Id,PostId,Score,Text,CreationDate,UserId,UserDisplayName
1,3,5,"Could be a poster child fo argumentative and subjective.  At the least, need to define 'valuable'.","2010-07-19 19:15:52",13,NULL
2,5,0,"Yes, R is nice- but WHY is it 'valuable'.","2010-07-19 19:16:14",13,NULL
3,9,0,"Again- why?  How would I convince my boss to use this over, say, Excel.","2010-07-19 19:18:54",13,NULL
4,5,11,"It's mature, well supported, and a standard within certain scientific communities (popular in our AI department, for example)","2010-07-19 19:19:56",37,NULL
5,3,1,"Define "valuable"...","2010-07-19 19:20:28",5,NULL
6,14,9,"why ask the question here?  All are community-wiki, why not just fix the canonical answer?","2010-07-19 19:22:27",23,NULL
7,18,1,"also the US census data http://www.census.gov/main/www/access.html","2010-07-19 19:25:47",36,NULL
9,16,0,"Andrew Gelman has a nice R library that links Bugs to R.","2010-07-19 19:30:24",78,NULL
10,23,4,"I am not sure I understand the difficulty. If the functional form is known just take the derivative otherwise take differences. Am I missing something here?","2010-07-19 19:31:18",NULL,user28
11,43,5,"There are many R GUI's available for Windows, so I'm not following your point.","2010-07-19 19:34:20",5,NULL
12,38,0,"That's just an example - it might have a median that is much smaller, on the order of 200 (it depends on how I partition the data). That would preclude using a normal distribution, right?","2010-07-19 19:37:33",54,NULL
13,20,2,"What levels of kurtosis and skewdness are acceptable to meet the assumption of normality?","2010-07-19 19:38:01",24,NULL
14,46,6,"this is an incredibly unclear response. Please try to write in English.","2010-07-19 19:38:30",74,NULL
15,3,2,"Maybe the focus shouldn't be on "valuable" but rather "pros" and "cons" of each project?","2010-07-19 19:44:47",24,NULL
16,8,2,"Probably better asked on meta.  People feel like they have to downvote it because it is an off-topic question.  But then the downvotes make it look like staticians have no sense of humor :(","2010-07-19 19:46:19",13,NULL
18,36,10,http://xkcd.com/552/,"2010-07-19 19:48:32",68,NULL
19,8,0,"@Jason Punyon in particular gets a humorless downvote for removing my "verboten" tag! ;-)","2010-07-19 19:50:40",6,NULL
20,54,3,"I am not sure if characterizing one or the other as the 'wrong' formula is the way to understand the issue. It is just that the second one is 'better' in in the sense that it is an unbiased estimator of the true standard deviation. So, if you care about unbiased estimates then the second one is 'better'/'correct'.","2010-07-19 19:51:06",NULL,user28
21,77,1,"I like the first example you give. That will certainly get the students talking ;)","2010-07-19 19:56:18",8,NULL
22,56,1,"I like the analogy. I would find it very useful if there were a defined question (based on a dataset) in which an answer was derived using frequentist reasoning and an answer was derived using Bayesian - preferably with R script to handle both reasonings. Am I asking too much?","2010-07-19 19:56:21",104,NULL
23,57,3,"-1 for wikipedia cut and paste","2010-07-19 19:56:28",74,NULL
24,73,0,"Very subjective question: this question cannot be answered, and is not suitable for a QA site.","2010-07-19 19:58:20",107,NULL
25,76,1,"+1 for ggplot2 - by far best graphs you can get","2010-07-19 19:59:04",22,NULL
27,77,0,"There's an interesting discussion by Steve Steinberg on his blog here: http://blog.steinberg.org/?p=11 about some of the implications of 1 and where it might lead in terms of Weak AI.","2010-07-19 20:01:15",55,NULL
28,57,5,"@el chief: On Stackexchange sites, it is allowed to copy and paste answers from other resources. I give the source link along with the answers. It's pointless to give -1 to a right answer. See meta for what is allowed here and what is not; and a big -1 from me to your behavior!","2010-07-19 20:02:25",69,NULL
29,79,0,"I wasn't suggesting it was, I was just curious as to why such a difference might have arisen, what sort of level of error following the wrong advice might give and whether there was a decent explanation of the difference I could give to my students.","2010-07-19 20:03:30",55,NULL
30,57,3,"Wikipedia cut and paste doesn't bother me when cited, as it is  here [is it compatible with the cc-sa license?]. However, these explanations are complicated for a beginner","2010-07-19 20:03:31",87,NULL
31,54,0,"I was characterising the formula as "wrong" purely in the sense that in an exam if you use the formula which isn't proscribed by the syllabus you'll end up with the "wrong" answer. Plus if the values are not a sample of population per se then surely the first formula gives the more accurate value.","2010-07-19 20:05:22",55,NULL
32,73,2,"Should probably be community wiki; useful question here but doesn't have definitive answer.","2010-07-19 20:05:57",5,NULL
33,73,2,"@Shane: good point. moved.
@ Egon: subjective indeed. but if the answers come from knowledgeable people i don't mind dose of subjectivity.
i've started learning R quite recently and have couple of dozens installed to explore, however i notice that there are tools that I use much more often irrespectively of the task at hand.","2010-07-19 20:06:56",22,NULL
34,56,8,"The simplest thing that I can think of that tossing a coin n times and estimating the probability of a heads (denote by p). Suppose, we observe k heads. Then the probability of getting k heads is:

P (k heads in n trials) = (n, k) p^k (1-p)^(n-k)

Frequentist inference would maximize the above to arrive at an estimate of p = k / n.

Bayesian would say: Hey, I know that p ~ Beta(1,1) (which is equivalent to assuming that p is uniform on [0,1]). So, the updated inference would be:

p ~ Beta(1+k,1+n-k) and thus the bayesian estimate of p would be 

p = 1+k / (2+n)

I do not know R, sorry.","2010-07-19 20:11:13",NULL,user28
35,43,0,"I wasn't aware of rapidminer. That looks nice, thanks!","2010-07-19 20:11:28",33,NULL
36,3,0,"Or maybe even "How X will help you get Y done faster/cheaper and kill the germs that cause bad breath."","2010-07-19 20:15:51",13,NULL
38,73,0,"It would be interesting if StackExchange could support some method of linking community wiki posts across sites.  Because I will bet this question has been asked on Stackoverflow and I also think that Statistical Analysis may attract some people that wouldn't usually visit SO.","2010-07-19 20:19:28",13,NULL
39,75,2,"This is really a Stackoverflow question as it has to do with learning the R programming language.  With the current wording the question is only associated with statistics by virtue of R's focus on statistical analysis.","2010-07-19 20:23:01",13,NULL
41,16,3,"I'd rephrase that "the most popular statistical tool in bioinformatics"... Bioinformaticians doing microarray analysis use it extensively, yes. But bioinformatics is not limited to that ;)","2010-07-19 20:25:52",120,NULL
42,65,4,"Colin, an unbiased estimator of the standard deviation does not have a closed form representation in the general case. What does exist is the unbiased estimator of the <i>variance</i> (s<sup>2</sup> in this case).
Noteworthy that both are consistent estimators of the population variance - and so by the continuous mapping theorem, are the two estimators of the standard deviations.
A related point is that s<sub>n</sub><sup>2</sup> has a lower MSE than s<sup>2</sup>. The additional advantage from imposing unbiasedness is arguable.","2010-07-19 20:30:23",47,NULL
43,75,0,"I guess you're right. Voting to close my own question.","2010-07-19 20:32:02",69,NULL
44,103,6,"i suggest everyone put their favourite image from the blog, so it's not just a collection of links...","2010-07-19 20:37:20",74,NULL
45,73,0,"@Sharpie: there have been several interesting SO posts like http://stackoverflow.com/questions/1295955/what-is-the-most-useful-r-trick or http://stackoverflow.com/questions/1535021/whats-the-biggest-r-gotcha-youve-run-across however they are not focused on packages. and i agree, linkage of community wiki could be really useful.","2010-07-19 20:37:27",22,NULL
46,44,6,"This is an incredibly general question.","2010-07-19 20:38:14",46,NULL
47,105,0,"+1 Beat me by 10 seconds with that one...","2010-07-19 20:39:12",5,NULL
48,103,0,"There is no single correct answer, should be wiki...","2010-07-19 20:39:39",122,NULL
50,16,0,"@Nicojo - Very good point!","2010-07-19 20:43:25",8,NULL
51,65,0,"@Tirthankar - very sloppy of me. I've altered the answer slightly. Thanks.","2010-07-19 20:45:49",8,NULL
52,38,0,"The normal approximation to the Poisson distribution is pretty robust, the difference between the CDFs is bounded by something like 0.75/sqrt(lambda), if I recall correctly.  I wouldn't be too worried about using lambda=200, but if you're more risk-averse then definitely go with the negative binomial.","2010-07-19 20:46:12",61,NULL
53,80,0,"thank you user93!","2010-07-19 20:46:46",58,NULL
54,80,0,"By "January 1st", I presume you mean the cut-off is an entire year and not 6 months period that applies to your children.","2010-07-19 20:48:04",58,NULL
55,59,4,"Do you get bonus points for the first graph in the stats exchange?","2010-07-19 20:53:53",8,NULL
56,101,1,"Take your time! I won't be thinking about selecting a "Best Answer" for a week or so.","2010-07-19 20:54:37",13,NULL
57,75,1,"I wouldn't vote to close it just yet- there could be a good question in there.  Perhaps something like "Where can I find useful tutorials that focus on putting statistical concepts into practice using a tool such as R?" or "Where can I find useful tutorials that teach statistics by example using tools such as R?"","2010-07-19 20:58:26",13,NULL
59,116,0,"It doesn't have an RSS feed though :(","2010-07-19 21:11:30",8,NULL
60,110,0,"Thanks for the great answer and for book advice. Also, do you know about relation of window function to generalized functions? It seems (from wikipedia article) they are suitable as domains for functionals.","2010-07-19 21:18:07",117,NULL
62,116,0,http://cscs.umich.edu/~crshalizi/weblog/index.rss,"2010-07-19 21:20:53",61,NULL
64,118,11,"In a way, the measurement you proposed is widely used in case of error (model quality) analysis -- then it is called MAE, "mean absolute error".","2010-07-19 21:30:23",88,NULL
65,130,7,"If you would taste R, it is highly probable that you will resign from MATLAB (as in my case).","2010-07-19 21:33:33",88,NULL
66,116,0,"@Rich - Thanks. I wonder why FF didn't pick it up?","2010-07-19 21:34:21",8,NULL
67,130,0,"IMO, this should be community wiki (language "versus" type questions are pretty subjective).","2010-07-19 21:34:28",5,NULL
68,130,0,"This is definitely a question concerning programming languages and should be asked on Stack Overflow.","2010-07-19 21:35:25",13,NULL
69,130,0,"I agree with Sharpie.  @Vivi: you should change the question title to be "advantages and disadvantages for data munging" or something along that line so that it's more on-topic.","2010-07-19 21:37:16",5,NULL
70,134,1,"I think this is the first candidate to be moved to Stack Overflow.","2010-07-19 21:38:27",88,NULL
71,120,9,"Nice analogy of euclidean space!","2010-07-19 21:38:48",83,NULL
73,138,9,"Should be community wiki.","2010-07-19 21:40:12",5,NULL
74,116,1,"@Colin: The author of the blog would need to put `<link rel="alternate" type="application/rss+xml" title="RSS" href="http://cscs.umich.edu/~crshalizi/weblog/index.rss" />` in his the `<head>` section of the HTML document for Firefox to pick it up automatically :)","2010-07-19 21:42:04",66,NULL
76,130,0,"I don't know what munging is  :(","2010-07-19 21:51:17",90,NULL
77,130,5,"@Sharpie, @Shane IMO to this extent it is a question about tools, so it is acceptable.","2010-07-19 21:54:08",88,NULL
78,145,0,"similar to http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/70#70","2010-07-19 21:54:47",22,NULL
79,145,0,"Voting to close as a duplicate.","2010-07-19 21:56:44",13,NULL
80,130,0,"@mbq, we definitely need a set of community guidelines on these sorts of questions, help us decide on meta: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions","2010-07-19 21:59:52",13,NULL
81,130,0,"@Sharpie It is not only in contex of R -- to this end I've made a separate meta discussion for that, http://meta.stats.stackexchange.com/questions/35/how-much-programming-here","2010-07-19 22:18:16",88,NULL
82,20,5,"Most statistical methods assume normality, not of the data, but rather of an assumed random variable, e.g. the error term in a linear regression. Checking involves looking at the residuals, not the original data!","2010-07-19 22:24:14",NULL,Statprof
83,145,0,"I agree, first duplicate ;-)","2010-07-19 22:28:22",88,NULL
84,7,2,"This should be community-wiki.","2010-07-19 22:31:13",88,NULL
85,75,1,http://meta.stats.stackexchange.com/questions/35/how-much-programming-here,"2010-07-19 22:39:45",88,NULL
86,110,0,"The full technical explanation is complicated.  The shorter, less technical explanation is that, since they are typically smooth, and either of compact support or they decay exponentially fast, many (but definitely not all) windowing functions make good 'test functions' to examine operator behavior with.  A good starting point might be the chapter on distributions in http://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf","2010-07-19 22:42:33",61,NULL
87,130,0,"@Sharpie : check this out: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions","2010-07-19 22:52:59",90,NULL
88,157,0,"+1 that problem twisted my brain when I first read and thought about it- and the solution is pretty simple but teaches a lot about probability.","2010-07-19 23:01:30",13,NULL
89,86,0,"It's probably best not to use the term "normal variable" here when you do not mean a normally distributed random variable.","2010-07-19 23:28:39",159,NULL
90,143,0,"Random Forest seems very interesting. Thanks. :)","2010-07-19 23:32:20",131,NULL
91,137,0,"This is a great answer! I'll look into your book suggestions, and the description of your process is also great. I especially like the suggestions for feature vectorizations.","2010-07-19 23:34:20",131,NULL
92,137,0,"(If anyone would like to elaborate even further on the vectorization part, that'd be great.)","2010-07-19 23:35:23",131,NULL
93,150,0,"curious that that book has no reviews on Amazon","2010-07-19 23:54:17",74,NULL
94,86,0,"Agreed.  Although I personally would look at someone funny for a few seconds if they said "normal variable" and didn't throw the word "random" or "distributed" in there somewhere to cue me that that is what they were discussing.  But I am also an engineer and not a statistician so I don't use that much domain-specific notation.","2010-07-19 23:56:45",13,NULL
95,145,3,"Actually, I think the previous question is more a subset of this one. EAMann was asking for a datasets with some particular characteristics (although I'm not sure anybody is paying attention to those criteria); this question is wide-open.  For example, I feel comfortable voting up many of these answers, because they are, in fact, datasets, but not any of the answers on the previous question, because I haven't opened them up to see if they suit EAMann's requests.","2010-07-20 00:10:26",71,NULL
96,101,0,"Now that I've had a chance to come back and read the whole answer- a big +1 for the student height example.  Very clear and well laid out.","2010-07-20 00:12:40",13,NULL
97,43,3,""Windows-based" may mean that it works on the Windows operating system (which R does) rather than meaning that it is a heavily GUI-oriented tool. Note the capital letter!","2010-07-20 01:01:12",173,NULL
98,5,10,"It's extensible and there's no statistical technique that can't be done in it.","2010-07-20 01:22:43",1356,NULL
99,134,0,"Possibly, but it'd need a lot more explanation on SO.","2010-07-20 01:29:42",174,NULL
100,86,5,"Random variables may be classified as *discreet* if they don't draw attention to themselves. If they're merely countable we say *discrete* :-P Also, you mean prescribe rather than proscribe, but I think *describe* might be more appropriate. Nice answer, anyway -- hopefully +1 will help mitigate the nitpicking!","2010-07-20 01:43:31",174,NULL
101,46,1,"maybe so. is a person asking this question a person who walked in off the street, or a person who has at least opened a statistics book. 

Telling someone the standard deviation is just the square root of the variance is completely begging the question.","2010-07-20 01:59:53",62,NULL
104,188,3,"That's a good explanation, but not for a nontechnical layperson. I suspect the OP wanted to know how to explain it to, say, the MBA who hired you to do some statistical analysis! How would you describe MCMC to someone who, at best, sorta understands the concept of a standard deviation (variance, though, may be too abstract)?","2010-07-20 02:18:02",6,NULL
105,196,0,"To give everyone a chance -- this should be wikified","2010-07-20 02:35:25",87,NULL
106,188,2,"@Harlan: It's a hard line to straddle; if someone doesn't at least know what a random variable is, why we might want to estimate probabilities, and have some hazy idea of a density function, then I don't think it *is* possible to meaningfully explain the how or why of MCMC to them, only the "what", which in this case would boil down to "it's a way of numerically solving an otherwise impossible problem by simulation, like flipping a coin a lot to estimate the probability that it lands on heads".","2010-07-20 02:39:24",61,NULL
108,25,0,"do you mean a GUI graphical tool that runs on Windows, or a command line based one that runs on Windows (or either)","2010-07-20 04:09:28",74,NULL
109,200,0,"coming up with a persuasive theoretical framework that explains the patterns is very hard, especially when there are thousands of variables involved. Real world is a complicated business.","2010-07-20 04:33:45",175,NULL
110,101,1,"Nice work ... but we need to add 

(C) our model (embodied in the formula/statistical routine) is wrong.","2010-07-20 05:07:08",187,NULL
112,54,12,"Srikant, I don't think that the second one is an unbiased estimator.  The square of it *is* an unbiased estimator of the true variance.  However, Jensen's Inequality establishes that the expectation of a curvilinear function of a random variable is not the same as the function of the expectation of the random variable.  Hence the second formula can't be an unbiased estimator of the true standard deviation.","2010-07-20 05:28:20",187,NULL
113,200,2,"@Ngu Nobody promised it would be easy. Expecting meaningful patterns just to magically emerge from a sea of data is what gives data miners a bad name.","2010-07-20 05:29:24",174,NULL
114,220,2,"Answer can be checked from most textbooks / google.","2010-07-20 05:41:15",195,NULL
115,167,0,"Wouldn't that be FA then?","2010-07-20 05:52:50",144,NULL
116,222,0,"el chef has a condensed answer over here -> http://stats.stackexchange.com/questions/146/pca-scores-in-multiple-regression. HTH","2010-07-20 05:53:59",144,NULL
117,224,0,"What do you mean, standalone application?","2010-07-20 06:05:54",5,NULL
118,225,0,"The title says 99th percentile, but obviously it should be 98th; sorry about that.  The question still holds.","2010-07-20 06:08:48",196,NULL
119,167,2,"No. FA is not regression. I am referring to a response variable regressed against the principal components computed from a large number of explanatory variables. The principal components themselves are closely related to the factors of FA.","2010-07-20 06:14:32",159,NULL
120,223,1,"What's the significance of the fact that your friend is an MD?","2010-07-20 06:25:48",5,NULL
121,225,1,"You should be able to edit the title (I did a similar thing myself earlier).","2010-07-20 06:26:28",173,NULL
122,224,0,"By standalone application I mean an executable program.","2010-07-20 06:32:54",128,NULL
123,223,0,"I think the significance of the MD is the person in question is a busy clinician, and not a researcher. Therefore we are being guided towards your less weighty tomes as source recommendations. Well, that's from the MD's I come across anyway.","2010-07-20 06:42:40",199,NULL
124,25,0,"I mean anything (GUI or command line based) that can run on Windows operating systems.","2010-07-20 06:57:11",69,NULL
126,167,0,"I'm sorry, I should have been more precise in my comment. Your writing that the explanatory variables can be reduced to a small number of PC rang me "factor analysis" bell.","2010-07-20 07:32:47",144,NULL
127,223,0,"Exactly! The fact that he is an MD poses a few restriction on (a) the volume of the introductory material (b) on what to assume about the "starting point" and (c) on the time willing to spend to reintroduce himself with basic stuff.","2010-07-20 07:48:49",79,NULL
128,118,2,"In accepting an answer it seems important to me that we pay attention to whether the answer is circular.  The normal distribution is based on these measurements of variance from squared error terms, but that isn't in and of itself a justification for using (X-M)^2 over |X-M|.","2010-07-20 07:59:54",196,NULL
129,10,2,"Technically Likert scales are the sum of Likert-type items and as such end up being a reasonable approximation (at least according to many psychometricians in Psychology) of an interval data point.","2010-07-20 08:03:30",196,NULL
130,240,0,"Univariate boxplots are useful for spotting univariate outliers. But they can completely miss multivariate outliers.

The regression idea is ok if I had a Y and a bunch of X variables. But as I said in the question, there is no Y so regression is inappropriate.","2010-07-20 08:12:10",159,NULL
132,241,0,"Yes, I could compute the Mahalanobis distance of each observation from the mean of the data. But the observations with the greatest distance from the mean are not necessarily multivariate outliers. Think of a bivariate scatterplot with high correlation. An outlier can be outside the main cloud of points but not that far from the mean.","2010-07-20 08:14:21",159,NULL
136,229,0,"Is there a way to express in plain language what the difference is between a given percentile and the maximum of N values?  From a lay perspective, it is hard to see why a datapoint that comes from a given (Y) percentile wouldn't be expected to be (on average) the same as the top scorer from a group of 100/Y.  

For example, if I found that your answers were ranked in the 90th percentile, I'd expect that your answer would usually be the top answer among any randomly selected group of 10 answers.","2010-07-20 08:45:52",196,NULL
137,242,0,"could you edit the title to something like "Using time series analysis to analyze/predict violent behavior"?","2010-07-20 08:56:46",87,NULL
138,134,2,"Most programmers know "median".  (sort(array))[length/2] is a big enough hint for those who forgot.  Also at its most basic for each new point you only need to do a bisection/insert on one half of the array...","2010-07-20 09:04:20",87,NULL
139,250,1,"R is also lazy evaluated.","2010-07-20 09:04:48",88,NULL
140,189,0,"Thats the same as taking the derivative, but just more inaccurate so why would you do it?","2010-07-20 09:39:03",214,NULL
141,261,0,"How about putting it in publication?","2010-07-20 09:59:57",217,NULL
142,263,0,"How to make a vector file usable in a publication using it?","2010-07-20 10:01:17",217,NULL
143,261,4,"R produces some of the best quality graphics around. As an editor of an international research journal, I would love all our authors to use R.","2010-07-20 10:02:05",159,NULL
144,263,0,"@Łukasz I added it to my answer","2010-07-20 10:11:50",190,NULL
145,263,0,"Could you put a code snippet for completness? It would be very useful for people in the future finding this page.","2010-07-20 10:19:14",217,NULL
146,262,0,"How about saving to file?","2010-07-20 10:19:47",217,NULL
147,254,0,"Thank you. Would you please explain the two lines of R code for those of us (me included) who don't even understand R syntax.","2010-07-20 10:33:59",213,NULL
148,250,0,"@mbq good to know, thanks!","2010-07-20 10:48:05",171,NULL
149,257,2,"How do you define 'publication-quality'? Please elaborate on what aspects you like to see covered... e.g. color use, line widths, etc. Should answers focus on font size, instead?","2010-07-20 10:51:44",107,NULL
150,263,0,"@Łukasz Hmm, some suggestion how to upload an svg figure?","2010-07-20 10:52:57",190,NULL
151,261,1,".. see my comment on the question... how do you define 'publication-quality', or 'best quality'... from a editor perspective?","2010-07-20 10:53:25",107,NULL
152,261,12,"I like to see vector graphics (no jpegs), graphical design following the principles of Tufte & Cleveland, readable fonts, uncluttered legends, no shaded backgrounds, sensible axis limits and tick intervals, labelled axes, no overlap of text and plotting characters or lines, etc. Most authors use the default settings of their software, so good software has good defaults. This is where Excel fails miserably and R does pretty well. But it is possible to produce lousy graphs in R and good graphs in Excel. It's just easier to produce high quality graphics in R.","2010-07-20 11:07:42",159,NULL
154,267,0,"You should use Colin's answer, still your idea of making Monte Carlo simulation is also correct.","2010-07-20 11:14:06",88,NULL
155,163,0,"It's a good example, but I'd rather see a proper definition (a function from a sample space to the real numbers, which you can make more general) followed up by an example.","2010-07-20 11:14:55",62,NULL
156,254,0,"Thank you again. Please see my edit to the original post that clarifies the problem even more","2010-07-20 11:21:08",213,NULL
157,274,0,"Good answer. I think you should go further into what you mean by "do your inference based on that". That's kind of the second part of my question.","2010-07-20 11:36:16",62,NULL
158,274,0,"mmm... I didn't really understand what you meant by what common variables and statistics... Oh, do you mean like you use z distribution if you have the population variance and the t-distribution if you only have the sample variance and the sample size is small? Something along those lines?","2010-07-20 12:03:37",90,NULL
159,278,1,"Your random reordering reminded me of this AI koan: In the days when Sussman was a novice Minsky once came to him as he sat hacking at the PDP-6. "What are you doing?", asked Minsky. "I am training a randomly wired neural net to play Tic-Tac-Toe." "Why is the net wired randomly?", asked Minsky. "I do not want it to have any preconceptions of how to play." Minsky shut his eyes. "Why do you close your eyes?", Sussman asked his teacher. "So the room will be empty." At that moment, Sussman was enlightened.","2010-07-20 12:40:15",56,NULL
160,263,3,"You could have mentioned in your answer that matplotlib allows rendering of all typography in the plot with LaTeX so it perfectly integrates visually.","2010-07-20 12:49:37",56,NULL
162,213,1,"If a scatterplot matrix won't catch it, you could try a 3D scatterplot.  That won't work out to 4D, of course, but then you could create a 4th dimension as time and make a movie.  :)","2010-07-20 13:06:49",5,NULL
163,298,1,"Are you asking about how to reduce the effect of outliers or when to use the log of some variable?","2010-07-20 13:14:06",56,NULL
164,298,7,"I think that the OP is saying "I've heard of people using the log on input variables: why do they do that?"","2010-07-20 13:24:23",5,NULL
165,274,0,"What I was getting at was that mean and standard deviation are parameters associated with the population, but they're estimated by the sample mean ((1/N)*\\sum(x_i)) and the sample standard deviation ((1/(N-1))*\\sum(x_i - x^bar)^2).","2010-07-20 13:42:11",62,NULL
168,249,3,"The m_i's do not have to be equal

Wikipedia has a simplified description of the model.","2010-07-20 14:17:12",8,NULL
170,163,1,"@Baltimark That's a valid comment which I generally agree with but would someone who is new to statistics be scared away by a formal definition and then not proceed on to the more intuitive example? I have edited my answer, but have still kept it simple :)","2010-07-20 14:38:40",81,NULL
171,262,0,"+1 for R and ggplot2","2010-07-20 14:38:52",22,NULL
172,119,4,"I agree. Standard deviation is the *right* way to measure dispersion if you assume normal distribution. And a lot of distributions and real data are an approximately normal.","2010-07-20 14:40:02",217,NULL
173,244,0,"+1 for comprehensive collection","2010-07-20 14:40:51",22,NULL
174,9,3,"If moving from Excel is the issue, you could try:

* http://www.coventry.ac.uk/ec/~nhunt/pottel.pdf 

* http://www.forecastingprinciples.com/files/McCullough.pdf 

* http://www.lomont.org/Math/Papers/2007/Excel2007/Excel2007Bug.pdf 

* http://www.csdassn.org/software_reports/gnumeric.pdf","2010-07-20 14:44:45",229,NULL
175,163,0,"You're right, but I'm still confused as to whether answers here are being addressed to rank beginners, or someone with a minimum of knowledge. Correct me if I'm wrong, but when I was in college, we usually took stats after a certain amount of other math classes (for me, it was after calculus, so I was familiar and comfortable with the definition of functions).","2010-07-20 14:44:49",62,NULL
176,301,0,"But still, using log changes the model -- for linear regression it is y~a*x+b, fo linear regression on log it is y~y0*exp(x/x0).","2010-07-20 14:50:05",88,NULL
177,309,4,"Especially where students are concerned.","2010-07-20 14:51:13",71,NULL
178,163,2,"@Baltimark I'm not too sure myself tbh, I've just been basing the level of my answers on how the questions are asked, so e.g. if someone asks a question showing that they have some knowledge of the field, then you can say things that won't scare them away. This question seemed basic so I assume a very simple reply is appropriate. I'm sure http://meta.stats.stackexchange.com/ has some more guidance.","2010-07-20 14:51:44",81,NULL
179,232,0,"+1 for Mondrian - very useful toy, especially for large data","2010-07-20 14:54:06",22,NULL
180,310,0,"You're not missing something if all you're trying to do is estimate the parameter from a set of observations. That was definitely the main idea of the OP's question. However, she was also asking generally (if not rigorously) "how to estimate poisson models". Perhaps she wants to know the value of the pdf at a specific point. In that case, the normal approx. is probably going to be better than scaling the parameter, and the observations by 100, or whatever, if the observations are large enough to make calculating the factorial impractical.","2010-07-20 14:54:19",62,NULL
182,220,0,"I have voted to close this question for the reasons mentioned in this meta thread (http://meta.stats.stackexchange.com/questions/67/what-should-be-our-criteria-for-closing-questions). Please go to the meta thread if you think that we should not close questions like the one above.","2010-07-20 15:01:21",NULL,user28
183,10,2,"@drknexus - So, multiple items serve as a measurement triangulation for construct scales?  If yes, what are the criteria for determining that a researcher has enough relevant data points (i.e., items) to use the scale as an interval measurement?","2010-07-20 15:06:18",24,NULL
185,313,0,"First of all, this is just a big example and doesn't really explain explain the concept of p-value and test-statistic. Second, you're just claiming that if you get fewer than 5 or more than 15 white marbles, you reject the null hypothesis. What's your distribution that you're calculating those probabilities from? This can be approximated with a normal dist. centered at 10, with a standard deviation of 3. Your rejection criteria is not nearly strict enough.","2010-07-20 15:21:27",62,NULL
186,224,4,"This should probably be community wiki since there is no definitive answer.","2010-07-20 15:30:58",5,NULL
188,304,1,"Answers will be reordered based on votes, so please try not to refer to other answers.","2010-07-20 15:54:34",220,NULL
189,301,0,"I agree - taking log's changes your model. But if you have to transform your data, that implies that your model wasn't suitable in the first place.","2010-07-20 16:00:09",8,NULL
190,99,0,"+1 for multicore - very useful indeed","2010-07-20 16:02:04",22,NULL
191,316,0,"+1 for RODBC and sqldf - i find them essential as well","2010-07-20 16:02:52",22,NULL
192,307,8,"I seen examples where a model has ten data points and nine parameters. On pointing out that the model has too many parameters, I was told that the R^2 was 0.999 so the model must be correct!","2010-07-20 16:03:44",8,NULL
193,321,0,"What other variants?  It might be helpful to tighten up this question a little (more specificity).","2010-07-20 16:03:59",5,NULL
195,138,6,"You should add your background. Programmers who came to R have different issues than people without a programming background.","2010-07-20 16:06:54",3807,NULL
196,323,0,"These were both listed in the original question...","2010-07-20 16:12:38",5,NULL
197,114,3,"I added the blogs in the question as answers to allow proper voting to find the most popular blogs.","2010-07-20 16:13:35",3807,NULL
198,285,0,"Thanks Jeromy. I explored several possible transformations of data using Stata's gladder function for the ladder of powers.

Teoreically based solution will be one way to go, but before that I wanted to check for the data driven solution.","2010-07-20 16:19:56",22,NULL
199,60,0,"Thanks Reed. k-means is definitely a way to go and I explored several solutions. However I am trying to go for a solution without subjective decision of the number of clusters.   Hierarchical approach on the other hand produced huge classification and again it was hard for me to specify where to stop.","2010-07-20 16:22:43",22,NULL
200,323,10,"By that reasoning another question which asks for the best blogs but lists other blogs in the starting question wouldn't be a duplicate of the question.
I think it makes sense to vote on all blogs to find out which are most popular including those already listed in the OP.","2010-07-20 16:22:54",3807,NULL
201,268,0,"Thanks Egon. SOM approach might be very interesting indeed. Will have a look at it.","2010-07-20 16:23:45",22,NULL
202,254,0,"Colin, I tried to figure out how to expand the Wikipedia formulae to non-equal m_i's, but didn't have any success. Can you please help me with this? (sorry)","2010-07-20 16:38:50",213,NULL
205,307,0,"All worship the mighty R^2!","2010-07-20 17:00:03",229,NULL
206,43,0,"fine, you win :)","2010-07-20 17:06:12",74,NULL
209,321,0,"Right; it is just one variant among others.","2010-07-20 17:11:25",88,NULL
211,333,1,"These are good references, but I disagree with your assessment of Ed Thrope as the founder of this field.  Statistical analysis of financial data and statistical arbitrage are not the same thing: one would perform statistical analysis for most financial analysis (e.g. modern portfolio theory).","2010-07-20 17:20:49",5,NULL
213,333,1,"I agree, Markowitz definitely invented portfolio theory","2010-07-20 17:55:23",74,NULL
214,86,0,"@walkytalky Thanks for the corrections- I have made some fixes.","2010-07-20 18:00:44",13,NULL
215,134,0,"@walkytalky I don't think it would require any more explanation than any other algorithm question on SO.  Probably less as the median is a relatively basic concept.","2010-07-20 18:15:39",13,NULL
216,127,4,"Agree strongly. Both great books. Start with Bayesian Computation With R, then get Gelman et al.","2010-07-20 19:12:45",247,NULL
217,321,0,"OK, I'll ask for comparison to adaboost since that is perhaps the best known.","2010-07-20 19:24:06",220,NULL
218,306,0,"Agreed - just to throw out some additional ideas on modeling: logistic to predict which patients will have 1+ violent outbursts, Poisson(esque) regression to predict which patients will have many outbursts,  multilevel to examine variations from room-to-room and/or ward-to-ward...","2010-07-20 20:13:46",71,NULL
219,7,0,"I didn't even see that checkbox.  Done :-)","2010-07-20 20:51:13",38,NULL
220,310,1,"@Srikant, you are right, to estimate the parameters the factorial is not an issue, but in general you will want the value of the likelihood for a given model, and you would have to use the factorial for that. Also, for hypothesis testing (e.g. likelihood ratio test) you will need the value of the likelihood.","2010-07-20 21:31:03",90,NULL
221,310,0,"@Baltimark: yes, I want to know in general, whether it is valid to change the unit of measurement of Poisson. I was asked this question and I didn't know what to say.","2010-07-20 21:33:48",90,NULL
222,302,0,"the two answers look too different to me. One says 20 to 30, the other says 20 to 30 times slopes. So if you have 5 slopes, one rule tells you 20 to 30, the other 100 to 150 observations. That doesn't seem right to me....","2010-07-20 21:37:31",90,NULL
223,313,0,"I would agree that this is just an example, and I it is true I just picked the numbers 5 and 15 out of the air for illustrative purposes. When I have time I will post a second answer, which I hope will be more complete.","2010-07-20 22:00:38",226,NULL
224,337,0,"I know, however that doesn't help me to decide whether using sample entropy or shannon entropy or some other kind of entropy is appropriate for the data that I'm working with.","2010-07-20 22:31:17",3807,NULL
226,337,1,"What I wrote in my post is just that for a certain type of data/process/system there is only one *true* entropy definition. Sample Entropy is *not* an entropy measure, it is just some statistic with a confusing name. Make a question where you define the data for which you want to calculate the entropy, and will get the formula.","2010-07-20 23:17:28",88,NULL
227,302,0,"They are pretty different guidelines.  I suspect the disconnect is whether you think that the test of the overall model matters (the lower N guideline) or the test of the individual slopes that matter (the higher N guideline).","2010-07-20 23:48:55",196,NULL
228,358,1,"The answer in that other thread don't really explain why 2 is a better value than other values that are very near to 2 but are no natural numbers.","2010-07-21 00:04:53",3807,NULL
229,10,1,"I'm not sure; that might be a worthy question for the community in general.  I'd guess that it is probably in part a value judgement on the part of the researcher & area.  Some areas are completely willing to treat a single Likert item as interval even though it clearly is ordinal.  A reasonable answer might be to use a different analysis method, e.g. a permutation or bootstrapped test.  Another answer might be to conduct a simple test of normality, so long as the aggregate doesn't significantly depart from normality you are probably okay.","2010-07-21 00:07:20",196,NULL
230,360,1,"Your point about month N's count not necessarily being correlated with N-1 is well-taken.  With a slow-growing disease like TB, that's something I'd have to look at carefully, but I'm pretty sure I could identify about how much lag there is between the time we report a source case and the time we report any secondary cases.","2010-07-21 00:11:05",71,NULL
231,360,1,"However, it's your point about analyzing the distribution of monthly counts that's at the heart of my question.  There is a definite decline in TB, both nationally in the US and in my district.  For example, when I compare 2009 to the previous years, there are decidedly fewer cases.  2010 is on track to have fewer still.  What I'm trying to identify (which I did a poor job of explaining in the question) is whether or not these declines are part of an ongoing downward trend, or just a downward wobble.  Thanks - you've gotten me to think much more carefully about the problem.","2010-07-21 00:15:26",71,NULL
232,358,0,"I think it does; still I'll try to extend the answer.","2010-07-21 00:21:35",88,NULL
234,135,3,"For R ks.test in the default "stats" package can conduct the KS test without installing additional packages.","2010-07-21 00:23:25",196,NULL
235,10,1,"... but in general it seems like one could evoke the central limit theorem and suggest that 20 to 30 items should be sufficient to use the scale as an interval measurement.","2010-07-21 00:26:48",196,NULL
236,310,0,"@Vivi: I am not sure why you would want to compute the likelihood with k_i! included as in most applications (e.g., likelihood ratio test, bayesian estimation) the constant will not matter. In any case, I do not think you can re-scale as you suggested. If I feel otherwise I will update my answer.","2010-07-21 00:28:39",NULL,user28
237,310,0,"@Srikant, I see your point, but some softwares (Eviews, for example) include this by default, and large numbers are an issue you like it or not. I guess I was really after an explanation of why you can or can't do it rather than a way around it, but the discussion has been interesting and instructive nonetheless  :)","2010-07-21 00:42:20",90,NULL
239,217,0,"Protovis looks awesome but do you know what browser support it has? particularly IE?","2010-07-21 01:39:03",191,NULL
240,134,0,"@Sharpie Perhaps not more than other SO algo questions. But certainly more than what it actually says here!","2010-07-21 02:58:14",174,NULL
241,227,0,"Sorry, but what are loadings (c in your formula) and how do you determine them?","2010-07-21 03:17:32",191,NULL
242,234,0,"Would there be any benefit or could you plot them on a 3-d scatter plot?","2010-07-21 03:18:06",191,NULL
243,226,0,"How does a higher Y value "explain" a bigger chunk of the variance? Is it how the PCA is computed? If so I think I've got another question to post ;)","2010-07-21 03:31:11",191,NULL
244,287,1,"See http://en.wikipedia.org/wiki/Method_of_moments_(statistics) and  http://en.wikipedia.org/wiki/Generalized_method_of_moments","2010-07-21 03:52:40",NULL,user28
245,30,0,"Similar question on SO: http://stackoverflow.com/questions/56411/how-to-test-randomness-case-in-point-shuffling","2010-07-21 05:09:44",68,NULL
246,215,0,"I agree completely with what you're saying, but the reason for looking at tests here is to satisfy others. The situation is modelling possible extreme operational losses bassed on historical loss experience and the regulator needs to be convinced that the choice of distribution is supported by the data. What the regulator thinks is reasonable and what the business thinks is reasonable for the results can differ quite a bit! Using a (reasonably) standard statistical test may provide a somewhat independent approach to the justifying a particular choice.","2010-07-21 05:40:47",173,NULL
247,372,2,"I think this is a too broad question. Because almost all statistics can be used in data mining, I don't see any reason for this question to exist.","2010-07-21 06:11:42",190,NULL
249,375,2,"Or this one http://stats.stackexchange.com/questions/30/","2010-07-21 06:16:52",56,NULL
250,368,0,"I don't have any requirements how correct my ASR should be. For example, when I change my model a bit my error goes down from 30% to 29.8%. For that change I want to now if it's significant","2010-07-21 06:17:05",190,NULL
252,215,0,"Then using a 'distance between distribution test' (like chi square or Kolmogorov-Smirnov or ... is a better idea because it is easely understood by the end user.","2010-07-21 06:57:31",223,NULL
253,372,1,"All statistics are usable, but if your goal is to study the *most* important (or often used) parts of statistics when applied to Data Mining - a subset would be useful.","2010-07-21 07:46:57",252,NULL
254,372,1,"Suggestion: why not made this a community wiki ? (p.s I wasn't the one who voted to close this - but I understand why someone voted for it)","2010-07-21 07:56:08",253,NULL
255,372,0,"Good idea. Community wiki = on!","2010-07-21 08:05:44",252,NULL
256,113,0,"What is the difference between model selection http://www.modelselection.org/ (hot topic in statistic during the past 20 years) and method selection.","2010-07-21 08:30:20",223,NULL
257,385,1,"I agree, but I was indeed worried about the k parameter -- if the unbalance will create some class-wise differences in the observation densities in the feature space, the same k will tend to a smaller sphere in feature space for an observation from denser class. Won't it influence the k parameter optimization then?","2010-07-21 08:45:38",88,NULL
258,372,0,"I agree with @Peter; way too vague.","2010-07-21 09:40:14",5,NULL
261,368,0,"In that case you would use test 2. See the table for "Two-proportion z-test, pooled for d0 = 0" at the wiki: http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Common_test_statistics","2010-07-21 10:44:25",NULL,user28
262,344,1,"Can you give a description a problem where RNB gave you good results?","2010-07-21 10:57:06",217,NULL
263,344,0,"No ;-) This was only to revive the pool.","2010-07-21 11:05:37",88,NULL
264,395,1,"Look at http://stats.stackexchange.com/questions/173/time-series-for-count-data-with-counts-20/ . It's not a duplicate, but the problem is similar at a first sight.","2010-07-21 11:20:20",88,NULL
265,386,0,"How would this deal with cases where you don't know how many outliers you have, i.e. when the N-1 points still have a bias since they include outliers?","2010-07-21 11:58:10",56,NULL
266,386,1,"if n is sufficiently large and the number of outlier is small then this bias is negligible. If there are a large number of outliers then, maibe it is not outliers and anyway, as I mentionned you can use leave k out strategy ... (in this case, you have to find out a strategy to avoid tracking all configurations which may be NP hard ... ) and if you don't know k, you can try many values for k and keep the most relevent.","2010-07-21 12:07:43",223,NULL
267,154,3,"Cool! Please post a link here to your thesis once it's complete and/or published!","2010-07-21 12:20:54",6,NULL
268,293,2,"Yes, rather than starting with the data, start with the question and the data generating process.","2010-07-21 12:35:24",46,NULL
269,192,3,"This question doesn't make a lot of sense - where is your statistical/scientific question?","2010-07-21 12:35:50",46,NULL
270,262,1,"Or a little more succinctly with melt and qplot: `m <- melt(d, id = "x"); qplot(variable, value, data = m, colour = variable)`","2010-07-21 12:39:58",46,NULL
271,400,13,"I think you mean sent by fax at some point in the **past** ;)","2010-07-21 12:42:57",46,NULL
272,165,7,"Here is my favorite paper about the topic: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pdf","2010-07-21 13:28:05",88,NULL
273,217,1,"That's unfortunately one of Protovis's weak points (but it's an issue with any SVG library because IE doesn't support that).  Fortunately, Jamie Love has come up with a solution using SVGWeb.  See here: http://groups.google.com/group/protovis/browse_thread/thread/1a80f98a16736658?pli=1.","2010-07-21 13:37:53",5,NULL
274,410,0,"I've updated the question with the information on kind of time series/mean I'm talking about.","2010-07-21 13:41:09",219,NULL
275,399,2,"Tufte's Visual Display of Quantitative Information (http://www.amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20) is better than Beautiful Evidence IMO. All four of his books are good though, and if you have an opportunity to attend one of his courses, do it.","2010-07-21 13:57:31",36,NULL
276,415,0,"Thanks!  I was out of town last week and missed that MO post.","2010-07-21 14:01:32",89,NULL
277,133,0,"I thought Chi-Sq was primarily for categorical data (contingency tables) vs. continuous?","2010-07-21 14:17:35",23,NULL
278,13,2,"it's similar to using planet gravitational models to urban traffic. I find it absurd, but it works quiet accurately actually","2010-07-21 14:25:01",59,NULL
279,405,0,"We have done this in two different ways and it has proved very useful with the large movements this week lcearly being outside the histogram of typical results.","2010-07-21 14:47:33",210,NULL
280,402,0,"Heh - this was actually one of the plots I made that didn't make it into the post.  The problem I had is deciding how to calculate the bounds - my initial attempt was with Poisson bounds, with lambda set to the mean of my data, but variance is too high to be a proper Poisson (high enough to matter in practice?  I don't know).","2010-07-21 15:01:32",71,NULL
281,402,0,"A further problem is that the center of the distribution can change over time - for example, it wouldn't make sense to set those bounds using data from the early 1900s, when Colorado was a haven for TB patients.  So what's an appropriate way to keep the lines up-to-date with long-term changes in the process, while still being able to identify deviations?","2010-07-21 15:02:21",71,NULL
282,217,0,"IE 9 will also support SVG- so as long as you don't need to work with IE 6...","2010-07-21 15:08:29",13,NULL
283,349,0,"The problem with the binning approach is that we do not have a good upper bound for the data, and so the midpoint for the largest bin would have to be huge. So, we'd need a huge number of bins (not enough memory for that), or have pretty wide bins (which would then lead to a fairly inaccurate answer.)

And the data is not very sparse.","2010-07-21 15:13:35",247,NULL
284,404,0,"I'll have to read it more thoroughly later on, but yes, this package is definitely addressing the kinds of problems I'm facing here.  Thanks!  And also, thanks for the kind words about the plots ;p","2010-07-21 15:16:07",71,NULL
285,352,2,"The data set is potentially too big to read in half of it...it is in a networking context where the device doing the processing can see tens of thousands of items per second, and probably has enough memory to store only a few hundred.

Also the data is definitely not Gaussian. In fact it does not fit well to any of the common distributions.","2010-07-21 15:17:56",247,NULL
286,424,3,"Absolutely love this one.","2010-07-21 15:23:11",13,NULL
287,346,3,"Perhaps, asking on Stackoverflow may get better answers.","2010-07-21 15:33:20",NULL,user28
288,113,2,"While model selection typically involves the scoring of models within a family of distributions, based on their fit and penalizing the number of parameters used (a la AIC and BIC), whereas method selection is more general.  Method selection involves being faced with a problem (e.g. test, classify, predict) for which we have some background knowledge (variables are known to be (e.g. independence, data type), and for which auxiliary assumptions are made (e.g. normality, homoscedasticity), and we must select a method.","2010-07-21 15:50:24",39,NULL
289,113,1,"Now there are mathematical prescriptions along the lines of measurement type, convergence results, optimality, and time/space complexity, but no framework for their systematic application, that I am aware of, thus the question.","2010-07-21 15:53:08",39,NULL
290,260,0,"Aptamer active motif selection, forest ground humidity forecasting, digit OCR, multispectral satellite image analysis, musical information retrieval, chemometry...","2010-07-21 15:58:58",88,NULL
291,348,0,"The way I wrote it up, specifically with the bayesian not knowing much about cat reproduction, at the beginning only the frequentist would bet on there being kittens.  The relevant points of my *very crude example* were mostly that the frequentist made his prediction based on the data at the beginning, then sat back without incorporating new supplementary data, while the bayesian didn't have much data to begin with, but continued to incorporate relevant data as it become available.","2010-07-21 16:09:12",24,NULL
292,135,0,"Thanks for the information, drknexus.","2010-07-21 16:53:25",39,NULL
294,36,4,"should be community wiki","2010-07-21 17:08:26",219,NULL
295,428,0,"This is interesting, and where some statistical advice could come in! Assume in total I've got (say) 500,000 i.i.d. points and I look at groups of (say) 1,000 of them, and calculate the median of each group. Now I've got 500 medians. Is there theory that could allow me to calculate a confidence interval for the overall median based on these 500 medians?","2010-07-21 17:10:16",247,NULL
298,223,2,"Is he looking to perform statistical analyses, interpret the output or critique published papers that use statistical methods?","2010-07-21 17:23:52",215,NULL
300,437,0,"I guess I should take the first suggestion as a vote of confidence.","2010-07-21 17:52:02",89,NULL
301,410,0,"@Silent: We would still need to guess what your problem really is. What is N? Number of sunflares in hundreds or the voltage you measure on some battery? It would suggest you plot your monthly means in a histogram. Fit this with a Gaussian an see if it fits and if you can justify a Gaussian fit from your model.","2010-07-21 19:24:05",56,NULL
302,244,0,"+1 for Graphviz which is often missed and really useful","2010-07-21 19:33:01",212,NULL
303,457,1,"Are you referring to a particular paper? I imagine I could find an answer to my question if I researched, studied, read a lot, but so could 95% of the questions other people ask here... Also, in some cases, particularly with macroeconomics data (which is my area), there is no more data to be collected. Data is scarce (the number of observations, I mean), and you just have to live with it. There is no "get more data" solution. I was hoping someone here would know the topic, but it doesn't seem like. Maybe once the website is opened to the general public?","2010-07-21 20:07:46",90,NULL
304,461,0,"Indeed some people are taking Tufte as gospel and not being particularly flexible...","2010-07-21 20:31:16",259,NULL
305,464,0,"No, nothing to do with Qnotifier, and that's not mine, it's a commercial product. The data I'm working with is very similar to that though.

Completely agree with the "color the part over the threshold" but the plotting tools in use currently here don't allow that level of control. :-(","2010-07-21 20:42:35",259,NULL
306,450,0,"Can you give examples of alternatives? I'd like to look into that.","2010-07-21 20:56:10",77,NULL
307,450,5,"The mostly known and the simplest is the Median-Median regression, well known from smart calculators (Sigh!). Consult also Wikipedia  http://en.wikipedia.org/wiki/Robust_regression and maybe CRAN's Robust task view http://cran.r-project.org/web/views/Robust.html","2010-07-21 21:20:20",88,NULL
308,464,0,"Too bad... Still you can make the same trick only with the color of the threshold line.","2010-07-21 21:23:35",88,NULL
310,133,1,"Hmmm I actually like the KS test answer better than mine !","2010-07-22 00:07:38",139,NULL
311,185,0,"I actually scanned that at work yesterday. It's an interesting read - I wish I had more time to absorb the material in it, but I had to get what I needed and move on.","2010-07-22 00:39:46",110,NULL
312,457,0,"I suspect the answer to your question will be domain/model specific and hence I am not sure I can recommend a specific paper.","2010-07-22 01:19:43",NULL,user28
313,247,3,"More specifically, the asker may be interested in [JFreeChart](http://www.jfree.org/jfreechart/) which powers a lot of Incanter graphics.","2010-07-22 03:23:01",13,NULL
314,262,0,"Actually, an even easier way is to use R+deducer with ggplot2 (there is a new release of this which is about to come out in the next few months.  A beta is currently available)","2010-07-22 03:57:44",253,NULL
315,423,2,"I do have to ask though- how come cartoons are in and jokes are out?","2010-07-22 05:09:53",13,NULL
316,423,0,"@sharpie: are jokes out?  We obviously don't want the entire site to be humor, but everyone benefits from a little educational humor in small doses.","2010-07-22 05:15:40",5,NULL
317,479,0,"Totally agree. By making it a background it is clearly not data, but my making it coloured it shows a clear change in 'state'.","2010-07-22 09:04:20",210,NULL
318,461,0,"Leaving the reader to figure things out has been a source of countless problems. You have to communicate your message properly and if your message is the data over this line is an issue you must shown the line which is concerning.","2010-07-22 09:47:28",210,NULL
319,151,25,""Squaring always gives a positive value, so the sum will not be zero." and so does absolute values.","2010-07-22 09:54:23",223,NULL
320,483,0,"Sorry, bu I don't really understand your answer. What do you mean by "In detail, each tree is build on a sample of objects drawn with replacement from the original set"

 Can you give more precision on where I find the details "here"?","2010-07-22 10:04:51",223,NULL
321,262,2,"Nice example, but the plot is hardly publication quality. Or at least none of the journals I publish in would accept it.","2010-07-22 11:02:42",214,NULL
322,411,1,"I like the question, there may be already most of the possible answer in the question... do you have an idea of the type of answer/developpement you want?","2010-07-22 11:18:44",223,NULL
323,485,2,"Should be community wiki.","2010-07-22 11:21:23",88,NULL
324,483,0,"This is how bagging works; check out http://en.wikipedia.org/wiki/Bootstrap_aggregating . Here is a link  (hardly visible in that theme I admit) to the detailed RF reference.","2010-07-22 11:24:12",88,NULL
325,48,2,"can you give one of those or advise a particular one in the book?","2010-07-22 11:30:13",223,NULL
326,490,0,"Is it a question or a pool? If the latter, it should be community wiki. If the first, give more details about what you want to achieve? For instance, is it all-relevant or rather  minimal-optimal selection? How much is many? How   hard is the classification problem?","2010-07-22 11:38:20",88,NULL
327,492,0,"What about slow (standard) Fourier transform.","2010-07-22 11:45:24",88,NULL
328,490,0,"pool... many means 1000 features or more and less than 100 observations.","2010-07-22 11:58:08",223,NULL
329,498,1,"Also, I know this might get flagged off-topic, or not statistical, but maybe not. I wouldn't mind seeing any SAS/R/SPPS/Stata question be fair game here. Statisticians should be the most experienced in these packages.","2010-07-22 11:59:44",62,NULL
330,498,0,"imho offtopic. Even though it is an "Analysis" program, the question has definitely nothing to do with statistical analysis. Suggestion: contact the manufacturer of SAS, they can fix the problem.","2010-07-22 12:05:58",190,NULL
331,498,1,"See also http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions . Option 3 seems to be favored here, which means that questions that have not anything to do with statistics are closed/move. I think this is the same case.","2010-07-22 12:12:06",190,NULL
332,503,0,"Yes, certainly when I need to I output what I want into excel, or csv, or whatever, but at times, I just want a quick  C & P. It behave very oddly. . .not even giving me the option of "copy" from the "edit" drop-down.","2010-07-22 12:33:19",62,NULL
333,168,1,"If I could vote up more than once, I would do it!","2010-07-22 12:38:01",223,NULL
334,460,0,"The procedure I already gave can be applied in large dimension, as I said, using a gaussian assumption.  If the dimension is really large with respect to the sample size (i.e. p>>n) then you can  make some sparcity assumption (assume that the parameters of your gaussian distribution lie in a low dimensional space for example)  and use a thresholding estimation procedure for the estimation of the parameters...","2010-07-22 13:16:30",223,NULL
335,151,13,"@robin girard: That is correct, hence why I preceded that point with "The benefits of squaring include". I wasn't implying that anything about absolute values in that statement. I take your point though, I'll consider removing/rephrasing it if others feel it is unclear.","2010-07-22 13:19:20",81,NULL
336,498,2,"I think questions about how to implement a statistical analysis in SAS/R/whatever are ok. But, purely interface questions are off-topic.","2010-07-22 13:38:33",NULL,user28
337,423,0,"@Sharpie, feel free to close or reopen according to your feelings! I agree with Shane, a bit is ok, but not too much. For example, this question already included a funny cartoon. The jokes question not really a funny joke....","2010-07-22 13:58:03",190,NULL
338,423,25,"These cartoons are useful too; they can be included in a lecture on a particular topic where you are trying to explain a concept (e.g. correlation/causation above).  A little humor can help to keep an audience engaged.","2010-07-22 14:22:11",5,NULL
339,411,1,"Not very specifically.  I'm quite ignorant of statistics and one of my reasons for asking is to learn which criteria statisticians would use to pick between different metrics.  Since I did already describe one important practical advantage of 1 (you can actually compute it) I'm especially interested in theoretical motivations.  Say, is the information provided by estimates of Kolmogorov distance frequently of direct use in applications?","2010-07-22 14:50:34",89,NULL
340,328,0,"Given the current state of national economies due to the rescue of the various financial institutions, one may question the value of accepted knowledge in this field, save for greater fool theory.","2010-07-22 15:06:12",229,NULL
341,36,4,"That pirates / global warming chart is clearly cooked up by conspiracy theorists - anyone can see they have deliberately plotted even spacing for unequal time periods to avoid showing the recent sharp increase in temperature as pirates are almost entirely wiped out.
We all know that as temperatures rise it makes the rum evaporate and pirates cannot survive those conditions.
;-)","2010-07-22 16:08:37",270,NULL
342,461,0,"... right.  I wasn't suggesting he leave it to the reader *if it was a part of his message*.  I was saying that if the threshold weren't a part of the plot, he could either leave it out entirely or incorporate a subtle visual cue to make the threshold easier to find if the reader wanted to look for it.","2010-07-22 16:09:04",71,NULL
343,155,6,"Should be community wiki.","2010-07-22 16:13:35",88,NULL
344,517,6,"First, two lines from wiki: "In computer science, semi-supervised learning is a class of machine learning techniques that make use of both labeled and unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data)." Does that help?","2010-07-22 16:25:36",NULL,user28
345,11,0,"Can you be a little more specific?","2010-07-22 16:26:29",88,NULL
346,517,0,"What do you have in mind with "Algorithmic approaches"? I gave some examples of applications in my answer, is that what you are looking for?","2010-07-22 16:49:10",190,NULL
347,520,0,"Why the  downvote? Is it not a good answer, or do you think it's not a good tool?","2010-07-22 17:06:47",190,NULL
348,411,0,"I forgot to end my previous comment with the more or less obvious: and if so, how?","2010-07-22 17:11:20",89,NULL
349,479,1,"If you don't want too much colour, just show a band of a neutral eg light grey - but do it in the *foreground* with some high level of transparency. This means that any part of the data line which shows above this threshold will automatically "pop" as it has greater contrast to the background than to your 'normal' band.","2010-07-22 17:11:24",270,NULL
350,26,21,"Is this really the kind of question we want on this site? A simple google search, opening any stats book, or checking out wikipedia would instantly provide an answer.","2010-07-22 17:36:12",247,NULL
352,26,2,"Totally agree with PeterR","2010-07-22 18:13:11",NULL,jjgibaja
353,113,0,"Can you give an example of method selection with more details (a link to a page or a paper could be fine), this could help me to figure out more precisely. Thanks in advance","2010-07-22 18:14:49",223,NULL
354,452,0,"I am not sure of what you meant by "Robust Standard Errors are reported as a matter of course" standard errors of what? You said testing for "it" what is the test you are talking about?","2010-07-22 18:21:09",223,NULL
355,498,1,"Vote to keep. Stat software questions have to go somewhere. Stackoverflow is not the right place. Either we need to answer them here, or start statssoftware.stackexchange.com","2010-07-22 18:22:12",74,NULL
356,520,0,"I second the python recommendation here, especially if you have trouble fitting the data set into r. If you do go the python route,  have a look at "Programming Collective Intelligence" by Toby Segaran","2010-07-22 18:29:15",247,NULL
358,519,0,"Interesting example, because it looks, at first glance, like a probable cause-and-effect relationship, unlike many of the silliest examples.","2010-07-22 19:10:45",77,NULL
359,498,0,"Chief makes a good point. If not here, where? Is this purely "statistical analysis" just because of the name? Or is it a place that people working in the field can come for answers. Sometimes I just need a piece of SAS syntax that I've forgotten. I can dig through my archives, but I wouldn't mind coming here to get an answer/an alternate/a better way/ or just leave mental residue for future questioners.","2010-07-22 19:51:37",62,NULL
360,26,0,"The same concept may have different explanations at different levels... A smart sixth grader's understanding of the concept might be very different from that of a phd student, who has thought about it a lot more. It would be nice to see the basics revisited in light of more advanced concepts. That would help me understand how everything connects","2010-07-22 20:23:56",35,NULL
361,494,0,"Even if the constant is included, the AIC (AICc) can be negative.","2010-07-22 23:15:41",159,NULL
362,155,4,"Should definitely be community wiki.","2010-07-23 01:54:10",5,NULL
364,532,0,"Thanks for this answer (should be a comment ? ) 

As I already mentionned, as a comment to Rich answer's High dimension are not a problem (even 1000 could work) if you make parametric structural assumption.","2010-07-23 06:43:46",223,NULL
365,532,0,"@rob "I'm not sure what would be a good threshold " this would be the purpose of the multiple testing procedure I mentionned .... but I fully agree that things have to be filled in and I really like the outlier detection in the outlier detection !  who wants to writte a paper :) ?","2010-07-23 06:47:22",223,NULL
367,494,0,"That's what I've written.","2010-07-23 08:17:51",88,NULL
370,501,0,"But the question says there are more variables than observations. So it is not possible to begin with the full set.","2010-07-23 09:50:30",159,NULL
371,534,12,"Correlation and a strong underlying reason for a link suggest causation until proven otherwise is probably the best you can get.","2010-07-23 09:54:58",229,NULL
372,452,0,"Good point....I'm talking about the Standard Errors of regression coefficients in OLS regression and the problem of heteroscedasticity. The traditional approach would be to test for the presence of heteroscedasticity using, for example, White's test or Breusch Pagan test. If heteroscedasticity is found then one would report Robust Standard Errors, usually White Standard Errors.","2010-07-23 10:09:35",215,NULL
373,486,2,"Thanks for the helpful answers.","2010-07-23 10:37:02",266,NULL
374,541,0,""[T]hen I am making very strong assumptions about the relative difference between consecutive values of the ordinal variable."

I think this is the key point, really.  i.e. how strongly can you argue that the difference between groups 1 and 2 is comparable to that between 2 and 3?","2010-07-23 10:43:29",266,NULL
375,518,0,"Nice list, it will be very useful.","2010-07-23 11:31:31",166,NULL
376,520,0,"What downvote? I'm not familiar with Python yet, but it seems promising, I'll take a look.","2010-07-23 11:32:09",166,NULL
377,520,0,"@Victor, the first vote was a downvote (so it's now 2 upvotes/1 downvote). Maybe someone didn't like this tool. Happy I could help :)","2010-07-23 11:34:05",190,NULL
378,26,7,"I don't think the purpose of this site is to answer 6th graders questions. And my kid, when faced with such a question, would google for the answer.

If there is a specific part of the definition you don't understand, ask away. But such an unfocused question on such a basic topic indicates (to me anyway) that the poster didn't even try to find an answer. What is going to be next "What is a number and how are they used?"","2010-07-23 12:05:21",247,NULL
382,519,1,"What I like is that you can provoke lots of discussion about whether the "effect" was to actually impact fertility (in a medical sense of ability to conceive) or was it social ("I don't want to bring a child into this bad world"). Then drop the bombshell about the Pill if no-one else has brought it up. And then point out that even this can only be one possible factor and discuss some of the others.","2010-07-23 13:32:48",270,NULL
383,501,0,"What's the problem?","2010-07-23 13:41:29",88,NULL
384,486,0,"So consider accepting one.","2010-07-23 13:48:15",88,NULL
385,541,0,"I think you should make some assumption about how the continuous variable should be distributed and then try to fit this "psudohistogram" of each categorical variable frequency (I mean find bin widths which will transform it into a fitted histogram). Still, I'm not an expert in this field, its a fast&dirty idea.","2010-07-23 13:56:36",88,NULL
387,555,2,"If I am not mistaken, 

linear regression is the estimation of coefficients that define a good linear map from X to Y.

 ANOVA is a test to know if there is significant differences in X when Y take two different values.  

Can you explain us why you think they are the same?","2010-07-23 15:29:16",223,NULL
388,541,0,"Recasting binary categories as {0,1} makes sense, but turning that into a continuous [0,1] interval seems like a bit of a leap. On the broader front, I'm totally with your reluctance to weight ordinals equally unless there are powerful arguments from the model.","2010-07-23 15:29:27",174,NULL
390,455,9,"Statistical voyeurism? And there we were wondering what to call the site...","2010-07-23 15:48:25",174,NULL
391,557,0,"Thanks for the Gelman reference. I will read his paper. But, can't we analyze multilevel models using classical maximum likelihood? I agree that OLS is inefficient/inappropriate for multi-level models.","2010-07-23 15:50:47",NULL,user28
392,557,1,"+1 just for the lovely lucid even-handedness of this answer.","2010-07-23 15:56:32",174,NULL
393,556,6,"Data can be discrete without being restricted to integers. Or numbers, for that matter. It's always possible to *represent* discrete data with integers, but that doesn't mean the data can only take such values.","2010-07-23 16:06:37",174,NULL
394,557,2,"@Srikant - there any many ways to deal with multilevel data and Gelman is "the king" of this field. His point is that ANOVA is a simple/clear method of capturing the key features of complex and hierarchical data structures or study designs and ANOVA is a simple/clear way of presenting the key results. In this sense it's role is complementary or exploratory.","2010-07-23 16:30:00",215,NULL
395,565,0,"That's a great example!","2010-07-23 17:03:23",5,NULL
396,566,3,"Indeed, blending is one of the possible ensemble techniques. In particular, there are two when you combine the same sort of classifier, boosting (like Adaboost) and bagging (like Random Forest), and blending, where you combine different classifiers (what was Shane's question about).","2010-07-23 17:10:02",88,NULL
397,470,6,"Why pay $70 for the book?  To support the authors and to have a physical copy.  That's why I did it, anyway!","2010-07-23 17:13:42",5,NULL
398,411,0,"I just reread my long comment above and realized that the last question I raised is as much a practical consideration as theoretical.  In any case, that's one of the kinds of issues I'd be interested to learn about.","2010-07-23 17:23:48",89,NULL
399,26,1,"you are right.. this question is way too basic.. (but there are others like http://stats.stackexchange.com/questions/118 which are things I has confused by at some point and am glad to see discussions on on a website such as this)","2010-07-23 17:32:09",35,NULL
400,566,3,"For blending, this paper from the netflix competition is worth reading: http://www.the-ensemble.com/content/feature-weighted-linear-stacking.","2010-07-23 17:42:07",5,NULL
401,563,3,"Don't you think that the Wikipedia article about it is enough?","2010-07-23 17:59:06",88,NULL
402,534,1,"@James said it best.","2010-07-23 18:30:50",260,NULL
404,563,1,"Questions such as this require a wiki / blog post type of response. I do think questions should not require such long answers.","2010-07-23 19:50:07",NULL,user28
406,563,0,"I'm not sure the right thing to do is to simply ignore this question and refer the asker to the wiki - especially during beta where we are trying to build up the content of the site.  Perhaps the question asker should submit each of these questions individually so that they can be better addressed.","2010-07-23 20:52:00",196,NULL
407,529,0,"Sorry, I was meaning an analytical measurement method.  I've re-worded the question.","2010-07-23 20:58:13",114,NULL
408,529,0,"In that case, I think the two-sample test of equality for means/proportions is what you may want to do.","2010-07-23 21:04:24",NULL,user28
412,579,0,"Does that mean that for certain sample sizes BIC may be less stringent than AIC?","2010-07-23 21:36:49",196,NULL
413,121,0,"said "it's continuously differentiable (nice when you want to minimize it)" do you mean that the absolute value is difficult to optimize ?","2010-07-23 21:40:12",223,NULL
414,118,1,"Do you think the term standard means this is THE standard today ? Isn't it like asking why principal component are "principal" and not secondary ?","2010-07-23 21:44:37",223,NULL
415,311,1,"My opinion is that this book is THE book about extrem value theory","2010-07-23 21:56:20",223,NULL
416,532,0,"The principal component is a good powerfull idea. It will work in a lot of case. However, it is to avoid when the distribution has a trend or is multimodal... in those cases the direction of largest variation are not related to outliers.","2010-07-23 22:00:48",223,NULL
417,579,1,"Stringent is not a best word here, rather more tolerant for parameters; still, yup, for the common definitions (with natural log) it happens for 7 and less objects.","2010-07-23 22:13:56",88,NULL
419,121,14,"@robin: while the absolute value function is continuous everywhere, its first derivative is not (at x=0). This makes analytical optimization more difficult.","2010-07-23 23:59:23",7,NULL
420,470,0,"I agree with Shane... and a hardcopy makes reading it much easier.","2010-07-24 00:07:57",7,NULL
421,581,0,"What do you mean by 'viterbi training' exactly?","2010-07-24 00:40:50",240,NULL
422,579,0,"AIC is asymptotically equivalent to cross-validation.","2010-07-24 01:47:58",159,NULL
423,121,1,"Yeah, finding quantiles in general (which includes optimizing absolute values) tends to churn up linear programming type problems, which -- while they're certainly tractable numerically -- can get fiddly.  They typically don't have an analytical closed-form solution, and are a bit slower and a bit more difficult to implement than least-square-type solutions.","2010-07-24 02:55:02",61,NULL
424,574,0,"There is a lot of training involved. Training a reasonable Finnish acoustic model can take 60-100 hours of computing time. Besides that, most corpora work with standardized training, development and evaluation sets.","2010-07-24 04:50:42",190,NULL
425,588,1,"If I'm right you mix up Viterbi training and Viterbi decoding.","2010-07-24 05:00:00",190,NULL
427,121,1,"I do not agree with this. First, theoretically, the problem may be of different nature (because of the discontinuity) but not necessarily harder (for example the median is easely shown to be arginf_m E[|Y-m|]).  Second, practically, using a L1 norm (absolute value) rather than a L2 norm makes it piecewise linear and hence at least not more difficult. Quantile regression and its multiple variante is an example of that.","2010-07-24 06:01:42",223,NULL
428,118,0,"My understanding of this question is that it could be shorter just be something like: what is the difference between the MAE and the RMSE ? otherwise it is difficult to deal with.","2010-07-24 06:08:14",223,NULL
429,527,0,"Can you give more details in your question? I don't understand what is "the concentration of a particular molecule in a matrix".","2010-07-24 06:41:14",223,NULL
430,566,1,"IT is fun that meteorologist also use the word "ensemble" but not for combination: they use it for an ensemble of prediction (like scenario) obtained by perturbation of the initial conditions of the numerical model.","2010-07-24 06:46:49",223,NULL
431,529,1,"Wouldn't a test of means/proportions only give you a point estimate of whether the two methods gave the same average response for a given set of responses?  Couldn't that approach yield a result of "equal" even if the two methods were actually negatively correlated with one another?","2010-07-24 07:44:03",196,NULL
432,579,0,"@Rob Can you give a reference? I doubt that it is general.","2010-07-24 08:03:12",88,NULL
433,579,0,"@Rob For what I could found, this is true only for linear models.","2010-07-24 08:13:15",88,NULL
434,587,0,"AIC is equivalent to K-fold cross-validation, BIC is equivalent to leve-one-out cross-validation. Still, both theorems hold _only_ in case of linear regression.","2010-07-24 08:23:58",88,NULL
436,574,0,"E, 100 cores and one may live with that. Seriously, this looks dubious from a ML point of view, still I can understand the reasons why you do it in such a way. So then stick to the Srikant solution.","2010-07-24 08:44:37",88,NULL
437,585,0,"I meant significantly different results. I also think there is none, at least real-world example. Still, I think I'll wait some time more.","2010-07-24 09:04:44",88,NULL
438,590,0,"This question is a repost of http://stats.stackexchange.com/questions/536/when-a-serious-statistician-calls-a-geometric-distribution-a-geometric-density-t (see comments there). @Hibernating is the original question-asker","2010-07-24 09:09:35",190,NULL
439,121,11,"Yes, but finding the actual number you want, rather than just a descriptor of it, is easier under squared error loss.  Consider the 1 dimension case; you can express the minimizer of the squared error by the mean: O(n) operations and closed form.

You can express the value of the absolute error minimizer by the median, but there's not a closed-form solution that tells you what the median value is; it requires a sort to find, which is something like O(n log n).

Least squares solutions tend to be a simple plug-and-chug type operation, absolute value solutions usually require more work to find.","2010-07-24 09:10:00",61,NULL
441,573,0,"The question is interesting. The typos make it hard to read.","2010-07-24 09:24:27",200,NULL
442,573,0,"@Ivo I am sorry, there were a lot of mistakes indeed. I corrected a lot of them.","2010-07-24 09:40:23",190,NULL
444,587,1,"mbq, it's AIC/LOO (not LKO or K-fold) and I don't think the proof in Stone 1977 relied on linear models.  I don't know the details of the BIC result.","2010-07-24 11:01:35",251,NULL
445,93,4,"I think that if you ask a question that can be understood by people that don't know what the hazard function is and if you elaborate a bit more on what you do (how do you estimate the parameter of you'r gausian process, how do you use the gaussian process at the end) you will increase the chances to get an answer and this will be an added value for stats.stackexchange :)","2010-07-24 11:01:43",223,NULL
446,529,0,"That is a good point.","2010-07-24 12:16:03",NULL,user28
447,590,0,"I am guessing it is a typo or an oversight.","2010-07-24 12:27:31",NULL,user28
448,587,3,"ars is correct. It's AIC=LOO and BIC=K-fold where K is a complicated function of the sample size.","2010-07-24 12:42:31",159,NULL
449,512,0,"We had a look at it. Worked okay but in this case the noise still seemed to be a bit too strong and if we changed the parameters to even out the distributions enough it appeared that the trend was damped down too much. Maybe in this case there just is no solution to the data and it is just a bit too noisy.","2010-07-24 17:10:43",210,NULL
450,588,0,"You're right.  I wasn't aware that there was a procedure that used only the Viterbi algorithm to compute the transition probabilities as well.  It looks -- on further reading -- like there's some overlap of nomenclature between discrete time/discrete state HMM analysis, and discrete time/continuous state analysis using Gaussian mixture distributions.  My answer speaks to the DTDS HMM setup, and not the mixture model setup.","2010-07-24 19:08:38",61,NULL
451,587,0,"Congratulations, you've got me; I was in hurry writing that and so I made this error, obviously it's how Rob wrote it. Neverthelss it is from Shao 1995, where was an assumption that the model is linear. I'll analyse Stone, still I think you, ars, may be right since LOO in my field has equally bad reputation as various *ICs.","2010-07-24 20:10:12",88,NULL
453,603,0,"No, no, no, it is about machine learning *not* model selection.","2010-07-24 23:09:02",88,NULL
454,605,0,"To get the behavior you wanted, try using a simple CART.","2010-07-24 23:29:48",88,NULL
455,604,0,"I am not sure why you feel that in regression you will get R2 = 1 if you try to predict the predicted variable. Can you clarify?","2010-07-25 01:37:03",NULL,user28
457,603,1,"Interesting distinction. I thought model selection was central to machine learning, in almost all meanings of the term.","2010-07-25 02:40:53",30,NULL
458,13,3,"I am interested in the last statement: "the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome". Which model was it? I believe that the first model was RiskCalc by Moody's, and even the first version was a logistic regression model. The developers of that model were not CS people with a background in ML, but rather in econometrics.","2010-07-25 02:58:55",30,NULL
459,587,0,"The description on Wikipedia (http://en.wikipedia.org/wiki/Cross-validation_(statistics)#K-fold_cross-validation) makes it seem like K-fold cross-validation is sort of like a repeated simulation to estimate the stability of the parameters. I can see why AIC would be expected to be stable with LOO (since LOO can wasily be conducted exhaustively), but I don't understand why the BIC would be stable with K-fold unless K is also exhaustive. Does the complex formula underlying the value for K make it exhaustive?  Or is something else happening?","2010-07-25 03:18:18",196,NULL
460,593,0,"Thanks, I think the Friedman test is interesting, but I can't quite figure out how it is doing that adjustment for Type I error in the post-hoc.  The comments say it is a "Wilcoxon-Nemenyi-McDonald-Thompson test" but I've never heard of that before could you explain it?","2010-07-25 04:29:21",196,NULL
461,501,1,"You can't fit a model that has more variables than observations. There are not enough degrees of freedom for parameter estimation.","2010-07-25 04:42:10",159,NULL
462,411,0,"I know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises  Anderson darling statistic (and include in particular higher criticism of Tukey)...","2010-07-25 06:38:39",223,NULL
463,604,1,"You could improve clarity in your question. Note that A (good) classification rule may make classification errors in the learning set, this is sometime a clue to avoide overfitting.","2010-07-25 07:37:44",223,NULL
464,596,1,"Can you elaborate on what you really want to do (what is the model you want to fit) ? If you don't give a real question there won't be a real answer.","2010-07-25 07:40:01",223,NULL
465,594,0,"I am not sure but I think it is possible to interpret it as a stocastic gradient descent optimization procedure. I'll think about that...","2010-07-25 07:41:33",223,NULL
466,534,6,"Isn't it Karl Popper that said man can't establish causality: scientific theories are abstract in nature. They can be falsifiable and the fact that we encouter difficulties in falsifying something make us think about causality...","2010-07-25 07:48:30",223,NULL
467,244,0,"+1 for matplotlib and the collection... which one is your favorite ?","2010-07-25 07:51:32",223,NULL
468,501,0,"This is machine learning, so you can.","2010-07-25 08:08:08",88,NULL
469,603,0,"All those things work for trivial (mostly linear) models when you have few parameters and you just want to fit them to data to say something about it, like you have y and x and you want to check whether y=x^2 or y=x. Here I talk about estimating error of models like SVMs or RFs which can have thousands of parameters and are still not overfitting due to complex heuristics.","2010-07-25 08:22:10",88,NULL
470,604,0,"I think it is a good question; if I haven't known what I know I would thought the same.","2010-07-25 08:36:42",88,NULL
471,590,0,"Please fix spelling of Geometric. None of us have enough points yet to do editing.","2010-07-25 09:04:57",159,NULL
473,512,1,"Exponentially weighted moving averages are a special case of a kernel smoother (assuming you used a 2-sided MA rather than 1-sided). Better estimates that are generalizations of this are loess or splines -- see my answer.","2010-07-25 09:15:25",159,NULL
474,50,2,"I think this question could be formulated with more precision. With your formulation, we don't know who is "they", and we don't know if you want a general abstract mathematical definition, or an intuitive explanation for someone that don't know anything about statistic or mathematic...","2010-07-25 11:10:57",223,NULL
475,47,0,"I don't see why you want to cluster your users without any subjective input (I refer to a comment you made @reed), you said you want "the most appropriate number of clusters", but unfortunatly you don't have a clear objective, if you want to cluster your population to show something particular, you should tell us what you want to show ? If you want statistic (the data) to tell you what you want to show this is another problem :)","2010-07-25 11:24:42",223,NULL
476,194,0,"Maybe you are asking a bit to much to statistic: you said "patterns that you extract out from the data are indeed true patterns, not statistical fluke" and then you ask if there is a statistical procedure to answer the question... can I send that to xkcd :) ? more seriously, I think you should try to see if the pattern are meaningfull by trying to understand if they mean something","2010-07-25 11:29:52",223,NULL
477,242,1,"I really like this type of question, I think this type of precise real worl problem will increase the interest of the site. It would be even better if you had the possibility to add a link to the data, or to tell us (as a complement to the post) what you finally did, what was the conclusions .... however I understand that this can be confidential ...","2010-07-25 11:38:04",223,NULL
478,242,0,"I whish I could vote up again to make you pass over the question about the definition of a random variable ;)","2010-07-25 11:41:11",223,NULL
479,608,0,"+1 The code would be simple to write, still I'm very interested in seeing a clear, illustrative dataset.","2010-07-25 12:31:16",88,NULL
481,614,3,"Community wiki!","2010-07-25 15:02:22",88,NULL
482,614,0,"On the other hand, can a book be open source? It rather applies to code, so probably the better word is "open book".","2010-07-25 15:08:59",88,NULL
483,614,0,"Yes, any list question like this should be community wiki.","2010-07-25 15:25:40",5,NULL
484,185,0,"agreed, that is an excellent book. It pretty much explains how Google works :)","2010-07-25 18:41:18",74,NULL
485,615,1,"I'm still struggling to understand equations (biologist ahoy), which is why I turned to the community here, hoping it will help me explain the difference in layman's terms.","2010-07-25 19:18:23",144,NULL
486,613,0,"Thanks for your answer, it makes sense :)","2010-07-25 19:29:14",90,NULL
487,611,0,"I am afraid I do not follow much of what you wrote as I am not a trained mathematician. Are you saying that all 'discrete distributions' are in some sense densities under a very general definition?","2010-07-25 19:39:03",NULL,user28
488,611,2,"Without much mathematic you could say that a continuous variable has a density with respect to the Lebesgue measure, and a discrete random variable has a density with respect to the counting measure.","2010-07-25 19:56:22",223,NULL
492,26,3,"+1 to Peter... I think that we should directly close questions that find a direct answer on wikipedia http://en.wikipedia.org/wiki/Standard_deviation. I voted to close.","2010-07-25 20:07:35",223,NULL
493,50,2,"In addition, did you try google before asking ? 
ranked second (after wikipedia which is a bit theoretical on this question) on my google: http://www.stats.gla.ac.uk/steps/glossary/probability_distributions.html","2010-07-25 20:10:04",223,NULL
494,2,1,"did you try google/ wikipedia first ? 
http://en.wikipedia.org/wiki/Normal_distribution","2010-07-25 20:13:20",223,NULL
495,206,2,"Did you try google first ? for me, it gives http://infinity.cos.edu/faculty/woodbury/stats/tutorial/Data_Disc_Cont.htm","2010-07-25 20:15:33",223,NULL
496,109,3,"Can you give a definition or a link to the definition of the different test. Can you state the hypothesis you want to test (otherwise it is difficult to discuss the power ...).","2010-07-25 20:20:36",223,NULL
497,244,0,"I tend to use some more than others- the tool I use most often for visualization is R and associated packages, but I left it off of this list because there is no easy way to compile R scripts to stand-alone "executables" that the OP wanted.  I can't really claim a single favorite- I would have to say it depends on 1) The task at hand and 2) The tools I am using","2010-07-25 20:26:16",13,NULL
498,603,0,"These results are valid for regression of general linear models with arbitrary number of independent variables. The variables can be arbitrary learners. The crucial assumption is that as the number of observations goes to infinity the number of learners describing the true model stays finite. All of this works for regression, so for a classification task like yours I am not sure it helps.","2010-07-25 20:57:47",30,NULL
499,615,0,"I think the ideology is that FA assumes that the process is driven by some 'hidden factors', while the data we have consists of some combinations of them. Because of that, the problem of FA is to reconstruct the hidden factors somehow. And there goes PCA -- a method which iteratively builds a new variables (PCs) by mixing the old ones such to greedy absorb the variance of the data. One may say the PCs are equal to the FA's factors, and here they will be indistinguishable. But one may also make some changes to the PCA to make it a base of some other 'FA sort', and so the problem begins.","2010-07-25 22:56:50",88,NULL
500,615,0,"So basically, you should think of what you want to do (not which buzzword you want to use). I know it is hard, especially while having biologists around (to some extend use-buzzword works well in biology, so they just assume that this is common to other disciplines); still this is the way science should be done. Than use Google (or this site) to assess the good algorithm for it. Finally, use the docks to find a function/button that does it and type/click it.","2010-07-25 23:19:04",88,NULL
502,596,0,"Ok, I admit that now I am also in deep confusion. Probably you could translate some of the document-retrieval jargon.","2010-07-25 23:45:52",88,NULL
503,566,0,"@robin It's not fun, its physics.","2010-07-26 00:03:50",88,NULL
504,596,0,"Doc retrieval? None of that. However, I can elaborate.","2010-07-26 00:25:00",240,NULL
505,608,0,"I'm not sure what all would need to be in a clear and illustrative dataset, but I've made an attempt to include a sample dataset.","2010-07-26 05:50:33",196,NULL
506,566,1,"@mbq in fact they call themselves forecaster and they use statistic quite a lot ...","2010-07-26 06:46:28",223,NULL
507,457,1,"Sorry for the late reply. I like your suggestion of simulation. That is not really easy, though. The truth is, what I see in practice is that researchers just do the test that is computationally easier or that give them the result they want.","2010-07-26 07:41:27",90,NULL
508,566,1,"@robin I know, this is just why it's called "ensemble" not a set or something like this.","2010-07-26 07:46:15",88,NULL
509,608,0,"So look: what you provided is an example of an useless set, because the BIC and AIC give the same results: 340 v. 342 for AIC and 349 v. 353 for BIC -- so good.model wins in both cases. The whole idea with that convergence is that certain cross-validation will select the same model as its corresponding IC.","2010-07-26 07:53:50",88,NULL
510,570,0,"The term "SEM" is vague. It could also mean "Search Engine Marketing", for instance, for someone looking for statistical analysis techniques for studying ad click data or evaluating advertising effectiveness. Consider making the title more verbose.","2010-07-26 09:10:42",87,NULL
511,608,0,"I've made a simple scanning and for instance for seed 76 the ICs disagree.","2010-07-26 09:45:12",88,NULL
512,625,0,"Thanks for the suggestions, I've actually came across your book a couple of years ago.","2010-07-26 12:30:56",59,NULL
513,600,1,"Two quick notes on your notes. 1. The C-vM distance is precisely the L^2 cousin of the Kolmogorov (L^infinity) and (univariate) K-R (L^1) distances, and hence interpolates between them.  2. One advantage I didn't mention of the K-R and B-L distances is that they generalize more naturally to higher dimensional spaces.","2010-07-26 13:31:30",89,NULL
514,609,0,"Yes, what I called the Kantorovitch-Rubinstein distance is also called the L^1 Wasserstein distance or W1.  It goes by many other names too.","2010-07-26 13:37:04",89,NULL
515,411,0,"All the answers so far are very nice and add some nice perspective from different directions.  I won't accept any because I see no reasonable criterion for singling out just one.","2010-07-26 13:39:31",89,NULL
517,25,0,"Should be community wiki.","2010-07-26 14:54:10",5,NULL
518,614,0,"Done. Sorry about not doing it in the first place.","2010-07-26 15:32:04",107,NULL
519,614,0,"Any policy on how to aggregate the four answers into one wiki answer?","2010-07-26 15:32:58",107,NULL
520,614,0,"They should be separate answers (so people can vote on them), but the moderator or the individuals should make their answers community wiki as well.  All new answers with be CW by default.","2010-07-26 15:36:44",5,NULL
522,93,0,"Upvoted for doing a bounty...","2010-07-26 15:37:59",107,NULL
527,581,1,"In my problem I have an array of real valued data which I am modeling as a HMM (speficially a mixture of multiple density functions each with unknown parameters).   For now I assume that I know the state transition probabilites.   What I mean by Viterbi Trainig is the following algorithm.

1) Arbitrarily assign a state to each data point ( initialization)
2) Perform MLE of the density function parameters.
3) Re-estimate state for each point ( can be done with Viterbi Alg).
4) Goto step 2 and repeat unless stopping criteria is met.","2010-07-26 16:05:22",99,NULL
528,632,0,"Isn't that a function of estimator is still an estimator? I still don't know \\sigma, only X_i.","2010-07-26 16:45:07",88,NULL
529,632,0,"ok, then you will possibly estimate the square root of the variance of the estimation of the square root of the variance... right :) should be something like $\\hat{\\sigma}/n$ ?","2010-07-26 17:09:48",223,NULL
530,567,0,"+1 For this resource. As the name says, excellent for a practical approach to engineering problems.","2010-07-26 17:42:34",77,NULL
531,608,0,"Thanks mbq - I didn't understand what you meant/needed in terms of an illustrative dataset.  Also, in general, in terms of empirical demonstration I'd imagine that a single example from a single seed won't really do the trick.  I was imagining something like a metric from the employment of a cross-validation method being shown to be correlated with the calculated corresponding information criterion.  In that way the ICs don't necessarily need to give different answers in order to demonstrate the relation between the IC and the cross validation method.","2010-07-26 18:04:31",196,NULL
532,636,1,"I didn't downvote, but I must say that some explanation or reasoning why to use random forest could make this answer very much more interesting. ;)","2010-07-26 18:14:28",190,NULL
533,636,0,"I didn't downvote either, but why random forest over SVM (for instance)?","2010-07-26 18:16:06",5,NULL
534,635,0,"Thanks a lot for your answer ! I have read this book from first to last page, but I think it was edition 1... I didn't know it was available online.","2010-07-26 18:24:35",223,NULL
535,636,0,"I didn't downvote either, it didn't let me ;-) . @Peter extended. @Shane and why binomial regression? This is just my favorite algorithm and it may work well in this case.","2010-07-26 18:25:43",88,NULL
536,636,0,"+1 @mbq because to some extent, that's the "standard" and/or "obvious" answer when it comes to regression. :) (hope that's vague enough...) When it comes to a machine-learning approach, it doesn't seem to me that there is an obvious answer; but your experience is more than enough.","2010-07-26 18:26:52",5,NULL
537,625,1,"Rob, your book is great!","2010-07-26 18:37:01",74,NULL
538,26,6,"I think this question is ok. Actually, it was the most upvoted example on topic question on Area 51. Basics are ok here!","2010-07-26 18:52:44",190,NULL
539,50,5,"@robin We don't have a policy that we can not ask question, before you have googled them. We want to be the resource that comes on top when you google this question :)","2010-07-26 18:55:12",190,NULL
540,609,0,"Just to clarify for anyone unfamiliar with Wasserstein distances who reads this and gappy's answer: the L^2 Wasserstein distance (W2) is *not* the same as the Cramer-von Mises distance.","2010-07-26 18:55:17",89,NULL
541,130,0,"This should be community wiki since it is subjective.","2010-07-26 18:55:47",5,NULL
542,641,0,"Should be community wiki?","2010-07-26 19:25:49",5,NULL
544,645,0,"This seems very vague to me.  What kind of data, and what kind of analysis?  Also, this should be community wiki if it is subjective.","2010-07-26 19:36:21",5,NULL
545,650,0,"Peter, you beat me to the punch! I completely agree with storing data as text, though depending on the size (hundreds of millions of obs) it may be necessary to move into a map-reducible database (e.g., Pig, Cassandra, or one of the NoSQL options).","2010-07-26 19:37:03",302,NULL
547,652,7,"Related: http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician","2010-07-26 19:40:56",5,NULL
549,652,3,"Should be community wiki.","2010-07-26 19:41:32",5,NULL
550,652,1,"You tagged this as bayesian and machine-learning.  What kind of data analysis are you interested in?","2010-07-26 19:46:47",5,NULL
551,650,0,"Oh ok interesting! So just take data for each variable and lay it out in row-column format and get to number crunching eh? Are there any tools I should be looking at or should I just be programming something?","2010-07-26 19:47:31",9426,NULL
552,652,0,"Shane: Honestly, I don't really know yet. What kind of data analysis is there? Should I be posing this as yet another question for the site?","2010-07-26 19:49:24",9426,NULL
553,647,0,"I can do Monte Carlo, I just wanted to do in a more 'sciency' way; still you're right that the distribution is not normal, so this sd will be useless for testing.","2010-07-26 19:50:17",88,NULL
554,650,0,"R is a very extensive (and free) toolkit/programming language/library for statistics. My favorite for most things is however Python with SciPy / NumPy","2010-07-26 19:50:23",190,NULL
555,648,0,"I thought it shows the binomial distribution; I don't think that its asymptotics have a direct link with CLT.","2010-07-26 19:52:46",88,NULL
556,655,1,"May be a bit too academic for myself being such a beginner...","2010-07-26 19:53:40",9426,NULL
558,652,0,"There are many kinds.  :)  Those are just two specific areas, so it seems odd for tags on such a general question.","2010-07-26 19:54:21",5,NULL
559,652,0,"@Justin If you don't know it yet, this question is too broad and vague. A nice thing to learn what a field contains, is to look on the tags of this sites and see what questions match with it. Also wikipedia and the books you bought can give you and idea of what you want (although the books maybe not name the field wherein they operate, more on the practical overall part)","2010-07-26 19:54:54",190,NULL
560,648,2,"bean machine by the author of the package animation... http://yihui.name/en/wp-content/uploads/2010/07/animation-useR2010-Xie.pdf","2010-07-26 19:55:09",223,NULL
562,643,0,"This should be a community wiki.","2010-07-26 19:57:08",88,NULL
565,648,1,"@mbq take a look at http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation","2010-07-26 20:01:14",223,NULL
566,658,0,"what is the relation between the Cauchy distribution and the CLT or the failling of the CLT ?","2010-07-26 20:02:15",223,NULL
567,652,0,"Why is this question not closed yet as duplication ?","2010-07-26 20:03:09",223,NULL
568,656,3,"That's great.  The standard way of dealing with outliers.","2010-07-26 20:06:32",5,NULL
569,660,0,"What do you mean by *types* of "textual" content?","2010-07-26 20:08:17",68,NULL
570,648,0,"@robin I have wrote about it, what's the problem?","2010-07-26 20:09:41",88,NULL
571,652,0,"@Peter Smit: Thanks I'll give that a shot.
@Shane: Thanks as well... I was trying to give some idea of context with my tags.

Beware the noobs!","2010-07-26 20:10:14",9426,NULL
572,658,1,"@robin Consult http://en.wikipedia.org/wiki/Cauchy_distribution#Properties","2010-07-26 20:12:30",88,NULL
573,652,0,"@robin There are only 7 users able to vote for closing... so it can take a small while","2010-07-26 20:17:54",190,NULL
574,600,0,"Regarding 1., that's correct. Regarding 2. In principle all of the above distances could carry over to R^n, however I don't know of popular non-parametric tests based on *any* distance. It would be interesting to know if there are any.","2010-07-26 20:18:19",30,NULL
575,641,0,"I think a general wiki for where to get data is great, with a section for survey data.","2010-07-26 20:21:15",302,NULL
577,165,0,"Should be community wiki?","2010-07-26 20:22:54",5,NULL
579,660,0,"Could you show some sample data?","2010-07-26 20:27:15",NULL,user28
581,672,4,"Related: http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english","2010-07-26 20:34:14",5,NULL
583,658,0,"great ! I did not know that !","2010-07-26 20:47:47",223,NULL
589,262,3,""Hardly publication quality"???? I realise that it isn't perfect - the phrase "...should you get you started.." covers that bit. But with a little additional work, i.e. axis labels, I would say it's fine. BTW, what journals do you publish in?","2010-07-26 21:20:59",8,NULL
590,652,0,"@robin @peter: It isn't clear to me that it's an exact duplicate either.  Are you both referring to the one I linked above or another one?","2010-07-26 21:23:14",5,NULL
592,667,1,"So probability is pure mathematics and statistics is applied mathematics?","2010-07-26 21:24:35",327,NULL
593,673,0,"So statistics is synonymous with data analysis?","2010-07-26 21:25:04",327,NULL
594,652,0,"@shane I agree that it is not exact duplicate, but then it is too broad or not enough and the title is quite similar. In addition, my view is that someone asking for a book on stat.stack could at least say for what purpose and for what level.","2010-07-26 21:29:12",223,NULL
595,652,0,"@robin: I agree 100% with that.","2010-07-26 21:33:06",5,NULL
597,675,0,"So statistics observes what happens in the physical world, theorizes about the underlying process, and then having found the process, uses it in the sense of probability to predict what will happen next?","2010-07-26 21:34:19",327,NULL
598,651,0,"I work in medical/epidemiological research, and I see this book on colleagues' shelves all the time.  Still haven't read it myself, but I can attest to its popularity.","2010-07-26 21:35:13",71,NULL
599,673,1,"I don't see any distinction.","2010-07-26 21:39:17",25,NULL
600,667,2,"Statistics may be applied and may be not; still the concept of data is always present.","2010-07-26 21:42:34",88,NULL
601,685,0,"I feel that your question is not precise enough to get a reasonable answer.","2010-07-26 21:44:31",NULL,user28
602,685,0,"It could be rephrased as: In what ways are statistics misleadingly reported or cited?","2010-07-26 21:51:18",327,NULL
603,479,0,"Found a way to do this (high transparency overlay on the "danger" part of the graph) with our toolset, thanks!","2010-07-26 21:53:17",259,NULL
604,685,1,"Even if it is not off-topic, it should be community wiki.","2010-07-26 22:08:47",88,NULL
605,687,0,"About the glass, I think that just the boundary between phases lies in the half of its height.","2010-07-26 22:10:48",88,NULL
607,685,0,"Your re-stated question is much better. I would either suggest asking another question along those lines or better still edit the current one along the lines of your comment.","2010-07-26 22:20:56",NULL,user28
610,13,1,"I bet they used discriminant analysis before logistic regression, as DA was invented well before LR","2010-07-26 22:56:40",74,NULL
612,194,0,"@robin, that would be the hard part. Sometimes when thousands of variables influencing something, it's hard to make sense of the patterns that emerged.","2010-07-27 00:09:31",175,NULL
613,675,0,"I'm not a statistician, but from my understanding I'd say, yes, that *part* of what statistics does.","2010-07-27 00:10:53",89,NULL
615,349,0,"Since you are only interested in the median why couldn't you make the bins wider at higher values of your variable?","2010-07-27 00:23:15",196,NULL
616,570,0,"drpaulbrewer, thanks for the suggestion.","2010-07-27 00:49:44",251,NULL
617,702,1,"With a millon points and an 8 parameter model, a goodness of fit test like chi-squared tells me that there is essentially no chance that the model is correct. (Which is not surprising, as there are endless factors influencing reality that are not in the model)

RMSE gives me a sense as to how good the model fits the data, but does not give me a sense of whether there is a better model","2010-07-27 01:07:20",72,NULL
618,702,0,"Well in order to find out if there is a better model, you could either experiment with different formulations or you could use various plots (e.g.,  exit times vs time) to see if the data is consistent with your model assumptions. You could also plot predicted exit times for a small sample selected at random vis-a-vis actual times to for model improvement ideas.","2010-07-27 01:17:13",NULL,user28
619,612,0,"Consider adding a tag that directly references the R package you are using.","2010-07-27 04:00:46",196,NULL
620,712,6,"What is your data, and what do you want to do with the anonymized data?","2010-07-27 04:42:39",190,NULL
621,495,2,"I have watched all of those videos. It's a very good introduction to probability and counting.","2010-07-27 06:12:47",339,NULL
623,732,12,"I don't mind the down vote, but I maintain that this is a deep statistical point, not to be taken lightly.  ;-)","2010-07-27 07:18:30",251,NULL
624,675,0,"great answer! +1 for Persi and Mark","2010-07-27 07:21:49",223,NULL
625,242,0,"I will come back to tell you what the results were, but it will be a while as I am working my way through this alongside lots of other tasks. Wasn't sure what you meant about "pass over the question about random variable"? Is there a question you recommend I look at?","2010-07-27 07:46:32",199,NULL
626,744,13,"I like this one, could be put as an advise when people  write questions on this site ?","2010-07-27 08:48:41",223,NULL
627,705,0,"Srikant, thanks for the response!  I'm not sure what you mean by contour plots of covariances (obs v est) -- could you elaborate?  Thanks.","2010-07-27 08:52:31",251,NULL
628,612,0,"Perhaps Peter can edit it - my rep is not high enough yet.","2010-07-27 08:54:56",144,NULL
629,675,7,"Induction vs Deduction?","2010-07-27 09:14:39",434,NULL
630,686,0,"I hadn't seen these ones before. They look good.","2010-07-27 09:40:01",183,NULL
631,749,1,"+1 Wow, nice list.","2010-07-27 09:51:43",88,NULL
632,734,1,"i think only one of the classes in the Iris data set is linearly separable. (From the OP's Question, i think he's after data sets w/ *only* linearly separable classes).","2010-07-27 09:52:30",438,NULL
634,734,0,"@doug, good point, still I don't think that one can get any non-synthetic fully linearly separable problem. Nevertheless I'll wait for the OP's reaction.","2010-07-27 10:13:30",88,NULL
636,705,0,"See this: http://en.wikipedia.org/wiki/Level_set. Let Sigma be a a 2 dimensional covariance matrix and Y ~ N(0, Sigma). An iso-contour line would plot the set of points Y for which f(Y|sigma) = c where c is a constant. Note that Y is a 2-dimensional vector. You would choose various values of c and hence obtain different iso-contour lines which would give you a sens of the spread of the distribution.","2010-07-27 10:54:50",NULL,user28
637,759,0,"thanks a lot for this answer!","2010-07-27 11:04:27",223,NULL
638,759,0,"You're welcome.","2010-07-27 11:16:21",88,NULL
640,748,29,"Still it looks promising.","2010-07-27 11:20:02",88,NULL
642,741,0,"Thanks, this is just what I was looking for.

This is essentially a million subjects each with an entry and exit time.

Yes we are conditioning to account for the censoring.","2010-07-27 12:03:40",72,NULL
643,693,3,"+1 I really like this explanation and think it is very clear.","2010-07-27 12:16:21",81,NULL
644,337,0,"I'm not interested into *truth* but in getting a function that works. I'm a bioinformatician and taught not to seek dogmatic *truth* but to seek statistics that work. I don't think that there work done with the kind of data that I want to work with that specifics what entropy works best. That's kind of the point why I want to work with the data.","2010-07-27 12:17:57",3807,NULL
645,337,1,"Right, but this is not a discussion about dogmatic truths but about words. You have asked about entropy, so I answered about entropy. Because now I see that you indeed need an answer about time series descriptors, write a question about time series descriptors, only then you'll get an useful answer.","2010-07-27 12:32:45",88,NULL
646,695,1,"Both of these are really excellent.","2010-07-27 12:35:16",247,NULL
647,768,0,"So we can only find patterns that we were looking for in the first place?","2010-07-27 12:49:35",327,NULL
648,769,3,"Statistics is not a subset of data analysis -- it is a theory that is used in data analysis.","2010-07-27 12:56:46",88,NULL
649,618,0,"Nice, but I can't see anyone trying to draw a conclusion of causality there. Or are mexican lemon-truck drivers notoriously dangerous once they get over the border?","2010-07-27 12:57:59",270,NULL
650,674,0,"What if the random errors become greater than the observable factors over time?","2010-07-27 12:59:24",327,NULL
652,579,0,"@mbq. I was thinking of Shao 1995 which is, indeed, only for linear models. I don't know if the result has been extended to other models.","2010-07-27 13:30:26",159,NULL
653,769,4,"Whatever you do in Excel does not count.  Just kidding...","2010-07-27 13:36:13",334,NULL
654,769,1,"@Dirk: Wow...your hatred towards Excel knows no bounds.  :)","2010-07-27 13:37:37",5,NULL
655,769,0,"This should probably be community wiki since it is subjective/argumentative.","2010-07-27 13:40:46",5,NULL
656,252,0,"+1 for omegahat - very good source","2010-07-27 13:41:47",22,NULL
657,780,2,"+1 Nice pointer to the OR exchange site.","2010-07-27 13:59:54",5,NULL
658,772,0,"Wow, Shane, thanks for the very fast response! I'll look into those references!","2010-07-27 14:02:33",445,NULL
660,744,6,"Absolutely...asking the right question is one of the most important skills.","2010-07-27 14:17:30",5,NULL
661,790,1,"maybe a reference to the paper could help ?","2010-07-27 14:52:37",223,NULL
662,790,0,"Slightly related question: http://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-var/","2010-07-27 14:53:37",5,NULL
664,349,0,"drknexus - because we don't know what the largest bin should be.","2010-07-27 14:58:50",247,NULL
665,47,0,"@robin: thanks for comment. i haven't seen much research in this area and the input from data provider was minimal [so far they were not interested/capable of investigating it further]. after initial exploration of data it was quite clear for me that there are couple of distinctive patterns [examples being 'heavy downloaders' in just a few locations or 'frequent hoppers' with lots of small sessions in large number of locations].
my goal at this stage was to try to use data itself to tell me how can i divide it best and minimize subjective input.","2010-07-27 14:59:00",22,NULL
666,790,0,"You may find this paper of interest, which discusses the log mean and how it refines the 'arithmetic mean - geometric mean' inequality: http://www.ias.ac.in/resonance/June2008/p583-594.pdf","2010-07-27 15:03:53",81,NULL
667,704,1,"but then how to choose eta, and what doe sthis then mean statistically? i.e. how to form confidence intervals for the median from this result?","2010-07-27 15:08:20",247,NULL
668,428,2,"So, according to a long lost colleague, the best apropoach seems to be Chiranjeeb Buragohain and Subhash Suri. Quantiles on Streams.
http://www.cs.ucsb.edu/~suri/psdir/ency.pdf

I also like Ian's approach, as these medians of smaller data sets will converge to a normal distribution, and so I can form conf intervals for the medians.","2010-07-27 15:10:34",247,NULL
669,1,4,"Although I've accepted an answer, I would recommend that interested people should look at all the answers.","2010-07-27 15:22:18",8,NULL
670,801,0,"sorry, i should have mentioned that i need to do this in an automated way. the option of "doing it multiple times until i find the one that best suits my purpose" won't work for me. has to be done computationally...","2010-07-27 15:34:03",476,NULL
671,794,0,"oh the bayesian is soooo Good...","2010-07-27 15:39:02",223,NULL
672,799,2,"... and in that vein also the lme4 package (which I find easier to use than lme or nlme) and related packages from Baayen's above referenced book, languageR.","2010-07-27 15:40:33",196,NULL
674,804,3,"always these bayesian guys...","2010-07-27 15:45:32",442,NULL
675,720,1,"I found all the answers to this question helpful, but I think that this one is the most practical.","2010-07-27 15:46:34",266,NULL
676,486,3,"Oddly enough, I was rather interested in hearing what other people had to say...","2010-07-27 15:52:13",266,NULL
677,779,0,"As I always understood it, the central limit theorem does not postulate something about averaging a large number of iid random variables. Rather, it states that when sampling means, the distribution of the means becomes normal (independent of the distribution underlying what is sampled from). So I question whether the antecedent for your question holds.","2010-07-27 15:57:43",442,NULL
678,779,0,"But, if the sampling mean becomes normal irrespective of the distribution of the underlying distribution then is that not the same as saying 'averaging a large number of iid random variables' get us a normal distribution. To me they seem to be equivalent statements.","2010-07-27 16:11:46",NULL,user28
679,660,0,"@Srikant Vadali - sample data could be press releases, news stories, etc .. the textual data would be free-form, likely obtained from rss feeds or similar. Market data for a given company is what I'm looking to analyze/correlate. So maybe Blogger Bill writes a story about an upcoming VMware feature release, and VMW jumps 10%. (Oversimplified, I know)","2010-07-27 16:11:57",292,NULL
680,677,0,"that looks like a really promising start :)","2010-07-27 16:12:31",292,NULL
681,750,13,"I hate this quote. It makes professions using statistics look like you could cheat. But, when someone profoundly uses statistics one knows that actually you cannot cheat. Because when provided with enough information about the statistical procedures used, one can draw a conclusion on the soundness of the procedures/results. If not enough information on the statistical (and other) procedures are provided, you should immediately question the results.","2010-07-27 16:15:40",442,NULL
682,811,0,"is it ? then we are all intelligent persons here:)","2010-07-27 16:22:37",223,NULL
683,799,0,"thanks for the comment, I totally agree with you. lme4 is simply the best around.","2010-07-27 16:22:39",447,NULL
684,779,0,"Not in my eyes (but i would like to be convinced otherwise).
In the one case (the one i think of being meant by CLT) you draw samples from one distribution. Their means are normally distributed.
What i understand from the question and the quote "average a large number of iid random variables" is sth differnt: individual instantiations from different iid random variables determine (or make up) a trait. Hence, no averaging (i.e., computing a mean) from a single distribution and, hence, no application of the CLT.
I think mbq's answers points to the same issue.","2010-07-27 16:27:57",442,NULL
685,815,0,"+1 for harmonic oscillator.","2010-07-27 16:35:39",88,NULL
686,779,1,"Well the distribution need not be identical if some conditions hold. See: http://en.wikipedia.org/wiki/Central_limit_theorem#Lack_of_identical_distribution","2010-07-27 16:42:43",NULL,user28
687,787,0,"Hillaire Belloc?  Nice work on digging that up.","2010-07-27 16:43:12",5,NULL
688,252,0,"Well, RCurl and XML and both on CRAN too...","2010-07-27 16:47:41",334,NULL
689,307,3,"As can be read in my and dave's post, saturated models do not per definition lead to perfect fit. but if you use the n-1  polynominal as the model they will. see Sue Doe Nihm's seminal paper on this topic http://psych.fullerton.edu/mbirnbaum/papers/Nihm_18_1976.pdf","2010-07-27 16:52:10",442,NULL
690,783,0,"I would love to take this one as my accepted answer ! too good to be true !","2010-07-27 17:10:56",223,NULL
691,825,1,"IMO, this question belongs on stackoverflow.  Voting to close as off-topic.","2010-07-27 17:11:36",5,NULL
692,820,0,"Some comments would be appreciated for the down-vote.","2010-07-27 17:12:21",488,NULL
693,827,0,"Hah. You tricked me. I saw your first (one-line, no links) answer and then filled mine in.  We need mutexes here :)","2010-07-27 17:12:51",334,NULL
694,825,1,"Dunno. This is middle ground and suitable for either.  Xrefs, maybe?","2010-07-27 17:13:41",334,NULL
695,828,0,"What, are you copying my answers now?  :)  This is of course, needless to say, your domain.","2010-07-27 17:14:16",5,NULL
696,828,0,"Well, mutexes needed. As I commented on your answer, I only saw the first (raw) version and figured well, I may expand on mc and Rmpi.  And then you did and I look like a copycat.  Such is life.","2010-07-27 17:15:55",334,NULL
697,828,0,"On the other hand, my answer is derived from reading your paper/presentation in the past.  So I guess I'm copying you as well.","2010-07-27 17:16:37",5,NULL
698,825,0,"@Shane Agree, this is for SO.","2010-07-27 17:18:46",88,NULL
699,827,0,"Yep...with basic questions like this, I find that it's best to follow the ol' "answer quickly and then improve" routine...may not be a good practice for everyone else though.","2010-07-27 17:18:53",5,NULL
700,825,0,"I just don't see how the fact that importing data and running SVM has any relevance to the question.  That's why I think it's more of an SO question.  But I could see Xrefs as being a good long-term solution since it is R...","2010-07-27 17:20:58",5,NULL
701,822,0,""market basket analysis" seems like it's what I'm looking for, thanks for input.","2010-07-27 17:22:33",488,NULL
702,827,0,"thanks for the prompt replies guys. multicore package looks like a good choice for my needs.","2010-07-27 17:27:49",480,NULL
703,830,4,"I disagree somewhat. Revolution does a great sales job in getting mindshare (as evidenced by your post) but as of right now there is very little in the product you would not already get with the normal R (at least on Linux). Intel MKL, sure, but you can get Goto Blas.  On Windows, they offer doSMP which helps as multicore cannot be built there.","2010-07-27 17:42:14",334,NULL
704,658,0,"The CLT requires that the MGF's exist in a neighborhood of 0. The Cauchy distribution does not have that property. CLT Win. 

Cauchy doesn't even satisfy the weaker requirements of a stronger version of CLT where all that is required is that mean and variance exist. The Cauchy distribution shows that the mean is required to exist for the CLT to hold. It doesn't make the CLT fail.","2010-07-27 17:46:38",62,NULL
705,744,3,"I remember once where a private industry company commissioned a mathematician to solve a garbage collection routing problem. Long story short, the mathematician complained that the company was only interested in finding a "close enough" solution rather than an optimal solution. I think, ultimately he was fired, and an operations researcher was brought in instead.","2010-07-27 17:59:21",59,NULL
706,658,0,"@Baltimark You have misunderstood my post -- its obvious that Cachy is not covered by CLT because of CLT assumptions, otherwise it would be impossible to prove CLT. I have gave this example because people believe that CLT works for all distributions; probably "fail" is not a perfect word, but still I don't think it is a reason for downvote. Ok, I have even changed it to not applicable.","2010-07-27 18:05:10",88,NULL
707,658,0,"I prefer your edit. The Cauchy distribution is definitely very cool.","2010-07-27 18:11:05",62,NULL
708,834,3,"I would suggest reviewing the wiki link: http://en.wikipedia.org/wiki/Akaike_information_criterion and then edit the question so that you can highlight the aspect of AIC that you do not understand.","2010-07-27 18:14:03",NULL,user28
709,658,0,"Yup; the another nice "paradox" comes from basic physics -- if you have a particle that has flown between two detectors with an uniform speed and you've measured the distance and time-of-flight with a normally distributed error, the speed obtained from v=s/t is Cauchy distributed, so one may say that it is undefined ;-)","2010-07-27 18:19:06",88,NULL
710,834,2,"Read also this question: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other","2010-07-27 18:22:29",88,NULL
711,827,0,"I mainly use multicore, still I like snowfall more than snow and Rmpi for its fault tolerance and clean interface.","2010-07-27 18:25:39",88,NULL
712,754,0,"I guess I should remove it... poor Karl Pearson, one of the inventor of hypothesis testing not understood by the 21st century ... I would vote up if I could, but it's me that put it here :)","2010-07-27 18:31:32",223,NULL
713,478,5,"The Statistical sleuth is used a the textbook on that great introductory course (there are 64 lectures in total)  http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#","2010-07-27 18:36:31",339,NULL
714,842,1,"+1 Great point about how this doesn't deal with splitting up the cross-validation.","2010-07-27 18:45:28",5,NULL
716,709,0,"There are also more advanced treatments on the course pages for PS 236 and PS 239 (graduate-level political science methods courses) at my website: http://cgibbons.berkeley.edu/teaching.html","2010-07-27 19:09:24",401,NULL
717,847,3,"Have you tried asking Google itself?","2010-07-27 19:24:25",88,NULL
718,846,0,"Maybe Wikipedia will be sufficient here: http://en.wikipedia.org/wiki/F-test#Regression_problems","2010-07-27 19:34:49",88,NULL
719,806,3,"What do you precisely mean by normalization?","2010-07-27 20:13:18",88,NULL
720,744,1,"@dassouki I think the quote is more about the question .... something like science is not about finding good answer but about finding good questions !","2010-07-27 20:21:49",223,NULL
721,680,0,"This was awesome. Thank you so much for this response. You've given me a great jumping off point. Any books you recommend since you seem to "get" where I'm at.","2010-07-27 20:24:17",9426,NULL
722,731,0,"Thanks for the paper, I upvoted you for it.","2010-07-27 20:24:58",9426,NULL
723,680,0,"you're very welcome. books:  

Statistics in Plain English to start. 
Multivariate Data Analysis by Hair after that .   
  
These are good web resources:  
  
http://www.itl.nist.gov/div898/handbook/  , 
http://www.statsoft.com/textbook/","2010-07-27 20:54:37",74,NULL
724,845,4,"gappy specifically stated that he has paired, i.e. non-independent samples, so assumption 2 is violated right away.","2010-07-27 22:08:42",279,NULL
725,349,0,"Do you have **any** intuition as to what the range will be?  If you're fairly sure that over half of the answers will be below number N, then you can make your last bin as large as you want.  Maybe your last bin is all numbers greater than 1 trillion - would that be high enough?  With the amount of memory in modern systems you can store a LOT of bins and achieve fairly high resolution. In terms of data structures, we're not talking anything fancy and memory intensive here.","2010-07-27 22:17:59",54,NULL
726,850,0,"It's a good idea, but

    "mtcars$dummy <- 1; lrm(am ~ dummy, data=mtcars);" 

gives back:

    singular information matrix in lrm.fit (rank= 1 ) Offending variable(s):
    dummy 
    Error in lrm(am ~ dummy, data = mtcars) : 
    Unable to fit model using “lrm.fit”","2010-07-27 23:03:52",501,NULL
727,858,0,"This will due for now.  Is the fact that this doesn't work in the Design package a bug?","2010-07-27 23:06:16",501,NULL
728,734,0,"yes - only one of the classes in the iris data set is linearly separable - so, I've now started experimenting with algorithmically generated datasets.","2010-07-27 23:22:51",130,NULL
729,845,0,"Sorry I missed that. I'll try and think of something else.","2010-07-27 23:41:17",8,NULL
730,845,0,"Thanks anyway. I learned something from the wrong answer as well.","2010-07-28 00:08:09",30,NULL
731,845,1,"@gappy: I've updated my answer. Hopefully this will get you started.","2010-07-28 00:09:12",8,NULL
732,841,2,"I'm not sure why the information that the observations are paired is important to the hypothesis being tested; could you explain?","2010-07-28 00:11:01",196,NULL
733,834,1,"Consider asking the general question about the AIC separately from the stata question.","2010-07-28 00:16:29",196,NULL
735,674,0,"In that case you re-work your model as it is no longer consistent with reality.","2010-07-28 01:06:13",NULL,user28
736,815,0,"the maximum entropy principle is also another reason why the Gaussian distribution is used.
For example, what are good reasons for using Gaussian errors in the linear model, except tractability ?","2010-07-28 02:10:27",368,NULL
737,868,0,"Also help with laying out tables on this site is much welcomed and appreciated.","2010-07-28 03:15:57",9426,NULL
738,869,0,"Interesting... I'm not exactly sure how to apply that to what I've done but I'll think on it a bit more so I can articulate a question. Thanks!","2010-07-28 04:22:26",9426,NULL
739,705,0,"@Srikant, thanks for the suggestion.  I spent some time trying it out and it seems like a good start at getting a quick visual comparison, especially when the fit is bad.","2010-07-28 04:35:31",251,NULL
740,870,0,"your reference about q-value should be http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1074290335","2010-07-28 05:54:26",223,NULL
741,866,0,""Say I want to estimate a large number of parameters" this could be made more precise: What is the framework ? I guess it is linear regression?","2010-07-28 05:59:31",223,NULL
742,870,0,"The Benjamini-Hochberg procedure is not for calculating the FDR, it is for controlling the FDR (keeping it under a predefined threshold)","2010-07-28 06:11:26",223,NULL
743,870,0,"Your question, as it stands, is difficult to understand. What do you mean by "referred to" ?","2010-07-28 06:15:01",223,NULL
744,858,1,"Rather a non-implemented feature, still you can send a bug report.","2010-07-28 09:23:10",88,NULL
745,497,0,"Change to community wiki.","2010-07-28 09:27:17",88,NULL
747,835,0,"aligatou gozaimasu","2010-07-28 09:35:29",223,NULL
748,497,0,"@mbq Done......","2010-07-28 09:36:32",190,NULL
749,252,0,"@Dirk yes, that is correct, have edited my answer to include this information, cheers.","2010-07-28 10:00:28",81,NULL
750,789,1,"This is Creative Commons but does not allow derivates... :( That way, I cannot use the bits in my own course material, just the bits I need them to know...","2010-07-28 11:00:41",107,NULL
751,880,0,"But still, what do you want to measure with CV and in what purpose? To get a cutoff of attribute number?","2010-07-28 11:15:17",88,NULL
752,880,0,"@mbq: thanks for the advice. I have edited the question accordingly, hope it is more clear now !","2010-07-28 11:26:47",223,NULL
753,887,0,"What do you mean by: "we would like to test whether it is an vis-a-vis the general population"? and "single sample S"?","2010-07-28 12:06:44",NULL,user28
755,886,2,"I don't see the interest of these questions about trying to bridge a fictive gap. what is the aim of all that ? in addition there are a lot of others idea that are fundamental in statistic... and loss function is at least 100 years old. can you reduce statistic like that ? maybe your question is about fondamental concept in datamining/statistic/machine learning however you call it ... Then the question already exists and is too wide http://stats.stackexchange.com/questions/372/what-are-the-key-statistical-concepts-that-relate-to-data-mining/381#381.","2010-07-28 12:19:52",223,NULL
756,890,0,"Maybe you can precise your problem?","2010-07-28 12:44:10",88,NULL
759,886,0,"Well, I do not know much about machine learning or its connections to statistics. In any case, look at this question: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning which suggests that at the very least that the approaches to answer the same questions are different. Is it that 'unnatural' to wonder if there is some sort of link between them? Yes, I agree that there lot of ideas in statistics. That is why I have fundamental in quotes and restricted the scope to estimating parameters of interest.","2010-07-28 13:13:32",NULL,user28
760,886,0,"@Srikant link between what ? note that I really like to search link between to well defined objects, I find it really natural.","2010-07-28 13:20:05",223,NULL
761,893,0,"An excellent link. Thanks for sharing this one!","2010-07-28 13:43:41",1356,NULL
762,894,7,"It is not quite correct to say "you cannot change z". In fact, you have to change z to make the sum equal 10. But you have no choice (no freedom) about what it changes to. You can change any two values, but not the third.","2010-07-28 14:29:49",25,NULL
764,899,0,"Can you assume that the other two groups are from different Normal distributions?","2010-07-28 14:37:00",8,NULL
765,908,0,"said "You miss one important issue -- there is almost never such thing as T[i]"

 I wanted the answer to focus on the problem of selecting the number of variables. Construction (which I agree are not perfect) of T[i]  are discussed here http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification

Sometime, it is also usefull to discuss problem separatly.","2010-07-28 15:21:55",223,NULL
766,902,0,"This is an interesting paper.  Could you add to your answer with some more references and include (for instance) the paper's title and author?  Are there any particular academic research labs working in this area that would be relevant?","2010-07-28 15:37:02",5,NULL
767,908,1,"@robin But here you can't tear those apart. The most of algorithms mentions in that question were created to address this issue -- forward selection is to remove correlated features, backward elimination is to stabilize the importance measure, mcmc is to include correlated features...","2010-07-28 15:37:10",88,NULL
768,908,0,"@robin the idea of making some exact importance measure was a base for so-called filter algorithms which are now mainly abandoned since they were just too weak. They have the advantage that they are computationally cheap, still this is not worth it.","2010-07-28 15:42:44",88,NULL
769,910,0,"I'm not sure that this is entirely correct.  In what sense do machine learning methods work without parameter estimation (within a parametric or distribution-free set of models)?","2010-07-28 15:48:18",39,NULL
770,910,1,"You are estimating/calculating something (the exact term may be different). For example, consider a neural network. Are you not calculating the weights for the net when you are trying to predict something? In addition, when you say that you train to match output to reality, you seem to be implicitly talking about some sort of loss function.","2010-07-28 15:59:39",NULL,user28
771,779,0,"@Henrik Is there any meaningful difference between a single sample from each of N independent and *identically distributed* RVs and N independent measurements of a single RV?","2010-07-28 16:03:13",174,NULL
772,899,0,"@cgillespie: it is the same group, just with two modes, I guess, and therefore I probably cannot assume this.","2010-07-28 16:03:55",219,NULL
773,910,0,"@John, @Srikant Learners have parameters, but those are not the parameters in a statistical sense. Consider linear regression y=a*x (without free term for simp.). a is a parameter that statistical methods will fit, feed by the assumption that y=a*x. Machine learning will just try produce a*x when asked for x within the range of train (this makes sense, since it is not assuming y=a*x); it may fit hundreds of parameters to do this.","2010-07-28 16:07:24",88,NULL
774,909,0,"`optimize` requires two distributions to be side-by-side as I understand. In my case one is inside the other, i.e., the values from the second population are on both side of the limits.","2010-07-28 16:09:25",219,NULL
775,11,1,"Interpolation involves 3 things: 1) a class of functions to interpolate, e.g. sound, pictures, terrain; 2) grids of known / unknown points, 4 cases regular <-> scattered; and 3) a model of noise added to 1). There are many many interpolation methods for various cases, most ad hoc, not "in common use";
even IDW has variants. Can you describe what you're interpolating ?","2010-07-28 16:11:35",557,NULL
776,910,0,"@Srikant About the second issue; yes, it can be called a loss, but on unseen data, not the training one. When statistical models are used for prediction, it is assumed that the train is a perfect representation of reality and contains the full scope of the true process, which is not the case of machine learning.","2010-07-28 16:18:22",88,NULL
778,349,0,"Any intuition? yes. And your approach could work in general. However, in this case we can not have a lot of memory/computation. It is in a networking application where the device could see tens of thousands of items per second, and have VERY little processing left over for this purpose. Not the ideal/typical scenario, I know, but that is what makes it interesting!","2010-07-28 16:43:50",247,NULL
781,894,0,"That's right, thanks for spotting an error!","2010-07-28 17:35:42",1356,NULL
782,924,0,"By the way, is this a duplicate of what you posted here? http://stats.stackexchange.com/questions/920/test-if-probabilities-are-statistically-different","2010-07-28 17:42:49",NULL,user28
783,924,0,"I didn't think it was a duplicate. One question deals with probabilities and this one deals with a discrete variable.","2010-07-28 17:43:42",559,NULL
784,926,0,"When I saw your original answer, for a second, I lost faith in my intuition! :-)","2010-07-28 17:49:37",NULL,user28
785,926,0,"It was surprising to me also!","2010-07-28 17:52:05",287,NULL
787,926,0,"#JoFrhwld, would you happen to know the Matlab function to calculate this? I don't have access to R :(","2010-07-28 18:01:10",559,NULL
788,930,0,"Indeed we have suggested something like this in this topic; I'll dig for a link.","2010-07-28 18:05:47",88,NULL
789,930,0,"And I think your criterion for 'intervality' is valid only for a uniform distribution.","2010-07-28 18:10:53",88,NULL
790,926,0,"Sorry, I don't have access to Matlab! I'd just google around for it. It's surprising you wouldn't have access to R, considering it's free and platform independent.","2010-07-28 18:14:50",287,NULL
791,931,0,"And probably not easily put on an iPod, if that is essential to you.","2010-07-28 18:18:25",561,NULL
792,929,1,"I don't think it is a valid question here; just check out wiki and references there: http://en.wikipedia.org/wiki/Probability_interpretations","2010-07-28 18:20:32",88,NULL
793,929,0,"That wikipedia article is filled with "dubious-discuss" and other tags, so the question would become how comprehensive is that wiki article, what interpretations are missing from it.","2010-07-28 18:26:20",560,NULL
795,935,0,"Not comprehensive at all then. Thanks.","2010-07-28 18:41:44",560,NULL
796,928,0,"By the way, P70 - P50 represents the percentage of people who are between the 70th percentile and 50th percentile and that percentage is 20. Clearly that is the same as P50 -P30. When assessing if differences are equal I do not think you should look at the underlying scores.","2010-07-28 18:44:11",NULL,user28
797,904,0,"Fully agree ! the questions that are asked are different. Registration, landmarking, estimation of derivatives can arise from the functional view. This convince me ! so the big deal with functional data (as it stands in statistical literature) would not be that it is defined on a continuous set but more that it is indexed on an ordered set?","2010-07-28 18:44:58",223,NULL
798,929,1,"Still references are quite informative. And nevertheless even if it is discussive and mentions something that is not present in the book it is some kind of a clue, isn't it?","2010-07-28 18:45:07",88,NULL
799,926,0,"@MJoFrhwld, I need to use Matlab so I can incorporate it into my simulation framework.","2010-07-28 18:53:16",559,NULL
801,439,0,"I've accepted this answer on the somewhat capricious basis that I now remember Wasserman's book being recommended to me by someone else several years ago.  The same person also recommended "The Cartoon Guide to Statistics" by Gonick and Smith.","2010-07-28 19:13:50",89,NULL
802,290,0,"I am tempted to say "R for Stata users", but I would get voted down for this :)","2010-07-28 19:19:36",253,NULL
803,917,1,"I'd argue from a standpoint of trying to create good guess as to what video game a person likes coding 1s and 0s for like and didn't like isn't a good approach.  A scale of 1-5 or 1-7 is quite easy to elicit and will require fewer datapoints to generate a good model (because each data point provides more information).  With caveats about treating ordinal data as interval data of course applying, but probably not really that important in this context.","2010-07-28 19:23:52",196,NULL
804,936,1,"Econtalk is one of the most intelligent podcasts out there.","2010-07-28 19:36:06",319,NULL
805,917,0,"Agreed. I just answered in terms of the question, which was about logistic regression. I've fit proportional odds logistic regressions for ordered data like this in R using `MASS:polr`.","2010-07-28 19:42:06",287,NULL
806,918,0,"I was actually going to discuss this same thing. I had a job evaluating CEP/ESP tools, such as Aurora, STREAM, and Esper once...","2010-07-28 20:40:34",110,NULL
808,939,0,"Read carefully: http://en.wikipedia.org/wiki/Yates'_correction_for_continuity","2010-07-28 20:55:03",88,NULL
810,898,0,"Bayesian methods are theoretically well suited for real time analyses because the essential Bayesian method is to take a prior belief, and using a bit of data, compute a posterior belief -- which can become the prior for a new cycle.  While that part sounds good, there are practical difficulties and significant hurdles in specifying interesting models on real, changing data that are tractable and easily computable.","2010-07-28 21:35:46",87,NULL
811,944,6,"IMO total off-topic. Voted to close.","2010-07-28 21:58:10",88,NULL
812,948,0,"Will t.test() compare the means across all levels of a factor?","2010-07-28 22:59:28",569,NULL
813,949,0,"Till we have latex support, could you please avoid using latex. It is hard to make out what you are saying. See this meta thread: http://meta.stats.stackexchange.com/questions/5/what-typographic-support-is-available-to-support-display-of-statistical-formula","2010-07-28 23:03:58",NULL,user28
814,917,0,"Thanks guys! TONS O' INFO. :D So then what I'm doing now isn't exactly logistic regression and is instead proportional odds logistic regression?","2010-07-28 23:06:39",9426,NULL
815,682,0,"I ordered Statistics in Plain English... I'll accept your answer when I get it on Thursday if I dig it as much as I think I will.","2010-07-28 23:08:37",9426,NULL
817,918,0,"VFDT looks very interesting; thanks for the references!","2010-07-29 00:21:10",5,NULL
818,928,0,"And the same stands for calculating correlation coefficients, I guess?","2010-07-29 00:26:04",1356,NULL
819,928,0,"Yes, that would be correct. In fact correlation would be ratio as 0 means no correlation and such a conclusion is scale invariant. By the way, I suspect that percentiles would also be classified as ratio as the 0 point is scale invariant but it does not really matter. Most statistic applications require interval level measurements not necessarily ratio.","2010-07-29 00:53:56",NULL,user28
820,939,0,"@mbq I did read the Wikipedia article but I wanted to confirm and verify it","2010-07-29 00:55:08",559,NULL
821,944,3,"I agree. I think R programming question would be a better fit on StackOverlfow.com than here. http://stackoverflow.com/questions/tagged/r","2010-07-29 02:45:20",319,NULL
822,808,0,"Thanks. Had the opportunity to work with a great statistician for a while and was always amazed by how much information he could get out of the least amount of data by asking very pointed questions. This quotation so reminds me of hm","2010-07-29 03:30:12",482,NULL
823,917,0,"What you _want_ to do is a proportional odds logistic regression. I'm not exactly sure what to call what you _have_ done,","2010-07-29 03:47:12",287,NULL
824,952,0,"How are you defining redundancy?","2010-07-29 04:15:57",NULL,user28
825,957,0,"I am not sure I understand what you are saying. I am not sure there is any relationship between a new draw x and the current sample mean mean(S) as x ~ N(mu,sigma^2). Clearly, the draw of x can be anywhere in the support of the distribution. It is more likely to be around mu and less likely to be in the tails but it does not have anything to do with mean(S).","2010-07-29 04:20:42",NULL,user28
826,955,1,"Imo this question is offtopic on this site as it not goes about statistics itself.","2010-07-29 04:36:54",190,NULL
827,917,0,"Hahaha ok thanks man","2010-07-29 05:35:37",9426,NULL
828,603,0,"It does not; GLM is not machine learning. True machine learning methods are wise enough to hold their level of complexity independent of growing number of objects (if it is sufficient of course); even for linear models this whole theory works quite bad since the convergence is poor.","2010-07-29 07:43:01",88,NULL
829,707,0,"Have you even read this paper? Nevertheless it is works only for linear models (even the title shows it!) it is about asymptotic behavior for infinite number of objects. 100 is way not enough.","2010-07-29 07:49:16",88,NULL
830,707,1,"And I wish you luck making 10-fold cross validation on set with 9 objects.","2010-07-29 07:50:50",88,NULL
831,644,2,"CV using full set for model selection, huh? It's a common error (still even Wikipedia mentions it), because it is a hidden overfit. You need to make a higher level CV or leave some test to do this right.","2010-07-29 08:00:22",88,NULL
832,934,1,"I'd love to see this loss minimizing in clustering, kNN or random ferns...","2010-07-29 08:09:31",88,NULL
833,922,0,"Statistical crowd is clicking randomly in SPSS until desired p-value appears...","2010-07-29 08:17:05",88,NULL
834,962,0,"+1 Very nice paper.","2010-07-29 08:59:46",88,NULL
835,955,0,"Have you voted for close? It is strange that since without Shane we couldn't close any question...","2010-07-29 10:33:19",88,NULL
836,750,3,"That would be true if everyone were knowledgeable enough in statistics to drive the correct conclusions. Alas, that quote is very applicable to many of those amusing human beings called politicians...","2010-07-29 11:22:35",582,NULL
837,955,0,"I do think this could be rephrased in such a way that it would be on-topic (eg about the kind of work that statistical consultants do), but this is more like a job request.","2010-07-29 11:30:08",5,NULL
838,607,7,"Great post! Note that Vapnick had a PhD in statistics. I'm not sure there are a lot of computer scientist that know the name Talagrand and I'm sure 0.01% of them can state by memory one result of talagrand :) can you ? I don't know the work of Valiant :)","2010-07-29 11:30:57",223,NULL
840,955,0,"@mbq Sorry, I don't understand what you mean. Yes I voted to close. All 500+ rep users can (which means 8 users).","2010-07-29 11:45:13",190,NULL
841,955,0,"@Shane So you're here... I've got an impression that you should be holidating.
@Shane, @Peter Still, the problem exists, because not all of this eight is active in closing.","2010-07-29 11:53:06",88,NULL
843,955,0,"@mbq: I am vacationing, but occasionally my iPhone gets service.  And let's face it: I'm addicted to this.","2010-07-29 13:18:13",5,NULL
844,929,0,"See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.","2010-07-29 13:25:38",NULL,user28
845,966,0,"Your last line should have been with the log(P)'s, right? And isn't this also a simple consequence of Markov processes and the related exponential distribution?","2010-07-29 13:47:51",56,NULL
846,980,0,"I forgot this: The points along the x-axis come with varying spacing.","2010-07-29 14:06:54",NULL,Pete
847,980,0,"I am not sure I understand. Don't you have a y-axis?","2010-07-29 14:07:59",NULL,user28
848,966,0,"In the notation I borrowed:  x_t := log(X_t) to the p_t are the logs of the P_t.","2010-07-29 14:08:04",334,NULL
849,980,0,"Ah, sorry. I misstyped. I have now changed it above.","2010-07-29 14:11:05",NULL,Pete
850,947,0,"Thanks, this worked.","2010-07-29 14:12:49",NULL,anonymous
851,905,3,"Yes, and a slight clarification is that online learning algorithms, at least as studied in Machine Learning, mostly make the assumption that your ability to store examples is very limited compared to the size of the data set. In the most limiting case, you only get to see one example at a time, and then you have to forget it after you've used it to update your classifier.","2010-07-29 14:14:16",6,NULL
852,944,1,"@mbq: @JohnD.Cook: This is not a programming question. It is also not a question about a common tool of programmers. It is a question about a common tool of statisticians. That is why I asked it here. I don't mind that it was closed, though, since I received and accepted the correct answer.","2010-07-29 14:16:46",NULL,anonymous
853,981,0,"Ah, interesting, but I need it to be predictable, i.e. to have the same result each time I view the data.","2010-07-29 14:18:32",NULL,Pete
854,980,0,"I also think you need to provide a bit more information. For example, I still cannot visualize the graph. What is your goal?","2010-07-29 14:19:04",NULL,user28
855,981,0,"In that case, generate the *n* indexes of the points you choose, and store those indexes.","2010-07-29 14:21:07",8,NULL
856,981,0,"Or store the seed to the RNG before sampling.","2010-07-29 14:25:18",334,NULL
857,981,0,"Dirk's solution regarding the seed is probably the better option.","2010-07-29 14:31:10",8,NULL
858,983,0,"Is there any way to do this without knowing in advance what the relative varience will be or is it just a case of having to take a guess at the relative varience to do anything at all?","2010-07-29 14:38:55",210,NULL
860,980,0,"Ok, sorry. I have added some more details above.","2010-07-29 15:11:17",NULL,Pete
861,982,0,"Yep, visualization is what I want. I have added some more info in the question.","2010-07-29 15:11:55",NULL,Pete
862,984,0,"Ok, sorry. I have added some more details above.","2010-07-29 15:12:12",NULL,Pete
863,980,0,"Ok, just so I understand better- your x-axis is time with the 0 point being the time of the first sample. Your y-axis is beats per minute. Is that right? I still do not know your goal: Why do you want to reduce the number of data points? - Reduce clutter? or See patterns better? or the existing points are too close to each other for you to figure out what is happening?","2010-07-29 15:17:06",NULL,user28
864,911,0,"Thanks! But I hoped for a statistic that is faster and more stable to calculate...","2010-07-29 15:25:23",506,NULL
866,963,3,"You should be very careful in interpreting p-values from these t-tests with this looping approach. If you have 20 columns, and there is no real difference between any of them, at least one comparison is likely to appear significantly different at p < 0.05. You should at least multiply the p-values by the number of tests, to produce the Bonferronoi correction. Or, you could multiply them by their inverse rank for the Holm correction.","2010-07-29 15:36:54",287,NULL
867,982,0,"seconding plotting raw data with a smoothing line.","2010-07-29 15:43:10",287,NULL
869,685,0,"See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.","2010-07-29 15:48:02",NULL,user28
870,971,0,"Thank you, that's exactly what I needed. I'm not completely sure of what you mean when you talk about the overdispersion (sorry, I'm not a statistician, maybe it's something very basic)... You say that the residual deviance should be equal to the residual degrees of freedom... how would I check that?","2010-07-29 16:10:48",582,NULL
871,944,0,"It is a question about using a computer program use; so I think you should try SuperUser. Dirk is there, so R questions will be answered in a blink of eye ;-)","2010-07-29 16:13:58",88,NULL
872,981,0,"Calculating averages per each second is ok, but what I do when there is no data for a specific second. I guess I could do some interpolation from the seconds before and after it, but it would be great with some specific (named) method for this, so I don't try to invent something already invented.","2010-07-29 16:24:46",NULL,Pete
873,948,0,"I'm not sure what you mean. If you have some continuous variable column A (say people's heights) and some grouping factor column B (say Country of origin), you really want to do a `pairwise.t.test()`. You would do this like `pairwise.t.test(df$A , df$B)`. This function will automatically correct for multiple comparisons.","2010-07-29 16:28:03",287,NULL
874,997,0,"excellent logic, but wouldn't that be binom.test(3,13,0.5) (which is the same as binom.test(10,13,0.5)","2010-07-29 17:31:51",605,NULL
875,982,0,"thirding plotting raw data with a smoothing line --- You might want to also plot the change in BPM over time as a separate visualization.","2010-07-29 17:52:51",601,NULL
876,981,0,"The interpolation you are talking about is the moving average or smoothing bit.","2010-07-29 18:21:06",8,NULL
877,993,0,"You should really should state would language you sample code is for.","2010-07-29 18:33:14",8,NULL
878,730,1,"I use this quote a lot to explain the difficulties in mathematicians transitioning to statistics","2010-07-29 18:48:03",549,NULL
879,971,0,"If you give `summary(model1)` you'll see something like `Residual deviance: -2.7768e-28  on 0  degrees of freedom`","2010-07-29 18:55:47",339,NULL
882,138,3,"Refer to SO. http://stackoverflow.com/questions/192369/books-for-learning-the-r-language/2270793","2010-07-29 19:17:06",1356,NULL
884,981,0,"But moving average will not reduce the number of data points, will it? As far as I know it will _only_ smoothen. Or are there optional moving averages that does both?","2010-07-29 19:29:56",NULL,Pete
885,763,0,"this is great information.  Do you know of any papers that cover this?","2010-07-29 19:39:40",5,NULL
886,981,0,"I rewrote the answer. Hopefully it's clear and useful now.","2010-07-29 20:07:35",8,NULL
887,993,0,"@csgillespie: sorry; it's Matlab code, as Elpezmuerto requested in his comment to the other answer.","2010-07-29 20:44:37",506,NULL
888,899,1,"Do you know that members of the second group aren't included in the first group or are you just willing to mistakenly label those members as belonging to the first group?","2010-07-29 20:54:53",3807,NULL
890,926,1,"There is a chi-square test function in Matlab Central File Exchange: http://www.mathworks.com/matlabcentral/fileexchange/4779","2010-07-29 21:19:19",128,NULL
891,1004,0,"KS test always turns out to be non-normal with very large sets (I have dataset larger than 1 million data points). Also, I would like to have a quantifying measure that tells me about goodness of fit rather than just a test.

Am I asking for too much?

Thanks in advance,
A","2010-07-29 21:21:41",608,NULL
892,1004,0,"I agree with the question answerer.  What you are looking for is exactly a KS test.  The heightened ability of a KS test to detect violations of normality in large datasets is not a reason to toss it aside.  But, you may want to set different thresholds in terms of the D statistic depending on your sample size or get into the guts of the equation and see if you can remove the increased likelihood of rejecting the null as a function of sample size.","2010-07-29 21:50:42",196,NULL
893,1001,0,"Spearman's Correlation Coefficient is used to compare relative rank orders.  It strikes me that when comparing normal distributions in this way you are particularly unlikely to detect differences in kurtosis.","2010-07-29 21:53:37",196,NULL
894,981,0,"Thanks, very clear! Random sampling sounds like a great tool. But what can I do if the original data points are very unevenly spread out - I still want, for example, one final data point per 5 second interval, and that might not be present (maybe not even before the random sampling). That means I need to do some kind of interpolation.","2010-07-29 22:09:51",NULL,Pete
895,1001,0,"Not adding that this code is a total junk; you must normalize histograms somehow, cutting the end is not a good idea (still I have no idea how to do it). And make code a code (indent with 4 spaces).","2010-07-29 22:13:58",88,NULL
896,1004,0,"And as drknexus wrote, it is easy to extract the statistic and use it for comparison. Even the p-values will do.","2010-07-29 22:15:59",88,NULL
898,1004,0,"Is it actually valid to use p-values as a qualitative measure if they are not statistically significant (i.e. below .05)?  Wouldn't it be much better to have some kind of effect size?","2010-07-29 23:25:52",608,NULL
899,1012,1,"What is "not meaningful" about the standard deviation of a uniform distribution? It is a measure of spread for the uniform, just as it is for almost every other distribution. It may not be the best measure of spread, but it is certainly meaningful.","2010-07-29 23:32:38",159,NULL
900,965,0,"+1 from me--domain-independent answers are particularly useful here. It seems to me that in interdisciplinary domains, the "why" is often ignored because of the (understandable) emphasis on practical application.","2010-07-29 23:32:42",438,NULL
901,1023,3,"community wiki?","2010-07-29 23:55:06",NULL,user28
902,1004,4,"Why below 0.05, not 0.04973? Statistical significance does not have any in-depth meaning, it is just an accepted probability of analysis failure.  The operation of transforming statistic into a p-value is monotonic, so there is no problem with comparison. Still obtaining a significance level of this comparison is problematic (I have no better idea than bootstrap).","2010-07-30 00:20:42",88,NULL
903,1004,0,"Hi drknexus, you said "set different thresholds in terms of the D statistic depending on your sample size". How can this be achieved with kstest. I did not find a way to manipulate thresholds. I assume that with "getting into the guts of the equation" you mean transformations before testing. My distribution looks multi-modal and I have already an idea what these multi-modal tendencies might be i.e. how I can single them out...","2010-07-30 00:42:25",608,NULL
904,429,1,"Besides... Shapiro-Wilk's test is often used when estimating departures from normality in small samples. Great answer, John! Thanks.","2010-07-30 01:24:13",1356,NULL
905,871,0,"See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.","2010-07-30 02:00:21",NULL,user28
906,997,0,"Thanks Andreas. I fixed it. You are right of course, and that is what I entered into R. I am not sure how I copy and pasted something totally different into this answer, but it is fixed now.","2010-07-30 03:47:01",25,NULL
907,957,0,"But you don't and can't know mu or sigma. What you know are the mean and SD of the sample, mean(S) and stdev(S).","2010-07-30 03:51:27",25,NULL
908,1027,0,"Thanks for your thoughts. Maybe I wasn't clear though. I do have paired sets. They are the standardized quadratic scores of the test-retest results, ie some distributions were elicited twice. I will edit the question.","2010-07-30 06:42:57",108,NULL
909,971,0,thanks!,"2010-07-30 06:49:40",582,NULL
910,1012,0,"All I meant was that the standard deviation is not really a parameter that defines or describes the uniform distribution well.  In my mind, the standard deviation refers to the spread of a normal, or near normal distribution.  

Simply because a value is calculable does not mean that it is interesting or meaningful.  For example, I might be able to calculate what rate parameter from an exponential distribution best matches a normal distribution, but to me such a value would not be particularly meaningful because the distribution being described is not actually exponential.","2010-07-30 06:51:57",196,NULL
911,949,1,"Could you put $ signs around the LaTex? It now will be rendered.","2010-07-30 07:04:38",190,NULL
912,916,0,"It also seems as if $y$ is always larger than some linear function of $x$, $y>C x$.","2010-07-30 10:11:17",56,NULL
914,981,0,"@Pete - see answer","2010-07-30 10:45:12",8,NULL
915,916,0,"Possibly; still it is hard to tell without a zoom on this dense area (points are not transparent, so one cannot judge the density, and so this plot may be deceiving).","2010-07-30 10:48:57",88,NULL
917,445,0,"In referring to other questions, don't say "above" or "below" as the order can change depending on votes or whether the answer is accepted, for example.","2010-07-30 11:33:28",159,NULL
918,1012,5,"I can define a uniform distribution with mean 0.5 and standard deviation 1/12. It is perfectly well defined, but it is not the most natural parameterization. There is nothing about standard deviations that implies normality.","2010-07-30 11:36:04",159,NULL
921,1031,0,"Please add $ around your LaTeX. It should now be rendered ok. See http://meta.math.stackexchange.com/questions/2/tex-math-markup-is-sorely-needed","2010-07-30 13:10:35",159,NULL
922,21,0,"What kind of demographic data are you trying to forecast? Every data could (and should) be treated differently.","2010-07-30 13:43:17",614,NULL
923,1040,0,"You can latex on this site. Please enclose the tex with $ $. See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats","2010-07-30 14:29:26",NULL,user28
924,1027,0,"Could you clarify how you are computing standardized quadratic scores? And, what exactly your are correlating?","2010-07-30 14:44:55",NULL,user28
927,1027,0,"@Srikant: I updated the question with some background information","2010-07-30 17:16:03",108,NULL
928,1045,0,"Not an answer (because I don't know) but I just saw this new book on the Springer web site: "A Comparison of the Bayesian and Frequentist Approaches to Estimation" http://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4419-5940-9 that (one would assume) may have some answers.","2010-07-30 17:20:19",247,NULL
929,1019,0,"I don't understand... one set per person?","2010-07-30 17:40:15",88,NULL
930,1019,0,"@robin & @mbq I would suggest keeping it one dataset per post. This so people can indicate with votes which of the suggested ones there also suggest/support","2010-07-30 17:59:24",190,NULL
931,1049,0,"Welcome -- Nice to see you here, Pat!","2010-07-30 18:35:19",334,NULL
932,949,0,"cool, mostly works, although \\{ is not recognized and \\ldots looks strange","2010-07-30 18:43:31",511,NULL
933,949,0,"mostly worked here on firefox/Linux.  the subscript 1 on x1,...,xd didnt appear as a subscript but the others did.  The issues with formatting are minor, mostly the latex appears higher than other characters in the line.","2010-07-30 19:52:01",87,NULL
934,1051,0,"I understand what you wrote but I am not sure that answers my question. The method you describe and the methods in the linked question in my ps avoid estimating the nuisance parameter on the grounds that the nuisance parameter is not of interest. Such an approach is fine but then without estimating the nuisance parameter we cannot construct a confidence interval for this parameter. Thus, the sentence in the wiki makes sense only if we estimate the nuisance parameter using standard maximum likelihood.","2010-07-30 22:45:06",NULL,user28
935,1051,0,"So my question is: if we estimate the nuisance parameter using maximum likelihood how is its treatment any different than estimating any other parameter. By the way there seems to be a typo in your eqns as the second term f(z|-) should not depend on theta, right?","2010-07-30 22:47:15",NULL,user28
937,1051,0,"The equation looks right to me -- I hope I'm not having a blind moment staring at the screen here.  In each case, we have one component that depends on theta and the other (nuisance) component may or may not.  The crucial point is that we must find a transformation that isolates theta to *some* extent to obtain either a marginal or conditional.  When we can't, we must work with profile and other estimated likelihoods.  We could argue that ML is losing information, but that's another question entirely.  Still, perhaps this sheds light on the differences?","2010-07-30 23:33:50",251,NULL
939,934,0,"Well, for a loss function characterization of k-means nearest neighbor, see the relevant subsection (2.5) of this paper: http://www.hpl.hp.com/conferences/icml2003/papers/21.pdf","2010-07-30 23:41:56",39,NULL
940,628,0,"Note that the last definition here is an *Integrated* (or Bayesian) Likelihood, not a Marginal Likelihood.","2010-07-31 00:09:58",251,NULL
941,628,0,"Is this correct in the RHS for partial likelihood: "L2(θ|theta)"?","2010-07-31 00:13:14",461,NULL
942,628,0,"oops. Thanks for spotting the typo. Fixed it now.","2010-07-31 00:34:57",NULL,user28
943,728,0,"It has actually led to very interesting articles... :)","2010-07-31 01:07:04",253,NULL
944,1053,2,"community wiki please. Your question does not have an 'objective best' answer.","2010-07-31 02:04:28",NULL,user28
946,1055,1,"Correlation is not necessarily linear - Spearman's rho relies on the monotonic function, and yet, we refer to it as a "correlation coefficient", not "mutual information coefficient". And for a good reason: it provides an information about association between two variables. Mutual information, redundant information, mutual variance, correlation - these terms are so similar, and this question refers to _network reconstruction_, so I guess that we ended up in the wrong area with right terminology. This is quite specific question...","2010-07-31 02:26:27",1356,NULL
947,1055,0,"Good point. I've edited my answer to include monotonic relationships. I don't know anything about network reconstruction.","2010-07-31 03:04:58",159,NULL
948,1051,0,"I am not sure how far we should take this discussion as SE is not a good outlet for discussions. I agree with what you said but I am not sure you are addressing the issue I raised. If you use a nuisance free likelihood how can you construct a confidence interval for the nuisance parameter? In any case, I should probably stop here and I will let you have the last comment.","2010-07-31 04:27:08",NULL,user28
949,1015,0,"Is your your probability score identical to what is called the Brier score? (See: http://en.wikipedia.org/wiki/Brier_score)","2010-07-31 04:34:21",NULL,user28
951,1051,0,"Sorry, I wrote we don't estimate the nuisance, let me try again with some examples that address it.  Partial likelihood just discards information about the nuisance.  Profile replaces it with the MLE at fixed theta, but this doesn't account for uncertainty in lambda.  Contrast with the Bayesian and specification of a prior.  I think I get what you're saying that it's just estimation like any other parameter.  But the treatment matters because it's how you account for the uncertainty due to the nuisance which affects the intervals, whether through the posterior or the likelihood.","2010-07-31 04:54:33",251,NULL
952,894,0,"+1 I think this is the nicest/simplest example here.","2010-07-31 05:18:59",251,NULL
953,1019,0,"@Peter, OK, I follow your idea, I have changed the question accordingly.","2010-07-31 06:14:49",223,NULL
954,728,0,"@Tal: Fully agree! I think the whole area on optimal separation in minimax testing is starting from this idea ... and it is still so confused for a lot of statistician. For those interested see the paper of donoho http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492 (and the references in the paper ! since things are much older than donoho's paper)","2010-07-31 06:19:15",223,NULL
955,1015,0,"@Srikant: Yes it is. It is known by both names.","2010-07-31 06:49:42",108,NULL
956,659,0,"+1 from me--nice explanation.","2010-07-31 10:37:32",438,NULL
957,1061,0,"Aww, you've beat me. +1","2010-07-31 10:44:50",88,NULL
958,1061,0,"Thanks! Is it the same as the df=6 that LogLik returns? Could I have used -2*logLik(fit)+2*(attr(logLik(fit),"df"))  ??","2010-07-31 10:58:19",339,NULL
959,255,4,"Check out the outlier function in the randomForest package http://lib.stat.cmu.edu/R/CRAN/web/packages/randomForest/randomForest.pdf","2010-07-31 11:01:22",339,NULL
960,213,8,"5th D in color hue, 6th D in color intensity, 7th D in point size and we can go ;-)","2010-07-31 12:36:14",88,NULL
961,963,0,"You are right. I was feeling that something was wrong. They constitute a family of confidence intervals. Perhaps its even better to adjust using Tukey Honest Significant Differences (TukeyHSD in R)","2010-07-31 15:45:10",339,NULL
962,1062,0,"Orthogonal parameterization has a broad range of uses.  Please be more specific.  What are you applying it to?","2010-07-31 15:48:18",601,NULL
963,1061,0,"@user603 Also, how did you "draw" that indexed sigma?","2010-07-31 15:48:24",339,NULL
964,1065,0,"On 2) I believe the confusion is what p(X=.01) means when X is a continuous random variable.  Intuitively, the probability seem to be zero everywhere because there is no chance X is exactly .01. The questioner should review the definition of a density function in the continuous case, which is defined as the derivative of the cumulative density function.","2010-07-31 17:15:21",493,NULL
965,827,0,"@mbq +1 for snowfall- abstracts snow even further and makes parallel computing with R pretty simple.","2010-07-31 18:14:41",13,NULL
966,823,0,"It's unfortunate that consensus has so much control over what kind of science gets funded.","2010-07-31 18:26:15",13,NULL
967,827,0,"You could give it to my answer not Shane's... still thanks ;-)","2010-07-31 18:28:48",88,NULL
968,1066,0,"You mean that you are looking at the distribution of subsample **lengths** of subsegments covering each position, right? On the other hand (probably I'm wrong) I'm guessing it has something to do with the sequencing?","2010-07-31 18:40:22",88,NULL
969,1066,0,"No, I'm looking at the distribution of the **number of sub-segments** covering a specific position. For example, if we focus on position 1 in the large segment, in the first simulation we have 4 sub-segments covering it; in the second simulation we have 1 sub-segment covering it, etc.

I don't care what are the lengths of the sub-segments covering each position, just how many sub-segments there are.

You are not completely wrong, this has started as a part of an exercise I've been working on in a biology course, but has gone to another domain :)","2010-07-31 18:45:20",634,NULL
971,1066,0,"Still, one more doubt -- what is a distribution of the lengths of subsamples? I think it is crucial to the answer. (I'm suspecting it is a binomial distribution)","2010-07-31 18:58:00",88,NULL
972,1066,0,"the lengths of the sub-segments? A list of lengths is given. I believe the lengths distribute quite normally, but I did not check it.","2010-07-31 19:01:45",634,NULL
973,1066,0,"sounds like a poisson distribution...","2010-07-31 19:08:22",601,NULL
974,1068,0,"I think I got the idea, although I will have to think about it some more. One other thing I intentionally did not mention at the beginning, is the fact we might have predefined "hot" subranges along our long segment. This means that every subrange that is drawn and completely includes one of the "hot" subranges will not be counted (it will be totally elimnated, not drawn again).","2010-07-31 19:12:59",634,NULL
975,1068,0,"For example, if we have a predefined "hot" subrange 20..25 and we are now drawing a subrange of length  40, and we happen to draw 11..50, than we don't count anything (we throw this subrange). However, if we happen to draw 21..60 we count as normal (since our subrange does not include the complete hot subrange, only part of it).","2010-07-31 19:13:25",634,NULL
976,1068,0,"If you want the pmf, then it seems the only sensible thing to do is to actually construct it.  Some sort of counting approach probably works best, similar to what I suggested where you don't need to actually count everything.  It's not clear what your goal is though.  If you're interested in the pmf, then an exact pmf is obviously better than some ad hoc approximation.  If you're interested in some underlying parameter, the fact that you can easily simulate from your distribution opens other possibilities.","2010-07-31 19:25:14",493,NULL
977,1068,0,"OK, so what I really want is the following: I do not know if where the hotspots are (how many are there or how long is each hotspit), but I do have a few hypotheses (each hypothesis is a set of hotspots; each hotspot is just a subrange). These are hidden variables.

I also have one "true" mapping - where all the lengths were drawn and mapped to the long range. This is my data.

What I aim to do is to check which of my hypotheses is most likely given the data (the "true" mapping).","2010-07-31 19:36:44",634,NULL
978,1068,0,"(and sorry I can't vote-up yet, I'm still new here :))","2010-07-31 19:40:30",634,NULL
979,1068,0,"I don't exactly understand your data.  It sounds like you'll end up wanting to do something like EM or set up a full Bayesian model. If the probability of exactly matching your data using simulation is reasonable (above one in a million or something), you could just do rejection sampling to get the posterior probabilities of your hypotheses.  Unless your application is simple though rejection sampling may not be feasible since the match probability is too small.  In that case, having the pmf will help.","2010-07-31 19:57:37",493,NULL
980,893,2,"It would be nice to have an explanation for *why* degrees of freedom is important, rather than just what it is.  For instance, showing that the estimate of variance with 1/n is biased but using 1/(n-1) yields an unbiased estimator.","2010-07-31 20:12:29",493,NULL
981,1068,0,"What I thought of doing is: for each scenario, get the pmf for each position, then calculate the likelihood of the data to be produced given the scenario (multiple probability of each position, assuming positions are independent, although they actually are not). Then we have a likelihood for the data given each of the scenarios and we can choose the most likely scenario (from the set of given scenarios).

There is no chance I will fully match my data using simulations. There is too much noise and the segments are quite long.","2010-07-31 20:20:15",634,NULL
982,1070,0,"I like your explanation (+1)-- it is probably more to the point for Alekk.  I'll leave mine up in the hope that it casts a different, perhaps useful, light on the role of orthogonality.","2010-07-31 20:41:58",39,NULL
983,1068,0,"I've done some thinking. If I understand your suggestion correctly, I need to keep for each position in my long range m bernouli variables (one for each length, saying what is the probabilty the i-th subrange will cover this position).
Since I usually work with n=200k and m=20k, this means 4*10^9 variables, which is way too much. This could perhaps be "compressed" since many subsequent positions will have the exact same set of variables, but I can not think immediately of an easy way to do that. On the other hand, perhaps I can do all the calculations on the fly and not save those variables...","2010-07-31 20:44:17",634,NULL
985,1026,0,"Actually, to the best of my memory, Harrell strongly discourages the use of AIC.  I guess cross-validation would probably be the safest method around.","2010-08-01 01:54:35",253,NULL
986,1072,1,"Is there some way to quantify what is " increasingly prefered " these days ?","2010-08-01 01:55:20",253,NULL
987,1070,0,"Can you comment on how your definition of marginal likelihood relates to the one given in the wiki link? (See: http://en.wikipedia.org/wiki/Marginal_likelihood)","2010-08-01 01:57:42",NULL,user28
988,1026,1,"AIC is asymptotically equivalent to CV. See answers to http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other. I checked Harrell before I wrote that answer, and I didn't see any discouragement of the AIC. He does warn about significance testing after variable selection, with the AIC or any other method.","2010-08-01 02:54:39",159,NULL
989,282,4,"Effort is commendable -- BUT -- neither PC1 nor PC2 tells you who did best in all subjects.  To do so the PC subject coeffcients would all have to be positive.  PC1 has positive weights for Math and Music but negative for Science and English.  PC2 has positive weights for Math and English but negative for Science and Music. What the PCs tell you is where the largest variance in the dataset lies.  So by weighting the subjects by the coefficients in PC1, and using that to score the students, you get the biggest variance or spread in student behaviors. It can classify types but not performance.","2010-08-01 02:58:57",87,NULL
991,1073,1,"Nice explanation.  Doesn't explain people's cognitive failings, but +1 anyway.","2010-08-01 03:17:33",87,NULL
997,1051,0,"[moving this comment from the orthogonal question] You might find this paper by Berger et al. interesting: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009211804 -- marginal likelihood is defined on p.5 before they try to sell the integrated form to "likelihoodists". Also check out the historical references on that page, Cox 1975 and Basu 1977. The former is the origin of partial likelihood and the latter is a critique of Fisher likelihood w.r.t. nuisance parameters.","2010-08-01 04:34:25",251,NULL
998,1070,0,"All the cites on that page (Bos; MacKay/p.29) are to Bayesian concepts of likelihood (post ~ lik x prior). There's a connection, but it's not the marginal likelihood in the sense of Fisher and Likelihood theory. [see comment on Berger paper here: http://stats.stackexchange.com/questions/1045/is-there-a-radical-difference-in-how-bayesian-and-frequentist-approaches-treat-nu/1051#1051 ]","2010-08-01 04:37:58",251,NULL
999,1026,0,"@Tal: Perhaps from one of his papers rather than the RMS book, I remember Harrell objecting to the use of AIC for simply choosing among a pool of *many* models.  I think his point was that you must add a variable at a time and compare two models methodically or use some similar strategy.  (To be clear, this is in line with Rob's answer.)","2010-08-01 05:14:48",251,NULL
1001,282,0,"+1 good comment, cheers. You are of course correct, I should have have written that better and have now edited the offending line to make it clear I hope.","2010-08-01 09:29:48",81,NULL
1002,1081,0,"One remark -- expand nMDS. Is it just non-metric or something more exotic?","2010-08-01 09:35:29",88,NULL
1003,825,2,"I have no problem with this sort of Q&A here.  R isn't such a mainstream language (like Python or Java) that a quant would naturally say, "Oh this is a general programming question so I should go to StackOverflow or similar and ask this or look there for solutions".  Actually it is more a question for an R mailing list or group site.  To serve those budding analysts who want to learn R we should be glad to have an answer here as well.","2010-08-01 10:32:26",87,NULL
1006,226,1,"That's right - if the variance of PC is, say 3.5, then that PC "explains" variability of 3.5 variables from the initial set. Since PCs are additive, `PC1 > PC2 > ... > PCn`, and the sum of their variances is equal to the sum of the variances of the initial variable set, since PCA is computed upon covariance matrix, i.e. variables are standardised (SD = 1, VAR = 1).","2010-08-01 11:51:45",1356,NULL
1007,282,0,"You could standardise the vars, hence calculate the sum, in order to see who's the best, or if you prefer, in R: `apply(dtf, 1, function(x) sum(scale(x)))`","2010-08-01 12:08:34",1356,NULL
1008,1072,0,"I think that it is recognized to be scientifically more correct in many field in the sense that the shrinkage approach is used more in recent applied stat papers than the *.IC approach. That shows a certain -at least tacit- theoretical consensus.","2010-08-01 12:12:41",603,NULL
1011,1086,0,"moving average is exactly what I needed, thanks!","2010-08-01 14:40:31",642,NULL
1012,981,0,"Great method for dealing with uneaven data. I will try that. A huge thanks for all your help!","2010-08-01 14:50:13",NULL,Pete
1014,632,0,"What Srikant found (and what seems confirmed at PhysicsForums) there should be $\\sqrt{2}$, so rather $\\hat{\\sigma}\\frac{\\sqrt{2}}{2n}$.","2010-08-01 15:34:01",88,NULL
1015,632,0,"Aww, those comments locks; $\\frac{\\hat{\\sigma}}{\\sqrt{2n}}$. At least this one gives the result in agreement with bootstrap.","2010-08-01 15:58:24",88,NULL
1016,1069,0,"[Ack, deleted a previous comment when I thought it would edit, back to try again.]  This is interesting though I don't understand information geometry +1.  I'm intrigued by this question at quora, which is unfortunately unanswered: http://www.quora.com/What-does-it-mean-to-say-In-information-geometry-the-E-step-and-the-M-step-are-interpreted-as-projections-under-dual-affine-connections","2010-08-01 17:59:05",251,NULL
1017,1069,0,"I had wondered where that went!  If I get some time, I think I'll try to answer that question over at quora, though I should admit that I an information geometry novice.  Most of what I know of it is incidental to an an effort to pin down how the features of mathematical spaces (topological, algebraic, and otherwise) enable the application of learning methods.  This, in turn, is part of a quite difficult pet project of mine.","2010-08-01 18:51:44",39,NULL
1018,1069,0,"If you would like to correspond about any of these subjects feel free to email me at johnnylogic at gmail.","2010-08-01 19:09:46",39,NULL
1019,113,0,"The aforementioned paper addresses method selection, generally. As for specific examples and more details they may be found scattered among specific meta-methodological disciplines (measurement theory, algorithmic learning theory, statistical learning theory, complexity theory), but I have not found a systematic treatment, thus the question. If you wish to discuss these issues generally, you may email me at johnnylogic at gmail.","2010-08-01 19:11:47",39,NULL
1020,1040,0,"Thanks for the pointer on Latex","2010-08-01 20:12:49",168,NULL
1021,1095,0,"Reg Latex: See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats","2010-08-01 22:58:47",NULL,user28
1022,1094,0,"I am already modeling the probability of each cell as a Dirichlet, $p \\sim Dirichlet(\\alpha)$.  Normalizing by $n$ wouldn't quite work since a cell with large $p_i$ would have small likelihood whenever $n$ is fairly large, even when we indeed observed $x_i=1$ (unnormalized).  Does that make any sense?","2010-08-01 23:08:31",647,NULL
1023,1094,0,"I am afraid you lost me by the mention of a 'cell' when the question does not mention this word at all. Could you provide some more context so that I can understand better?","2010-08-01 23:13:24",NULL,user28
1024,1094,0,"Sorry: By cell I mean each $x_i$.","2010-08-01 23:14:44",647,NULL
1025,1093,0,"By the way, will the binomial not work for you? You have k trials and n successes. right?","2010-08-01 23:16:12",NULL,user28
1026,1094,0,"In other words, I want to sample $n$ integers between 1 and $k$ without replacement using the probability vector $p$.","2010-08-01 23:17:14",647,NULL
1027,1094,0,"This getting confusing to me. Your $X_i$ is either 0 or 1 but now you want to sample $n$ integers between 1 and $k$?","2010-08-01 23:23:32",NULL,user28
1028,1094,0,"Sorry: I'm saying that it's an equivalent formulation to sample the indices of $X$ without replacement.  Then $x_i=1$ for those integers that are sampled and $x_i=0$ for those integers that are not.","2010-08-01 23:39:34",647,NULL
1029,1094,0,"In that case, you have basically the following scenario: You flip a coin $k$ times and your constraint is that you need $n$ successes as that will ensure that $X_i$ sum to $n$. This is a binomial distribution. If this satisfies your requirements I will change my answer. If it does not can you explain why this will not work?","2010-08-02 00:04:19",NULL,user28
1030,219,0,"Would this be good for implementing on GPUs?  Link to details, references?","2010-08-02 00:27:47",646,NULL
1031,146,0,"Why did you put `r` tag and what do you mean by "why this is so"? PC's are not correlated, i.e. they're orthogonal, additive, you cannot predict one PC with the another. Are you looking for a formula?","2010-08-02 00:38:33",1356,NULL
1032,167,0,"In the set with n variables, n PCs can be extracted, but you can decide how many you'd like to keep, e.g. Guttman-Keiser criterion says: keep all PCs that have eigenvalue (variance) larger than 1. So there...","2010-08-02 00:43:29",1356,NULL
1033,1094,0,"Slightly different: I have $k$ different coins, each with a different probability (ie. $p_i$), and I have $n$ successes.  Sorry for the confusion!","2010-08-02 01:14:41",647,NULL
1035,1093,0,"Slightly different: I have $k$ different coins, each with a different probability (ie. $p_i$), and I have $n$ successes. Sorry for the confusion!","2010-08-02 01:43:26",647,NULL
1036,1094,0,"For the second proposal, what is the form of this posterior?  Another Dirichlet?  It makes sense that the Dirichlet would be conjugate here, just like if we were using a multinomial.  But we're not using a multinomial.  So what is the pdf of the distribution you propose?  (It's a multivariate bernoulli that has been conditioned on observing n successes, right?)","2010-08-02 02:07:56",647,NULL
1037,1094,0,"Unfortunately, it is not a dirichlet. The bernoulli can be written as: $f(X_i;p_i) = p_i^{X_i} (1-p_i)^{1-X_i}$ which is not conjugate to the dirichlet. You will have to use Metropolis-Hastings to estimate the parameters.","2010-08-02 02:14:54",NULL,user28
1038,1094,0,"Thanks for your help.","2010-08-02 02:19:15",647,NULL
1039,910,2,"[citation needed]. In other words, intriguing answer, although it doesn't jive (at least) with a lot of ML literature.","2010-08-02 05:03:16",30,NULL
1040,1081,0,"Indeed, its non-metric multidimensional scaling.","2010-08-02 06:09:19",144,NULL
1041,146,0,"I was wondering about the principles behind the logic (in my quest to understand PCA). I used R tag because R people might read this and maybe show R examples. :)","2010-08-02 06:13:21",144,NULL
1042,910,1,"Classical one is Breiman's "Statistical Modeling: The Two Cultures".","2010-08-02 06:42:56",88,NULL
1043,1025,0,"Interesting posting. I will look into that too. You are probably right.","2010-08-02 07:53:58",NULL,Pete
1045,146,0,"Oh, why didn't you say so? Have you seen http://www.statmethods.net/advstats/factor.html","2010-08-02 11:01:14",1356,NULL
1046,1113,0,"Thanks for your answer. That solves the problem of bounding.

For my data it goes to 1 very quickly for my data so I guess the next thing I need to do is to scale this information to concentrate on the interesting range which I could do based on the history of it without fear of leaving the bound, just hitting the limit.","2010-08-02 15:19:49",652,NULL
1047,1114,0,"erf is not a very handy function, provided you don't want to rather use it for its derivative.","2010-08-02 15:26:48",88,NULL
1048,1026,0,"Doing a quick search, I found Harrell writing the following "Beware of doing model selection on the basis of P-values, R-square, partial R-square, AIC, BIC, regression coefficients, or Mallows' Cp."  He wrote that on 12/14/08, on a mailing list titled [R] Obtaining p-values for coefficients from LRM function (package Design) - plaintext.  I guess I misunderstood his meaning.","2010-08-02 16:20:19",253,NULL
1049,1115,3,"Could you please make your question a little more understandable?","2010-08-02 16:38:41",88,NULL
1050,1118,0,"Thank you- mbq and stoplan - I computed mean in time1 and time2, and percent change. There are several obs for each idno, but not an equal  no. of obs for each idno in each time period.  There are 2 or 3 vars that cause change in dep. var, but the means are diff from time1 to time2. I am not clear on repeated measures - in SAS I used proc mixed w/repeated option but I am not clear on results!","2010-08-02 17:32:45",474,NULL
1051,1118,0,"There is a specific change in 1 indep. var, X1 in time2. What I want to do is show statistically what is causing change in dep var from time1 to time2.  How much is due to change in X1? Am I still unclear?","2010-08-02 17:37:02",474,NULL
1052,1118,0,"@Tailltu I think, ideally, editing the question itself to lend clarity is preferable. That way your question would show up in searches by other people who may have an issue similar to yours.","2010-08-02 17:58:52",NULL,user28
1053,1126,2,"Community wiki? I do not think there is a 'correct' answer for your questions.","2010-08-02 18:28:30",NULL,user28
1054,763,0,"(didn't get a notifier from SA of your comment) Well, i wasn't referring to any papers when i wrote that, rather just informally summing pieces of my experience relevant to your Question. I'll look through my files and see what i have that is relevant though.","2010-08-02 18:32:24",438,NULL
1055,1126,2,"Community wiki...","2010-08-02 19:18:14",5,NULL
1056,1128,0,"+1 Very good point; my number 1.","2010-08-02 19:29:21",88,NULL
1057,1126,1,"Fascinating question - thank you for it!","2010-08-02 20:27:48",253,NULL
1058,1115,0,"Or for example mention what stat package you are using ? (R, SAS, SPSS, etx ?)","2010-08-02 20:30:33",253,NULL
1059,1133,0,"I once asked the same question on the R mailing list, and didn't get a response.  I'd suggest you to change your title since your question is regarding "post hoc analysis of chi square - to detect the cause of the significance" (a shorter titles then the one I proposed would be better :) )","2010-08-02 20:33:00",253,NULL
1060,1144,0,"Thank you for your response, but what if the signal exhibits a high seasonality (i.e. a lot of network measurements are characterized by a daily and weekly pattern at the same time, for example night vs day or weekend vs working days)? An approach based on standard deviation will not work in that case.","2010-08-02 20:57:49",667,NULL
1061,1144,0,"For example, if I get a new sample every 10 minutes, and I'm doing an outlier detection of the network bandwidth usage of a company, basically at 6pm this measure will fall down (this is an expected an totaly normal pattern), and a standard deviation computed over a sliding window will fail (because it will trigger an alert for sure). At the same time, if the measure falls down at 4pm (deviating from the usual baseline), this is a real outlier.","2010-08-02 20:58:18",667,NULL
1062,1132,2,"I reckon loads of computer games are modelling problems dressed in disguise. SimCity for example - the goal of the game is to build as good a model as possible of the hidden game mechanics, then use that model to build a functioning city! (This is all probably a gross over-justification for wasting my youth playing SimCity)","2010-08-02 21:03:44",668,NULL
1065,1147,1,"Yeah, this is exactly what I am doing: until now I manually split the signal into periods, so that for each of them I can define a confidence interval within which the signal is supposed to be stationary, and therefore I can use standard methods such as standard deviation, ...
The real problem is that I can not decide the expected pattern for all the signals I have to analyze, and that's why I'm looking for something more intelligent.","2010-08-02 21:37:03",667,NULL
1066,1142,1,"Just for clarity, here's the original question on SO: http://stackoverflow.com/questions/3390458/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series","2010-08-02 21:42:06",71,NULL
1067,1142,1,"I think we should encourage posters to post links as part of the question if they have posted the same question at another SE site.","2010-08-02 21:47:31",NULL,user28
1068,886,2,"As, arguably, a machine learner, I'm here to tell you we maximise the heck out of likelihoods. All the time. Loads of machine learning papers start with "hey look at my likelihood, look how it factorises, watch me maximise". I'd suggest that it's dangerous to claim a fundamental basis of either discipline in terms of inference techniques. It's more about which conference you go to!","2010-08-02 21:47:52",668,NULL
1069,1142,0,"yes, you're completely right. Next time I'll mention that the message is crossposted.","2010-08-02 21:53:14",667,NULL
1070,1148,0,"Again, this works pretty well if the signal is supposed to have a seasonality like that, but if I use a completely different time series (i.e. the average TCP round trip time over time), this method will not work (since it would be better to handle that one with a simple global mean and standard deviation using a sliding window containing historical data).","2010-08-02 22:02:16",667,NULL
1071,1148,1,"Unless you are willing to implement a general time series model (which brings in its cons in terms of latency etc) I am pessimistic that you will find a general implementation which at the same time is simple enough to work for all sorts of time series.","2010-08-02 22:06:07",NULL,user28
1072,1148,0,"Another comment: I know a good answer might be "so you might estimate the periodicity of the signal, and decide the algorithm to use according to it", but I didn't find a real good solution to this other problem (I played a bit with spectral analysis using DFT and time analysis using the autocorrelation function, but my time series contain a lot of noise and such methods give some crazy results mosts of the time)","2010-08-02 22:06:34",667,NULL
1073,1148,0,"A comment to your last comment: that's why I'm looking for a more generic approach, but I need a kind of "black box" because I can't make any assumption about the analyzed signal, and therefore I can't create the "best parameter set for the learning algorithm".","2010-08-02 22:09:22",667,NULL
1074,1147,0,"Here is a one idea: Step 1: Implement and estimate a generic time series model on a one time basis based on historical data. This can be done offline. Step 2: Use the resulting model to detect outliers. Step 3: At some frequency (perhaps every month?), re-calibrate the time series model (this can be done offline) so that your step 2 detection of outliers does not go too much out of step with current traffic patterns. Would that work for your context?","2010-08-02 22:24:42",NULL,user28
1075,108,0,"I love that blog!","2010-08-02 22:31:03",582,NULL
1076,1147,0,"Yes, this might work. I was thinking about a similar approach (recomputing the baseline every week, which can be CPU intensive if you have hundreds of univariate time series to analyze).
BTW the real difficult question is "what is the best blackbox-style algorithm for modeling a completely generic signal, considering noise, trend estimation and seasonality?".
AFAIK, every approach in literature requires a really hard "parameter tuning" phase, and the only one automatic method I found is an ARIMA model by Hyndman (http://robjhyndman.com/software/forecast/). Am I missing something?","2010-08-02 22:38:45",667,NULL
1077,1147,0,"Please keep in mind I'm not too lazy for investigating these parameters, the point is that these values need to be set according to the expected pattern of the signal, and in my scenario I can't make any assumption.","2010-08-02 22:40:16",667,NULL
1078,1147,0,"ARIMA models are classic time series models that can be used to fit time series data. I would encourage you to explore the application of ARIMA models. You could wait for Rob to be online and perhaps he will chime in with some ideas.","2010-08-02 22:44:41",NULL,user28
1079,1026,0,"@Tal. Thanks for finding that. While it sounds like a general statement, I *think* he probably meant "Beware of looking at p-values after doing model selection on the basis of ...." I don't think it makes sense otherwise. Certainly the discussion in that thread was all about p-values","2010-08-02 23:12:30",159,NULL
1080,1128,1,"+1 Ha. Speaking of overlap, this may be the only answer with none over Epstein's reasons.","2010-08-02 23:49:07",251,NULL
1081,1069,0,"Cool, thanks. :)","2010-08-02 23:49:50",251,NULL
1082,1098,0,"Interesting, thanks.  I wonder though if it's not the Fisher that's more appropriate here -- since in the urn formulation of the problem, N is observed after the experiment, and you condition on the sum.  Quite likely, I'm one of the "confused" the wikipedia article refers to. :-)","2010-08-02 23:52:10",251,NULL
1083,795,0,"Do you mean normalize to (0,1)? Please explain clearly what you are asking.","2010-08-03 01:08:43",159,NULL
1084,1126,0,"I think I like the overall classification given by the answers here more than Epstein's.","2010-08-03 01:44:08",251,NULL
1086,1149,1,"Really great question.  The best way to understand something is from multiple direction of explanation.","2010-08-03 02:29:31",253,NULL
1087,1026,2,"@Tal, @Rob: In that thread, he does say "Be sure to use the hierarchy principle".  Perhaps of interest, this discussion from medstats (scroll down for Harrell's response): http://groups.google.com/group/medstats/browse_thread/thread/86c44163b849572","2010-08-03 02:38:20",251,NULL
1088,1153,0,"+1 from me, excellent. So > 1.5 X inter-quartile range is the consensus definition of an outlier for time-dependent series? That would be nice to have a scale-independent reference.","2010-08-03 03:06:39",438,NULL
1089,1153,0,"The outlier test is on the residuals, so hopefully the time-dependence is small. I don't know about a consensus, but boxplots are often used for outlier detection and seem to work reasonably well. There are better methods if someone wanted to make the function a little fancier.","2010-08-03 03:45:06",159,NULL
1090,538,0,"I have some trouble understanding why arrow directions in a corresponding Bayesian network have any relation to causation. For instance, A->B and B->A represent different directions for causality, but Bayesian networks for those two structures are equivalent","2010-08-03 06:41:21",511,NULL
1091,1160,0,"Probably better suited to stackoverflow since it has no particular data analysis relevance.","2010-08-03 08:28:02",5,NULL
1092,795,0,"This question is proposed to be closed. See this meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed","2010-08-03 08:58:14",NULL,user28
1093,1160,0,"This question is proposed to be closed. See this meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed","2010-08-03 08:58:48",NULL,user28
1102,1171,0,"Ah, yes, I knew that overlapping CI's didn't necessitate zero difference, but hadn't connected that to this example, where clearly the above procedure would always compute difference "CI" that includes zero whenever the individual CIs overlap. Thanks!","2010-08-03 12:39:56",364,NULL
1103,1061,1,"@gd047. LaTeX code: `$\\sigma_{\\epsilon}$`","2010-08-03 13:23:24",159,NULL
1104,353,0,"I think that you're right, Rich.  I can't think of any other reasons.  Thanks!","2010-08-03 13:36:15",187,NULL
1105,1170,2,""sometimes" is an understatement... the authors logic isn't often this direct but the stimulus / reward scenario is such that people will do this as a matter of conditioning","2010-08-03 13:52:52",601,NULL
1106,1175,0,"The data isn't necessary rates. It could be anything.","2010-08-03 14:07:10",8,NULL
1107,1171,2,"A useful fact given in these references is that non-overlapping 84% confidence intervals *do* approximate a 95% level test.","2010-08-03 14:08:29",279,NULL
1108,1176,0,"Is it available as a video? It sounds great.","2010-08-03 14:12:15",442,NULL
1109,1176,0,"I think the word is "will be eventually" -- keynotes got recorded.","2010-08-03 14:18:07",334,NULL
1110,1173,3,"Helping your field come to a consensus on just the se v. sd question would be a huge advance.  They mean completely different things.","2010-08-03 14:41:26",601,NULL
1111,1170,0,"I don't researchers are being dishonest so much as acting out of ignorance.  They don't understand what statistics mean or what assumptions they require, but as you said they clearly understand the stimulus/reward: p > 0.05 => no publication.","2010-08-03 14:44:25",319,NULL
1112,1175,0,"Subscription link, unfortunately.","2010-08-03 14:57:26",71,NULL
1113,1176,0,"this is easy in ggplot I think, i.e. http://had.co.nz/ggplot2/geom_jitter.html","2010-08-03 14:58:29",668,NULL
1115,1175,0,"... but here's the Wikipedia link on funnel plots: http://en.wikipedia.org/wiki/Funnel_plot","2010-08-03 14:59:02",71,NULL
1116,1180,2,"And so if the mean is the same as the variance, could you conclude that the data was Poisson? Hardly!","2010-08-03 14:59:46",247,NULL
1118,1182,0,"I should add that my "plot only the data and uncertainty" recommendation should be qualified: when presenting data to an audience that has experience/expertise with the variable being plotted, plot only the data and uncertainty. When presenting data to a naieve audience and when zero is a meaningful data point, I'd first show the data extending to zero so that the audience can get oriented to the scale, then zoom in to show just the data and uncertainty.","2010-08-03 15:10:49",364,NULL
1119,1176,0,"`jitter` is also in plain R.","2010-08-03 15:14:10",88,NULL
1120,1176,0,"Well, I didn't mention a need to jitter. Frank's example had relatively few points.  I simply mentions alpha blending because it may make mixing the dots and the bars easier.","2010-08-03 15:18:15",334,NULL
1121,1173,0,"I agree - se is usually chosen because it gives a smaller region!","2010-08-03 15:20:39",8,NULL
1122,1182,0,"since you've went to trouble of writing R code, could you include a jpeg image of the final plot. I find just uploading the image to http://img84.imageshack.us/ and linking to it is fairly easy. Oh thanks for the answer :)","2010-08-03 15:26:43",8,NULL
1123,1179,0,"I've added a small section on why I dislike these plots.","2010-08-03 15:34:58",8,NULL
1124,1121,0,"This can all be implemented in Max/MSP/Jitter with the [peak] and [trough] objects for the first example and with [jit.3m] for the second example.","2010-08-03 15:40:01",162,NULL
1126,1182,0,"@csgillespie: done.","2010-08-03 15:46:09",364,NULL
1127,1153,0,"Really thank you for your help, I really appreciate. I'm quite busy at work now, but I'm going to test an approach like yours as soon as possible, and I will come back with my final considerations about this issue. One only thought: in your function, from what I see, I have to manually specify the frequency of the time series (when constructing it), and the seasonality component is considered only when the frequency is greater than 1. Is there a robust way to deal with this automatically?","2010-08-03 15:59:18",667,NULL
1128,1180,0,"True. Necessary but not sufficient.","2010-08-03 16:44:18",319,NULL
1129,1173,0,"Maybe some more informative title?","2010-08-03 18:36:41",88,NULL
1130,1182,0,"I've found that it's easier to read a plot like this with `geom_ribbon()` indicating the error. If you don't like producing apparent estimates for regions between 1 and 2, at least reduce the width of the error bar.","2010-08-03 19:20:49",287,NULL
1131,752,1,"I don't get what is so deep in that one, is it only playing with words ?","2010-08-03 19:29:45",223,NULL
1132,1187,0,"Thank you, I will check these out.","2010-08-03 19:40:35",573,NULL
1133,595,0,"Agreed; wavelets are excellent for picking out non-stationary behavior in high amounts of noise.  You do have to be careful with the DWT, though.  It's not rotation-invariant (although there are modifications of the DWT that are, see e.g. Percival and Walden 2000), so you can lose sharp transients depending on the starting point for your data.  Also, most implementations of the DWT do implicit circularization of the data, so you'd still need to control for that.","2010-08-03 19:57:01",61,NULL
1134,1193,0,"Edited to disambiguate 'above.'","2010-08-03 20:10:46",455,NULL
1135,1194,2,"This question is proposed to be closed. See: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed I see that it has 2 votes. Could the up-voters or the OP comment on why they would like to see the question stay open at the meta thread?","2010-08-03 20:33:45",NULL,user28
1136,1194,3,"Rather than saying "this should be closed. Someone should defend it" how about starting with explaining why you want it closed. Too vague? Then ask for clarification. This seems a reasonable question to me. The asker presents a paper and asks about the difference is between predictive and explanatory statistics. The only change I would make to the question is to clarify exactly the question thus making it easier to vote.","2010-08-03 20:39:24",29,NULL
1137,1194,2,"I have already offered a reason on the meta thread. I feel that 'meta discussions' about the question would clutter up this particular page.","2010-08-03 20:41:09",NULL,user28
1138,1160,1,"I wish we could move a question to Stack Overflow the way you can move a question from Super User...","2010-08-03 20:42:08",29,NULL
1139,1194,1,"@Srikant @JD I'll beef up the question. Thanks for the feedback. I do think that this is a topic that merits discussion.","2010-08-03 20:44:41",11,NULL
1140,1194,1,"Your question would good for the community if, instead of telling us your life,  you could define what is (according to you) predictive model and explanatory model. I think nice debates start with clear definitions...","2010-08-03 20:49:23",223,NULL
1141,1194,1,"Could you pose a question here?  Is the question whether the paper is right?","2010-08-03 20:52:48",5,NULL
1142,595,0,"If my memory is good, package wavethresh contains translation invariant denoising (my reference was Coifman 1995) (Note that you talked about rotation, arn't we talking about temporal signals?).","2010-08-03 20:53:23",223,NULL
1143,1194,0,"@srikant you seem to not understand the nature of comments under the questions. They are, by definition, meta. They are not answers. They are not questions. They are meta. Having a convention where comments become a pointer to meta conversations in some other location is wasteful and silly.","2010-08-03 20:59:14",29,NULL
1144,1194,0,"I've made the question more specific","2010-08-03 21:18:07",11,NULL
1145,1194,1,"@JD Perhaps. But, in order to get some control on the process of closing questions there is a meta thread devoted to this issue. If I do not make a mention of the fact that this question is proposed to be closed then the community will not get a chance to have a say whether it should stay open or not. One other issue is consider a person who stumbles on this question in the distant future. All this discussion about whether and why we should keep the question open is a bit irrelevant. I feel that comments should be used to clarify the question not to debate its merits.","2010-08-03 21:19:05",NULL,user28
1146,1194,0,"@srikant. That's articulately stated and clear. This discussion probably belongs in the Meta area, however, as it is not specific to the question above. :) yeah, ok, that was a bit of a jackass comment... I could not resist it!   You make a good point. I think we can agree that @wahalulu needed to clarify his question. I think he's moving in the right direction.","2010-08-03 21:46:16",29,NULL
1147,1194,4,"Nevertheless, it should be community wiki.","2010-08-03 22:17:23",88,NULL
1148,1031,0,"Since I am not a mathematician nor a statistician, I would like to restate what you were saying to make sure I did not mis-understand. 

So, you are saying that taking ds^2 (twice the KL) would have a similar meaning as R^2 (in a regression model) for a general distribution. And that this could actually be used to quantify  distances geometrically? Does ds^2 have a name so I can do more reading about this. Is there a paper that directly describes this metric and shows applications and examples?","2010-08-03 22:22:05",608,NULL
1149,608,1,"Wow, this is something that will be even harder do obtain I'm afraid; my general point in the whole discussion is that the convergence of those theorems is too weak so the difference may emerge from random fluctuations. (And that it is not working for machine learning, but I hope this is obvious.)","2010-08-03 23:19:47",88,NULL
1150,1153,0,"Yes, I have assumed the frequency is known and specified. There are methods to estimate the frequency automatically, but that would complicate the function considerably. If you need to estimate the frequency, try asking a separate question about it -- and I'll probably provide an answer! But it needs more space than I have available in a comment.","2010-08-03 23:40:47",159,NULL
1151,608,0,"That is interesting.  If the convergence is so weak I wonder why people even brought it up in the other question about AIC/BIC, or how they figured out that the cross validation and IC methods were actually convergent; I'm not really involved in machine learning, so the last bit that you hope is obvious isn't obvious to me, could you expand on that point?  Besides, the entire thing may be moot, as I say below - a correlational approach to the problem doesn't look like it will work (at least not while sample sizes are constant).","2010-08-04 00:05:11",196,NULL
1152,1153,0,"Thank you, I'll post a different question.","2010-08-04 00:21:03",667,NULL
1153,109,0,"I've been watching this question for a while waiting for clarification from the question asker, but none has been forthcoming.  I've nominated this question to be closed: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed","2010-08-04 00:38:26",196,NULL
1154,1185,1,"But this raises an interesting pedagogical problem, at least in Psychology, because as far as I know most introductory statistics books being used in my field do not really discuss robust measures except as an aside.","2010-08-04 00:43:19",196,NULL
1155,1205,0,"What is N? What is the nature of the weighting scheme? Are the weights known?","2010-08-04 01:25:01",NULL,user28
1156,1182,0,"@JoFrwld: I like ribbons too, though I tend to reserve them for cases where the x-axis variable it truly numeric; my version of the "don't draw lines unless the x-axis variable is numeric" rule that I profess violating in my answer above :Op","2010-08-04 01:40:54",364,NULL
1157,1206,1,"Why/how would an important explanatory variable reduce predictive accuracy?","2010-08-04 02:34:10",NULL,user28
1158,1210,0,"Interesting.  Did you ever come across an R implementation of this ?","2010-08-04 03:16:52",253,NULL
1159,1206,1,"@Srikant. This can happen when the explanatory variable has a weak but significant relationship with the response variable. Then the coefficient can be statistically significant but hard to estimate. Consequently, the MSE of predictions can increase when the variable is included compared to when it is omitted. (The bias is reduced with its inclusion but the variance is increased.)","2010-08-04 05:15:22",159,NULL
1160,1181,0,"+1 thanks for the thorough example!","2010-08-04 06:49:26",634,NULL
1161,1178,0,"+1 thank you. often I get some "weired" results, for example, a normal distribution gets a higher p-value then a poisson one, where lambda is relatively small (so by looks only the normal and poisson are not similiar at all)","2010-08-04 06:51:10",634,NULL
1162,1212,0,"+1 thanks for the suggestions!","2010-08-04 07:04:35",634,NULL
1163,562,0,"i added what i think is a pretty good source (unfortunately a Textbook) w/ annotation, in light of your comment/question below my answer. I edited my original answer, so it appears at the end.","2010-08-04 07:07:48",438,NULL
1164,1216,0,"ok, let's size I need to calculate for size 45. Every delivery is about once every 3 months.
I'm using excel here. =POISSON(4;98;FALSE). Ressult: 0... Could you give me some more clues?","2010-08-04 07:16:39",698,NULL
1165,1216,0,"If the deliveries are every 3 months, and the data in the question are annual, then the average sales between deliveries is 95/4. So you want y such that 1-POISSON(y,95/4,TRUE) is smaller than 0.05. Choosing y=32 will give the out-of-stock probability of 4.1%.","2010-08-04 08:12:41",159,NULL
1166,752,0,"I like to think of it as the statisticians equivalent of "guns don't kill people, people kill people" not very deep, but important to realise from time to time","2010-08-04 09:17:53",127,NULL
1167,1222,0,"Good point, still the main differences are somewhere else;  first, statistics is about fitting a model to the data one has, ML is about fitting a model to data one will have; second, statistics ASSUME that a process one observes is fully driven by some embarassingly trivial "hidden" model that they want to excavate, while ML TRIES to make some complex enough to be problem-independent model behave like reality.","2010-08-04 09:28:38",88,NULL
1168,934,0,"@John Still, this is mixing aims with reasons. To great extent you can explain each algorithm in terms of minimizing something and call this something "loss". kNN wasn't invented in such a way: Guys, I have thought of loss like this, let's optimize it and see what will happen!; rather Guys, let's say that decision is more less continuous over the feature space, then if we would have a good similarity measure... and so on.","2010-08-04 09:38:57",88,NULL
1169,1176,0,"@Dirk I was commenting Mike's comment about ggplot. As a general comment I also used to like these seas of dots for the sake that nothing will better represent the distribution -- until we've made figures that were crushing Acrobat Reader ;-) .","2010-08-04 09:54:30",88,NULL
1170,1222,0,"@mbq. That's a rather harsh caricature of statistics. I've worked in five university statistics departments and I don't think I've met anybody who would think of statistics like that.","2010-08-04 09:59:17",159,NULL
1172,1223,1,"I don't think this is a duplicate because of the nature of the data. The problem discussed on the other question concerned regularly observed time series with occasional outliers (at least that's how I interpreted it). The nature of tick-by-tick data would lead to different solutions due to the exchange opening effect.","2010-08-04 10:09:10",159,NULL
1173,1223,0,"possible duplicate of [Simple algorithm for online outlier detection of a generic time series](http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series) This question is proposed to be closed as a duplicate. Could you please let us know at the meta thread if and how your context is different from the question I linked?","2010-08-04 10:10:59",NULL,user28
1174,1223,0,"@Rob But the exchange opening effect only determines when you have to run the algorithm. The fundamental issue remains the same. Even in network data you have the 'office opening effect' where traffic peaks as soon as an office opens. At the very least, the OP should link to that question, scan the answers there and explain why the solutions there do not work so that a suitable answer can be posted for this question.","2010-08-04 10:12:49",NULL,user28
1176,1224,1,"I think we need a bit more information. What are you comparing? What sort of data do you have? Is it a single outcome you a measuring per centre?","2010-08-04 10:41:18",8,NULL
1177,1223,1,"I agree with @Rob. This kind of data can pose unique challanges, so this is not a duplicate.","2010-08-04 10:46:15",5,NULL
1178,1222,0,"@Rob Caricature? I think this is what makes statistics beautiful! You assume all those gaussians and linearities and it just works -- and there is a reason for it which is called Taylor expansion. World is hell of a complex, but in linear approx. (which is often ninety-something% of complexity) embarrassingly trivial. ML (and nonparametric statistics) comes in in these few per cent of situations where some more subtle approach is needed. This is just no free lunch -- if you want theorems, you need assumptions; if you don't want assumptions, you need approximate methods.","2010-08-04 10:54:35",88,NULL
1179,1223,0,"This question might eventually get served better here due to its domain-specificity: http://area51.stackexchange.com/proposals/117/quantitative-finance","2010-08-04 11:09:19",5,NULL
1180,1222,0,"@mbq. Fair enough. I must have misinterpreted your comment.","2010-08-04 11:21:07",159,NULL
1181,1205,0,"see also this related question: http://stats.stackexchange.com/questions/856/fishers-exact-test-with-weights","2010-08-04 11:22:20",442,NULL
1182,1126,0,"I don't want to pick just one right answer, so I've made this a community wiki page, based on two suggestions above. Thanks for all the ideas!","2010-08-04 11:41:05",660,NULL
