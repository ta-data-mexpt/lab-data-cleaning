Id,PostHistoryTypeId,PostId,RevisionGUID,CreationDate,UserId,Text,Comment,UserDisplayName
1,2,1,e58bf7fd-e60f-4c58-a6e4-dfc91cf98a69,"2010-07-19 19:12:12",8,"How should I elicit prior distributions from experts when fitting a Bayesian model?",,
2,1,1,e58bf7fd-e60f-4c58-a6e4-dfc91cf98a69,"2010-07-19 19:12:12",8,"Eliciting priors from experts",,
3,3,1,e58bf7fd-e60f-4c58-a6e4-dfc91cf98a69,"2010-07-19 19:12:12",8,<bayesian><prior><elicitation>,,
4,2,2,18bf9150-f1cb-432d-b7b7-26d2f8e33581,"2010-07-19 19:12:57",24,"In many different statistical methods there is an "assumption of normality".  What is "normality" and how do I know if there is normality?",,
5,1,2,18bf9150-f1cb-432d-b7b7-26d2f8e33581,"2010-07-19 19:12:57",24,"What is normality?",,
6,3,2,18bf9150-f1cb-432d-b7b7-26d2f8e33581,"2010-07-19 19:12:57",24,<normality>,,
7,2,3,6320bb0f-c792-4a8c-8083-89507c28d375,"2010-07-19 19:13:28",18,"What are the most valuable Statistical Analysis open source projects available right now?",,
8,1,3,6320bb0f-c792-4a8c-8083-89507c28d375,"2010-07-19 19:13:28",18,"What are the most valuable Statistical Analysis open source projects available?",,
9,3,3,6320bb0f-c792-4a8c-8083-89507c28d375,"2010-07-19 19:13:28",18,<open-source>,,
10,16,3,6320bb0f-c792-4a8c-8083-89507c28d375,"2010-07-19 19:13:28",18,,,
11,2,4,ccbe017d-d22c-4c0a-83b0-1d82a792a098,"2010-07-19 19:13:31",23,"I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  \\n\\nWhat tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?",,
12,1,4,ccbe017d-d22c-4c0a-83b0-1d82a792a098,"2010-07-19 19:13:31",23,"Assessing the significance of differences in distributions",,
13,3,4,ccbe017d-d22c-4c0a-83b0-1d82a792a098,"2010-07-19 19:13:31",23,<distributions><statistical-significance>,,
14,2,5,5631dbb1-2781-4924-b7bd-5a67c06c5e4e,"2010-07-19 19:14:43",23,"The R-project\\n\\nhttp://www.r-project.org/",,
15,16,5,5631dbb1-2781-4924-b7bd-5a67c06c5e4e,"2010-07-19 19:14:43",-1,,,
16,2,6,18914b59-1d73-4c14-a8b6-25d429a1888e,"2010-07-19 19:14:44",5,"Last year, I read a blog post from [Bendan O'Connor][1] entitled ["Statistics vs. Machine Learning, fight!"][2] that discussed some of the differences between the two fields.  [Andrew Gelman responded to favorably to this][3]:\\n\\nSimon Blomberg: \\n> From R's fortunes\\n> package: To paraphrase provocatively,\\n> 'machine learning is statistics minus\\n> any checking of models and\\n> assumptions'.\\n> -- Brian D. Ripley (about the difference between machine learning\\n> and statistics) useR! 2004, Vienna\\n> (May 2004) :-) Season's Greetings!\\n\\nAndrew Gelman:\\n\\n> In that case, maybe we should get rid\\n> of checking of models and assumptions\\n> more often. Then maybe we'd be able to\\n> solve some of the problems that the\\n> machine learning people can solve but\\n> we can't!\\n\\nThere was also the [**"Statistical Modeling: The Two Cultures"** paper][4] by Leo Breiman in 2001 which argued that Statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the *predictive accuracy* of models.\\n\\nHas the Statistics field changed over the last decade in response to these critiques?  Do the *two cultures* still exist or has Statistics grown to embrace machine learning techniques such as neural networks and support vector machines?\\n\\n\\n  [1]: http://anyall.org/\\n  [2]: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\\n  [3]: http://www.stat.columbia.edu/~cook/movabletype/archives/2008/12/machine-learnin.html\\n  [4]: http://www.stat.osu.edu/~bli/dmsl/papers/Breiman.pdf",,
17,1,6,18914b59-1d73-4c14-a8b6-25d429a1888e,"2010-07-19 19:14:44",5,"The Two Cultures: statistics vs. machine learning?",,
18,3,6,18914b59-1d73-4c14-a8b6-25d429a1888e,"2010-07-19 19:14:44",5,<statistics><machine-learning>,,
19,2,7,ad3deff5-e4fd-446f-abce-77984eb5eeeb,"2010-07-19 19:15:59",38,"I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics.  While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data.\\n\\nWhat I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source.  Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means.  Where would I go to find such information?",,
20,1,7,ad3deff5-e4fd-446f-abce-77984eb5eeeb,"2010-07-19 19:15:59",38,"Locating freely available data samples",,
21,3,7,ad3deff5-e4fd-446f-abce-77984eb5eeeb,"2010-07-19 19:15:59",38,<dataset><sample><population>,,
22,2,8,8b903ea3-538d-4cbf-be6e-9aefc1ad0019,"2010-07-19 19:16:21",37,"Sorry, but the emptyness was a bit overwhelming. And this has been stuck in my head since it got asked at Area51!",,
23,1,8,8b903ea3-538d-4cbf-be6e-9aefc1ad0019,"2010-07-19 19:16:21",37,"So how many staticians *does* it take to screw in a lightbulb?",,
24,3,8,8b903ea3-538d-4cbf-be6e-9aefc1ad0019,"2010-07-19 19:16:21",37,<lightbulb><staticians>,,
25,2,9,8baa8e0d-94e3-4315-884c-de31731f8e03,"2010-07-19 19:16:27",50,"[Incanter][1] is a Clojure-based, R-like platform (environment + libraries) for statistical computing and graphics. \\n\\n\\n  [1]: http://incanter.org/",,
26,16,9,8baa8e0d-94e3-4315-884c-de31731f8e03,"2010-07-19 19:16:27",-1,,,
27,2,10,350cf958-2957-43bd-86f8-909a6d2b19cd,"2010-07-19 19:17:47",24,"Many studies in the social sciences use Likert scales?  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?",,
28,1,10,350cf958-2957-43bd-86f8-909a6d2b19cd,"2010-07-19 19:17:47",24,"Under what conditions should Likert scales be used as ordinal or interval data?",,
29,3,10,350cf958-2957-43bd-86f8-909a6d2b19cd,"2010-07-19 19:17:47",24,<measurement><scales>,,
30,2,11,c77bd587-f3b4-406d-9a84-24a37851dc85,"2010-07-19 19:18:30",34,"\\nIs there a good, modern treatment covering the various methods of multivariable interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.",,
31,1,11,c77bd587-f3b4-406d-9a84-24a37851dc85,"2010-07-19 19:18:30",34,"Multivariable Interpolation Approaches",,
32,3,11,c77bd587-f3b4-406d-9a84-24a37851dc85,"2010-07-19 19:18:30",34,<interpolation><multivariable>,,
33,2,12,2cc68cee-98a8-47e0-b7c9-9479d86c7a0e,"2010-07-19 19:18:41",5,"See my response to ["Datasets for Running Statistical Analysis on"][1] in reference to datasets in R.\\n\\n\\n  [1]: http://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/2252450#2252450",,
34,2,13,e2528eb9-342a-46fb-b85a-e7e61e92d9b5,"2010-07-19 19:18:56",23,"Machine Learning seems to have its basis in the pragmatic - a Practical observation or simulation of reality.  Even within statistics, mindless "checking of models and assumptions" can lead to discarding methods that are useful.\\n\\nFor example, years ago, the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome.  Technically, that's a bad approach, but practically, it worked.",,
35,2,14,b425ff8b-ef40-4b57-84cd-2167e96b2764,"2010-07-19 19:19:03",36,"I second that Jay. Why is R valuable? Here's a short list of reasons. http://www.inside-r.org/why-use-r. Also check out [ggplot2][1] - a very nice graphics package for R. Some nice tutorials [here][2].\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/ggplot2",,
36,16,14,b425ff8b-ef40-4b57-84cd-2167e96b2764,"2010-07-19 19:19:03",-1,,,
37,2,15,ccdaaf70-98b2-49ae-a2b4-08960ebcee50,"2010-07-19 19:19:46",6,"John Cook gives some interesting recommendations. Basically, get percentiles/quantiles (not means or obscure scale parameters!) from the experts, and fit them with the appropriate distribution.\\n\\nhttp://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/",,
38,6,8,d8ccbc08-0867-4ab7-b135-74cf7ce741a3,"2010-07-19 19:20:33",6,<lightbulb><verboten>,"edited tags",
39,5,5,d919c31f-f443-4bfa-9b5c-c306970ae92e,"2010-07-19 19:21:15",23,"The R-project\\n\\nhttp://www.r-project.org/\\n\\nR is valuable and significant because it was the first widely-accepted Open-Source alternative to big-box packages.  It's mature, well supported, and a standard within many scientific communities.\\n\\n - [Some reasons why it is useful and valuable][1] \\n - There are some nice tutorials [here][2].\\n\\n\\n  [1]: http://www.inside-r.org/why-use-r\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/ggplot2","add content from the comments;",
40,2,16,7f07993b-9fa1-4359-8dec-f4e9aadfe8e2,"2010-07-19 19:22:31",8,"Two projects spring to mind:\\n\\n 1. [Bugs][1] - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC.\\n 1. [Bioconductor][2] - perhaps the most popular tool in Bioinformatics. I know it's a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none.\\n\\n\\n  [1]: http://www.mrc-bsu.cam.ac.uk/bugs/\\n  [2]: http://www.bioconductor.org/",,
41,16,16,7f07993b-9fa1-4359-8dec-f4e9aadfe8e2,"2010-07-19 19:22:31",-1,,,
42,2,17,3e975162-dbea-4bcc-af2f-86c04a2adb1f,"2010-07-19 19:24:12",NULL,"I have four competing models which I use to predict a binary outcome variable (say, employment status after graduating, 1 = employed, 0 = not-employed) for n subjects. A natural metric of model performance is hit rate which is the percentage of correct predictions for each one of the models. \\n\\nIt seems to me that I cannot use ANOVA in this setting as the data violates the assumptions underlying ANOVA. Is there an equivalent procedure I could use instead of ANOVA in the above setting to test for the hypothesis that all four models are equally effective?\\n\\n",,user28
43,1,17,3e975162-dbea-4bcc-af2f-86c04a2adb1f,"2010-07-19 19:24:12",NULL,"How can I adapt ANOVA for binary data?",,user28
44,3,17,3e975162-dbea-4bcc-af2f-86c04a2adb1f,"2010-07-19 19:24:12",NULL,<anova>,,user28
45,2,18,917d8213-ef39-440b-b446-cec478d38b04,"2010-07-19 19:24:18",36,"Also see the UCI machine learning Data Repository.\\n\\nhttp://archive.ics.uci.edu/ml/",,
46,2,19,d14373a3-820e-4d7c-aec6-e87297be7405,"2010-07-19 19:24:21",55,"[Gapminder][1] has a number (430 at the last look) of datasets, which may or may not be of use to you.\\n\\n\\n  [1]: http://www.gapminder.org/data/",,
47,2,20,cc8e15db-c7ce-451a-af1e-81ef63a6b882,"2010-07-19 19:24:35",37,"The assumption of normality assumes your data is normally distributed (the bell curve, or gaussian distribution). You can check this by plotting the data or checking the measures for kurtosis (how sharp the peak is) and skewdness (?) (if more than half the data is on one side of the peak).",,
48,2,21,e63ccde2-b130-421e-a122-28b7ad228f51,"2010-07-19 19:24:36",59,"What are some of the ways to forecast demographic census with some validation and calibration techniques?\\n\\n\\nSome of the concerns:\\n\\n - Census blocks vary in sizes as rural\\n   areas are a lot larger than condensed\\n   urban areas. Is there a need to account for the area size difference?\\n - if let's say I have census data\\n   dating back to 4 - 5 census periods,\\n   how far can i forecast it into the\\n   future?\\n - if some of the census zone change\\n   lightly in boundaries, how can i\\n   account for that change?\\n - What are the methods to validate\\n   census forecasts? for example, if i\\n   have data for existing 5 census\\n   periods, should I model the first 3\\n   and test it on the latter two? or is\\n   there another way?\\n - what's the state of practice in\\n   forecasting census data, and what are\\n   some of the state of the art methods?",,
49,1,21,e63ccde2-b130-421e-a122-28b7ad228f51,"2010-07-19 19:24:36",59,"forecasting demographic census",,
50,3,21,e63ccde2-b130-421e-a122-28b7ad228f51,"2010-07-19 19:24:36",59,<population><census><forecast>,,
51,2,22,b235a8fa-e621-4456-a6c2-8c8e1eef48a2,"2010-07-19 19:25:39",66,"How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?",,
52,1,22,b235a8fa-e621-4456-a6c2-8c8e1eef48a2,"2010-07-19 19:25:39",66,"Bayesian and Frequentist reasoning in Plain English",,
53,3,22,b235a8fa-e621-4456-a6c2-8c8e1eef48a2,"2010-07-19 19:25:39",66,<baysian><frequentist>,,
54,2,23,f737130b-6f40-4902-b38d-4a91fd7a300c,"2010-07-19 19:26:04",69,"How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?",,
55,1,23,f737130b-6f40-4902-b38d-4a91fd7a300c,"2010-07-19 19:26:04",69,"Finding the PDF given the CDF",,
56,3,23,f737130b-6f40-4902-b38d-4a91fd7a300c,"2010-07-19 19:26:04",69,<distributions><pdf><cdf>,,
57,2,24,2d29e8f3-f301-478b-be9e-331a24ac5d8e,"2010-07-19 19:26:13",61,"For doing a variety of MCMC tasks in Python, there's [PyMC][1], which I've gotten quite a bit of use out of.  I haven't run across anything that I can do in BUGS that I can't do in PyMC, and the way you specify models and bring in data seems to be a lot more intuitive to me.\\n\\n\\n  [1]: http://code.google.com/p/pymc/",,
58,16,24,2d29e8f3-f301-478b-be9e-331a24ac5d8e,"2010-07-19 19:26:13",-1,,,
59,2,25,e9605664-43bc-4ad7-8114-2cb5fd614d40,"2010-07-19 19:27:13",69,"What modern tools (Windows-based) do you suggest for modeling financial time series?",,
60,1,25,e9605664-43bc-4ad7-8114-2cb5fd614d40,"2010-07-19 19:27:13",69,"Tools for modeling financial time series",,
61,3,25,e9605664-43bc-4ad7-8114-2cb5fd614d40,"2010-07-19 19:27:13",69,<software-rec><time-series><financial><modeling>,,
62,2,26,b2197fb4-3f17-4fe0-8d27-24d8e84ac96a,"2010-07-19 19:27:43",75,"What is a standard deviation, how is it calculuated and what is its use in statistics?",,
63,1,26,b2197fb4-3f17-4fe0-8d27-24d8e84ac96a,"2010-07-19 19:27:43",75,"What is a standard deviation?",,
64,3,26,b2197fb4-3f17-4fe0-8d27-24d8e84ac96a,"2010-07-19 19:27:43",75,<statistics><standard-deviation>,,
65,2,27,9a55d623-4b24-4697-9a71-153b5bcbde95,"2010-07-19 19:28:12",68,http://mathforum.org/workshops/sum96/data.collections/datalibrary/data.set6.html,,
66,2,28,7e616274-086a-4e71-9dfa-5a4e495917a6,"2010-07-19 19:28:12",NULL,"[GSL][1] for those of you who wish to program in C / C++ is a valuable resource as it provides several routines for random generators, linear algebra etc. While GSL is primarily available for Linux there are also ports for Windows. (See: http://gladman.plushost.co.uk/oldsite/computing/gnu_scientific_library.php and http://david.geldreich.free.fr/dev.html)\\n\\n\\n  [1]: http://www.gnu.org/software/gsl/",,user28
67,16,28,7e616274-086a-4e71-9dfa-5a4e495917a6,"2010-07-19 19:28:12",-1,,,
68,2,29,bfc58ad7-1542-423f-851a-95b8f2196c9e,"2010-07-19 19:28:15",36,"Contingency table (chi-square). Also Logistic Regression is your friend - use dummy variables. ",,
69,2,30,83659a4b-a1da-4479-9b30-06d2ff7ca936,"2010-07-19 19:28:34",69,"Which methods are used for testing random variate generation algorithms?",,
70,1,30,83659a4b-a1da-4479-9b30-06d2ff7ca936,"2010-07-19 19:28:34",69,"Testing random variate generation algorithms",,
71,3,30,83659a4b-a1da-4479-9b30-06d2ff7ca936,"2010-07-19 19:28:34",69,<random-variate><hypothesis-testing><random-generation><algorithms>,,
72,2,31,a79cca70-fdf1-4c86-b7bb-93f5562a5dc5,"2010-07-19 19:28:44",13,"After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results.  Many computerized tools report test results in terms of "p values" or "t values".\\n\\nHow would you explain the following points to college students taking their first course in statistics:\\n\\n  * What does a "p-value" mean in relation to the hypothesis being tested?  Are there cases when one should be looking for a high p-value or a low p-value?\\n\\n  * What is the relationship between a p-value and a t-value?\\n",,
73,1,31,a79cca70-fdf1-4c86-b7bb-93f5562a5dc5,"2010-07-19 19:28:44",13,"What is the meaning of p values and t values in statistical tests?",,
74,3,31,a79cca70-fdf1-4c86-b7bb-93f5562a5dc5,"2010-07-19 19:28:44",13,<hypothesis-testing><p-value><t-value>,,
75,2,32,6fd8f953-2f27-4c66-bb40-6476dfa9dd09,"2010-07-19 19:29:06",5,"I recommend R (see [the time series view on CRAN][1]).  \\n\\nSome useful references:\\n\\n - [Econometrics in R][2], by Grant Farnsworth\\n - [Multivariate time series modelling in R][3]\\n\\n\\n  [1]: http://cran.r-project.org/web/views/TimeSeries.html\\n  [2]: http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf\\n  [3]: http://stackoverflow.com/questions/1714280/multivariate-time-series-modelling-in-r/1715488#1715488",,
76,2,33,f397f5c0-7376-4ef5-96e2-cbfdf7229bc6,"2010-07-19 19:30:03",69,"What R packages should I install for seasonality analysis?",,
77,1,33,f397f5c0-7376-4ef5-96e2-cbfdf7229bc6,"2010-07-19 19:30:03",69,"R packages for seasonality analysis",,
78,3,33,f397f5c0-7376-4ef5-96e2-cbfdf7229bc6,"2010-07-19 19:30:03",69,<r><seasonality-analysis><statistical-analysis>,,
79,2,34,baa7406c-26fd-4eae-9555-2bba79e64573,"2010-07-19 19:30:07",79,"[Schools of thought in Probability Theory][1]\\n\\n\\n  [1]: http://scienceblogs.com/goodmath/2008/04/schools_of_thought_in_probabil.php",,
80,6,22,d3653def-c161-4c06-a57e-733baf37da6c,"2010-07-19 19:30:08",80,<bayesian><frequentist>,"edited tags",
81,2,35,4e2f657c-3787-4d39-80ce-2f065d16c9f3,"2010-07-19 19:30:30",54,"I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.\\n\\n    ## assuming a median value of 1500\\n    med = 1500\\n    rawdist = rpois(1000000,med)\\n    oDdist = rawDist + ((rawDist-med)*3)\\n\\nVisually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a [negative binomial distribution, as described here](http://en.wikipedia.org/wiki/Overdispersion#Poisson)? (If so, any pointers or links on doing so would be much appreciated).\\n\\nOh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.\\n",,
82,1,35,4e2f657c-3787-4d39-80ce-2f065d16c9f3,"2010-07-19 19:30:30",54,"Modelling a Poisson distribution with overdispersion",,
83,3,35,4e2f657c-3787-4d39-80ce-2f065d16c9f3,"2010-07-19 19:30:30",54,<modeling><poisson><distribution><overdispersion>,,
84,2,36,89191c5b-9ead-40f4-b423-cc211fcb9fbd,"2010-07-19 19:31:47",8,"We all know the old saying "Correlation does not mean causation". When I'm teaching I tend to use these standard examples to illustrate this point:\\n\\n1. Number of storks and birth rate in Denmark;\\n1. Number of priests in America and alcoholism\\n1. In the start of the 20th century it was noted that there was a strong correlation between `Number of radios' and 'Number of people in Insane Asylums'\\n1. and my favourite: [pirates cause global warming][1] \\n\\nHowever, I don't have any references for these examples and whilst amusing, they are obviously false.\\n\\nDoes anyone have any other good examples?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/File:PiratesVsTemp_English.jpg",,
85,1,36,89191c5b-9ead-40f4-b423-cc211fcb9fbd,"2010-07-19 19:31:47",8,"Correlation does not mean causation",,
86,3,36,89191c5b-9ead-40f4-b423-cc211fcb9fbd,"2010-07-19 19:31:47",8,<correlation><teaching>,,
88,2,38,f6d036e2-69e9-4537-b8c4-7b3434ba6650,"2010-07-19 19:32:28",61,"If your mean value for the Poisson is 1500, then you're very close to a normal distribution; you might try using that as an approximation and then modelling the mean and variance separately.",,
89,2,39,61be4085-ad02-4873-ab15-d659e428a31c,"2010-07-19 19:32:29",59,"I'm looking for worked out solutions using Bayesian and/or logit analysis similar to a workbook or an annal. \\n\\nThe worked out problems could be of any field; however, I'm interested in urban planning / transportation related fields. ",,
90,1,39,61be4085-ad02-4873-ab15-d659e428a31c,"2010-07-19 19:32:29",59,"Sample problems on logit modeling and Bayesian methods",,
91,3,39,61be4085-ad02-4873-ab15-d659e428a31c,"2010-07-19 19:32:29",59,<bayesian><logit><modeling><transportation>,,
92,2,40,9d1c7dc9-43ca-4e75-b513-ab70b3ec1760,"2010-07-19 19:32:47",69,"What algorithms are used in modern and good-quality random number generators? ",,
93,1,40,9d1c7dc9-43ca-4e75-b513-ab70b3ec1760,"2010-07-19 19:32:47",69,"Pseudo-random number generation algorithms",,
94,3,40,9d1c7dc9-43ca-4e75-b513-ab70b3ec1760,"2010-07-19 19:32:47",69,<random-variate><random-generation><algorithms>,,
95,6,8,8b4cd9c4-5abe-4c33-bb9b-3da717a1b3f6,"2010-07-19 19:32:54",60,<lightbulb>,"edited tags",
96,2,41,ce0b902a-cd13-49de-bd5b-b1ff0cb41082,"2010-07-19 19:33:13",83,"A quote from [wiki][1].\\n\\n> It shows how much variation there is from the "average" (mean, or expected/budgeted value). A low standard deviation indicates that the data points tend to be very close to the mean, whereas high standard deviation indicates that the data is spread out over a large range of values.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Standard_deviation",,
97,2,42,fdd08f15-ae52-4361-965e-4bb79b36dc54,"2010-07-19 19:33:19",80,"[Weka][1] for data mining - contains many classification and clustering algorithms in Java.\\n\\n [1]: http://www.cs.waikato.ac.nz/ml/weka  ",,
98,16,42,fdd08f15-ae52-4361-965e-4bb79b36dc54,"2010-07-19 19:33:19",-1,,,
99,2,43,917d5d7b-47be-4256-980e-a34bf413348c,"2010-07-19 19:33:37",74,"I wouldn't really call R "windows based" :) That's like saying the cmd prompt is windows based. \\n\\nRapidMiner is far better [1]. It's a free, open-source, multi-platform, GUI. Here's a video on time series forecasting:\\n\\nhttp://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1\\n\\nAlso, don't forget to read:\\n\\nhttp://www.forecastingprinciples.com/\\n\\n\\n[1] No, I don't work for them. ",,
100,4,3,780804d6-b18f-4180-875e-31e397c4acf1,"2010-07-19 19:34:13",13,"What are some valuable Statistical Analysis open source projects?","Removed the word "most" and used "some"- most is way too subjective.",
101,2,44,32504bd0-c677-41d6-8a96-9246b231feb9,"2010-07-19 19:34:42",68,"How would you explain data visualization and why it is important to a layman?",,
102,1,44,32504bd0-c677-41d6-8a96-9246b231feb9,"2010-07-19 19:34:42",68,"Explain data visualization",,
103,3,44,32504bd0-c677-41d6-8a96-9246b231feb9,"2010-07-19 19:34:42",68,<data-visualization>,,
104,2,45,132c2391-8a68-4f88-8615-213a0316784e,"2010-07-19 19:34:44",55,"The [Mersenne Twister][1] is one I've come across and used before now.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Mersenne_twister",,
105,2,46,801baf8c-38c6-4ac7-9634-9c06dd0ace9f,"2010-07-19 19:35:04",62,"A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. \\n\\nTo put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. ",,
106,5,3,780804d6-b18f-4180-875e-31e397c4acf1,"2010-07-19 19:34:13",13,"What are some valuable Statistical Analysis open source projects available right now?","Removed the word "most" and used "some"- most is way too subjective.",
107,2,47,47eb2be3-688d-4b50-b187-047bfe45decd,"2010-07-19 19:36:12",22,"I have a dataset of 130k internet users characterized by 4 variables describing users' number of sessions, locations visited, avg data download and session time aggregated from four months of activity.\\n\\nDataset is very heavy-tailed. For example third of users logged only once during four months, whereas six users had more than 1000 sessions.\\n\\nI wanted to come up with a simple classification of users, preferably with indication of the most appropriate number of clusters.\\n\\nIs there anything you could recomend as a soultion?\\n\\n",,
108,1,47,47eb2be3-688d-4b50-b187-047bfe45decd,"2010-07-19 19:36:12",22,"Clustering of large, heavy-tailed dataset  ",,
109,3,47,47eb2be3-688d-4b50-b187-047bfe45decd,"2010-07-19 19:36:12",22,<clustering>,,
110,2,48,385bebc4-2ba0-4c16-a2f9-3b600b1b181c,"2010-07-19 19:36:20",36,"Flip through Freakonomics for some great examples. Their bibliography is chock full of references.",,
111,2,49,a85fcdbb-34cd-493c-95ab-aab491932394,"2010-07-19 19:36:52",5,"You don't need to install any packages because this is possible with base-R functions.  Have a look at [the arima function][1].  \\n\\nThis is a basic function of [Box-Jenkins analysis][2], so you should consider reading one of the R time series text-books for an overview; my favorite is Shumway and Stoffer. "<a href="http://www.amazon.com/gp/product/0387293175?ie=UTF8&tag=statalgo-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=0387293175">Time Series Analysis and Its Applications: With R Examples</a><img src="http://www.assoc-amazon.com/e/ir?t=statalgo-20&l=as2&o=1&a=0387293175" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />".\\n\\n\\n  [1]: http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/ts/html/arima.html\\n  [2]: http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins",,
112,2,50,98be07be-76c9-49e6-838f-014ab51d5c51,"2010-07-19 19:37:31",62,"What do they mean when they say "random variable"? ",,
113,1,50,98be07be-76c9-49e6-838f-014ab51d5c51,"2010-07-19 19:37:31",62,"What is meant by a "random variable"? ",,
114,3,50,98be07be-76c9-49e6-838f-014ab51d5c51,"2010-07-19 19:37:31",62,<random><variable><random-variable>,,
115,2,51,bda0f58b-2e17-4f83-8c1b-1d6d8381bbe1,"2010-07-19 19:37:55",68,"Are there any objective methods of assessment or standardized tests available to measure the effectiveness of a software that does pattern recognition?",,
116,1,51,bda0f58b-2e17-4f83-8c1b-1d6d8381bbe1,"2010-07-19 19:37:55",68,"Measuring the effectiveness of a pattern recognition software",,
117,3,51,bda0f58b-2e17-4f83-8c1b-1d6d8381bbe1,"2010-07-19 19:37:55",68,<pattern-recognition>,,
118,2,52,d3d7a55d-52d3-4f0f-b1f3-d1d3a9271464,"2010-07-19 19:39:06",36,"Usually one (p=.042), but it also depends on power.",,
119,2,53,52c767e7-1b74-4a46-8021-ebc11e02ea39,"2010-07-19 19:39:08",17,"What are the main differences between performing Principal Components Analysis on a correlation and covariance matrix? Do they give the same results?",,
120,1,53,52c767e7-1b74-4a46-8021-ebc11e02ea39,"2010-07-19 19:39:08",17,"PCA on Correlation or Covariance?",,
121,3,53,52c767e7-1b74-4a46-8021-ebc11e02ea39,"2010-07-19 19:39:08",17,<multivariable><statistical-analysis><modeling>,,
122,2,54,9db7fd28-939f-4429-8823-b7cf3c2237d3,"2010-07-19 19:41:19",55,"As I understand UK Schools teach that the Standard Deviation is found using:\\n\\n![alt text][1]\\n\\nwhereas US Schools teach:\\n\\n![alt text][2]\\n\\n(at a basic level anyway).\\n\\nThis has caused a number of my students problems in the past as they have searched on the Internet, but found the wrong explanation.\\n\\nWhy the difference?\\n\\nWith simple datasets say 10 values, what degree of error will there be if the wrong method is applied (eg in an exam)?\\n\\n\\n  [1]: http://upload.wikimedia.org/math/e/e/4/ee485814ab9e19908f2b39d5d70406d5.png\\n  [2]: http://upload.wikimedia.org/math/8/3/a/83a7338b851dcaafaf5b64353af56596.png",,
123,1,54,9db7fd28-939f-4429-8823-b7cf3c2237d3,"2010-07-19 19:41:19",55,"Why do US and UK Schools Teach Different methods of Calculating the Standard Deviation?",,
124,3,54,9db7fd28-939f-4429-8823-b7cf3c2237d3,"2010-07-19 19:41:19",55,<standard-deviation><error><accuracy>,,
125,2,55,3551a041-31ed-4147-9d1d-2d4ba5fc1ab3,"2010-07-19 19:41:39",56,"The [Diehard Test Suite][1] is something close to a Golden Standard for testing random number generators. It includes a number of tests where a good random number generator should produce result distributed according to some know distribution against which the outcome using the tested generator can then be compared.\\n\\n[1]: http://en.wikipedia.org/wiki/Diehard_tests",,
126,2,56,fc2eedef-6d1c-4870-8258-6484234cf64b,"2010-07-19 19:42:28",NULL,"Here is how I would explain the basic difference to my grandma:\\n\\nI have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping.\\n\\nProblem: Which area of my home should I search?\\n\\nFrequentist Reasoning: \\n\\nI can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming from. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone.\\n\\nBayesian Reasoning:\\n\\nI can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone.\\n\\n ",,user28
127,2,57,a35bc82b-b294-42ad-9b5a-acb7eca62393,"2010-07-19 19:42:34",69,"From [Wikipedia][1]:\\n\\n> In mathematics (especially probability\\n> theory and statistics), a random\\n> variable (or stochastic variable) is\\n> (in general) a measurable function \\n> that maps a probability space into a\\n> measurable space. Random variables\\n> mapping all possible outcomes of an\\n> event into the real numbers are\\n> frequently studied in elementary\\n> statistics and used in the sciences to\\n> make predictions based on data\\n> obtained from scientific experiments.\\n> In addition to scientific\\n> applications, random variables were\\n> developed for the analysis of games of\\n> chance and stochastic  events. The\\n> utility of random variables comes from\\n> their ability to capture only the\\n> mathematical properties necessary to\\n> answer probabilistic  questions.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable",,
128,2,58,316d8f18-8ac6-400f-bcb5-2e0778ecf554,"2010-07-19 19:42:57",68,"What is the backpropogation algorithm and how does it work?",,
129,1,58,316d8f18-8ac6-400f-bcb5-2e0778ecf554,"2010-07-19 19:42:57",68,"Can someone please explain the backpropogation algorithm?",,
130,3,58,316d8f18-8ac6-400f-bcb5-2e0778ecf554,"2010-07-19 19:42:57",68,<algorithms><neural-networks>,,
131,2,59,fba078cb-5c67-4766-b3cc-16074bba0e1a,"2010-07-19 19:43:20",39,"The assumption of normality is just the supposition that data is distributed [normally][1], vis:\\n![alt text][2]\\n\\n\\nThis can be checked in [multiple ways][3], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][4]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [2]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [3]: http://en.wikipedia.org/wiki/Normality_test\\n  [4]: http://en.wikipedia.org/wiki/Q-Q_plot",,
132,2,60,e3d6dd9f-676c-4462-88e2-999ff5dcd02c,"2010-07-19 19:43:20",41,"[K-Means clustering][1] should work well for this type of problem.  However, it does require that you specify the number of clusters in advance.\\n\\nGiven the nature of this data, however, you may be able to work with a [hierarchical clustering algorithm][2] instead.  Since all 4 variables are most likely fairly highly correlated, you can most likely break out clusters, and stop when you reach a small enough distance between clusters.  This may be a much simpler approach in this specific case, and allows you to determine "how many clusters" by just stopping as soon as you've broken your set into fine enough clusters.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/K-means_algorithm\\n  [2]: http://en.wikipedia.org/wiki/Cluster_analysis#Hierarchical_clustering",,
133,2,61,ead6c1b1-f746-486b-9809-13b293bc6938,"2010-07-19 19:44:35",74,"The standard deviation is a metric, meaning it is a number that represents a concept. The concept in this case is the "spread" or "dispersion" of the data.\\n\\nThere are other metrics for spread, including range and variance. \\n\\nA distribution with a mean of 1 and a standard deviation of 2 has more "spread" than a distribution with a mean of 1 and a standard deviation of 1. \\n\\nIf there is no spread, then the standard deviation is zero. \\n\\nStandard deviation is often used as it is in the same units as the mean, unlike variance. ",,
134,2,62,d78853d7-3f4f-4901-8112-de657608a2e2,"2010-07-19 19:44:58",58,"With the recent FIFA world cup, I decided to have some fun and determine which months produced world cup football players. Turned out, most footballers in the 2010 world cup were born in the first half of the year.\\n\\nSomeone pointed out, that children born in the first half of the year had a physical advantage over others and hence "survivorship bias" was involved in the equation. Is this an accurate observation? Can someone please explain why he says that?\\n\\nAlso, when trying to understand the concept, I found most examples revolved around the financial sector. Are they any other everyday life examples explaining it?\\n\\nThanks!",,
135,1,62,d78853d7-3f4f-4901-8112-de657608a2e2,"2010-07-19 19:44:58",58,"A case of survivorship bias?",,
136,3,62,d78853d7-3f4f-4901-8112-de657608a2e2,"2010-07-19 19:44:58",58,<survivorship><bias><statistical>,,
137,2,63,c711f9c2-3747-4603-b6b3-0369e2c7bfdb,"2010-07-19 19:45:19",87,"It might be useful to explain that "causes" is an asymmetric relation (X causes Y is different from Y causes X), whereas "is correlated with" is a symmetric relation.\\n\\nFor instance, homeless population and crime rate might be correlated, in that both tend to be high or low in the same locations.  It is equally valid to say that homelesss population is correlated with crime rate, or crime rate is correlated with homeless population.  To say that crime causes homelessness, or homeless populations cause crime are different statements.  And correlation does not imply that either is true.  For instance, the underlying cause could be a 3rd variable such as drug abuse, or unemployment.  \\n\\nThe mathematics of statistics is not good at identifying underlying causes, which requires some other form of judgement.\\n\\n",,
138,2,64,3475f714-f1ed-4538-b78b-96c993f8dd34,"2010-07-19 19:46:08",5,"Yes, there are many methods.  You would need to specify which model you're using, because it can vary.  \\n\\nFor instance, Some models will be compared based on the [AIC][1] or [BIC][2] criteria.  In other cases, one would look at the [MSE from cross validation][3] (as, for instance, with a support vector machine).\\n\\nI recommend reading [Pattern Recognition and Machine Learning][4] by Christopher Bishop.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Akaike_information_criterion\\n  [2]: http://en.wikipedia.org/wiki/Bayesian_information_criterion\\n  [3]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)\\n  [4]: http://research.microsoft.com/en-us/um/people/cmbishop/prml/",,
139,2,65,e57bcbae-5a2f-4279-bd4c-cd728e4de6b8,"2010-07-19 19:46:11",8,"The first formula is the *population* standard deviation and the second formula is the the *sample* standard deviation. The second formula is also the unbiased maximum likelihood estimator of the standard deviation.\\n\\nI suppose (here) in the UK they don't make the distinction between sample and population at high school. They certainly don't touch concepts such as biased estimators. ",,
140,2,66,82485ae5-2703-4c0c-9d7d-94a009444128,"2010-07-19 19:46:11",41,"This is [Bessel's Correction][1].  The US version is showing the formula for the *sample standard deviation*, where the UK version above is the *standard deviation of the sample*.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bessel's_correction",,
141,2,67,f5d6ef6b-a2d6-47e6-bfe4-c3d82324427f,"2010-07-19 19:47:16",36,"From [Wikipedia][1]: Data visualization is the study of the visual representation of data, meaning "information which has been abstracted in some schematic form, including attributes or variables for the units of information"\\n\\nData viz is important for visualizing trends in data, telling a story - See [Minard's map of Napoleon's march][2] - possibly one of the best data graphics ever printed.\\n\\nAlso see any of Edward Tufte's books - especially [Visual Display of Quantitative Information.][3]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Data_visualization\\n  [2]: http://www.edwardtufte.com/tufte/posters\\n  [3]: http://www.amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20",,
143,2,69,64f3ab8a-029a-4fc9-b241-979e0ae2fcc7,"2010-07-19 19:47:49",56,"Since N is the number of points in the data set, one could argue that by calculating the mean one has reduced the degree of freedom in the data set by one (since one introduced a dependency into the data set), so one should use N-1 when estimating the standard deviation from a data set for which one had to estimate the mean before.",,
144,5,57,054a085f-97d0-4de4-91e5-2770cbc9d23e,"2010-07-19 19:47:54",69,"From [Wikipedia][1]:\\n\\n> In mathematics (especially probability\\n> theory and statistics), a random\\n> variable (or stochastic variable) is\\n> (in general) a measurable function \\n> that maps a probability space into a\\n> measurable space. Random variables\\n> mapping all possible outcomes of an\\n> event into the real numbers are\\n> frequently studied in elementary\\n> statistics and used in the sciences to\\n> make predictions based on data\\n> obtained from scientific experiments.\\n> In addition to scientific\\n> applications, random variables were\\n> developed for the analysis of games of\\n> chance and stochastic  events. The\\n> utility of random variables comes from\\n> their ability to capture only the\\n> mathematical properties necessary to\\n> answer probabilistic  questions.\\n\\nFrom [cnx.org][2]:\\n\\n> A random variable is a function, which assigns unique numerical values to all possible\\n> outcomes of a random experiment under fixed conditions. A random variable is not a \\n> variable but rather a function that maps events to numbers.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://cnx.org/content/m13418/latest/","added 310 characters in body",
145,2,70,ff76a5d4-d8ec-4a3b-9dbb-c2a3ba6cc68f,"2010-07-19 19:48:45",22,"[World Bank][1] offers quite a lot of interesting data and has been recently very active in developing nice [API][2] for it.\\n\\n\\n  [1]: http://data.worldbank.org/data-catalog\\n  [2]: http://data.worldbank.org/developers/api-overview",,
146,2,71,952cc7e1-3709-42b8-bf10-c8e60d99446d,"2010-07-19 19:50:33",36,"It's an algorithm for training feedforward multilayer neural networks (multilayer perceptrons). There are several nice java applets around the web that illustrate what's happening, like this one: http://neuron.eng.wayne.edu/bpFunctionApprox/bpFunctionApprox.html. Also, [Bishop's book on NNs][1] is the standard desk reference for anything to do with NNs.\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0198538642/ref=nosim/gettgenedone-20",,
147,6,62,1f068b07-c91e-4a83-8a5d-d836258cbdb6,"2010-07-19 19:50:45",58,<statistical-bias>,"edited tags",
148,5,59,22f9cc27-c828-4fc6-b41c-dbe3dae4afee,"2010-07-19 19:50:46",39,"The assumption of normality is just the supposition that the underlying [random variable][1] of interest is distributed [normally][2], or approximately so; vis:\\n\\n![alt text][3]\\n\\nFrom the following function:\\n![alt text][4]\\n\\n\\nThis can be checked in [multiple ways][5], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][6]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [3]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [4]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [5]: http://en.wikipedia.org/wiki/Normality_test\\n  [6]: http://en.wikipedia.org/wiki/Q-Q_plot","added 251 characters in body",
149,2,72,16e0816c-cddb-41ce-893e-70aeb2f13bc5,"2010-07-19 19:51:05",96,"for overdispersed poisson, use the negative binomial, which allows you to parameterize the variance as a function of the mean precisely.  rnbinom(), etc. in R.",,
150,2,73,498e2950-a5f1-429f-ae59-b29c1d9e71f2,"2010-07-19 19:51:32",22,"What are the R packages you couldn't imagine your daily work with data?\\nBoth general and specific tools.",,
151,1,73,498e2950-a5f1-429f-ae59-b29c1d9e71f2,"2010-07-19 19:51:32",22,"What R packages do you find most useful in your daily work?",,
152,3,73,498e2950-a5f1-429f-ae59-b29c1d9e71f2,"2010-07-19 19:51:32",22,<r>,,
153,2,74,7d4db1ed-7a4f-41cf-b26c-af0053630fa6,"2010-07-19 19:51:34",88,"In such a discussion, I always recall the famous Ken Thompson quote \\n\\n> When in doubt, use brute force.\\n\\nIn this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. ",,
154,2,75,c27e7f0a-5bb3-4044-9986-ad143dcfcdb5,"2010-07-19 19:52:31",69,"I'm using [**R**][1] and the manuals on the R site are really informative. However, I'd like to see some more examples and implementations with R which can help me develop my knowledge faster. Any suggestions?\\n\\n\\n  [1]: http://www.r-project.org/",,
155,1,75,c27e7f0a-5bb3-4044-9986-ad143dcfcdb5,"2010-07-19 19:52:31",69,"Where can I find useful R tutorials with various implementations?",,
156,3,75,c27e7f0a-5bb3-4044-9986-ad143dcfcdb5,"2010-07-19 19:52:31",69,<r><books><implementation>,,
157,2,76,03001983-95cf-4200-8dd4-256dff09b148,"2010-07-19 19:52:49",5,"I use [**plyr**][1] and [**ggplot2**][2] the most on a daily basis.\\n\\nI also rely heavily on time series packages; most especially, the [**zoo**][3] package.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/plyr/index.html\\n  [2]: http://cran.r-project.org/web/packages/ggplot2/index.html\\n  [3]: http://cran.r-project.org/web/packages/zoo/index.html",,
158,2,77,c3190e4b-6f57-4e8f-9892-5905fdafabf2,"2010-07-19 19:54:03",74,"1. Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally.\\n\\n2. To find causation, you need generally experimental data, not observational data. ",,
159,2,78,cf3824a4-3054-41b2-ae14-faa3ad0c9faf,"2010-07-19 19:54:38",8,"You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales. Using the covariance matrix *standardises* the data.\\n\\nIn general they give different results. Especially when the scales are different.",,
160,2,79,1ead3165-e78d-482d-abf0-761c357687a4,"2010-07-19 19:56:04",25,"I am not sure this is purely a US vs. British issue. Here is a brief page I wrote [explaining the difference between using n vs. n-1 when computing a Standard Deviation](http://www.graphpad.com/faq/viewfaq.cfm?faq=1382).\\n",,
161,5,59,126a8681-d0ff-4442-a8b5-5d7e19e9a23a,"2010-07-19 19:56:05",39,"The assumption of normality is just the supposition that the underlying [random variable][1] of interest is distributed [normally][2], or approximately so; vis:\\n\\n![alt text][3]\\n\\nFrom the following function:\\n![alt text][4]\\nwhere μ and σ^2 are the mean and the variance, respectively.\\n\\nIntuitively, normality may be understood as the result of the sum of a large number of independent random events.\\n\\nThis can be checked in [multiple ways][5], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][6]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [3]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [4]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [5]: http://en.wikipedia.org/wiki/Normality_test\\n  [6]: http://en.wikipedia.org/wiki/Q-Q_plot","added 115 characters in body; added 62 characters in body",
162,2,80,7ba9c721-1380-4b0e-8e7a-16eb101b4f6b,"2010-07-19 19:56:43",93,"The basic idea behind this is that football clubs have an age cut-off when determining teams.  In the league my children participate in the age restrictions states that children born after July 31st are placed on the younger team.  This means that two children that are effectively the same age can be playing with two different age groups.  The child born July 31st will be playing on the older team and theoretically be the youngest and smallest on the team and in the league.  The child born on August 1st will be the oldest and largest child in the league and will be able to benefit from that.\\n\\nThe survivorship bias comes because competitive leagues will select the best players for their teams.  The best players in childhood are often the older players since they have additional time for their bodies to mature.  This means that otherwise acceptable younger players are not selected simply because of their age.  Since they are not given the same opportunities as the older kids, they don’t develop the same skills and eventually drop out of competitive soccer.\\n\\nIf the cut-off for competitive soccer in enough countries is January 1st, that would support the phenomena you see.  A similar phenomena has been observed in several other sports including baseball and ice hockey.",,
163,2,81,55bf06bc-ca75-4915-8310-9513c335053c,"2010-07-19 19:58:56",69,"I use the **[xtable][1]** package. The xtable package turns tables produced by R (in particular, the tables displaying the anova results) into LaTeX tables, to be included in an article. \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/xtable/index.html",,
165,5,59,fd1cd967-cf1c-4b92-88cc-50646bf3337f,"2010-07-19 20:02:04",39,"The assumption of normality is just the supposition that the underlying [random variable][1] of interest is distributed [normally][2], or approximately so.  Intuitively, normality may be understood as the result of the sum of a large number of independent random events.\\n\\nMore specifically, normal distributions are defined by the following function:\\n\\n![alt text][4]\\n\\nwhere μ and σ^2 are the mean and the variance, respectively, and which appears as follows:\\n\\n![alt text][3]\\n\\nThis can be checked in [multiple ways][5], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][6]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [3]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [4]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [5]: http://en.wikipedia.org/wiki/Normality_test\\n  [6]: http://en.wikipedia.org/wiki/Q-Q_plot","added 73 characters in body; added 4 characters in body",
166,5,77,67cc4076-c675-464c-be49-3d77d9885e7c,"2010-07-19 20:02:48",74,"1. Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally.\\n\\n2. To find causation, you generally need experimental data, not observational data [1]\\n\\n\\n[1] Though, in economics, they often use observed "shocks" to the system to test for causation, like if a CEO dies suddenly and the stock price goes up, you can assume causation. ","added 187 characters in body",
167,2,83,966fa512-a91d-48ff-89ec-b97a7012ab24,"2010-07-19 20:02:51",5,"R is designed around ideas such as "reproducible research" and "trustworthy software", as John Chambers says <a href="http://books.google.com/books?id=UXneuOIvhEAC&printsec=frontcover">in his excellent book "Software for Data Analysis: Programming with R"</a>.  \\n\\nOne of the best ways to learn R is to look at the wealth of source code that available on [CRAN][1] (with 2461 packages and counting).  Simple `install.packages`, load a `library()`, and start browsing the code.\\n\\n\\n  [1]: http://cran.r-project.org/",,
168,2,84,f601b199-5fdf-4795-beca-38630dad62b6,"2010-07-19 20:03:34",41,"I would explain it to a layman as:\\n\\n> Data visualization is taking data, and making a picture out of it.  This allows you to easily see and understand relationships within the data much more easily than just looking at the numbers.\\n\\n",,
169,16,73,029c0265-e658-4927-b63b-a53ddd478892,"2010-07-19 20:07:49",22,,,
170,5,73,029c0265-e658-4927-b63b-a53ddd478892,"2010-07-19 20:07:49",22,"What are the R packages you couldn't imagine your daily work with data?\\nPlease list both general and specific tools.","added 12 characters in body",
171,2,85,4885a394-9de8-478b-803c-3eeabecfdd64,"2010-07-19 20:08:00",87,"A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as "state", and then the random variable is a function of the state.\\n\\nExample:  Suppose we have three dice rolls (D1,D2,D3).  Then the state S=(D1,D2,D3).  A ramdom variable X is the number of 5s. X=(D1==5?)+(D2==5?)+(D3==5?).   Another random variable Y is the sum of the dice rolls Y=D1+D2+D3.  ",,
172,2,86,9bad9ae7-3b1f-4ec6-b275-69f71b0b6a98,"2010-07-19 20:08:37",13,"Unlike a normal variable, a random variable may not be substituted for a single, unchanging value.  Rather **statistical properties** such as the **distribution** of the random variable may be proscribed.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.  \\n\\nRandom variables may be classified as *discreet* if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is *continuous* and is used if the distribution covers values from an uncountable set such as the real numbers.",,
175,2,89,afc1d7bc-11ff-4989-9179-cea3f4766e34,"2010-07-19 20:11:47",55,"When I teach very basic statistics to Secondary School Students I talk about evolution and how we have evolved to spot patterns in pictures rather than lists of numbers and that data visualisation is one of the techniques we use to take advantage of this fact. \\n\\nPlus I try to talk about recent news stories where statistical insight contradicts what the press is implying, making use of sites like [Gapminder][1] to find the representation before choosing the story.\\n\\n\\n  [1]: http://www.gapminder.org/",,
176,5,64,c095b3e2-d112-4ee9-bab3-421f99d3bb25,"2010-07-19 20:12:16",5,"Yes, there are many methods.  You would need to specify which model you're using, because it can vary.  \\n\\nFor instance, Some models will be compared based on the [AIC][1] or [BIC][2] criteria.  In other cases, one would look at the [MSE from cross validation][3] (as, for instance, with a support vector machine).\\n\\n 1. I recommend reading [Pattern Recognition and Machine Learning][4] by Christopher Bishop.\\n 2. This is also discussed in Chapter 5 on Credibility, and particularly section 5.5 "Comparing data mining methods" of [Data Mining: Practical Machine Learning Tools and Techniques][5] by Witten and Frank (which discusses Weka in detail).\\n 3. Lastly, you should also have a look at [The Elements of Statistical Learning][6] by Hastie, Tibshirani and Friedman which is available for free online.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Akaike_information_criterion\\n  [2]: http://en.wikipedia.org/wiki/Bayesian_information_criterion\\n  [3]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)\\n  [4]: http://research.microsoft.com/en-us/um/people/cmbishop/prml/\\n  [5]: http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0120884070/ref=ntt_at_ep_dpi_1\\n  [6]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/","added 568 characters in body",
177,2,90,680dcd60-5a29-4d16-a617-4cb7c9d2d027,"2010-07-19 20:12:24",22,"[R bloggers][1] has been steadily supplying me with a lot of good pragmatic content.  \\nFrom the author:\\n\\n    R-Bloggers.com is a central hub (e.g: A blog aggregator) of content \\n    collected from bloggers who write about R (in English). \\n    The site will help R bloggers and users to connect and follow \\n    the “R blogosphere”.\\n\\n\\n  [1]: http://www.r-bloggers.com/",,
178,2,91,f9d47ca6-7a93-4012-9169-2ac7851b6fc3,"2010-07-19 20:15:54",87,"As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous variable, and the difference for a discrete variable. \\n\\nIn the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta "functions" can be used to represent these atoms.    \\n\\n ",,
180,2,93,f802855b-9345-484f-9570-d3532ee5d0d7,"2010-07-19 20:17:07",61,"We're trying to use a Gaussian process to model h(t) -- the hazard function -- for a very small initial population, and then fit that using the available data.  While this gives us nice plots for credible sets for h(t) and so on, it unfortunately is also just pushing the inference problem from h(t) to the covariance function of our process.  Perhaps predictably, we have several reasonable and equally defensible guesses for this that all produce different result.  \\n\\nHas anyone run across any good approaches for addressing such a problem?  Gaussian-process related or otherwise?",,
181,1,93,f802855b-9345-484f-9570-d3532ee5d0d7,"2010-07-19 20:17:07",61,"Robust nonparametric estimation of hazard/survival functions based on low count data",,
182,3,93,f802855b-9345-484f-9570-d3532ee5d0d7,"2010-07-19 20:17:07",61,<survivorship><hazard-function><survival-analysis><nonparametric>,,
183,2,94,ef5c28a1-db1a-4b42-9645-9bd0f259dfa7,"2010-07-19 20:18:24",88,"Quick R site is basic, but quite nice for start http://www.statmethods.net/index.html . ",,
185,10,8,abf0405c-3e41-4bab-a3b0-3710520084bc,"2010-07-19 20:19:46",-1,"{"Voters":[{"Id":60,"DisplayName":"Jason Punyon"},{"Id":68,"DisplayName":"Ami"},{"Id":55,"DisplayName":"Amos"},{"Id":103,"DisplayName":"rcs"},{"Id":69,"DisplayName":"Mehper C. Palavuzlar"}]}",2,
186,5,78,b9e34a27-912c-4ee1-b3d5-4afabeda9000,"2010-07-19 20:19:52",8,"You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales. Using the covariance matrix *standardises* the data.\\n\\nIn general they give different results. Especially when the scales are different.\\n\\nAs example, take a look a look at this R heptathlon data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (200m) are around 20.\\n\\n    library(HSAUR)\\n    # look at heptathlon data\\n    heptathlon\\n    \\n    # correlations\\n    round(cor(heptathlon[,-8]),2)   # ignoring "score" \\n    # covariance\\n    round(cov(heptathlon[,-8]),2)\\n    \\n    # PCA\\n    # scale=T bases the PCA on the correlation matrix\\n    hep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\\n    hep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\\n    \\n    # PC scores per competitor\\n    hep.scores.cor = predict(hep.PC.cor)\\n    hep.scores.cov = predict(hep.PC.cov)\\n    \\n    # Plot of PC1 vs PC2\\n    par(mfrow = c(2, 1))\\n    plot(hep.scores.cov[,1],hep.scores.cov[,2],\\n         xlab="PC 1",ylab="PC 2", pch=NA, main="Covariance")\\n    text(hep.scores.cov[,1],hep.scores.cov[,2],labels=1:25) \\n    \\n    plot(hep.scores.cor[,1],hep.scores.cor[,2],\\n         xlab="PC 1",ylab="PC 2", pch=NA, main="Correlation")\\n    text(hep.scores.cor[,1],hep.scores.cor[,2],labels=1:25) \\n\\n\\nNotice that the outlying individuals (in *this* data set) are outliers regardless of whether the covariance or correlation matrix is used.","Added R code",
187,2,95,43effdb3-3898-4c0a-88c9-9e21e9340fc6,"2010-07-19 20:21:35",57,"I have been using various GARCH-based models to forecast volatility for various North American equities using historical daily data as inputs.\\n\\nAsymmetric GARCH models are often cited as a modification of the basic GARCH model to account for the 'leverage effect' i.e. volatility tends to increase more after a negative return than a similarly sized positive return.\\n\\nWhat kind of a difference would you expect to see between a standard GARCH and an asymmetric GARCH forecast for a broad-based equity index like the S&P 500 or the NASDAQ-100?\\n\\nThere is nothing particularly special about these two indices, but I think it is helpful to give something concrete to focus the discussion, as I am sure the effect would be different depending on the equities used.",,
188,1,95,43effdb3-3898-4c0a-88c9-9e21e9340fc6,"2010-07-19 20:21:35",57,"How Large a Difference Can Be Expected Between Standard GARCH and Asymmetric GARCH Volatility Forecasts?",,
189,3,95,43effdb3-3898-4c0a-88c9-9e21e9340fc6,"2010-07-19 20:21:35",57,<garch><volatility-forecasting>,,
190,2,96,fc27569f-8ee8-41ad-ba12-2794b21c2239,"2010-07-19 20:22:44",5,"Another great resource is [the LearnR blog][1], which went through an extensive study of visualizations with lattice and ggplot2.\\n\\n\\n  [1]: http://learnr.wordpress.com/",,
191,2,97,6dc08214-db6a-4a26-9e69-4e59e6ce94a3,"2010-07-19 20:23:22",114,"I have some [ordinal data][1] gained from survey questions.  In my case they are [Likert style][2] responses (Strongly Disagree-Disagree-Neutral-Agree-Strongly Agree).  In my data they are coded as 1-5.\\n\\nI  don't think means would mean much here, so what basic summary statistics are considered usefull?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Ordinal_scale#Ordinal_scale\\n  [2]: http://en.wikipedia.org/wiki/Likert_scale",,
192,1,97,6dc08214-db6a-4a26-9e69-4e59e6ce94a3,"2010-07-19 20:23:22",114,"What are good basic statistics to use for Ordinal data",,
193,3,97,6dc08214-db6a-4a26-9e69-4e59e6ce94a3,"2010-07-19 20:23:22",114,<ordinal><summary-statistics>,,
194,2,98,d807e739-dba0-4db0-9be0-a60f1949f99b,"2010-07-19 20:23:57",39,"Eliciting priors is a tricky business. \\n\\n[This][1] and [this][2] are quite good practical guides for prior elicitation.  The process is outlined as follows:\\n\\n 1. background and preparation;\\n 2. identifying and recruiting the expert(s);\\n 3. motivation and training the expert(s);\\n 4. structuring and decomposition (typically deciding precisely what variables should\\nbe elicited, and how to elicit joint distributions in the multivariate case);\\n 5. the elicitation itself.\\n\\nThe elicitation results in parameters to fit to distributions modeling expert knowledge (i.e. in the Bayesian context, [Beta distributions][3] are quite popular )\\n\\n\\n  [1]: http://www.stat.cmu.edu/tr/tr808/tr808.pdf\\n  [2]: http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf\\n  [3]: http://en.wikipedia.org/wiki/Beta_distribution",,
195,5,43,fd088df0-cd62-496a-a8ed-bfa7b84575ab,"2010-07-19 20:24:23",74,"R is great, but I wouldn't really call it "windows based" :) That's like saying the cmd prompt is windows based. I guess it is technically in a window...\\n\\nRapidMiner is far easier to use [1]. It's a free, open-source, multi-platform, GUI. Here's a video on time series forecasting:\\n\\nhttp://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1\\n\\nAlso, don't forget to read:\\n\\nhttp://www.forecastingprinciples.com/\\n\\n\\n[1] No, I don't work for them. ","added 64 characters in body",
196,2,99,4381a266-d99d-4860-b452-69b189aff068,"2010-07-19 20:25:08",88,"[multicore][1] is quite nice for tool for making faster scripts faster.  \\n[cacheSweave][2] saves a lot of time when using `Sweave`.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/multicore/index.html \\n  [2]: http://cran.r-project.org/web/packages/cacheSweave/index.html",,
197,16,99,4381a266-d99d-4860-b452-69b189aff068,"2010-07-19 20:25:08",-1,,,
198,2,100,1f7e287e-a7cf-46e0-aa66-37fa813f32cc,"2010-07-19 20:30:23",117,"I'd like to see the answer with qualitative view on the problem, not just definition. Examples and analogous from other areas of applied math also would be good.\\n\\nI understand, my question is silly, but I can't find good and intuitive introduction textbook on signal processing — if someone would suggest one, I will be happy.\\n\\nThanks.",,
199,1,100,1f7e287e-a7cf-46e0-aa66-37fa813f32cc,"2010-07-19 20:30:23",117,"What's the purpose of window function in spectral analysis?",,
200,3,100,1f7e287e-a7cf-46e0-aa66-37fa813f32cc,"2010-07-19 20:30:23",117,<signal-processing>,,
201,2,101,961d9b70-28f4-4279-b2e9-1075ee1d8995,"2010-07-19 20:32:08",NULL,"Understanding p-value\\n\\nSuppose, that you want to test the hypothesis that the average height of male students at your University is 5 ft 7 inches. You collect heights of 100 students selected at random and compute the sample mean (say it turns out to be 5 ft 9 inches). Using an appropriate formula/statistical routine you compute the p-value for your hypothesis and say it turns out to be 0.06. \\n\\nIn order to interpret p=0.06 appropriately, we should keep several things in mind:\\n\\n1. The first step under classical hypothesis testing is the assumption that the hypothesis under consideration is true. (In our context, we assume that the **true** average height is 5 ft 7 inches.\\n\\n2. Imagine doing the following calculation: Compute the probability that the sample mean is greater than 5 ft 9 inches assuming that our hypothesis is in fact correct (see point 1). \\n\\nIn other words, we want to know P(Sample mean >= 5 ft 9 inches | Given true value = 5 ft 7 inches). \\n\\nThe calculation in step 2 is what is called the p-value. Therefore, a p-value of 0.06 would mean that if we were to repeat our experiment many, many times (each time we select 100 students at random and compute the sample mean) then 6 times out of 100 we can expect to see a sample mean greater than 5 ft 9 inches.\\n\\nGiven the above understanding, should we still retain our assumption that our hypothesis is true (see step 1). Well, a p=0.06 indicates that one of two things have happened: \\n\\n(A) Either our hypothesis is correct and an extremely unlikely event has occurred (e.g., all 100 students are student athletes)\\n\\nor\\n\\n(B) Our assumption is incorrect and the sample we have obtained is not that unusual. \\n\\nThe traditional way to choose between (A) and (B) is to choose an arbitrary cut-off for p. We choose (A) if p > 0.05 and (B) if p < 0.05.\\n\\nPS: Need to run now. Will add about the t-test when I find some more time.",,user28
202,2,102,abd65999-5783-4839-a1d8-c759970b1a8c,"2010-07-19 20:32:14",22,"For me [Illuminating the Path][1] report has been always good point of reference.   \\nFor more recent overview you can also have a look at good [article][2] by Heer and colleagues.\\n\\n\\n  [1]: http://nvac.pnl.gov/agenda.stm\\n  [2]: http://queue.acm.org/detail.cfm?id=1805128",,
203,2,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,"2010-07-19 20:33:26",5,"What is the best blog on data visualization?\\n\\nI'm making this question a community wiki since it is highly subjective.  Please limit each answer to one link.",,
204,1,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,"2010-07-19 20:33:26",5,"What is your favorite data visualization blog?",,
205,3,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,"2010-07-19 20:33:26",5,<data-visualization><blog>,,
206,16,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,"2010-07-19 20:33:26",5,,,
207,2,104,862150c8-ed85-492e-b806-83d706588cf6,"2010-07-19 20:34:46",74,"A frequency table is a good place to start. You can do the count, and relative frequency for each level. Also, the total count, and number of missing values may be of use. \\n\\nYou can also use a contingency table to compare two variables at once. Can display using a mosaic plot too.\\n\\n\\n\\n",,
208,6,73,997aaa66-6101-41ed-a0bb-775b5a284c5b,"2010-07-19 20:35:22",69,<r><subjective>,"edited tags",
209,2,105,b036b8cf-e030-4db8-9b61-262e23edae87,"2010-07-19 20:35:34",46,http://flowingdata.com/,,
210,16,105,b036b8cf-e030-4db8-9b61-262e23edae87,"2010-07-19 20:35:34",-1,,,
214,2,107,06cf2365-faf1-4477-b975-faf0ba618162,"2010-07-19 20:36:01",46,http://infosthetics.com/,,
215,16,107,06cf2365-faf1-4477-b975-faf0ba618162,"2010-07-19 20:36:01",-1,,,
216,5,98,21d0b1e0-5f79-40e8-8740-283b529dee65,"2010-07-19 20:36:42",39,"Eliciting priors is a tricky business. \\n\\n[This][1] and [this][2] are quite good practical guides for prior elicitation.  The process is outlined as follows:\\n\\n 1. background and preparation;\\n 2. identifying and recruiting the expert(s);\\n 3. motivation and training the expert(s);\\n 4. structuring and decomposition (typically deciding precisely what variables should\\nbe elicited, and how to elicit joint distributions in the multivariate case);\\n 5. the elicitation itself.\\nOf course, the papers also review how the elicitation results in parameters that may be fit to or define distributions for modeling expert knowledge (for instance, in the Bayesian context, [Beta distributions][3]), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow distributions) .\\n\\n\\n  [1]: http://www.stat.cmu.edu/tr/tr808/tr808.pdf\\n  [2]: http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf\\n  [3]: http://en.wikipedia.org/wiki/Beta_distribution","added 163 characters in body; added 9 characters in body",
217,2,108,0db02b44-3cab-4bd0-9c43-d7dab5a7a4e7,"2010-07-19 20:36:47",5,http://www.informationisbeautiful.net/,,
218,16,108,0db02b44-3cab-4bd0-9c43-d7dab5a7a4e7,"2010-07-19 20:36:47",-1,,,
219,2,109,8d56adbc-decc-49ef-a841-f11b52cc4a84,"2010-07-19 20:37:21",25,"Following one-way ANOVA, there are many possible follow-up multiple comparison tests. Holm's test (or better, the Holm-Sidak) test has lots of power, but because it works in a stepwise manner, it cannot compute confidence intervals. Its advantage over the tests than can compute confidence intervals (Tukey, Dunnett) is that is has more power. But is it fair to say that the Holm method *always* has more power than the methods of Tukey and Dunnet? Or does it depend...?",,
220,1,109,8d56adbc-decc-49ef-a841-f11b52cc4a84,"2010-07-19 20:37:21",25,"Power of Holm's multiple comparison testing compared to others.",,
221,3,109,8d56adbc-decc-49ef-a841-f11b52cc4a84,"2010-07-19 20:37:21",25,<multiple-comparisons><power>,,
222,2,110,6cece4e4-4654-4c82-99f1-38c5e36c0c77,"2010-07-19 20:40:36",61,"It depends on where you apply the window function.  If you do it in the time domain, it's because you only want to analyze the periodic behavior of the function in a short duration.  You do this when you don't believe that your data is from a stationary process.  \\n\\nIf you do it in the frequency domain, then you do it to isolate a specific set of frequencies for further analysis; you do this when you believe that (for instance) high-frequency components are spurious.\\n\\nThe first three chapters of "A Wavelet Tour of Signal Processing" by Stephane Mallat have an excellent introduction to signal processing in general, and chapter 4 goes into a very good discussion of windowing and time-frequency representations in both continuous and discrete time, along with a few worked-out examples.",,
223,2,111,d9a70998-ef1b-4422-a92b-6c37c060bab1,"2010-07-19 20:41:24",8,"In R, the default setting for random number generation are:\\n\\n1.  For U(0,1), use the Mersenne-Twister algorithm\\n2.  For Guassian numbers use  the numerical inversion of the standard normal distribution function. \\n\\nYou can easily check this, viz.\\n\\n    > RNGkind()\\n    [1] "Mersenne-Twister" "Inversion"\\n\\nIt is possible to change the default generator to other PRNGs, such as Super-Duper,Wichmann-Hill, Marsaglia-Multicarry or even a user-supplied PRNG. See the ?RNGkind for further details. I have never needed to change the default PRNG.\\n\\nThe [C GSL][1] library also uses the [Mersenne-Twister][2] by default.\\n\\n\\n  [1]: http://www.gnu.org/software/gsl/manual/html_node/Random-number-environment-variables.html\\n  [2]: http://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html",,
224,5,16,6a7eb0c9-d732-4106-9d16-558372254714,"2010-07-19 20:43:02",8,"Two projects spring to mind:\\n\\n 1. [Bugs][1] - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC.\\n 1. [Bioconductor][2] - perhaps the most popular statistical tool in Bioinformatics. I know it's a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none.\\n\\n\\n  [1]: http://www.mrc-bsu.cam.ac.uk/bugs/\\n  [2]: http://www.bioconductor.org/","added 12 characters in body",
225,5,65,116a8336-b0dc-47c5-890a-23cae0b14435,"2010-07-19 20:44:53",8,"The first formula is the *population* standard deviation and the second formula is the the *sample* standard deviation. The second formula is also related to the unbiased estimator of the variance - see [wikipedia][1] for further details.\\n\\nI suppose (here) in the UK they don't make the distinction between sample and population at high school. They certainly don't touch concepts such as biased estimators. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance","Adding wikipedia link and changing sd to var",
226,6,2,501bfb02-715c-4b57-8557-87b7945312ef,"2010-07-19 20:51:58",124,<normality><distribution>,"edited tags",
227,2,112,61662989-9e3a-40d9-aeca-3892ef038fd7,"2010-07-19 20:52:27",12,"I see all my favorite blogs have been listed. So I'll give you this one:\\n\\nhttp://ilovecharts.tumblr.com/\\n\\nit's a bit light hearted.",,
228,16,112,61662989-9e3a-40d9-aeca-3892ef038fd7,"2010-07-19 20:52:27",-1,,,
229,2,113,055bc3f7-18bf-43f5-b830-0a67a8f3544b,"2010-07-19 20:54:23",39,"I have been looking into theoretical frameworks for method selection and have found very little systematic, mathematically-motivated work. By 'method selection', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type.\\n\\nWhat I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. [Inductive Policy: The Pragmatics of Bias Selection][1]). I may be unrealistic at this early stage of machine learning's development, but I was hoping to find something like what [measurement theory][2] does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems.\\n\\nAny suggestions?\\n\\n\\n  [1]: http://portal.acm.org/citation.cfm?id=218546\\n  [2]: ftp://ftp.sas.com/pub/neural/measurement.html",,
230,1,113,055bc3f7-18bf-43f5-b830-0a67a8f3544b,"2010-07-19 20:54:23",39," What are some good frameworks for method selection?",,
231,3,113,055bc3f7-18bf-43f5-b830-0a67a8f3544b,"2010-07-19 20:54:23",39,<machine-learning><methodology><theory>,,
232,5,98,41368fc2-fc87-494c-9d23-d6d297cb2026,"2010-07-19 20:56:56",39,"Eliciting priors is a tricky business. \\n\\n[Statistical Methods for Eliciting Probability Distributions][1] and [Eliciting Probability Distributions][2] are quite good practical guides for prior elicitation.  The process in both papers is outlined as follows:\\n\\n 1. background and preparation;\\n 2. identifying and recruiting the expert(s);\\n 3. motivation and training the expert(s);\\n 4. structuring and decomposition (typically deciding precisely what variables should\\nbe elicited, and how to elicit joint distributions in the multivariate case);\\n 5. the elicitation itself.\\n\\nOf course, they also review how the elicitation results in information that may be fit to or otherwise define distributions (for instance, in the Bayesian context, [Beta distributions][3]), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow and small-tailed distributions, etc.).\\n\\n\\n  [1]: http://www.stat.cmu.edu/tr/tr808/tr808.pdf\\n  [2]: http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf\\n  [3]: http://en.wikipedia.org/wiki/Beta_distribution","added 2 characters in body; added 22 characters in body; added 76 characters in body",
233,5,91,9a905f43-cc63-4c62-bfff-4265a1ae556a,"2010-07-19 20:58:11",87,"As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. \\n\\nIn the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta "functions" can be used to represent these atoms.    \\n\\n random","changed "variable" to "random variable"",
235,2,114,a6a670ff-46cc-440e-98cd-b6be2d0bee68,"2010-07-19 21:00:53",8,"I currently subscribe to \\n\\n1. Andrew Gelman's [blog][1]\\n2. Christian Roberts' [blog][2]\\n\\nWhat other research level statistical blogs are there. \\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/blog/\\n  [2]: http://xianblog.wordpress.com/",,
236,1,114,a6a670ff-46cc-440e-98cd-b6be2d0bee68,"2010-07-19 21:00:53",8,"What statistical blogs would you recommend?",,
237,3,114,a6a670ff-46cc-440e-98cd-b6be2d0bee68,"2010-07-19 21:00:53",8,<blog>,,
238,2,115,d15020c9-befc-4d3d-8167-b04b4f21501e,"2010-07-19 21:01:35",127,http://datavis.tumblr.com/,,
239,16,115,d15020c9-befc-4d3d-8167-b04b4f21501e,"2010-07-19 21:01:35",-1,,,
241,2,116,cb435917-1818-4068-8b5b-3123b0c37497,"2010-07-19 21:04:16",72,"Cosma Shalizi's [blog][1], often talks about statistics, and is always interesting.\\n\\n\\n  [1]: http://www.cscs.umich.edu/~crshalizi/weblog/",,
242,2,117,6bb1ee1d-c5aa-476e-b9bd-1c275d7fec78,"2010-07-19 21:04:24",36,"http://www.r-bloggers.com/ is an aggregated blog from lots of blogs that talk about statistics using R. Also the [#rstats][1] hashtag on twitter is helpful. I write quite a bit about [statistics and R in genetics research][2].\\n\\n\\n  [1]: http://search.twitter.com/search?q=%23rstats\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/R",,
243,2,118,73b29f38-18d0-4de0-8e6a-d577fed6ecd0,"2010-07-19 21:04:39",83,"In the definition of standard deviation, why do we have to **square** the difference from the mean to get the mean(E) and take the **square root back** at the end? Can't we just simply take **the absolute value** of the difference instead and get the expected value(mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method(the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard?\\n\\nThe Definition of Standard Deviation:\\n\\n![alt text][1]\\n\\n\\nCan't we just take the absolute value instead and still be a good measurement?\\n\\nσ = E[**|X-μ|**]\\n\\n\\n\\n  [1]: http://upload.wikimedia.org/math/9/2/7/9276926b3f1d5663490bc3611300d9e4.png",,
244,1,118,73b29f38-18d0-4de0-8e6a-d577fed6ecd0,"2010-07-19 21:04:39",83,"[Standard deviation] : Why square the difference instead of taking the absolute value?",,
245,3,118,73b29f38-18d0-4de0-8e6a-d577fed6ecd0,"2010-07-19 21:04:39",83,<standard-deviation><definition>,,
246,6,2,7a47c5ea-db3f-4cb1-962d-f67ccab04d50,"2010-07-19 21:04:58",24,<distributions><normality>,"edited tags",
247,6,35,2840a795-c52e-461c-b6c5-e65721dbf40f,"2010-07-19 21:06:54",24,<distributions><modeling><poisson><overdispersion>,"edited tags",
248,2,119,a6b9b9a4-75a3-4922-adf5-21c8b58cbcef,"2010-07-19 21:11:44",88,"There are many reasons; probably the main is that it is a natural parameter of normal distribution.",,
249,5,114,7aa952cb-f74d-4823-89de-e92c5c58be4c,"2010-07-19 21:12:43",8,"I currently subscribe to \\n\\n1. Andrew Gelman's [blog][1]\\n2. Christian Roberts' [blog][2]\\n3. Darren Wilkinson's [blog][3]\\n\\nWhat other research level statistical blogs are there. \\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/blog/\\n  [2]: http://xianblog.wordpress.com/\\n  [3]: http://darrenjw.wordpress.com/","added 72 characters in body",
250,2,120,5675991b-9bcf-4271-bf38-87526d560f36,"2010-07-19 21:14:07",41,"One way you can think of this is that standard deviation is similar to a "distance from the mean".  \\n\\nCompare this to distances in euclidean space - this gives you the true distance, where what you suggested (which, btw, is the [absolute deviation][1]) is more like a [manhattan distance][2] calculation.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Average_absolute_deviation\\n  [2]: http://en.wikipedia.org/wiki/Manhattan_distance_transform",,
251,2,121,759fa59c-050a-4292-ba8b-124da8cc9cca,"2010-07-19 21:14:25",61,"The squared difference has nicer mathematical properties; it's continuously differentiable (nice when you want to minimize it), it's a sufficient statistic for the Gaussian distribution, and it's (a version of) the L2 norm which comes in handy for proving convergence and so on.\\n\\nThe mean absolute deviation (the absolute value notation you suggest) is also used as a measure of dispersion, but it's not as "well-behaved" as the squared error.",,
255,2,123,f2cbcdd7-a858-45fa-8251-441409307447,"2010-07-19 21:15:20",130,"Squaring the difference from the mean has a couple of reasons.\\n\\n - Variance is defined as the 2nd moment of the deviation (the R.V here is (x-$\\mu$) ) and thus the square as moments are simply the expectations of higher powers of the random variable.\\n\\n - Having a square as opposed to the absolute value function gives a nice continuous and differentiable function (absolute value is not differentiable at 0) - which makes it the natural choice, especially in the context of estimation and regression analysis.\\n\\n - The squared formulation also naturally falls out of parameters of the Normal Distribution. ",,
256,2,124,e1474585-8be7-4edb-8ee1-c1aee77ddbd4,"2010-07-19 21:17:30",131,"I'm a programmer without statistical background, and I'm currently looking at different classification methods for a large number of different documents that I want to classify into pre-defined categories. I've been reading about kNN, SVM and NN. However, I have some trouble getting started. What resources do you recommend? I do know single variable and multi variable calculus quite well, so my math should be strong enough. I also own Bishop's book on Neural Networks, but it has proven to be a bit dense as an introduction.",,
257,1,124,e1474585-8be7-4edb-8ee1-c1aee77ddbd4,"2010-07-19 21:17:30",131,"Statistical classification of text",,
258,3,124,e1474585-8be7-4edb-8ee1-c1aee77ddbd4,"2010-07-19 21:17:30",131,<neural-networks><support-vector-machines><k-nearest-neighbour><classification><tutorials>,,
259,2,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,"2010-07-19 21:18:12",5,"Which is the best introductory textbook for Bayesian statistics?\\n\\nOnce book per answer.",,
260,1,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,"2010-07-19 21:18:12",5,"What is the best introductory bayesian statistics textbook?",,
261,3,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,"2010-07-19 21:18:12",5,<bayesian><best-of><textbook>,,
262,16,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,"2010-07-19 21:18:12",5,,,
263,16,114,08537f1e-91e8-43f4-ab43-5840cfedb19e,"2010-07-19 21:18:51",8,,,
264,2,126,d48f2548-bad6-452a-81bc-71dcc6f1764e,"2010-07-19 21:19:43",5,"My favorite is ["Bayesian Data Analysis"][1] by Gelman, et al.\\n\\n\\n  [1]: http://www.amazon.com/exec/obidos/ISBN=158488388X/",,
265,16,126,d48f2548-bad6-452a-81bc-71dcc6f1764e,"2010-07-19 21:19:43",-1,,,
266,2,127,842e06ec-39f7-4110-9190-d26c94ceb3ae,"2010-07-19 21:23:20",61,"Another vote for Gelman et al., but a close second for me -- being of the learn-by-doing persuasion -- is [Bayesian Computation with R][1].\\n\\n\\n  [1]: http://bayes.bgsu.edu/bcwr/",,
267,16,127,842e06ec-39f7-4110-9190-d26c94ceb3ae,"2010-07-19 21:23:20",-1,,,
268,2,128,99075edd-d6f6-42ca-8341-96ae02ce25cc,"2010-07-19 21:23:57",132,"In Plain English, how does one interpret a Bland-Altman plot?  \\n\\nWhat are the advantages of using a Bland-Altman plot over other methods of comparing two different measurement methods?",,
269,1,128,99075edd-d6f6-42ca-8341-96ae02ce25cc,"2010-07-19 21:23:57",132,"How doe one interpret a Bland-Altman plot?",,
270,3,128,99075edd-d6f6-42ca-8341-96ae02ce25cc,"2010-07-19 21:23:57",132,<untagged>,,
271,2,129,7b343dfd-2975-444d-8859-4f21c99f3499,"2010-07-19 21:24:58",8,"I quite like [Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference][1] by Gamerman and Lopes.\\n\\nIt more an introduction to (advanced) Bayesian computation though.\\n\\n\\n  [1]: http://www.amazon.co.uk/o/ASIN/1584885874",,
272,16,129,7b343dfd-2975-444d-8859-4f21c99f3499,"2010-07-19 21:24:58",-1,,,
273,6,103,0b037236-e380-444c-960e-d1f2d130e778,"2010-07-19 21:25:22",5,<data-visualization><blog><best-of>,"edited tags",
274,2,130,7d38aff2-0b66-4ee8-809e-b03963b19a72,"2010-07-19 21:26:27",90,"I had a plan of learning R in the near future. Reading [another question][1] I found out about Clojure. Now I don't know what to do.\\n\\nI think a big **advantage of R** for me is that some people in Economics use it, including one of my supervisors (though the other said: stay away from R!). One **advantage of Clojure** is that it is Lisp-based, and as I have started learning Emacs and I am keen on writing my own customisations, it would be helpful (yeah, I know Clojure and Elisp are different dialects of Lisp, but they are both Lisp and thus similar I would imagine).\\n\\nI can't ask which one is better, because I know this is very personal, but could someone give me the advantages (or advantages) of Clojure x R, especially in practical terms? For example, which one should be easier to learn, which one is more flexible or more powerful, which one has more libraries, more support, more users, etc?\\n\\n**My intended use**: The bulk of my estimation should be done using Matlab, so I am not looking for anything too deep in terms of statistical analysis, but rather a software to substitute Excel for the initial data manipulation and visualisation, summary statistics and charting, but also some basic statistical analysis or the initial attempts at my estimation.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/3/what-are-some-valuable-statistical-analysis-open-source-projects",,
275,1,130,7d38aff2-0b66-4ee8-809e-b03963b19a72,"2010-07-19 21:26:27",90,"Clojure versus R: advantages and disadvantages",,
276,3,130,7d38aff2-0b66-4ee8-809e-b03963b19a72,"2010-07-19 21:26:27",90,<r>,,
277,2,131,69cb5f92-ef39-4fa8-9fac-f627215a08e5,"2010-07-19 21:28:41",5,"For basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as [SICP][1]).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.\\n\\nIn general:\\n\\nThe main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.\\n\\nClojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.\\n\\n\\n  [1]: http://mitpress.mit.edu/sicp/",,
278,2,132,39074a92-6f5b-46ad-91d2-0f14fb8e1428,"2010-07-19 21:29:37",22,"Coming from non-statistical background I found [Introduction to Applied Bayesian Statistics and Estimation for Social Scientists][1] quite informative and easy to follow.\\n\\n\\n  [1]: http://www.princeton.edu/~slynch/bayesbook/bookinfo.html",,
279,16,132,39074a92-6f5b-46ad-91d2-0f14fb8e1428,"2010-07-19 21:29:37",-1,,,
280,2,133,01aabde1-c707-4ff9-b48d-2b6d95944892,"2010-07-19 21:31:53",139,"I don't know how to use SAS/R/Orange, but it sounds like the kind of test you need is a [chi-square test][1]. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Chi-square_test",,
281,5,125,a1d81e68-c963-4c2f-a46e-f864c0acc8c2,"2010-07-19 21:32:09",8,"Which is the best introductory textbook for Bayesian statistics?\\n\\nOne book per answer.","deleted 1 characters in body",
282,2,134,657b3316-c103-42cc-bdd9-a52a5fedc530,"2010-07-19 21:32:38",138,"On smaller window sizes, `n log n` sorting might work. Are there any better algorithms to achieve this?",,
283,1,134,657b3316-c103-42cc-bdd9-a52a5fedc530,"2010-07-19 21:32:38",138,"Algorithms to compute the running median?",,
284,3,134,657b3316-c103-42cc-bdd9-a52a5fedc530,"2010-07-19 21:32:38",138,<algorithms><running><median>,,
285,5,131,258c695e-b291-4fcc-b958-e7aeedf39191,"2010-07-19 21:35:55",5,"Let me start by saying that I love both languages: you can't go wrong with either, and they are certainly better than something like C++ or Java for doing data analysis.\\n\\nFor basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as [SICP][1]).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.  Also, keep in mind that R (or S) is largely derived from Scheme, so you would benefit from Lisp knowledge when using it.\\n\\nIn general:\\n\\nThe main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.\\n\\nClojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.\\n\\n\\n  [1]: http://mitpress.mit.edu/sicp/","added 295 characters in body",
286,2,135,0c522a65-4bb8-4d51-a9f2-fb1e84f7cea4,"2010-07-19 21:36:12",39,"I believe that this calls for a [two-sample Kolmogorov–Smirnov test][1], or the like.  The two-sample Kolmogorov–Smirnov test is based on comparing differences in the [empirical distribution functions][2] (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples.\\n\\nThis test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them, and run it on your sample data.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm\\n  [2]: http://en.wikipedia.org/wiki/Empirical_distribution_function",,
290,6,75,c50a6445-bbdb-481f-8ba0-2a065559b2fa,"2010-07-19 21:37:34",13,<r><books><implementation><possibly-off-topic>,"edited tags",
291,6,130,83e0555c-5fbe-4a16-9703-1d5c16bb9227,"2010-07-19 21:37:58",13,<r><possibly-off-topic>,"edited tags",
292,2,137,066fc9cf-557f-4b54-88cf-51d698c3a63c,"2010-07-19 21:38:09",74,"Do you have a college, university, or city library card? If so you may have free access to Books24x7.com and SpringerLink.com which are e-book repositories. If so, I recommend these books:\\n\\n"Text Mining" by Weiss\\nhttp://www.springerlink.com/content/k46654/?p=b318fa02471245298ce4a9fba9104dd5&pi=0\\n\\n"Text Mining Application Programming", by Konchady\\nhttp://library.books24x7.com/toc.asp?bookid=26322\\n\\nFor software, I recommend RapidMiner or GATE, both free and open-source. \\n\\n\\nThis is my "text mining process":\\n\\n    * collect the documents (usually a web crawl)\\n          o [sample if too large]\\n          o timestamp\\n          o strip out markup\\n    * tokenize\\n    * break into characters, words, n-grams, or sliding windows\\n    * stemming (aka lemmatization)\\n          o [include synonyms]\\n          o see porter algorithm\\n          o pronouns and articles are usually bad predictors\\n    * remove stopwords\\n    * feature vectorization\\n          o binary (appears or doesn’t)\\n          o word count\\n          o relative frequency: tf-idf\\n          o information gain, chi square\\n          o [have a minimum value for inclusion]\\n    * weighting\\n          o weight words at top of document higher?\\n\\nThen you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. ",,
293,2,138,97c3baad-6884-40b3-a72c-429bc15cd147,"2010-07-19 21:38:10",142,"I'm looking to learn R on the cheap. What's the best free resource/book/tutorial for learning R?",,
294,1,138,97c3baad-6884-40b3-a72c-429bc15cd147,"2010-07-19 21:38:10",142,"Resources for Learning R ",,
295,3,138,97c3baad-6884-40b3-a72c-429bc15cd147,"2010-07-19 21:38:10",142,<r>,,
296,2,139,780d9f7a-979b-424e-b746-5a17ad7eb099,"2010-07-19 21:39:17",5,"If I had to choose one thing, make sure that you read <a href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">"The R Inferno"</a>.\\n\\nThere are many good resources on <a href="http://www.r-project.org">the R homepage</a>, but in particular, read <a href="http://cran.r-project.org/doc/manuals/R-intro.pdf">"An Introduction to R"</a> and <a href="http://cran.r-project.org/doc/manuals/R-lang.pdf">"The R Language Definition"</a>.",,
297,6,138,9348f066-4a87-4578-a425-a908fc13c102,"2010-07-19 21:39:33",61,<r><tutorials><open-source>,"edited tags",
299,2,140,4c8b0283-3855-47fb-bdfa-f12323808f95,"2010-07-19 21:39:35",88,"The official guides are pretty nice; check out http://cran.r-project.org/manuals.html . There is also a lot of contributed documentation there.",,
301,2,141,932070fa-305f-4eb1-afc3-e6dd94c83759,"2010-07-19 21:40:02",142,"Light-hearted: http://indexed.blogspot.com/",,
302,16,141,932070fa-305f-4eb1-afc3-e6dd94c83759,"2010-07-19 21:40:02",-1,,,
303,2,142,4c53c0ab-279e-4372-9372-797d9f42e264,"2010-07-19 21:42:57",8,"After you learn the basics, I find the following sites very useful:\\n\\n1. [R-bloggers][1]. \\n1. Subscribing to the [Stack overflow R tag][2].\\n\\n\\n  [1]: http://www.r-bloggers.com/\\n  [2]: http://stackoverflow.com/questions/tagged/R",,
304,6,138,f30d4ad1-6771-46bb-9289-b721d33794ce,"2010-07-19 21:47:25",13,<r><possibly-off-topic><open-source><tutorials>,"edited tags",
305,5,10,a36d340c-e18b-4f66-8268-b51a0188c153,"2010-07-19 21:47:34",24,"Many studies in the social sciences use Likert scales.  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?","fixed punctuation ",
306,2,143,dad82f23-4065-4a78-a65b-a196b92427a0,"2010-07-19 21:48:28",88,"Neural network may be to slow for a large number of documents (also this is now pretty much obsolete).   \\nAnd you may also check Random Forest among classifiers; it is quite fast, scales nice and does not need complex tuning.",,
307,2,144,120be3f7-7b4e-4055-92de-f28358f1f35b,"2010-07-19 21:48:52",22,"[Quick-R][1] can be a good place to start.\\n\\n\\n  [1]: http://www.statmethods.net/index.html",,
308,2,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,"2010-07-19 21:50:16",138,"Where can I find freely accessible data sources?\\n\\nI'm thinking of sites like\\n\\n* [http://www2.census.gov/census_2000/datasets/][1]?\\n\\n\\n\\n  [1]: http://www2.census.gov/census_2000/datasets/",,
309,1,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,"2010-07-19 21:50:16",138,"Free Dataset Resources?",,
310,3,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,"2010-07-19 21:50:16",138,<dataset><linked><data>,,
311,6,130,fe023c9f-0ca0-482a-b4c2-31ed29c4ebf3,"2010-07-19 21:50:20",22,<r><possibly-off-topic><subjective>,"edited tags",
312,16,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,"2010-07-19 21:50:16",138,,,
313,5,135,a5e35103-d092-46c6-a9ba-f70fb8125705,"2010-07-19 21:52:08",39,"I believe that this calls for a [two-sample Kolmogorov–Smirnov test][1], or the like.  The two-sample Kolmogorov–Smirnov test is based on comparing differences in the [empirical distribution functions][2] (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples.  It also generalizes out to a multivariate form.\\n\\nThis test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them (e.g. [fBasics][3]), and run it on your sample data.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm\\n  [2]: http://en.wikipedia.org/wiki/Empirical_distribution_function\\n  [3]: http://cran.r-project.org/web/packages/fBasics/fBasics.pdf","added 136 characters in body",
314,2,146,a78be343-46f5-4238-ac5f-efd0c7bcfc35,"2010-07-19 21:52:51",144,"A while ago a user on R-help mailing list asked about the soundness of using PCA scores in a regression. The user is trying to use one PC scores to explain variation in another PC (see full discussion <a href="http://r.789695.n4.nabble.com/PCA-and-Regression-td2280038.html">here</a>). The answer was that no, this is not sound because PC are orthogonal to each other. Can someone explain in a bit more detail why this is so?",,
315,1,146,a78be343-46f5-4238-ac5f-efd0c7bcfc35,"2010-07-19 21:52:51",144,"PCA scores in multiple regression",,
316,3,146,a78be343-46f5-4238-ac5f-efd0c7bcfc35,"2010-07-19 21:52:51",144,<r><pca><scores><regression>,,
317,2,147,9095f4cf-a12f-43d8-b58a-d4b215201ebb,"2010-07-19 21:53:02",142,"Amazon has free Public Data sets for use with EC2. \\n\\nhttp://aws.amazon.com/publicdatasets/\\n\\nHere's a list: http://developer.amazonwebservices.com/connect/kbcategory.jspa?categoryID=243",,
318,16,147,9095f4cf-a12f-43d8-b58a-d4b215201ebb,"2010-07-19 21:53:02",-1,,,
319,4,130,2a642ba3-fc3b-4750-a155-168ed66fde10,"2010-07-19 21:56:05",90,"Clojure versus R: advantages and disadvantages for data analysis","edited title",
320,2,148,58a77a36-2728-4a65-a3fc-b2d1cce46af6,"2010-07-19 21:58:51",130,"http://infochimps.org/ - is a good resource for free data sets.",,
321,16,148,58a77a36-2728-4a65-a3fc-b2d1cce46af6,"2010-07-19 21:58:51",-1,,,
322,2,149,bbd31e1c-4168-4c50-ab55-07734ebba0e4,"2010-07-19 22:02:10",74,"A principal component is a weighted linear combination of all your variables (X's).\\n\\nexample: PCA1 = 0.1X1 + 0.3X2\\n\\nThere will be one principal component for each X (though in general a small number are selected). \\n\\nThe principal components are created such that they have zero correlation (are orthogonal).\\n\\nTherefore, principal component 1 should not explain any variation in principal component 2.\\n\\nFor a book, I recommend Multivariate Data Analysis by Hair\\n\\nThis is also good: http://www.statsoft.com/textbook/principal-components-factor-analysis/",,
323,6,134,a6573b2a-0409-4d07-98ba-52887586892e,"2010-07-19 22:05:23",88,<algorithms><possibly-off-topic><running-median>,"edited tags",
324,2,150,3175fbeb-0fb0-4828-8b29-5e20e5fdcbd1,"2010-07-19 22:13:29",25,"For complete beginners, try William Briggs [Breaking the Law of Averages: Real-Life Probability and Statistics in Plain English](http://www.amazon.com/Breaking-Law-Averages-Probability-Statistics/dp/0557019907/ref=sr_1_1?ie=UTF8&s=books&qid=1279577542&sr=8-1)",,
325,16,150,3175fbeb-0fb0-4828-8b29-5e20e5fdcbd1,"2010-07-19 22:13:29",-1,,,
326,5,137,d2e7b795-a941-4750-a038-dee8ecba8d72,"2010-07-19 22:23:21",74,"Do you have a college, university, or city library card? If so you may have free access to Books24x7.com and SpringerLink.com which are e-book repositories. If so, I recommend these books:\\n\\n"Text Mining" by Weiss\\nhttp://www.springerlink.com/content/k46654/?p=b318fa02471245298ce4a9fba9104dd5&pi=0\\n\\n"Text Mining Application Programming", by Konchady\\nhttp://library.books24x7.com/toc.asp?bookid=26322\\n\\nFor software, I recommend RapidMiner (with the text plugin), free and open-source. There is also GATE for advanced stuff. \\n\\n\\nThis is my "text mining process":\\n\\n    * collect the documents (usually a web crawl)\\n          o [sample if too large]\\n          o timestamp\\n          o strip out markup\\n    * tokenize\\n    * break into characters, words, n-grams, or sliding windows\\n    * stemming (aka lemmatization)\\n          o [include synonyms]\\n          o see porter algorithm\\n          o pronouns and articles are usually bad predictors\\n    * remove stopwords\\n    * feature vectorization\\n          o binary (appears or doesn’t)\\n          o word count\\n          o relative frequency: tf-idf\\n          o information gain, chi square\\n          o [have a minimum value for inclusion]\\n    * weighting\\n          o weight words at top of document higher?\\n\\nThen you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. ","added 49 characters in body",
327,2,151,75f4377a-c1f0-4599-aa96-f4e97a7721e1,"2010-07-19 22:31:12",81,"If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.\\n\\nThe benefits of squaring include:\\n\\n - Squaring always gives a positive\\n   value, so the sum will not be zero.\\n - Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).\\n\\nSquaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.\\n\\nI suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes.\\n\\nIt's important to note however that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.\\n\\nMy view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.\\n\\nAn much more indepth analysis can be read [here][1].\\n\\n \\n\\n\\n  [1]: http://www.leeds.ac.uk/educol/documents/00003759.htm",,
328,2,152,199802a8-012c-455e-b18a-f5f4d075fdff,"2010-07-19 22:37:38",NULL,"Label switching (i.e., the posterior distribution is invariant to switching component labels) is a problematic issue when using MCMC to estimate mixture models. \\n\\n1. Is there a standard (as in widely accepted) methodology to deal with the issue?\\n\\n2. If there is no standard approach then what are the pros and cons of the leading approaches to solve the label switching problem?",,user28
329,1,152,199802a8-012c-455e-b18a-f5f4d075fdff,"2010-07-19 22:37:38",NULL,"Is there a standard method to deal with label switching problem in MCMC estimation of mixture models?",,user28
330,3,152,199802a8-012c-455e-b18a-f5f4d075fdff,"2010-07-19 22:37:38",NULL,<bayesian><mcmc>,,user28
331,2,153,7d59cfbb-85fe-4883-b0e4-41c98d275c49,"2010-07-19 22:39:27",145,"The simple answer is that Likert scales are always ordinal. The intervals between positions on the scale are monotonic but never so well-defined as to be numerically uniform increments.\\n\\nThat said, the distinction between ordinal and interval is based on the specific demands of the analysis being performed. Under special circumstances, you may be able to treat the responses as if they fell on an interval scale. To do this, typically the respondents need to be in close agreement regarding the meaning of the scale responses and the analysis (or the decisions made based on the analysis) should be relatively insensitive to problems that may arise.",,
332,2,154,b8495b04-69fb-47e6-a7ee-e72dc2bf1271,"2010-07-19 22:40:47",108,"I am currently researching the *trial roulette method* for my masters thesis as an elicitation technique. This is a graphical method that allows an expert to represent her subjective probability distribution for an uncertain quantity.\\n\\nExperts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up to 1 - for example 20 chips of probability = 0.05 each. They are then instructed to arrange them on a pre-printed grid, with bins representing result intervals. Each column would represent their belief of the probability of getting the corresponding bin result.\\n\\nSome reasons in favour of using this technique are:\\n\\n1. Many questions about the shape of the expert's subjective probability distribution can be answered without the need to pose a long series of questions to the expert - the statistician can simply read off density above or below any given point, or that between any two points. \\n2. During the elicitation process, the experts can move around the chips if unsatisfied with the way they placed them initially - thus they can be sure of the final result to be submitted. \\n3. It forces the expert to be coherent in the set of probabilities that are provided. If all the chips are used, the probabilities must sum to one.\\n4. Graphical methods seem to provide more accurate results, especially for participants with modest levels of statistical sophistication.\\n",,
333,2,155,53e57bd0-d1a8-4c0d-92da-11a2f6a954d0,"2010-07-19 22:43:50",154,"I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explain a difficult statistical concept?",,
334,1,155,53e57bd0-d1a8-4c0d-92da-11a2f6a954d0,"2010-07-19 22:43:50",154,"What is your favorite layman's explanation for a difficult statistical concept?",,
335,3,155,53e57bd0-d1a8-4c0d-92da-11a2f6a954d0,"2010-07-19 22:43:50",154,<layman>,,
336,2,156,eace5f99-4af8-494a-83b2-b1736167e248,"2010-07-19 22:50:13",148,"I know this must be standard material, but I had difficulty in finding the proof in this form in the statistics books that I picked up in the library.\\n\\nLet e be a white Gaussian vector of size N.  Let all the other matrices in the following be constant.\\n\\nLet v = X*y + e, where X is an N x L matrix and y is an N x 1 vector, and let\\n\\ny_bar = (X^T*X)^(-1)X^T*v\\n\\ne_bar = v - X*y_bar. \\n\\nIf c is any constant vector, J = N - rank(X), and \\n\\nu = c^T*y_bar\\n\\ns^2 = e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c,\\n\\nthen the random variable defined as\\n\\nt = u/sqrt(s^2/J)  \\n\\nfollows a normalized student t distribution with J degrees of freedom.\\n\\nI would be grateful if you could provide an outline for its proof. \\n\\n",,
337,1,156,eace5f99-4af8-494a-83b2-b1736167e248,"2010-07-19 22:50:13",148,"From the general linear model to a t variable",,
338,3,156,eace5f99-4af8-494a-83b2-b1736167e248,"2010-07-19 22:50:13",148,<untagged>,,
339,5,151,7fadd5e8-e116-444c-b4b8-084b020c9475,"2010-07-19 22:50:32",81,"If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.\\n\\nThe benefits of squaring include:\\n\\n - Squaring always gives a positive\\n   value, so the sum will not be zero.\\n - Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).\\n\\nSquaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.\\n\\nI suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)\\n\\n***It's important to note*** however that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.\\n\\nMy view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.\\n\\nAn much more indepth analysis can be read [here][1].\\n\\n \\n\\n\\n  [1]: http://www.leeds.ac.uk/educol/documents/00003759.htm","added 304 characters in body",
340,2,157,a837e749-584f-4ed4-a120-5be4ae1d16e1,"2010-07-19 22:52:22",36,"Definitely the Monty Hall Problem. http://en.wikipedia.org/wiki/Monty_Hall_problem",,
341,5,155,efc25937-3798-4e5b-aca0-c49f4623ea73,"2010-07-19 22:55:13",154,"I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept?\\n\\nMy favorite is <a href="http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Co-integration/Murray93DrunkAndDog.pdf">Murray's</a> explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their first differences are stationary.\\n\\n> The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But\\n> periodically she intones "Oliver, where are you?", and Oliver interrupts his aimless\\n> wandering to bark. He hears her; she hears him. He thinks, "Oh, I can't let her get too far\\n> off; she'll lock me out." She thinks, "Oh, I can't let him get too far off; he'll wake\\n> me up in the middle of the night with his barking." Each assesses how far\\n> away the other is and moves to partially close that gap.\\n\\n\\n","added 895 characters in body; added 1 characters in body",
343,2,159,b36b2449-95b2-4aeb-9439-c78823ac4c0b,"2010-07-19 23:00:30",145,"[Junk Charts][1] is always interesting and thought-provoking, usually providing both criticism of visualizations in the popular media and suggestions for improvements.\\n\\n\\n  [1]: http://junkcharts.typepad.com/",,
344,16,159,b36b2449-95b2-4aeb-9439-c78823ac4c0b,"2010-07-19 23:00:30",-1,,,
345,2,160,60f83d0d-4ec7-4b21-ae7f-428ec8f76919,"2010-07-19 23:06:43",158,http://dataspora.com/blog/,,
346,16,160,60f83d0d-4ec7-4b21-ae7f-428ec8f76919,"2010-07-19 23:06:43",-1,,,
347,2,161,ad921ca4-df6a-4994-8201-0b71b8d3ca1f,"2010-07-19 23:11:36",154,"Econometricians often talk about a time series being *integrated with order k, I(k)*. *k* being the minimum number of differences required to obtain a stationary time series.\\n\\nWhat methods or statistical tests can be used to determine, given a level of confidence, the *order of integration* of a time series?",,
348,1,161,ad921ca4-df6a-4994-8201-0b71b8d3ca1f,"2010-07-19 23:11:36",154,"What methods can be used to determine the Order of Integration of a time series?",,
349,3,161,ad921ca4-df6a-4994-8201-0b71b8d3ca1f,"2010-07-19 23:11:36",154,<timeseries>,,
350,2,162,215c226e-ad7c-4e4d-8641-b92b06d2e5aa,"2010-07-19 23:13:32",74,"These aren't exactly "difficult" concepts. Depends how lay the man is I suppose...\\n\\nIf you carved your distribution out of wood, and tried to balance it on your finger, the balance point would be the mean.\\n\\nIf you put a stick in the middle of your scatter plot, and attached the stick to each data point with a spring, the resting point of the stick would be your regression line.\\n\\n",,
351,2,163,0070f18f-0884-4f02-b30f-e07608c0bfe7,"2010-07-19 23:17:53",81,"Lets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.\\n\\nWhichever number the die lands on is the number of free text-books I will give you.\\n\\nIn this case, the amount of text books that I give you is the ***random variable*** because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die).",,
352,2,164,08cb40e3-a8e1-4e86-a923-8efd52158cd0,"2010-07-19 23:19:44",158,"For governmental data:\\n\\nUS: http://www.data.gov/\\n\\nWorld: http://www.guardian.co.uk/world-government-data\\n",,
353,16,164,08cb40e3-a8e1-4e86-a923-8efd52158cd0,"2010-07-19 23:19:44",-1,,,
354,2,165,30dd10d3-03ec-4e8d-93ed-857371beea0c,"2010-07-19 23:21:05",74,"I've never found a great explanation of this technique. Thanks in advance!",,
355,1,165,30dd10d3-03ec-4e8d-93ed-857371beea0c,"2010-07-19 23:21:05",74,"Please explan Markov Chain Monte Carlo (MCMC) in plain English",,
356,3,165,30dd10d3-03ec-4e8d-93ed-857371beea0c,"2010-07-19 23:21:05",74,<mcmc>,,
357,5,156,47ef8869-bcb4-4e23-90b8-c495ece4d609,"2010-07-19 23:21:08",148,"I know this must be standard material, but I had difficulty in finding a proof in this form.\\n\\nLet e be a white Gaussian vector of size N.  Let all the other matrices in the following be constant.\\n\\nLet v = X*y + e, where X is an N x L matrix and y is an N x 1 vector, and let\\n\\ny_bar = (X^T*X)^(-1)X^T*v\\n\\ne_bar = v - X*y_bar. \\n\\nIf c is any constant vector, J = N - rank(X), and \\n\\nu = c^T*y_bar\\n\\ns^2 = e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c,\\n\\nthen the random variable defined as\\n\\nt = u/sqrt(s^2/J)  \\n\\nfollows a normalized student t distribution with J degrees of freedom.\\n\\nI would be grateful if you could provide an outline for its proof. \\n\\n","deleted 58 characters in body",
358,2,166,327722a1-82d7-47c2-a31a-4bf18f58364c,"2010-07-19 23:21:35",154,"Australia is currently having an election and understandably the media reports new political poll results daily. In a country of 22 million what percentage of the population would need to be sampled to get a statistically valid result?\\n\\nIs it possible that using too large a sample could affect the results, or does statistical validity monotonically increase with sample size?",,
359,1,166,327722a1-82d7-47c2-a31a-4bf18f58364c,"2010-07-19 23:21:35",154,"How do you decide the sample size when polling a large population?",,
360,3,166,327722a1-82d7-47c2-a31a-4bf18f58364c,"2010-07-19 23:21:35",154,<polling>,,
361,5,155,22c29e8a-c1d5-40d1-b281-2fb3ec053f50,"2010-07-19 23:22:33",154,"I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept?\\n\\nMy favorite is <a href="http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Co-integration/Murray93DrunkAndDog.pdf">Murray's</a> explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their joint first differences are stationary.\\n\\n> The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But\\n> periodically she intones "Oliver, where are you?", and Oliver interrupts his aimless\\n> wandering to bark. He hears her; she hears him. He thinks, "Oh, I can't let her get too far\\n> off; she'll lock me out." She thinks, "Oh, I can't let him get too far off; he'll wake\\n> me up in the middle of the night with his barking." Each assesses how far\\n> away the other is and moves to partially close that gap.\\n\\n\\n","added 6 characters in body",
362,2,167,6c2eb023-f320-4f0e-b5fd-d11e89589648,"2010-07-19 23:26:31",159,"Principal components are orthogonal by definition, so any pair of PCs will have zero correlation.\\n\\nHowever, PCA can be used in regression if there are a large number of explanatory variables. These can be reduced to a small number of principal components and used as predictors in a regression.",,
363,2,168,4b64323b-7e66-4e8c-afd6-2313c9e8a4ae,"2010-07-19 23:26:44",8,"For univariate kernel density estimators (KDE), I use Silverman's rule for calculating h:\\n\\n0.9 min(sd, IQR/1.34)*n^(-0.2)\\n\\nWhat are the standard rules for multivariate KDE (assuming a Normal kernel).\\n\\n",,
364,1,168,4b64323b-7e66-4e8c-afd6-2313c9e8a4ae,"2010-07-19 23:26:44",8,"Choosing a Bandwidth for kernel density estimators",,
365,3,168,4b64323b-7e66-4e8c-afd6-2313c9e8a4ae,"2010-07-19 23:26:44",8,<kde><kernel>,,
366,6,95,a95e4349-8eda-4acc-9397-8b34780cfaec,"2010-07-19 23:27:03",57,<time-series><garch><volatility-forecasting>,"edited tags",
367,2,169,4a818ce5-fa98-40da-9e53-2988eff93f75,"2010-07-19 23:27:36",159,"For time series data, try the [Time Series Data Library][1].\\n\\n\\n  [1]: http://robjhyndman.com/TSDL",,
368,16,169,4a818ce5-fa98-40da-9e53-2988eff93f75,"2010-07-19 23:27:36",-1,,,
369,2,170,7791570c-de35-41e0-98d2-55119430df0e,"2010-07-19 23:29:54",8,"Are there any free statistical textbooks available? \\n\\n",,
370,1,170,7791570c-de35-41e0-98d2-55119430df0e,"2010-07-19 23:29:54",8,"Free statistical textbooks",,
371,3,170,7791570c-de35-41e0-98d2-55119430df0e,"2010-07-19 23:29:54",8,<teaching><textbook>,,
372,2,171,03f6e514-4551-4772-bbe3-ea8c5a244b49,"2010-07-19 23:32:30",159,"There are a number of statistical tests (known as "unit root tests") for dealing with this problem. The most popular is probably the "Augmented Dickey-Fuller" (ADF) test, although the Phillips-Perron (PP) test and the KPSS test are also widely used. \\n\\nBoth the ADF and PP tests are based on a null hypothesis of a unit root (i.e., an I(1) series). The KPSS test is based on a null hypothesis of stationarity (i.e., an I(0) series). Consequently, the KPSS test can give quite different results from the ADF or PP tests.",,
373,2,172,7a5a2d5b-b5d1-460d-8dc2-abbee8df947e,"2010-07-19 23:34:18",74,"It doesn't much depend on the population size, which is counter-intuitive to many.\\n\\nMost polling companies use 400 or 1000 people in their samples.\\n\\nThere is a reason for this:\\n\\nA sample size of 400 will give you a confidence interval of +/-5% 19 times out of 20 (95%)\\n\\nA sample size of 1000 will give you a confidence interval of +/-3% 19 times out of 20 (95%)\\n\\nWhen the proportion that you are measuring is near 50% anyways. \\n\\nThis calculator isn't bad:\\n\\nhttp://www.raosoft.com/samplesize.html\\n",,
374,2,173,f37eaaad-cc84-4e94-b158-83ba39fe93ae,"2010-07-19 23:37:22",71,"I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc.  I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not.  Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:\\n\\n![alt text][1]\\n\\n![alt text][2]\\n\\n\\nI've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.\\n\\n\\n  [1]: http://img827.imageshack.us/img827/1927/activetbcases.png "Cases by month"\\n  [2]: http://img827.imageshack.us/img827/4348/tbcasedistribution.png "Distribution of counts"",,
375,1,173,f37eaaad-cc84-4e94-b158-83ba39fe93ae,"2010-07-19 23:37:22",71,"Time series for count data, with counts < 20",,
376,3,173,f37eaaad-cc84-4e94-b158-83ba39fe93ae,"2010-07-19 23:37:22",71,<r><time-series><count-data><epidemiology>,,
377,2,174,f0c672c7-b4ee-43ee-9e32-cd0366e43192,"2010-07-19 23:37:43",159,"The most widely used and probably the best of what is available is\\nhttp://www.statsoft.com/textbook/\\n\\nOther online stats books include\\n\\n - http://davidmlane.com/hyperstat/\\n - http://faculty.vassar.edu/lowry/webtext.html\\n - http://www.psychstat.missouristate.edu/multibook2/mlt.htm\\n - http://bookboon.com/uk/student/statistics\\n - http://www.freebookcentre.net/SpecialCat/Free-Statistics-Books-Download.html",,
378,6,161,1a3b3122-9113-4ac4-bad7-f624f3299ed4,"2010-07-19 23:39:49",159,<time-series>,"edited tags",
379,2,175,dd786a6b-9a21-4304-839f-a31b55ba8146,"2010-07-19 23:39:49",13,"Often times a statistical analyst is handed a set dataset and asked to fit a modle using a technique such as linear regression.  Very frequently the dataset is accompanied with a disclaimer similar to "Oh yeah, we messed up collecting some of these data points- do what you can".\\n\\nThis situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following:\\n\\n  - It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it "makes the fit look bad".\\n\\n  - In real life, the people who collected the data are frequently not available to answer questions such as "when generating this data set, which of the points did you mess up, exactly?"\\n\\nWhat statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis?\\n\\nAre there any special considerations for multilinear regression?",,
380,1,175,dd786a6b-9a21-4304-839f-a31b55ba8146,"2010-07-19 23:39:49",13,"How should outliers be dealt with in linear regression analysis?",,
381,3,175,dd786a6b-9a21-4304-839f-a31b55ba8146,"2010-07-19 23:39:49",13,<outliers><regression>,,
382,2,176,c50d8fa9-d796-4343-8ca0-69f273c28dfd,"2010-07-19 23:40:01",81,"Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book\\n\\nThe Frequentist would say that each outcome has an equal 1 in 6 chance of occurring.\\n\\nThe Bayesian however would say hang on a second, I know that man, he's David Blane, a famous trickster, I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on Heads BUT I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll increase the chance from 1% to something slightly higher, otherwise I'll reduce it. \\n",,
383,2,177,0a265080-0dc2-44ce-b0bd-05e812201416,"2010-07-19 23:45:44",159,"Rather than exclude outliers, you can use a robust method of regression. In R, for example, the [`rlm()` function from the MASS package][1] can be used instead of the `lm()` function. The method of estimation can be tuned to be more or less robust to outliers.\\n\\n\\n  [1]: http://sekhon.berkeley.edu/library/MASS/html/rlm.html",,
384,2,178,2c98faff-dc16-4f10-98c6-904ad806334b,"2010-07-19 23:48:50",74,"[RapidMiner][1] for data mining\\n\\n[GATE][2] for text mining\\n\\n\\n  [1]: http://rapid-i.com/\\n  [2]: http://gate.ac.uk/",,
385,16,178,2c98faff-dc16-4f10-98c6-904ad806334b,"2010-07-19 23:48:50",-1,,,
386,5,156,e4a1683e-8793-46ad-bfe2-11bff2d228d0,"2010-07-19 23:52:14",148,"I know this must be standard material, but I had difficulty in finding a proof in this form.\\n\\nLet e be a standard white Gaussian vector of size N.  Let all the other matrices in the following be constant.\\n\\nLet v = X*y + e, where X is an N x L matrix and y is an N x 1 vector, and let\\n\\ny_bar = (X^T*X)^(-1)X^T*v\\n\\ne_bar = v - X*y_bar. \\n\\nIf c is any constant vector, J = N - rank(X), and \\n\\nu = c^T*y_bar\\n\\ns^2 = e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c,\\n\\nthen the random variable defined as\\n\\nt = u/sqrt(s^2/J)  \\n\\nfollows a normalized student t distribution with J degrees of freedom.\\n\\nI would be grateful if you could provide an outline for its proof. \\n\\n","added 9 characters in body",
387,5,86,6d5e4008-cee7-4285-8337-8c35ca0b8bc4,"2010-07-19 23:58:01",13,"Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather **statistical properties** such as the **distribution** of the random variable may be proscribed.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.  \\n\\nRandom variables may be classified as *discreet* if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is *continuous* and is used if the distribution covers values from an uncountable set such as the real numbers.","added 1 characters in body",
388,2,179,1545a29e-3032-4028-92ed-0c92a1927aa8,"2010-07-19 23:59:29",159,"For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example,\\n\\n    plot(density(precip, bw="SJ"))\\n\\nThe situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The [ks package in R][1] provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/ks/",,
389,2,180,d8b94140-6161-4562-9581-83febc83ccfd,"2010-07-20 00:06:20",90,"I really like the [FRED][1], from the St. Louis Fed (economics data). You can chart the series or more than one series, you can do some transformations to your data and chart it, and the NBER recessions are shaded.\\n\\n\\n  [1]: http://research.stlouisfed.org/fred2/",,
390,16,180,d8b94140-6161-4562-9581-83febc83ccfd,"2010-07-20 00:06:20",-1,,,
391,2,181,b1d1b7fc-6b47-42b2-8ce3-4e3af217fbb5,"2010-07-20 00:15:02",159,"Is there a standard and accepted method for selecting the number of layers, and the number of nodes in each layer, in a FF NN? I'm interested in automated ways of building neural networks.",,
392,1,181,b1d1b7fc-6b47-42b2-8ce3-4e3af217fbb5,"2010-07-20 00:15:02",159,"How to choose the number of hidden layers and nodes in a feedforward neural network?",,
393,3,181,b1d1b7fc-6b47-42b2-8ce3-4e3af217fbb5,"2010-07-20 00:15:02",159,<neural-networks>,,
394,2,182,3f2efcec-0d7a-416d-a895-cd77c774e37b,"2010-07-20 00:15:47",74,"Sometimes outliers are bad data, and should be excluded, such as typos. Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept. \\n\\nOutlier detection methods include:\\n\\nUnivariate -> boxplot\\n\\nBivariate -> scatterplot with confidence ellipse\\n\\nMultivariate -> Mahalanobis D2 distance\\n\\nMark those observations as outliers.\\n\\nRun a logistic regression (on Y=IsOutlier) to see if there are any systematic patterns. \\n\\nRemove ones that you can demonstrate they are not representative of any sub-population. \\n",,
395,6,21,de547e71-54d9-4f9f-b9e0-800b23e1ad9f,"2010-07-20 00:19:20",159,<population><census><forecasting>,"edited tags",
396,6,33,4e4ca71f-03be-4224-aa7b-06cc89027900,"2010-07-20 00:20:37",159,<r><statistical-analysis><seasonality>,"edited tags",
397,2,183,7c3b015d-c52b-4c9f-a107-a67c71be82c8,"2010-07-20 00:20:51",166,"I need to analyze the 100k MovieLens dataset for clustering with two algorithms of my choice, between the likes of k-means, agnes, diana, dbscan, and several others. What tools (like Rattle, or Weka) would be best suited to help me make some simple clustering analysis over this dataset?",,
398,1,183,7c3b015d-c52b-4c9f-a107-a67c71be82c8,"2010-07-20 00:20:51",166,"What tools could be used for applying clustering algorithms on MovieLens?",,
399,3,183,7c3b015d-c52b-4c9f-a107-a67c71be82c8,"2010-07-20 00:20:51",166,<clustering>,,
400,10,145,fa10f885-f225-4626-a896-0f0b6123c342,"2010-07-20 00:21:48",-1,"{"OriginalQuestionIds":[7],"Voters":[{"Id":74,"DisplayName":"el chief"},{"Id":13,"DisplayName":"Sharpie"},{"Id":8,"DisplayName":"Colin Gillespie"},{"Id":88,"DisplayName":"mbq"},{"Id":80,"DisplayName":"Fabian Steeg"}]}",1,
401,5,145,e222a997-773f-4b6a-b4da-57895b07a4e0,"2010-07-20 00:21:48",-1,"> **Possible Duplicate:**  
> [Locating freely available data samples](http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples)  

<!-- End of automatically inserted text -->

Where can I find freely accessible data sources?\\n\\nI'm thinking of sites like\\n\\n* [http://www2.census.gov/census_2000/datasets/][1]?\\n\\n\\n\\n  [1]: http://www2.census.gov/census_2000/datasets/","insert duplicate link",
402,2,184,8553d06a-a42c-47c7-ba1e-c233c802d9cb,"2010-07-20 00:21:58",159,"Try using the `stl()` function for time series decomposition. It provides a very flexible method for extracting a seasonal component from a time series.",,
403,5,165,fc59b87b-60cf-482e-bf69-b444ef495128,"2010-07-20 00:22:24",74,"Maybe the concept, why it's used, and an example. Thanks!","deleted 17 characters in body; edited title",
404,4,165,fc59b87b-60cf-482e-bf69-b444ef495128,"2010-07-20 00:22:24",74,"How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?","deleted 17 characters in body; edited title",
405,2,185,8676cc2b-d6d0-4998-8575-5aeecf93ae8b,"2010-07-20 00:30:00",80,"A great introductory text covering the topics you mentioned is [Introduction to Information Retrieval][1], which is available online in full text for free.\\n\\n![Introduction to Information Retrieval][2]\\n\\n  [1]: http://www.informationretrieval.org\\n  [2]: http://nlp.stanford.edu/IR-book/iir.jpg",,
407,2,187,3d84abd0-1e90-4c09-b816-0def604184dd,"2010-07-20 00:47:45",119,"As far as I know there is no way to select automatically the number of layers and neurons in each layer. But there are networks that can build automatically their topology, like EANN (Evolutionary Artificial Neural Networks, which use Genetic Algorithms to evolved the topology).\\n\\nThere are several approaches, a more or less modern one that seemed to give good results was NEAT (Neuro Evolution of Augmented Topologies). You can get more info:\\n\\nhttp://nn.cs.utexas.edu/?neat",,
408,2,188,284bc08b-1cfa-4136-9f6d-4992a16ebe4e,"2010-07-20 00:52:13",61,"I'd probably say something like this:\\n\\n"Anytime we want to talk about probabilities, we're really integrating a density.  In Bayesian analysis, a lot of the densities we come up with aren't analytically tractable: you can only integrate them -- if you can integrate them at all -- with a great deal of suffering.  So what we do instead is simulate the random variable a lot, and then figure out probabilities from our simulated random numbers.  If we want to know the probability that X is less than 10, we count the proportion of simulated random variable results less than 10 and use that as our estimate.  That's the "Monte Carlo" part, it's an estimate of probability based off of random numbers.  With enough simulated random numbers, the estimate is very good, but it's still inherently random.\\n\\n"So why "Markov Chain"?  Because under certain technical conditions, you can generate a memoryless process (aka a Markovian one) that has the same limiting distribution as the random variable that you're trying to simulate.  You can iterate any of a number of different kinds of simulation processes that generate correlated random numbers (based only on the current value of those numbers), and you're guaranteed that once you pool enough of the results, you will end up with a pile of numbers that looks "as if" you had somehow managed to take independent samples from the complicated distribution you wanted to know about.\\n\\n"So for example, if I want to estimate the probability that a standard normal random variable was less than 0.5, I could generate ten thousand independent realizations from a standard normal distribution and count up the number less than 0.5; say I got 6905 that were less than 10000; my estimate for P(Z<0.5) would be 0.6905, which isn't that far off from the actual value.  That'd be a Monte Carlo estimate.\\n\\n"Now imagine I couldn't draw independent normal random variables, instead I'd start at 0, and then with every step add some uniform random number between -0.5 and 0.5 to my current value, and then decide based on a particular test whether I liked that new value or not; if I liked it, I'd use the new value as my current one, and if not, I'd reject it and stick with my old value.  Because I only look at the new and current values, this is a Markov chain.  If I set up the test to decide whether or not I keep the new value correctly (it'd be a random walk Metropolis-Hastings, and the details get a bit complex), then even though I never generate a single normal random variable, if I do this procedure for long enough, the list of numbers I get from the procedure will be distributed like a large number of draws from something that generates normal random variables.  This would give me a Markov Chain Monte Carlo simulation for a standard normal random variable.  If I used this to estimate probabilities, that would be a MCMC estimate."",,
410,2,189,473253fd-8480-4717-ac83-e91d48ca25f4,"2010-07-20 00:59:34",173,"For a continuous random variable you can always approximate the pdf by calculating (CDF(x2) - CDF(x1))/(x2 - x1) where x1 and x2 are on either side of the point where you want to know the pdf and the distance |x2 - x1| is small.",,
411,2,190,a2ab6a04-abdb-418c-a30c-b9e5abf70195,"2010-07-20 01:07:38",25,"[A New View of Statistics](http://www.sportsci.org/resource/stats/) by Will G. Hopkins is great!",,
412,4,156,e052614d-50a9-4dfe-88fa-5c99fe930f23,"2010-07-20 01:13:21",148,"How to get to a t variable from linear regression","edited title",
413,2,191,1b9cce05-ba3a-41e5-a3c8-1c4c881ead89,"2010-07-20 01:17:17",173,"The Bland-Altman plot is more widely known as the **Tukey Mean-Difference Plot** (one of many charts devised by John Tukey http://en.wikipedia.org/wiki/John_Tukey).\\n\\nThe idea is that x-axis is the mean of your two measurements, which is your best guess as to the "correct" result and the y-axis is the difference between the two measurement differences. The chart can then highlight certain types of anomalies in the measurements. For example, if one method always gives too high a result, then you'll get all of your points above or all below the zero line. It can also reveal, for example, that one method over-estimates high values and under-estimates low values.\\n\\nIf you see the points on the Bland-Altman plot scattered all over the place, above and below zero, then the suggests that there is no consistent bias of one approach versus the other (of course, there could be hidden biases that this plot does not show up).\\n\\nEssentially, it is a good first step for exploring the data. Other techniques can be used to dig into more particular sorts of behaviour of the measurements.",,
414,2,192,2d896c09-3484-41ad-ae2f-3f6c0e6f0bb8,"2010-07-20 01:18:11",1356,"I'm aware that this one is far from *yes or no* question, but I'd like to know which techniques do you prefer in categorical data analysis - i.e. cross tabulation with two categorical variables.\\n\\nI've come up with: \\n\\n - &chi;<sup>2</sup> test - well, this is quite self-explanatory\\n  - Fisher's exact test - when n < 40,\\n  - Yates' continuity correction - when n > 40,\\n - Cramer's V - measure of association for tables which have more than *2 x 2* cells,\\n - &Phi; coefficient - measure of association for *2 x 2* tables,\\n - contingency coefficient (C) - measure of association for *n x n* tables,\\n - odds ratio - independence of two categorical variables,\\n - McNemar marginal homogeniety test,\\n\\nAnd my question here is: ***which statistical techniques for cross-tabulated data (two categorical variables) do you consider relevant (and why)?***",,
415,1,192,2d896c09-3484-41ad-ae2f-3f6c0e6f0bb8,"2010-07-20 01:18:11",1356,"Cross tabulation of two categorical variables: recommended techniques",,
416,3,192,2d896c09-3484-41ad-ae2f-3f6c0e6f0bb8,"2010-07-20 01:18:11",1356,<statistical-analysis><nonparametric><contingency-tables><categorical-data>,,
417,2,193,ef966819-8b18-491b-8c0c-d983c5d98dc7,"2010-07-20 01:45:12",NULL,"Suppose that you want to know what percentage of people would vote for a particular candidate (say, pi. Note: by definition pi is between 0 and 100). You sample N voters at random to find out how they would vote and your survey of these N voters tells you that the percentage is p. So, you would like to establish a confidence interval for the true percentage. \\n\\nIf you assume that p is normally distributed (an assumption that may or may not be justified depending on how 'big' N is) then your confidence interval for pi would be of the following form:\\n\\nCI = [ p - k * sd(p) , p + k * sd(p)]\\n\\nwhere k is a constant that depends on the extent of confidence you want (i.e., 95% or 99% etc).\\n\\nFrom a polling perspective, you want the width of your confidence interval to be 'low'. Usually, pollsters work with the margin of error which is basically one-half of the CI. In other words:\\n\\nMargin of Error = k * sd(p). \\n\\nHere is how we would go about calculating sd(p).\\n\\nBy definition, p = Sum X_i / N \\n\\nwhere,\\n\\nX_i = 1 if voter i votes for candidate and 0 otherwise.\\n\\nSince, we sampled the voters at random, we could assume that X_i is a i.i.d bernoulli random variable. Therefore, \\n\\nVariance(P) =  V ( Sum X_i / N) = [ Sum V(X_i) ] / N^2 = N pi (1-pi) / N^2 = pi  (1-pi) / N.\\n\\nThus, \\n\\nsd(p) = sqrt ( pi * (1-pi) / N)\\n\\nNow to estimate margin of error we need to know pi which we do not know obviously. But, an inspection of the numerator suggests that the 'worst' estimate for sd(p) in the sense that we get the 'largest' standard deviation is when pi = 50. Therefore,\\n\\nWorst possible sd(p) = sqrt (50 * 50 / N ) = 50 / sqrt(N)\\n\\nSo, you see that the margin of error falls off exponentially with N and thus you really do not need very big samples to reduce your margin of error or in other words N need not be very large for you to obtain a narrow confidence interval.\\n\\nFor example, for a 95 % confidence interval (i.e., k= 1.96) and N = 1000, the confidence interval is: \\n\\n[p - 1.96 * 50/sqrt(1000), p + 1.96 * 50/sqrt(1000)] = [p - 3, p + 3] \\n\\nAs we increase N the costs of polling go up linearly but the gains go down exponentially. That is the reason why pollsters usually cap N at 1000 as that gives them a reasonable error of margin under the worst possible assumption of pi = 50%.\\n\\n ",,user28
418,2,194,1be7c2b5-98fd-4a0c-be46-adabb288d853,"2010-07-20 01:47:36",175,"I am sure that everyone who's trying to find patterns in historical stock market data or betting history would like to know about this. Given a huge sets of data, and thousands of random variables that may or may not affect it, it makes sense to ask any patterns that you extract out from the data are indeed true patterns, not statistical fluke.\\n\\nA lot of patterns are only valid when they are tested in the samples. And even those that are patterns that are valid out of samples may cease to become valid when you apply it in the real world. \\n\\nI understand that it is not possible to completely 100% make sure a pattern is valid all the time, but besides in and out of samples tests, are their any tests that could establish the validness of a pattern?",,
419,1,194,1be7c2b5-98fd-4a0c-be46-adabb288d853,"2010-07-20 01:47:36",175,"Data Mining-- How to Tell Whether the Pattern Extracted is Meaningful?",,
420,3,194,1be7c2b5-98fd-4a0c-be46-adabb288d853,"2010-07-20 01:47:36",175,<data-mining>,,
421,2,195,23df9034-fb0b-453f-9242-3bf5e3c5c649,"2010-07-20 02:01:05",173,"I am looking at fitting distributions to data (with a particular focus on the tail) and am leaning towards Anderson-Darling tests rather than Kolmogorov-Smirnov. What do you think are the relative merits of these or other tests for fit (e.g. Cramer-von Mises)?",,
422,1,195,23df9034-fb0b-453f-9242-3bf5e3c5c649,"2010-07-20 02:01:05",173,"What do you think is the best goodness of fit test?",,
423,3,195,23df9034-fb0b-453f-9242-3bf5e3c5c649,"2010-07-20 02:01:05",173,<hypothesis-testing><fitting>,,
424,5,46,c80e33a5-c893-44bf-9989-11fe09d120a9,"2010-07-20 02:13:12",62,"A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. \\n\\nTo put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. \\n\\nStandard deviation is a property of a population. It measures how much average "dispersion" there is to that population. Are all the obsrvations clustered around the mean, or are they widely spread out? \\n\\nTo estimate the standard deviation of a population, we often calculate the standard deviation of a "sample" from that population. To do this, you take observations from that population, calculate a mean of those observations, and then calculate the square root of the average squared deviation from that "sample mean". \\n\\nTo get an unbiased estimator of the variance, you don't actually calculate the average squared deviation from the sample mean, but instead, you divide by (N-1) where N is the number of observations in your sample. Note that this "sample standard deviation" is not an unbiased estimator of the standard deviation, but the square of the "sample standard deviation" is an unbiased estimator of the variance of the population. \\n\\n\\n","added 963 characters in body",
425,2,196,246ebe84-5e41-4b73-bfe6-7a05843da6bd,"2010-07-20 02:17:24",87,"Besides gnuplot and [ggobi][1], what open source tools are people using to for visualizing multi-dimensional data?\\n\\nGnuplot is more or less a basic plotting package. \\n\\nGgobi can do a number of nifty things, such as:\\n\\n -  animate data along a dimension or among discrete collections\\n -  animate linear combinations varying the coefficients\\n -  compute principal components and other transformations\\n -  visualize and rotate 3 dimensional data clusters\\n -  use colors to represent a different dimension\\n\\nI would like to hear about other useful approaches that are based in open source and thus freely reusable or customizable.  Thanks.\\n\\n\\n  [1]: http://www.ggobi.org/",,
426,1,196,246ebe84-5e41-4b73-bfe6-7a05843da6bd,"2010-07-20 02:17:24",87,"Open source tools for visualizing multi-dimensional data ?",,
427,3,196,246ebe84-5e41-4b73-bfe6-7a05843da6bd,"2010-07-20 02:17:24",87,<data-visualization>,,
428,2,197,125d6609-4d33-41c8-8359-35e9bbf3a4fc,"2010-07-20 02:24:38",5,"How about R with ggplot2?",,
429,2,198,b90e2003-1aca-4102-8df8-ddd49289bfef,"2010-07-20 02:32:43",61,"Start with the distribution of y_bar, show that since v is normal, y_bar is multivariate normal and that consequently u must also be a multivariate normal; also show that the covariance matrix of y_bar is of the form sigma^2*(X^T * X)^-1 and thus -- if sigma^2 were known -- the variance of u would be sigma^2*c^T*(X^T*X)^(-1)*c.  Show that the distribution of e_bar^T * e_bar must be chi-squared and (*carefully*) find the degrees of freedom.  Think about how what the operation e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c must therefore produce, and what it's distribution and degrees of freedom are.\\n\\nThe result follows (almost) immediately from the definition of the t-distribution.",,
430,2,199,8f52d8e2-b0ba-43fc-bd17-c517a659e6e8,"2010-07-20 02:32:53",5,"You could try:\\n\\n - Bagging http://en.m.wikipedia.org/wiki/Bootstrap_aggregating\\n - Boosting http://en.m.wikipedia.org/wiki/Boosting\\n - Cross validation http://en.m.wikipedia.org/wiki/Cross-validation_(statistics)",,
431,16,196,4ce06585-d0c4-43e2-9fa6-76b59109fe63,"2010-07-20 02:35:32",87,,,
432,5,197,53148ad4-41cd-4c44-b1f4-0af5b45c825d,"2010-07-20 02:42:01",5,"How about R with [ggplot2][3]?\\n\\nOther tools that I really like:\\n\\n - [Processing][10]\\n - [Prefuse][11]  \\n - [Protovis][12]\\n\\n  [3]: http://had.co.nz/ggplot2/\\n  [10]: http://www.processing.org/\\n  [11]: http://prefuse.org/\\n  [12]: http://vis.stanford.edu/protovis/\\n ","added 249 characters in body",
433,2,200,c7ff66cb-91a1-42be-9a91-436681bbf21d,"2010-07-20 02:48:45",174,"If you want to know that a pattern is meaningful, you need to show what it actually *means*. Statistical tests do not do this. Unless your data can be said to be in some sense "complete", inferences draw from the data will always be provisional.\\n\\nYou can increase your *confidence* in the validity of a pattern by testing against more and more out of sample data, but that doesn't protect you from it turning out to be an artefact. The broader your range of out of sample data -- eg, in terms of how it is acquired and what sort of systematic confounding factors might exist within it -- the better the validation.\\n\\nIdeally, though, you need to go beyond identifying patterns and come up with a persuasive theoretical framework that *explains* the patterns you've found, and then test *that* by other, independent means. (This is called "science".)",,
434,6,165,d5320234-6c94-45a8-85b7-2f91aeda7178,"2010-07-20 03:01:29",61,<bayesian><mcmc><teaching>,"edited tags",
435,4,118,1d7716e8-ba6c-4741-9368-fb18c91e932d,"2010-07-20 03:05:58",83,"Standard deviation : Why square the difference instead of taking the absolute value?","edited title",
436,2,201,cf198915-d4d3-4b73-a71f-10c1219b61fb,"2010-07-20 03:11:36",183,"Start R and type `data()`. This will show all datasets in the search path.\\nMany additional datasets are available in add-on packages.\\nFor example, there are some interesting real-world social science datasets in the `AER` package.",,
437,2,202,84825c72-d4d9-4ea2-8c88-e953acc18cdf,"2010-07-20 03:13:22",183,"If you like learning through videos, I collated a list of R training videos:\\nhttp://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html",,
438,2,203,cc5187e0-636e-4611-8a01-124a7fb06973,"2010-07-20 03:31:45",183,"Following on from [this question][1]:\\nImagine that you want to test for differences in central tendency between two groups (e.g., males and females)\\non a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied).\\nI think a t-test would be sufficiently accurate for most purposes,\\n but that a bootstrap test of differences between group means would often provide more accurate p-values.\\nWhat statistical test would you use?\\n\\n  [1]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data",,
439,1,203,cc5187e0-636e-4611-8a01-124a7fb06973,"2010-07-20 03:31:45",183,"Group differences on a five point Likert item",,
440,3,203,cc5187e0-636e-4611-8a01-124a7fb06973,"2010-07-20 03:31:45",183,<scales><ordinal><t-test><interval>,,
441,2,204,94778fd8-d5b8-4126-94bd-3abecaed9143,"2010-07-20 03:35:58",183,"The lattice package in R\\n\\n",,
442,16,204,94778fd8-d5b8-4126-94bd-3abecaed9143,"2010-07-20 03:35:58",-1,,,
443,5,203,33e11b2d-6cd4-49da-84a3-3e52378fac8a,"2010-07-20 03:43:57",183,"Following on from [this question][1]:\\nImagine that you want to test for differences in central tendency between two groups (e.g., males and females)\\non a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied).\\nI think a t-test would be sufficiently accurate for most purposes,\\n but that a bootstrap test of differences between group means would often provide more accurate estimate of confidence intervals.\\nWhat statistical test would you use?\\n\\n  [1]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data","added 24 characters in body",
444,2,205,c9587fc0-c357-412d-8efa-c1bc25773530,"2010-07-20 03:51:24",187,"I'm curious about why we treat fitting GLMS as though they were some special optimization problem.  Are they?  It seems to me that they're just maximum likelihood, and that we write down the likelihood and then ... we maximize it!  So why do we use Fisher scoring instead of any of the myriad of optimization schemes that has been developed in the applied math literature? ",,
445,1,205,c9587fc0-c357-412d-8efa-c1bc25773530,"2010-07-20 03:51:24",187,"Why do we make a big fuss about using Fisher scoring when we fit a GLM?",,
446,3,205,c9587fc0-c357-412d-8efa-c1bc25773530,"2010-07-20 03:51:24",187,<generalized-linear-model>,,
447,2,206,1b62c7aa-1144-4376-8309-3e10a492c891,"2010-07-20 03:53:54",188,"What is the difference between discrete data and continuous data?",,
448,1,206,1b62c7aa-1144-4376-8309-3e10a492c891,"2010-07-20 03:53:54",188,"Discrete and Continuous",,
449,3,206,1b62c7aa-1144-4376-8309-3e10a492c891,"2010-07-20 03:53:54",188,<discrete-data><continuous-data>,,
450,6,196,2aeb3f60-ec23-4142-b345-17dbab1257c0,"2010-07-20 03:56:44",18,<data-visualization><open-source>,"edited tags",
451,2,207,2ad73f24-6ddb-4e60-ae03-ff8efdd07ad8,"2010-07-20 04:00:14",NULL,"First, we need to understand what is a markov chain. Consider the following [weather][1] example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:\\n\\nProbability(Next day is sunny | Given today is rainy ) = 0.50\\n\\nSince, the next day's weather is either sunny or rainy it follows that:\\n\\nProbability(Next day is Rainy | Given today is rainy ) = 0.50 \\n\\nSimilarly, let:\\n\\nProbability(Next day is rainy | Given today is sunny ) = 0.10\\n\\nTherefore, it follows that:\\n\\nProbability(Next day is sunny | Given today is sunny ) = 0.90\\n\\nThe above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:\\n\\n             S   R\\n    P = S [ 0.9 0.1\\n        R   0.5 0.5]\\n\\nWe might ask several questions whose answers follow:\\n\\nQ1: If the weather is sunny today then what is the weather likely to be tomorrow?\\n\\nA1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. \\n\\nQ2: What about two days from today?\\n\\nA2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now:\\n\\nFirst day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9  0.9. \\n\\nOr\\n\\nFirst day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 * 0.5\\n\\nTherefore, the probability that the weather will be sunny in two days is:\\n\\nProb(Sunny two days from now) = 0.9  0.9 + 0.1  0.5 = 0.81 + 0.05 = 0.86 \\n\\nSimilarly, the probability that it will be rainy is:\\n\\nProb(Rainy two days from now) = 0.1 * 0.5 + 0.9 0.1 = 0.05 + 0.09 = 0.14\\n\\nIf you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities:\\n\\nProb(Sunny) = 0.833\\nProb(Rainy) = 0.167\\n\\nIn other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.\\n\\nThe above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions):\\n\\nIrrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.\\n\\nMarkov Chain Monte Carlo exploits the above feature as follows: \\n\\nWe want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. \\n\\nIf we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. \\n\\nWe then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monet carlo component.\\n\\nThere are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm).\\n\\n  [1]: http://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_very_simple_weather_model",,user28
452,2,208,632070fc-2901-4863-8e2b-9f9c9cf7cc4e,"2010-07-20 04:07:11",74,"Temperatures are continuous. It can be 23 degrees, 23.1 degrees, 23.100004 degrees. \\n\\nGender is discrete. You can only be male or female (insert san francisco joke here). Something you would represent with a whole number like 0, 1, 2, etc\\n\\nThe difference is important as many statistical and data mining algorithms can handle one type but not the other. For example in regular regression, the Y must be continuous. In logistic regression the Y is discrete. ",,
453,2,209,b25dbbbd-7e13-40a0-8721-287576bc5356,"2010-07-20 04:16:52",174,"Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.\\n\\nContinuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric.\\n",,
454,2,210,946596bb-bbf3-4d39-9b17-099fdce34968,"2010-07-20 04:19:12",183,"Data is always discrete. Given a sample of `n` values on a variable, the maximum number of distinct values the variable can take is equal to  `n`.\\n\\nData on a variable are typically assumed to be drawn from a random variable.\\nThe random variable is continuous over a range if there is an infinite number of possible values that the variable can take between any two different points in the range.\\nFor example, height, weight, and time are typically assumed to be continuous.\\nOf course, any measurement of these variables will be finitely accurate and in some\\n sense discrete.\\n\\nIt is useful to distinguish between ordered (i.e., ordinal), unordered (i.e., nominal),  \\nand binary discrete variables.\\n\\nSome introductory textbooks confuse a continuous variable with a numeric variable.\\nFor example, a score on a computer game is discrete even though it is numeric.\\n\\nSome introductory textbooks confuse a ratio variable with continuous variables. A count variable is a ratio variable, but it is not continuous.\\n\\nIn actual practice, a variable is often treated as continuous when it can take on a sufficiently large number of different values.\\n\\n",,
455,2,211,fcb56ad8-88a9-47f1-bb69-68754a96161e,"2010-07-20 04:49:07",187,"I have written a document that is freely available at my website and on CRAN. See the linked page:\\n\\n[icebreakeR][1]\\n\\nThe datasets that are used in the document are also linked from that page.  Feedback is welcome and appreciated!\\n\\nAndrew\\n\\n\\n  [1]: http://www.ms.unimelb.edu.au/~andrewpr/r-users/",,
456,2,212,b0426c88-7ec6-4b42-8cf3-d298cdd56ec7,"2010-07-20 04:54:20",190,"I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate.\\n\\nWhat methods do I have to test for statistical significance of my new results?",,
457,1,212,b0426c88-7ec6-4b42-8cf3-d298cdd56ec7,"2010-07-20 04:54:20",190,"What method to use to test Statistical Significance of ASR results",,
458,3,212,b0426c88-7ec6-4b42-8cf3-d298cdd56ec7,"2010-07-20 04:54:20",190,<statistical-significance>,,
459,2,213,c2c56819-8032-44fc-93a9-3dd2ec1a30d4,"2010-07-20 05:02:33",159,"Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.\\n\\nI am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.\\n\\nOne possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches.",,
460,1,213,c2c56819-8032-44fc-93a9-3dd2ec1a30d4,"2010-07-20 05:02:33",159,"What is the best way to identify outliers in multivariate data?",,
461,3,213,c2c56819-8032-44fc-93a9-3dd2ec1a30d4,"2010-07-20 05:02:33",159,<multivariable><outliers>,,
462,2,214,18295242-39ea-4237-be6a-1d6d26d94f00,"2010-07-20 05:02:42",40,"Some free Stats textbooks are also available [here][1].\\n\\n\\n  [1]: http://www.e-booksdirectory.com/mathematics.php",,
463,2,215,9e0b9514-5295-446c-af10-b858ddfd2b43,"2010-07-20 05:03:12",187,"I'm not sure about these tests, so this answer may be off-topic.  Apologies if so.  But, are you sure that you want a test?  It really depends on what the purpose of the exercise is.  Why are you fitting the distributions to the data, and what will you do with the fitted distributions  afterward?  \\n\\nIf you want to know what distribution fits best just because you're interested, then a test may help.  \\n\\nOn the other hand, if you want to actually do something with the distribution, then you'd be better off developing a loss function based on your intentions, and using the distribution that gives you the most satisfactory value for the loss function.  \\n\\nIt sounds to me from your description (particular focus on the tail) that you want to actually do something with the distribution.  If so, it's hard for me to imagine a situation where an existing test will provide better guidance than comparing the effects of the fitted distributions in situ, somehow.",,
464,2,216,192c804a-f93b-43c2-8c3d-5e4493cb3c3d,"2010-07-20 05:04:40",191,"What are some good visualization libraries for online use? Are they easy to use and is there good documentation?\\n\\nThanks.",,
465,1,216,192c804a-f93b-43c2-8c3d-5e4493cb3c3d,"2010-07-20 05:04:40",191,"Web visualization libraries",,
466,3,216,192c804a-f93b-43c2-8c3d-5e4493cb3c3d,"2010-07-20 05:04:40",191,<data-visualization><library><protovis>,,
467,2,217,9391e1ba-d506-4ba4-b3cf-b8e761f698d3,"2010-07-20 05:10:08",5,"IMO, **[Protovis][12]** is the best and is very well documented and supported.  It is the basis for my [webvis][http://cran.r-project.org/web/packages/webvis/index.html] R package.  \\n\\nThese are also very good, although they have more of a learning curve:\\n\\n - [Processing][10]\\n - [Prefuse][11]  \\n\\n  [10]: http://www.processing.org/\\n  [11]: http://prefuse.org/\\n  [12]: http://vis.stanford.edu/protovis/",,
468,2,218,e22633d6-e8c8-4440-be33-61ece2f8c455,"2010-07-20 05:13:21",187,"This interesting question is the subject of some research in [ACERA][1].  The lead researcher is Andrew  Speirs-Bridge, and his work is eminently google-able :)\\n\\n\\n  [1]: http://www.acera.unimelb.edu.au/",,
469,5,217,a223f60f-fc0d-4352-8974-9870efb9c35b,"2010-07-20 05:15:51",5,"IMO, **[Protovis][12]** is the best and is very well documented and supported.  It is the basis for my [webvis][1] R package.  \\n\\nThese are also very good, although they have more of a learning curve:\\n\\n - [Processing][10]\\n - [Prefuse][11]  \\n\\n  [1]: http://cran.r-project.org/web/packages/webvis/index.html\\n  [10]: http://www.processing.org/\\n  [11]: http://prefuse.org/\\n  [12]: http://vis.stanford.edu/protovis/","added 10 characters in body",
470,2,219,25f497bf-a2fe-4e6a-ac44-8172ed315824,"2010-07-20 05:21:14",154,"The Xorshift PNG designed by George Marsaglia. Not as good as the Mersenne-Twister but very simple to implement and highly parallelizable. Performs well on many-core architectures such as DSP chips and Nvidia's Tesla.",,
471,2,220,c6cc7bd6-93fa-4eb1-bee2-f5905b2eb908,"2010-07-20 05:23:04",85,"If X1, ..., Xn are independent identically-distributed random variables, what can be said about the distribution of min(X1, ..., Xn) in general?\\n\\n",,
472,1,220,c6cc7bd6-93fa-4eb1-bee2-f5905b2eb908,"2010-07-20 05:23:04",85,"How is the minimum of a set of rvs distributed?",,
473,3,220,c6cc7bd6-93fa-4eb1-bee2-f5905b2eb908,"2010-07-20 05:23:04",85,<distributions><random-variables><minimum>,,
474,5,209,0cd94239-601a-40e0-bbf7-f2b5dbea9589,"2010-07-20 05:25:22",174,"Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.\\n\\nContinuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric.\\n\\nIt sometimes makes sense to treat numeric data that is properly of one type as being of the other. For example, something like *height* is continuous, but often we don't really care too much about tiny differences and instead group heights into a number of discrete **bins**. Conversely, if we're counting large amounts of some discrete entity -- grains of rice, or termites, or pennies in the economy -- we may choose not to think of 2,000,006 and 2,000,008 as crucially different values but instead as nearby points on an approximate continuum.\\n\\nIt can also sometimes be useful to treat numeric data as categorical, eg: underweight, normal, obese. This is usually just another kind of binning.\\n\\nIt seldom makes sense to consider categorical data as continuous.","added stuff about treating one kind of data as another",
475,5,213,fb6d122c-d4d0-4c59-bf15-00ea2882499e,"2010-07-20 05:28:56",159,"Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.\\n\\nI am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.\\n\\nOne possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches?","edited body",
476,5,219,8b758f55-4a59-45be-b02b-e0edf268f580,"2010-07-20 05:33:00",154,"The Xorshift PNG designed by George Marsaglia. Its period (2^128-1) is much shorter than the Mersenne-Twister but the algorithm is very simple to implement and lends itself to parallelization. Performs well on many-core architectures such as DSP chips and Nvidia's Tesla.","added 20 characters in body; added 24 characters in body; added 6 characters in body; added 4 characters in body",
477,2,221,9c2c5269-92ca-481d-8438-395b1237da36,"2010-07-20 05:35:48",159,"If the cdf of Xi is denoted by F(x), then the cdf of the minimum is given by [1-F(x)]^n.\\n\\n",,
478,2,222,07f7ceae-2a2b-4498-b2ca-c217ce7aa13b,"2010-07-20 05:37:46",191,"I know this is probably simplistic but what are Principal component scores?\\n\\nThis question originates from my attempt to understand this question [here][1].\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/213/what-is-the-best-way-to-identify-outliers-in-multivariate-data",,
479,1,222,07f7ceae-2a2b-4498-b2ca-c217ce7aa13b,"2010-07-20 05:37:46",191,"What are principal component scores?",,
480,3,222,07f7ceae-2a2b-4498-b2ca-c217ce7aa13b,"2010-07-20 05:37:46",191,<component-scores><fundamentals>,,
481,4,220,96945a15-87e9-4f80-94f7-c7ce8640532d,"2010-07-20 05:43:42",85,"How is the minimum of a set of random variables distributed?","edited title",
482,2,223,46b8f548-570b-4bec-a930-510fb003068b,"2010-07-20 05:54:15",79,"I have a friend who is an MD and wants to refresh his Statistics. So is there any recommended resource online (or offline) ? He did stats ~20 years ago.",,
483,1,223,46b8f548-570b-4bec-a930-510fb003068b,"2010-07-20 05:54:15",79,"Intro to statistics for an MD?",,
484,3,223,46b8f548-570b-4bec-a930-510fb003068b,"2010-07-20 05:54:15",79,<textbook><online><introductory>,,
485,2,224,c37eed7c-d60c-4dfd-a834-aee0dc15eb5c,"2010-07-20 06:03:59",128,"Which visualization libraries would you suggest to use in a standalone application (Linux, .Net, Windows, whatever). Reasonable performance would be nice as well.",,
486,1,224,c37eed7c-d60c-4dfd-a834-aee0dc15eb5c,"2010-07-20 06:03:59",128,"Visualization Libraries",,
487,3,224,c37eed7c-d60c-4dfd-a834-aee0dc15eb5c,"2010-07-20 06:03:59",128,<data-visualization>,,
488,2,225,42db41e9-e1f9-4e06-985e-27ba324f8b9f,"2010-07-20 06:07:37",196,"Why is the average of the highest value from 100 draws from a normal distribution different from the 98% percentile of the normal distribution?  It seems that by definition that they should be the same.  But...\\n\\nCode in R:\\n\\n    NSIM <- 10000\\n    x <- rep(NA,NSIM)\\n    for (i in 1:NSIM)\\n    {\\n    	x[i] <- max(rnorm(100))\\n    }\\n    qnorm(.98)\\n    qnorm(.99)\\n    mean(x)\\n    median(x)\\n    hist(x)\\n\\nI imagine that I'm misunderstanding something about what the maximum of a 100 draws from the normal distribution should be.  As is demonstrated by an unexpectedly asymetrical distribution of maximum values.\\n",,
489,1,225,42db41e9-e1f9-4e06-985e-27ba324f8b9f,"2010-07-20 06:07:37",196,"Why is the average of the highest value from 100 draws from a normal distribution different from the 99% percentile of the normal distribution?",,
490,3,225,42db41e9-e1f9-4e06-985e-27ba324f8b9f,"2010-07-20 06:07:37",196,<r><distributions><maximum>,,
491,6,95,2ee192b9-7866-4b5d-a8b5-4b6518349139,"2010-07-20 06:14:40",154,<time-series><garch><volatility-forecasting><finance>,"edited tags",
492,2,226,7a6ececc-db65-4a12-ac6a-c5b403c951b2,"2010-07-20 06:23:21",173,"Principal component analysis (PCA) is one popular approach analyzing variance when you are dealing with multivariate data. You have random variables X1, X2,...Xn which are all correlated (positively or negatively) to varying degrees, and you want to get a better understanding of what's going on. PCA can help.\\n\\nWhat PCA gives you is a change of variable into Y1, Y2,..., Yn (i.e. the same number of variables) which are linear combinations of the Xs. For example, you might have Y1 = 2.1 X1 - 1.76 X2 + 0.2 X3...\\n\\nThe Ys the nice property that each of these have zero correlation with each other. Better still, you get them in decreasing order of variance. So, Y1 "explains" a big chunk of the variance of the original variables, Y2 a bit less and so on. Usually after the first few Ys, the variables become somewhat meaningless. The PCA score for any of the Xi is just it's coefficient in each of the Ys. In my earlier example, the score for X2 in the first principal component (Y1) is 1.76.\\n\\nThe way PCA does this magic is by computing eigenvectors of the covariance matrix.\\n\\nTo give a concrete example, imagine X1,...X10 are changes in 1 year, 2 year, ..., 10 year Treasury bond yields over some time period. When you compute PCA you generally find that the first component has scores for each bond of the same sign and about the same sign. This tells you that most of the variance in bond yields comes from everything moving the same way: "parallel shifts" up or down. The second component typically shows "steepening" and "flattening" of the curve and has opposite signs for X1 and X10.",,
493,2,227,96318b27-a589-4d1a-a261-513ee81214e6,"2010-07-20 06:24:32",144,"Let i be N rows and j be M columns. Suppose you linearize the combination of variables (columns):\\n\\n    Z_i1 = c_11×Y_i1 + c_12×Yi2 + ... + c_1M×YiM\\n\\nThe above formula basically says to multiply row elements with a certain value c (loadings) and sum them by columns.\\n\\nA principal component (PC) is a linear combination Z_1 = (Z_11, ..., Z_N1) (values by columns). In essence, the PC should present the most important features of variables (columns). Ergo, you can extract as many PC as there are variables (or less).\\n\\nAn output from <a href="http://cran.r-project.org/">R</a> on PCA (a fake example) looks like this. PC1, PC2... are principal components 1, 2... The example below is showing only the first 8 principal components (out of 17). You can also extract other elements from PCA, like loadings and scores.\\n\\n    Importance of components:\\n                              PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8\\n    Standard deviation     1.0889 1.0642 1.0550 1.0475 1.0387 1.0277 1.0169 1.0105\\n    Proportion of Variance 0.0697 0.0666 0.0655 0.0645 0.0635 0.0621 0.0608 0.0601\\n    Cumulative Proportion  0.0697 0.1364 0.2018 0.2664 0.3298 0.3920 0.4528 0.5129\\n\\n",,
494,2,228,c5dd83a0-7925-42f5-8894-05b50be424fa,"2010-07-20 06:27:16",10," - http://insideria.com/2009/12/28-rich-data-visualization-too.html 28 Rich Data Visualization Tools\\n - http://www.rgraph.net/ R graph\\n - http://vis.stanford.edu/protovis/\\n \\n ",,
495,2,229,abf06717-575e-468a-82b3-01ab4216582e,"2010-07-20 06:28:47",159,"The maximum does not have a normal distribution. Its cdf is Phi^100(x) where Phi(x) is the standard normal cdf. In general the moments of this distribution are tricky to obtain analytically. There is an ancient paper on this by [Tippett (*Biometrika*, 1925)][1].\\n\\n\\n  [1]: http://www.jstor.org/stable/2332087",,
496,2,230,fae807e1-530c-42f5-83e8-f763dcfdeec0,"2010-07-20 06:35:21",173,"You could have a look at Processing: http://processing.org/",,
497,5,176,28ca4081-210f-4fb7-a0ac-c884314aaf42,"2010-07-20 06:40:59",81,"Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book\\n\\nThe Frequentist would say that each outcome has an equal 1 in 6 chance of occurring.\\n\\nThe Bayesian however would say hang on a second, I know that man, he's David Blane, a famous trickster, I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on 3 BUT I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll increase the chance from 1% to something slightly higher, otherwise I'll reduce it even further. \\n","deleted 4 characters in body; added 13 characters in body",
498,2,231,6234f949-9a78-4de6-8a87-a8895a18a071,"2010-07-20 06:41:13",199,"This is one I've used successfully:\\n\\nhttp://www.amazon.co.uk/Statistics-without-Psychology-Christine-Dancey/dp/013124941X\\n\\nI just stumlbed on this too, this might be useful:\\n\\nhttp://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm\\n\\nI'm sure I knew of a free pdf that some doctors I know use, but I can't seem to find it at the moment. I will try to dig it out.",,
499,2,232,37d9d13c-cda0-4ed0-a585-2a9fc033f782,"2010-07-20 06:41:25",103,"\\n\\n - [Mondrian][1]: Exploratory data analysis with focus on large data and databases.\\n - [iPlots][2]: a package for the R statistical environment which provides high interaction statistical graphics, written in Java.\\n\\n\\n  [1]: http://rosuda.org/mondrian/\\n  [2]: http://rosuda.org/iplots/",,
500,16,232,37d9d13c-cda0-4ed0-a585-2a9fc033f782,"2010-07-20 06:41:25",-1,,,
502,2,234,bbd70139-028c-407a-993b-0c57dd9f8ba7,"2010-07-20 06:47:14",198,"Say you have a cloud of N points in, say, 3D (which can be listed in a 100x3 array). Then, the principal components analysis (PCA) fits an arbitrarily oriented ellipsoid into the data. The principal component score is the length of the diameters of the ellipsoid. \\n\\nIn the direction in which the diameter is large, the data varies a lot, while in the direction in which the diameter is small, the data varies litte. If you wanted to project N-d data into a 2-d scatter plot, you plot them along the two largest principal components, because with that approach you display most of the variance in the data.",,
503,5,224,f1b8b1b9-8e3a-4df5-92d1-0dc793e5f525,"2010-07-20 06:49:14",128,"Which visualization libraries (plots, graphs, ...) would you suggest to use in a standalone application (Linux, .Net, Windows, whatever). Reasonable performance would be nice as well.","added 21 characters in body",
504,2,235,949b6bbf-368b-4f34-ba45-8c42bcd5fb9a,"2010-07-20 07:17:58",13,"[RaphaelJS](http://raphaeljs.com/) can do some pretty amazing stuff and it just got some major backing from [Sencha](http://www.sencha.com/) (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, [gRaphael](http://g.raphaeljs.com/), that focuses on drawing charts and graphs.\\n\\nThe [MIT SIMILE Project](http://simile.mit.edu/) also has some interesting JavaScript libraries:\\n\\n  - [Timeplot](http://www.simile-widgets.org/timeplot/)\\n  - [Timeline](http://www.simile-widgets.org/timeline/)\\n\\nThere is also a project to port Processing to JavaScript: [ProcessingJS](http://processingjs.org/)",,
505,6,222,3bebe9fd-0a7d-4adc-a92a-b63895ebc2ca,"2010-07-20 07:20:49",144,<scores><fundamentals><principal><components>,"edited tags",
506,5,232,e11f550f-4c97-4fb3-bd65-3702bfd8bc67,"2010-07-20 07:23:11",103,"\\n - [Mondrian][1]: Exploratory data analysis with focus on large data and databases.\\n - [iPlots][2]: a package for the R statistical environment which provides high interaction statistical graphics, written in Java. The package provides interactive mosaic plots, bar plots, box plots, parallel plots, scatter plots and histograms that can be linked and color brushed. \\n\\n\\n  [1]: http://rosuda.org/mondrian/\\n  [2]: http://rosuda.org/iplots/","added 151 characters in body",
507,8,232,f3f66fe3-be8c-41fa-a793-436d2c449924,"2010-07-20 07:26:11",103,"\\n\\n - [Mondrian][1]: Exploratory data analysis with focus on large data and databases.\\n - [iPlots][2]: a package for the R statistical environment which provides high interaction statistical graphics, written in Java.\\n\\n\\n  [1]: http://rosuda.org/mondrian/\\n  [2]: http://rosuda.org/iplots/","Rollback to [37d9d13c-cda0-4ed0-a585-2a9fc033f782]",
508,5,235,60c7e1b8-0c37-4f5e-a2a9-d74365bed696,"2010-07-20 07:29:42",13,"[RaphaelJS](http://raphaeljs.com/) can do some pretty amazing stuff and it just got some major backing from [Sencha](http://www.sencha.com/) (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, [gRaphael](http://g.raphaeljs.com/), that focuses on drawing charts and graphs.\\n\\nThe [MIT SIMILE Project](http://simile.mit.edu/) also has some interesting JavaScript libraries:\\n\\n  - [Timeplot](http://www.simile-widgets.org/timeplot/)\\n  - [Timeline](http://www.simile-widgets.org/timeline/)\\n\\nThere is also a project to port Processing to JavaScript: [ProcessingJS](http://processingjs.org/)\\n\\nAnd for an open source alternative to Google Maps, there is the excellent [OpenLayers](http://www.openlayers.org) JavaScript library which powers the frontend of the equally excellent [OpenStreetMap](http://www.openstreetmap.org/).","added 137 characters in body; added 98 characters in body",
509,5,227,312b33fa-0a1c-44ac-96c1-ef311151f5da,"2010-07-20 07:30:08",144,"Let i be N rows and j be M columns. Suppose you linearize the combination of variables (columns):\\n\\n    Z_i1 = c_11×Y_i1 + c_12×Yi2 + ... + c_1M×YiM\\n\\nThe above formula basically says to multiply row elements with a certain value c (loadings) and sum them by columns. Resulting values (Y values times the loading) are scores.\\n\\nA principal component (PC) is a linear combination Z_1 = (Z_11, ..., Z_N1) (values by columns which are called scores). In essence, the PC should present the most important features of variables (columns). Ergo, you can extract as many PC as there are variables (or less).\\n\\nAn output from <a href="http://cran.r-project.org/">R</a> on PCA (a fake example) looks like this. PC1, PC2... are principal components 1, 2... The example below is showing only the first 8 principal components (out of 17). You can also extract other elements from PCA, like loadings and scores.\\n\\n    Importance of components:\\n                              PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8\\n    Standard deviation     1.0889 1.0642 1.0550 1.0475 1.0387 1.0277 1.0169 1.0105\\n    Proportion of Variance 0.0697 0.0666 0.0655 0.0645 0.0635 0.0621 0.0608 0.0601\\n    Cumulative Proportion  0.0697 0.1364 0.2018 0.2664 0.3298 0.3920 0.4528 0.5129\\n\\n","added 82 characters in body",
510,2,236,d86b02cf-7f1d-4f12-ad37-577452cee632,"2010-07-20 07:30:34",138,"Unfortunately, it only runs on macs, but otherwise a great application:\\n\\n* [http://nodebox.net/code/index.php/Home][1]\\n\\n> NodeBox is a Mac OS X application that lets you create 2D visuals (static, animated or interactive) using Python programming code and export them as a PDF or a QuickTime movie. NodeBox is free and well-documented.\\n\\n\\n  [1]: http://nodebox.net/code/index.php/Home",,
511,2,237,b6928025-802c-45a6-ae4f-94057b537a50,"2010-07-20 07:31:08",199,"I'm going to leave the main question alone, because I think I will get it wrong (although I too analyse data for a healthcare provider, and to be honest, if I had these data, I would just analyse them using standard techniques and hope for the best, they look pretty okay to me).\\n\\nAs for R packages, I have found the TSA library and its accompanying book:\\n\\nhttp://www.amazon.co.uk/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=sr_1_2?ie=UTF8&s=books&qid=1279610835&sr=8-2\\n\\nvery useful indeed. The armasubsets command, particularly, I think is a great timesaver.",,
512,2,238,254e93c3-776e-4df4-bcab-8852c29678dc,"2010-07-20 07:33:09",138,"There is always lovely gnuplot:\\n\\n* [http://www.gnuplot.info/][1]\\n\\n> Gnuplot is a portable command-line driven graphing utility for linux, OS/2, MS Windows, OSX, VMS, and many other platforms. The source code is copyrighted but freely distributed (i.e., you don't have to pay for it). It was originally created to allow scientists and students to visualize mathematical functions and data interactively, but has grown to support many non-interactive uses such as web scripting. It is also used as a plotting engine by third-party applications like Octave. Gnuplot has been supported and under active development since 1986.\\n\\n> Gnuplot supports many types of plots in either 2D and 3D. It can draw using lines, points, boxes, contours, vector fields, surfaces, and various associated text. It also supports various specialized plot types. \\n\\n\\n  [1]: http://www.gnuplot.info/",,
514,2,240,4ac6cc20-2703-494a-81a0-3bc82faf211f,"2010-07-20 07:55:30",114,"My first response would be that if you can do multivariate regression on the data, then to use the residuals from that regression to spot outliers. (I know you said it's not a regression problem, so this might not help you, sorry !)\\n\\nI'm copying some of this from a [Stackoverflow question I've previously answered][1] which has some example [R][2] code\\n\\nFirst, we'll create some data, and then taint it with an outlier;\\n\\n    > testout<-data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5),Y=rnorm(50,mean=200,sd=25)) \\n    > #Taint the Data \\n    > testout$X1[10]<-5 \\n    > testout$X2[10]<-5 \\n    > testout$Y[10]<-530 \\n     \\n    > testout \\n             X1         X2        Y \\n    1  44.20043  1.5259458 169.3296 \\n    2  40.46721  5.8437076 200.9038 \\n    3  48.20571  3.8243373 189.4652 \\n    4  60.09808  4.6609190 177.5159 \\n    5  50.23627  2.6193455 210.4360 \\n    6  43.50972  5.8212863 203.8361 \\n    7  44.95626  7.8368405 236.5821 \\n    8  66.14391  3.6828843 171.9624 \\n    9  45.53040  4.8311616 187.0553 \\n    10  5.00000  5.0000000 530.0000 \\n    11 64.71719  6.4007245 164.8052 \\n    12 54.43665  7.8695891 192.8824 \\n    13 45.78278  4.9921489 182.2957 \\n    14 49.59998  4.7716099 146.3090 \\n    <snip> \\n    48 26.55487  5.8082497 189.7901 \\n    49 45.28317  5.0219647 208.1318 \\n    50 44.84145  3.6252663 251.5620 \\n\\nIt's often most usefull to examine the data graphically (you're brain is much better at spotting outliers than maths is)\\n\\n    > #Use Boxplot to Review the Data \\n    > boxplot(testout$X1, ylab="X1") \\n    > boxplot(testout$X2, ylab="X2") \\n    > boxplot(testout$Y, ylab="Y") \\n\\nYou can then use stats to calculate critical cut off values, here using the Lund Test (See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476. and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.)\\n\\n    > #Alternative approach using Lund Test \\n    > lundcrit<-function(a, n, q) { \\n    + # Calculates a Critical value for Outlier Test according to Lund \\n    + # See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476. \\n    + # and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132. \\n    + # a = alpha \\n    + # n = Number of data elements \\n    + # q = Number of independent Variables (including intercept) \\n    + F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE) \\n    + crit<-((n-q)*F/(n-q-1+F))^0.5 \\n    + crit \\n    + } \\n     \\n    > testoutlm<-lm(Y~X1+X2,data=testout) \\n     \\n    > testout$fitted<-fitted(testoutlm) \\n     \\n    > testout$residual<-residuals(testoutlm) \\n     \\n    > testout$standardresid<-rstandard(testoutlm) \\n     \\n    > n<-nrow(testout) \\n     \\n    > q<-length(testoutlm$coefficients) \\n     \\n    > crit<-lundcrit(0.1,n,q) \\n     \\n    > testout$Ynew<-ifelse(testout$standardresid>crit,NA,testout$Y) \\n     \\n    > testout \\n             X1         X2        Y    newX1   fitted    residual standardresid \\n    1  44.20043  1.5259458 169.3296 44.20043 209.8467 -40.5171222  -1.009507695 \\n    2  40.46721  5.8437076 200.9038 40.46721 231.9221 -31.0183107  -0.747624895 \\n    3  48.20571  3.8243373 189.4652 48.20571 203.4786 -14.0134646  -0.335955648 \\n    4  60.09808  4.6609190 177.5159 60.09808 169.6108   7.9050960   0.190908291 \\n    5  50.23627  2.6193455 210.4360 50.23627 194.3285  16.1075799   0.391537883 \\n    6  43.50972  5.8212863 203.8361 43.50972 222.6667 -18.8306252  -0.452070155 \\n    7  44.95626  7.8368405 236.5821 44.95626 223.3287  13.2534226   0.326339981 \\n    8  66.14391  3.6828843 171.9624 66.14391 148.8870  23.0754677   0.568829360 \\n    9  45.53040  4.8311616 187.0553 45.53040 214.0832 -27.0279262  -0.646090667 \\n    10  5.00000  5.0000000 530.0000       NA 337.0535 192.9465135   5.714275585 \\n    11 64.71719  6.4007245 164.8052 64.71719 159.9911   4.8141018   0.118618011 \\n    12 54.43665  7.8695891 192.8824 54.43665 194.7454  -1.8630426  -0.046004311 \\n    13 45.78278  4.9921489 182.2957 45.78278 213.7223 -31.4266180  -0.751115595 \\n    14 49.59998  4.7716099 146.3090 49.59998 201.6296 -55.3205552  -1.321042392 \\n    15 45.07720  4.2355525 192.9041 45.07720 213.9655 -21.0613819  -0.504406009 \\n    16 62.27717  7.1518606 186.6482 62.27717 169.2455  17.4027250   0.430262983 \\n    17 48.50446  3.0712422 228.3253 48.50446 200.6938  27.6314695   0.667366651 \\n    18 65.49983  5.4609713 184.8983 65.49983 155.2768  29.6214506   0.726319931 \\n    19 44.38387  4.9305222 213.9378 44.38387 217.7981  -3.8603382  -0.092354925 \\n    20 43.52883  8.3777627 203.5657 43.52883 228.9961 -25.4303732  -0.634725264 \\n    <snip> \\n    49 45.28317  5.0219647 208.1318 45.28317 215.3075  -7.1756966  -0.171560291 \\n    50 44.84145  3.6252663 251.5620 44.84145 213.1535  38.4084869   0.923804784 \\n           Ynew \\n    1  169.3296 \\n    2  200.9038 \\n    3  189.4652 \\n    4  177.5159 \\n    5  210.4360 \\n    6  203.8361 \\n    7  236.5821 \\n    8  171.9624 \\n    9  187.0553 \\n    10       NA \\n    11 164.8052 \\n    12 192.8824 \\n    13 182.2957 \\n    14 146.3090 \\n    15 192.9041 \\n    16 186.6482 \\n    17 228.3253 \\n    18 184.8983 \\n    19 213.9378 \\n    20 203.5657 \\n    <snip> \\n    49 208.1318 \\n    50 251.5620 \\n\\nObviosuly there are other outlier tests than the Lund test (Grubbs springs to mind), but I'm not sure which are better suited to multivariate data.\\n  [1]: http://stackoverflow.com/questions/1444306/how-to-use-outlier-tests-in-r-code/1444548#1444548\\n  [2]: http://en.wikipedia.org/wiki/R_(programming_language)",,
515,2,241,8ebce7bf-456e-4aee-9e9b-cafd0c3e1f43,"2010-07-20 07:56:06",196,"I'm not sure what you mean when you say you aren't thinking of a regression problem but of "true multivariate data".  My initial response would be to calculate the Mahalanobis distance since it doesn't require that you specify a particular IV or DV, but at its core (as far as I understand it) it is related to a leverage statistic.",,
516,2,242,2ffb1296-9d20-45ca-84d8-baee3922addd,"2010-07-20 07:56:16",199,"This is a bit of a flippant question, but I have a serious interest in the answer. I work in a psychiatric hospital and I have three years' of data, collected every day across each ward regarding the level of violence on that ward.\\n\\nClearly the model which fits these data is a time series model. I had to difference the scores in order to make them more normal. I fit an ARMA model with the differenced data, and the best fit I think was a model with one degree of differencing and first order auto-correlation at lag 2.\\n\\nMy question is, what on earth can I use this model for? Time series always seems so useful in the textbooks when it's about hare populations and oil prices, but now I've done my own the result seems so abstract as to be completely opaque. The differenced scores correlate with each other at lag two, but I can't really advise everyone to be on high alert two days after a serious incident in all seriousness.\\n\\nOr can I?",,
517,1,242,2ffb1296-9d20-45ca-84d8-baee3922addd,"2010-07-20 07:56:16",199,"Uses for time series analysis?",,
518,3,242,2ffb1296-9d20-45ca-84d8-baee3922addd,"2010-07-20 07:56:16",199,<time-series>,,
519,2,243,42d6af73-b512-4c32-b721-11f25ef7bfb6,"2010-07-20 08:05:04",196,"Depending on the size of the dataset in question, a permutation test might be preferable to a bootstrap in that it may be able to provide an exact test of the hypothesis.",,
520,2,244,7b89b9e3-12e8-48a8-b922-4ae0f62fc0f2,"2010-07-20 08:13:31",13,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.  \\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface to `Fortran`!  Not open source or free for commercial use though.",,
521,2,245,08a85590-40b9-460d-b680-3761325fe8cd,"2010-07-20 08:20:36",196,"Principal component scores are a group of scores that are obtained following a Principle Components Analysis (PCA).  In PCA the relationships between a group of scores is analyzed such that an equal number of new "imaginary" variables (aka principle components) are created.  The first of these new imaginary variables is maximally correlated with all of the original group of variables.  The next is somewhat less correlated, and so forth until the point that if you used all of the principal components scores to predict any given variable from the initial group you would be able to explain all of its variance.  The way in which PCA proceeds is complex and has certain restrictions.  Among these is the restriction that the correlation between any two principal components (i.e. imaginary variables) is zero; thus it doesn't make sense to try to predict one principal component with another.",,
522,2,246,26b71b7e-a6c4-4197-83e4-6941a06d3b18,"2010-07-20 08:28:52",198,"You fitted the model to the differences, which means that you're describing the change in levels of violence. You get a lag of 2 days. A lag is indicative of the memory of the process. In other words, the change in levels of violence today has some dependency on the change in levels of violence in the last two days. For longer time-scales, the contribution of random influences becomes strong enough so that there is no clear link anymore.\\n\\nIs the auto-correlation positive? Then a change of levels of violence today suggests a similar change in levels of violence in two days. Is it negative? Then violence might stay higher for two days.\\n\\nOf course, you may want to have to control for confounding effects. For example, after a serious incident, people may be more likely to report minor incidents, but this "sensitization" would be going away after two days.",,
523,5,244,d126bab9-5e21-474e-bb75-67f9dd369d3b,"2010-07-20 08:31:04",13,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.  \\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nIf we were to bend the concept of "standalone application" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:\\n\\n  - [Asymptote](http://asymptote.sourceforge.net/) can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader ([example](http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf)).\\n\\n  - [PGF/TikZ](http://sourceforge.net/projects/pgf/) provides a wonderful vector drawing language to TeX documents.  The [manual](http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf) is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  [PGFPlots](http://sourceforge.net/projects/pgfplots/) provides an abstraction layer for drawing plots.  A wondeful showcase can be found at [TeXample](http://www.texample.net/tikz/examples/all/).\\n\\n  - [PSTricks](http://www.tug.org/PSTricks/main.cgi/) served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface to `Fortran`!  Not open source or free for commercial use though.","added 1242 characters in body",
524,5,244,3cc6a11a-2529-4f38-9542-05c13d8a91d1,"2010-07-20 08:36:07",13,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.\\n\\n[igraph](http://igraph.sourceforge.net/) can also be used for visualization of tree-like data structures.  Contains nice interfaces to scripting languages such as R and Python along with a stand-alone C library.\\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nIf we were to bend the concept of "standalone application" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:\\n\\n  - [Asymptote](http://asymptote.sourceforge.net/) can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader ([example](http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf)).\\n\\n  - [PGF/TikZ](http://sourceforge.net/projects/pgf/) provides a wonderful vector drawing language to TeX documents.  The [manual](http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf) is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  [PGFPlots](http://sourceforge.net/projects/pgfplots/) provides an abstraction layer for drawing plots.  A wondeful showcase can be found at [TeXample](http://www.texample.net/tikz/examples/all/).\\n\\n  - [PSTricks](http://www.tug.org/PSTricks/main.cgi/) served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface to `Fortran`!  Not open source or free for commercial use though.","added 213 characters in body",
525,2,247,a9936605-e7e1-4d44-95d4-d4544b0e3fbc,"2010-07-20 08:36:36",171,"Might be a bit narrow in scope, but if you're doing any work in Clojure on the JVM there's the excellent [Incanter][1]:\\n\\n> Incanter is a Clojure-based, R-like platform for statistical computing and graphics.\\n\\n [1]: http://incanter.org",,
526,2,248,b7cdb094-f1d7-4ada-8872-d586ad0488eb,"2010-07-20 08:38:38",211,"ggobi and the R links to Ggobi are really rather good for this.   There are simpler visualisations (iPlots is very nice, also interactive, as mentioned).\\n\\nBut it depends whether you are doing something more specialised.   For example TreeView lets you visualise the kind of cluster dendrograms you get out of microarrays.",,
527,16,248,b7cdb094-f1d7-4ada-8872-d586ad0488eb,"2010-07-20 08:38:38",-1,,,
528,5,235,8b5fa715-2835-44c2-a621-6519ee972f02,"2010-07-20 08:45:07",13,"[RaphaelJS](http://raphaeljs.com/) can do some pretty amazing stuff and it just got some major backing from [Sencha](http://www.sencha.com/) (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, [gRaphael](http://g.raphaeljs.com/), that focuses on drawing charts and graphs.\\n\\nThe [MIT SIMILE Project](http://simile.mit.edu/) also has some interesting JavaScript libraries:\\n\\n  - [Timeplot](http://www.simile-widgets.org/timeplot/)\\n  - [Timeline](http://www.simile-widgets.org/timeline/)\\n\\nThere is also a project to port Processing to JavaScript: [ProcessingJS](http://processingjs.org/)\\n\\n[Jmol](http://jmol.sourceforge.net/) is a Java applet for viewing chemical structures, but it is used as the display engine for 3D graphics in the [SAGE](http://www.sagemath.org/) system, which has a completely browser-based GUI.\\n\\nAnd for an open source alternative to Google Maps, there is the excellent [OpenLayers](http://www.openlayers.org) JavaScript library which powers the frontend of the equally excellent [OpenStreetMap](http://www.openstreetmap.org/).","added 229 characters in body; added 4 characters in body",
529,5,176,865f3b7e-640f-4e5e-bc6d-1c28dcaaec37,"2010-07-20 08:46:03",81,"Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book.\\n\\nThen informally:\\n\\nThe ***Frequentist*** would say that each outcome has an equal 1 in 6 chance of occurring.\\n\\nThe ***Bayesian*** however would say hang on a second, I know that man, he's David Blaine, a famous trickster! I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on a 3  *BUT*  I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll iteratively increase the chance from 1% to something slightly higher, otherwise I'll reduce it even further. \\n","improved meaning; added 1 characters in body; added 1 characters in body; added 1 characters in body",
530,4,225,924c68e5-40e9-4384-8190-4f4339f1298a,"2010-07-20 08:47:33",196,"Why is the average of the highest value from 100 draws from a normal distribution different from the 98th percentile of the normal distribution?","Fixed a value in the title to match the question",
531,5,244,fd42158b-2fdc-48bb-9069-c7f3b4351bc3,"2010-07-20 08:47:44",13,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.\\n\\n[igraph](http://igraph.sourceforge.net/) can also be used for visualization of tree-like data structures.  Contains nice interfaces to scripting languages such as R and Python along with a stand-alone C library.\\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nIf we were to bend the concept of "standalone application" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:\\n\\n  - [Asymptote](http://asymptote.sourceforge.net/) can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader ([example](http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf)).\\n\\n  - [PGF/TikZ](http://sourceforge.net/projects/pgf/) provides a wonderful vector drawing language to TeX documents.  The [manual](http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf) is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  [PGFPlots](http://sourceforge.net/projects/pgfplots/) provides an abstraction layer for drawing plots.  A wondeful showcase can be found at [TeXample](http://www.texample.net/tikz/examples/all/).\\n\\n  - [PSTricks](http://www.tug.org/PSTricks/main.cgi/) served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface for `Fortran`!  Not open source or free for commercial use though.","added 1 characters in body",
532,2,249,302c254f-60c1-45ca-9604-73042d55303f,"2010-07-20 08:49:13",213,"I have a set of *N* bodies, which is a random sample from a population whose mean and variance I want to estimate. A property of each body is being measured m_i times (m_i>1) and different for each body index i  identifies which body it is; the property is expected to be distributed around zero). I would like to describe the resulting measurement. Particularly I'm interested in average property value and in the variance.\\n\\nThe average value is simple. First calculate the mean values for each body and then calculate the mean of means.\\n\\nThe variance is more tricky. There are two variances: the variance of measurement and the variance of property values. In order to have an idea on the confidence we have in any single measurement, we need to account for both the sources. Unfortunately, I can't think of a good method. It is obvious that putting all the numbers in a single pool and calculating the stdev of this pool isn't a good idea.\\n\\nAny suggestion?\\n\\n",,
533,1,249,302c254f-60c1-45ca-9604-73042d55303f,"2010-07-20 08:49:13",213,"Variance components",,
534,3,249,302c254f-60c1-45ca-9604-73042d55303f,"2010-07-20 08:49:13",213,<standard-deviation><anova><variance>,,
535,5,163,06b96a64-039a-4b09-ab09-a4c14017ced2,"2010-07-20 08:50:33",81,"Lets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.\\n\\nWhichever number the die lands on is the number of free text-books I will give you.\\n\\nIn this case, the *final amount* of free text books that I give you is the **random variable** because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die) and is not known before I roll the die.","deleted 2 characters in body; added 2 characters in body; added 51 characters in body; deleted 1 characters in body",
536,5,91,73c7e7a1-18c8-46ae-9c56-06e51a161fd4,"2010-07-20 08:52:27",87,"As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. \\n\\nIn the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta "functions" can be used to represent these atoms.    \\n\\n ","typo -- word random at end -- no reason",
537,2,250,4adb1fae-ed9e-4481-8c19-c98b9ec62cbe,"2010-07-20 08:53:15",171,"Here's a data point: R has a ["big data ceiling"][1], useful to know if you plan on working with huge data sets.\\n\\nI'm unsure whether the same limitations apply to Clojure/Incanter, whether it outperforms R or is actually worse. I imagine the JVM can probably handle large datasets, especially if you manage to harness the power of Clojure's lazy features.\\n\\n [1]: http://www.bytemining.com/2010/05/hitting-the-big-data-ceiling-in-r/",,
541,5,243,c2d09690-a22d-4b8c-8747-9351e041d329,"2010-07-20 09:09:01",196,"Depending on the size of the dataset in question, a permutation test might be preferable to a bootstrap in that it may be able to provide an exact test of the hypothesis (and an exact CI).","added 18 characters in body",
542,2,252,26214128-c858-4b89-b234-b5dab46962b1,"2010-07-20 09:13:29",81,"For me personally, I use the following three packages the most, all available from the awesome [Omega Project for Statistical Computing][1] (I do not claim to be an expert, but for my purposes they are very easy to use):\\n\\n - **RCurl**: It has lots of options which allows access to websites that the default functions in base R would have difficulty with I think it's fair to say. It is an R-interface to the libcurl library, which has the *added* benefit of a whole community outside of R developing it. \\n\\n - **XML**: It is very forgiving of parsing malformed XML/HTML. It is an R-interface to the libxml2 library and again has the *added* benefit of a whole community outside of R developing it\\n - **RJSONIO**: It allows one to parse the text returned from a json call and organise it into a list structure for further analysis.The competitor to this package is rjson but this one has the advantage of being vectorised, readily extensible through S3/S4, fast and scalable to large data.  \\n\\n\\n\\n  [1]: http://www.omegahat.org",,
543,16,252,26214128-c858-4b89-b234-b5dab46962b1,"2010-07-20 09:13:29",-1,,,
544,2,253,0f83430f-d739-4f96-a47f-46b6e67d9bfa,"2010-07-20 09:26:05",215,"If you're an economist/econometrician then Grant Farnworth's paper is indispensable and is available on CRAN at:\\nhttp://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf",,
545,2,254,3c9aff0c-d41b-45d9-bb06-0495e5d88123,"2010-07-20 09:30:18",8,"I think if I understand your description correctly, you need to use a [linear mixed model][1]. However, this maybe overkill, since these models are used to find differences between groups. For example, if you have two types of bodies and you wish to determine if they are different.\\n\\nBasically, you have *between* subject variation and *within* subject variation. To fit these models in R, you can use the `lmer` function from the `lme4` library. So if I understand you correctly, your function will look something like this:\\n\\n    fm1 = lmer(measurement ~ (1|Subject), data)\\nIf you are looking for differences between bodies, then it will look something like:\\n    \\n    fm2 = lmer(measurement ~ body + (body|Subject), data)\\n\\nThe command `summary(fm1)` should give the values you are after.\\n\\nBTW, the subject part is usually called the random effect. However, there a many different  views on what a random effect is. See Ch11.4 of [Data analysis using regression][2] by Gelman and Hill for more details.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_effects_model\\n  [2]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/0521867061",,
546,2,255,79f9b990-3e28-47dc-8c3f-9a39d92c011f,"2010-07-20 09:30:51",88,"It may be an overshoot, but you may train an unsupervised Random Forest on the data and use the object proximity measure to detect outliers. More details [here][1].\\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#outliers",,
547,2,256,621e2a12-827f-4729-bdf0-57f1033f923c,"2010-07-20 09:34:22",217,"What is the easiest way to understand boosting?\\n\\nWhy doesn't it boost very weak classifiers "to infinity" (perfectness?) ?",,
548,1,256,621e2a12-827f-4729-bdf0-57f1033f923c,"2010-07-20 09:34:22",217,"How boosting works?",,
549,3,256,621e2a12-827f-4729-bdf0-57f1033f923c,"2010-07-20 09:34:22",217,<boosting>,,
550,2,257,2b0f781d-dd6f-4f63-8799-bb33e6fdab91,"2010-07-20 09:38:44",217,"We may assume that we have CSV file and we want a very basic line plot with several lines on one plot and a simple legend.",,
551,1,257,2b0f781d-dd6f-4f63-8799-bb33e6fdab91,"2010-07-20 09:38:44",217,"What is the easiest way to create publication-quality plots under Linux?",,
552,3,257,2b0f781d-dd6f-4f63-8799-bb33e6fdab91,"2010-07-20 09:38:44",217,<data-visualization><plotting><csv-file>,,
553,2,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,"2010-07-20 09:43:23",217,"Rules:\\n\\n * one classifier per answer\\n * vote up if you agree \\n * downvote/remove duplicates.\\n * put your application in the comment",,
554,1,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,"2010-07-20 09:43:23",217,"Poll: What is the best out-of-the-box 2-class classifier for your application?",,
555,3,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,"2010-07-20 09:43:23",217,<classification><application>,,
556,16,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,"2010-07-20 09:43:23",217,,,
557,2,259,1c25a269-6ecc-48ab-a779-d04957c522ae,"2010-07-20 09:44:11",217,"[Support vector machine][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Support_vector_machine",,
558,16,259,1c25a269-6ecc-48ab-a779-d04957c522ae,"2010-07-20 09:44:11",-1,,,
559,2,260,de6a66dd-8396-40d1-bcfb-a62ef4ee5720,"2010-07-20 09:45:06",217,"[Random forest][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_forest",,
560,16,260,de6a66dd-8396-40d1-bcfb-a62ef4ee5720,"2010-07-20 09:45:06",-1,,,
561,2,261,cddda61d-81b8-44e2-92c2-aa808dff32b9,"2010-07-20 09:45:18",159,"It's hard to go past R for graphics. You could do what you want in 3 lines. For example, assuming the csv file has four columns:\\n\\n    x <- read.csv("file.csv")\\n    matplot(x[,1],x[,2:4],type="l",col=1:3)\\n    legend("topleft",legend=c("A","B","C"),lty=1,col=1:3)\\n\\n",,
562,2,262,42070c99-3512-4032-9f45-5b4ede8e90d2,"2010-07-20 09:50:45",8,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then you can a combination of `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or lattice. In `ggplot2` the following commands should get you started.\\n\\n    require(ggplot2)\\n    \\n    #You would use read.csv here\\n    N = 20\\n    d = data.frame(x=1:N,y=runif(N),z=rnorm(N))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y, colour="Type 1"))\\n    p = p+geom_point(aes(x, z, colour="Type 2"))\\n    \\n    #Optional, if you want your own colours\\n    p = p+scale_colour_manual("Type",c('blue','red'))\\n    p\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n",,
563,2,263,ff419f0c-d347-4cbf-af52-5e9ed20f29fd,"2010-07-20 09:53:11",190,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/",,
564,5,262,90822f7e-201a-4594-82d2-a888a977c0bf,"2010-07-20 09:57:26",8,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then you can a combination of `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or lattice. In `ggplot2` the following commands should get you started.\\n    \\n    require(ggplot2)\\n    #You would use read.csv here\\n    N = 10\\n    d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y1, colour="Type 1"))\\n    p = p+geom_line(aes(x, y2, colour="Type 2"))\\n    p = p+geom_line(aes(x, y3, colour="Type 3"))\\n    #Add points\\n    p = p+geom_point(aes(x, y3, colour="Type 3"))\\n    p   \\n\\nThis would give you the following plot:\\n\\n![Line plot][2]\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://img84.imageshack.us/img84/6393/tmpq.jpg","added 161 characters in body; deleted 10 characters in body",
565,2,264,60321bd8-f86c-4f8f-82ff-fbdd073007c0,"2010-07-20 09:59:00",217,"[Here][1] is an article describing one possible algorith. Source code included and a quite serious application (gravitational wave detection based on laser interferometry), so you can expect it to be well tested.\\n\\n  [1]: http://www.ligo.caltech.edu/docs/T/T030168-00.pdf",,
566,2,265,d2e69d67-ce13-4ce5-97d8-66407a6b4aed,"2010-07-20 10:05:48",88,"In plain English: If your classifier misclassifies some data, train another copy of it only on this misclassified part with hope that it will discover something subtle. And then, as usual, iterate. On the way there are some voting schemes that allow to combine all those classifiers' predictions in sensible way.\\n\\nBecause sometimes it is impossible (the noise is just hiding some of the information, or it is not even present in the data); on the other hand, boosting too much may lead to overfitting.",,
567,5,263,f02959d2-6138-44f9-a3a1-fbee808cac75,"2010-07-20 10:11:30",190,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n\\n\\nSpecifically, for different file formats like svg and eps, use the format parameter of [savefig][2]\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/\\n  [2]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig","added 193 characters in body",
568,5,265,425b3728-b705-4abb-8708-67c9c77b7e8f,"2010-07-20 10:17:16",88,"In plain English: If your classifier misclassifies some data, train another copy of it mainly on this misclassified part with hope that it will discover something subtle. And then, as usual, iterate. On the way there are some voting schemes that allow to combine all those classifiers' predictions in sensible way.\\n\\nBecause sometimes it is impossible (the noise is just hiding some of the information, or it is not even present in the data); on the other hand, boosting too much may lead to overfitting.","added 2 characters in body",
569,6,256,43f53a05-26a2-4a93-8332-51b4bb458079,"2010-07-20 10:18:12",88,<machine-learning><boosting>,"edited tags",
570,6,222,f6190f90-acb9-4116-9b87-f3ed31d4d6d1,"2010-07-20 10:18:50",144,<scores><fundamentals><principal-components>,"edited tags",
571,2,266,9097a62f-a7df-446c-96ef-c329308171fe,"2010-07-20 10:23:56",128,"As you mentioned sorting would be `O(n·log n)` for a window of length `n`. Doing this moving adds another `l=vectorlength` making the total cost `O(l·n·log n)`.\\n\\nThe simplest way to push this is by keeping an ordered list of the last n elements in memory when moving from one window to the next one. As removing/inserting one element from/into an ordered list are both `O(n)` this would result in costs of `O(l·n)`.\\n\\nPseudocode:\\n\\n    l = length(input)\\n    aidvector = sort(input(1:n))\\n    output(i) = aid(n/2)\\n    for i = n+1:l\\n        remove input(i-n) from aidvector\\n        sort aid(n) into aidvector\\n        output(i) = aid(n/2)\\n    \\n        \\n    ",,
572,2,267,cb3c3703-bf18-4a1c-a139-67cc6cf96e60,"2010-07-20 10:35:55",194,"If I have two lists A and B, both of which are subsets of a much larger list C, how can I determine if the degree of overlap of A and B is greater than I would expect by chance?\\n\\nShould I just randomly select elements from C of the same lengths as lists A and B and determine that random overlap, and do this many times to determine some kind or empirical p-value? Is there a better way to test this?",,
573,1,267,cb3c3703-bf18-4a1c-a139-67cc6cf96e60,"2010-07-20 10:35:55",194,"How do I calculate if the degree of overlap between two lists is significant?",,
574,3,267,cb3c3703-bf18-4a1c-a139-67cc6cf96e60,"2010-07-20 10:35:55",194,<statistical-significance>,,
575,5,262,3354eb64-08ac-487c-8d8c-9a237dedde9a,"2010-07-20 10:43:46",8,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then you can a combination of `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or lattice. In `ggplot2` the following commands should get you started.\\n    \\n    require(ggplot2)\\n    #You would use read.csv here\\n    N = 10\\n    d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y1, colour="Type 1"))\\n    p = p+geom_line(aes(x, y2, colour="Type 2"))\\n    p = p+geom_line(aes(x, y3, colour="Type 3"))\\n    #Add points\\n    p = p+geom_point(aes(x, y3, colour="Type 3"))\\n    print(p)   \\n\\nThis would give you the following plot:\\n\\n![Line plot][2]\\n\\n**Saving plots in R**\\n\\nSaving plots in R is straightforward:\\n\\n    #Look at ?jpeg to other different saving options\\n    jpeg("figure.jpg")\\n    print(p)#for ggplot2 graphics\\n    dev.off()\\n\\nInstead of `jpeg`'s you can also save as a `pdf` or postscript file:\\n\\n    #This example uses R base graphics\\n    #Just change to print(p) for ggplot2\\n    pdf("figure.pdf")\\n    plot(d$x,y1, type="l")\\n    lines(d$x, y2)\\n    dev.off()\\n\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://img84.imageshack.us/img84/6393/tmpq.jpg\\n","added 449 characters in body",
576,5,254,27669397-2ee1-4a29-aca5-13baa3719588,"2010-07-20 10:50:24",8,"I think if I understand your description correctly, you need to use a [linear mixed model][1]. However, this maybe overkill, since these models are used to find differences between groups. For example, if you have two types of bodies and you wish to determine if they are different.\\n\\nBasically, you have *between* subject variation and *within* subject variation.\\n\\nTo fit these models in R, you can use the `lmer` function from the `lme4` library. So if I understand you correctly, your function will look something like this:\\n\\n    #Load the R library\\n    library(lme4)\\n\\n    #data is a R data frame that contains your data\\n    #measurement and Subject are variables\\n    fm1 = lmer(measurement ~ (1|Subject), data)\\nIf you are looking for differences between bodies, then it will look something like:\\n    \\n    fm2 = lmer(measurement ~ body + (body|Subject), data)\\n\\nThe command `summary(fm1)` should give the values you are after.\\n\\nHere are some resources that will help you get started:\\n\\n1. [Documentation][2] for the lme4 package\\n1. [Statistics with R][3]\\n\\nMost statistical software will be able to fit models of this type.\\n\\n\\nBTW, the subject part is usually called the random effect. However, there a many different  views on what a random effect is. See Ch11.4 of [Data analysis using regression][4] by Gelman and Hill for more details.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_effects_model\\n  [2]: http://lme4.r-forge.r-project.org/\\n  [3]: http://zoonek2.free.fr/UNIX/48_R/all.html\\n  [4]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/0521867061","added 443 characters in body",
577,5,263,c3a4169f-30c9-46e6-bc7e-c4bae33b0384,"2010-07-20 10:51:40",190,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n\\n\\nSpecifically, for different file formats like svg and eps, use the format parameter of [savefig][2]\\n\\nAn example:\\ninput.csv\\n<pre>"Line 1",0.5,0.8,1.0,0.9,0.9\\n"Line 2",0.2,0.7,1.2,1.1,1.1</pre>\\n\\nCode:\\n\\n    import csv\\n    import matplotlib.pyplot as plt\\n\\n    legends = []\\n    for row in csv.reader(open('input.csv')):\\n        legends.append(row[0])\\n        plt.plot(row[1:])\\n    \\n    plt.legend(legends)\\n    plt.savefig("out.svg", format='svg')\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/\\n  [2]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig","Added some code",
578,2,268,c4123c1f-2c56-4f2f-8d49-10bcdeb7709b,"2010-07-20 11:01:06",107,"You could of for a supervised self-organizing map (e.g. with [kohonen][1] package for R), and use the login frequency as dependent variable. That way, the clustering will focus on separating the frequent visitors from the rare visitors. By plotting the number of users on each map unit, you may get an idea in clusters present in your data.\\n\\nBecause SOMs are non-linear mapping methods, this approach is particularly interesting for tailed data.\\n\\n  [1]: http://cran.r-project.org/web/packages/kohonen/index.html",,
579,2,269,c6791c41-3dcc-45a7-ab56-7f8a02e82392,"2010-07-20 11:07:42",62,"What is the difference between a population and a sample? What common variables and statistics are used for each one, and how do those relate to each other? ",,
580,1,269,c6791c41-3dcc-45a7-ab56-7f8a02e82392,"2010-07-20 11:07:42",62,"What is the difference between a population and a sample? ",,
581,3,269,c6791c41-3dcc-45a7-ab56-7f8a02e82392,"2010-07-20 11:07:42",62,<standard-deviation><population><sample><variance><sample-variance>,,
582,2,270,f4c0576e-e6e2-4578-a05b-8bcdd100e048,"2010-07-20 11:08:47",90,"Due to the factorial in a poisson distribution, it becomes unpractical to estimate poisson models (for example, using maximum likelihood) when the observations are large. So, for example, if I am trying to estimate a model to explain the number of suicides in a given year (only annual data are available), and say, there are thousands of suicides every year, is it wrong to express suicides in hundreds, so that 2999 would be 29.99 ~= 30? In other words, is it wrong to change the unit of measurement to make the data manageable? ",,
583,1,270,f4c0576e-e6e2-4578-a05b-8bcdd100e048,"2010-07-20 11:08:47",90,"Poisson regression with large data: is it wrong to change the unit of measurement?",,
584,3,270,f4c0576e-e6e2-4578-a05b-8bcdd100e048,"2010-07-20 11:08:47",90,<modeling><data><poisson>,,
585,2,271,d5a0fe51-fff2-4def-a129-f3826d28ebe6,"2010-07-20 11:10:42",8,"If I understand your question correctly, you need to use the [Hypergeometric distribution][1]. This distribution is usually associated with urn models, i.e there are n balls in an urn, *y* are painted red, and you draw *m* balls from the urn. Then if *X* is the number of balls in your sample of *m* that are red, *X* has a hyper-geometric distribution.\\n\\nFor your specific example, let n_A, n_B and n_C denote the lengths of your three lists and let n_A_B denote the overlap between A and B. Then \\n\\nn_A_B ~ HG(n_A, n_C, n_B)\\n\\nTo calculate a p-value, you could use this R command: \\n\\n    #Some example values\\n    n_A = 100;n_B = 200; n_C = 500; n_A_B = 50\\n    1-phyper(n_A_B, n_B, n_C-n_B, n_A)\\n    [1] 0.008626697\\n\\nWord of caution. Remember multiple testing, i.e. if you have lots of *A* and *B* lists, then you will need to adjust your p-values with a correction. For the example the FDR or Bonferroni corrections.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Hypergeometric_distribution",,
586,2,272,8409944f-ff4e-49ef-97fa-d9e7d8d03670,"2010-07-20 11:14:41",216,"Also, for some elaborate discussion (including bashing of ADF / PP / KPSS :) you might want to have a look at the book by Maddala and Kim:\\n\\nhttp://www.amazon.com/Cointegration-Structural-Change-Themes-Econometrics/dp/0521587824\\n\\nQuite extensive and not very easy to read sometimes, but a useful reference.",,
587,2,273,87602318-e5c3-40f5-ae80-e0060cec839d,"2010-07-20 11:18:01",216,"* Clearly R\\n\\n* RadidMiner is nice, but switching to thinking in terms of operators takes a moment\\n\\n* Matlab / Octave\\n\\nIf you describe a specific problem, I may be able to get more specific.",,
588,5,249,819480f7-fed2-4d92-8e64-ca88471b8f2b,"2010-07-20 11:19:46",213,"I have a set of *N* bodies, which is a random sample from a population whose mean and variance I want to estimate. A property of each body is being measured m_i times (m_i>1) and different for each body index i  identifies which body it is; the property is expected to be distributed around zero). I would like to describe the resulting measurement. Particularly I'm interested in average property value and in the variance.\\n\\nThe average value is simple. First calculate the mean values for each body and then calculate the mean of means.\\n\\nThe variance is more tricky. There are two variances: the variance of measurement and the variance of property values. In order to have an idea on the confidence we have in any single measurement, we need to account for both the sources. Unfortunately, I can't think of a good method. It is obvious that putting all the numbers in a single pool and calculating the stdev of this pool isn't a good idea.\\n\\nAny suggestion?\\n\\n**EDIT**\\nColin Gillespie suggests applying Random Effects Model. This model seems to be the right solution for my case, except for the fact that it is described (in Wikipedia) for the cases where each group (body in my case) is sampled equally (m_i is constant for all the bodies), which is not correct in my case\\n\\n","added 318 characters in body; edited tags; edited tags",
589,6,249,819480f7-fed2-4d92-8e64-ca88471b8f2b,"2010-07-20 11:19:46",213,<standard-deviation><variance><anova><random-effects-model>,"added 318 characters in body; edited tags; edited tags",
590,2,274,c3465352-1611-492d-ae45-ce668c8d9b84,"2010-07-20 11:21:59",90,"The population is the whole set of values, or individuals, you are interested in. The sample is a subset of the population, and is the set of values you actually use in your estimation.\\n\\nSo, for example, if you want to know the average height of the residents of China, that is your population, ie, the population of China. The thing is, this is quite large a number, and you wouldn't be able to get data for everyone there. So you draw a sample, that is, you get some observations, or the height of some of the people in China (a subset of the population, the sample) and do your inference based on that. ",,
591,2,275,d96deb70-98d1-408f-9db2-2222b7e621be,"2010-07-20 11:29:53",88,"In case of Poisson it is bad, since counts are counts -- their unit is an unity. On the other hand, if you'd use some advanced software like R, its Poisson handling functions will be aware of such large numbers and would use some numerical tricks to handle them.",,
592,5,253,3ef84eac-5e28-41ca-8704-cf97c7e17fdc,"2010-07-20 11:37:25",215,"If you're an economist/econometrician then Grant Farnworth's paper on using R is indispensable and is available on CRAN at:\\nhttp://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf","added 11 characters in body",
593,2,276,7b985e90-5544-4f51-9c19-3de8d9f83968,"2010-07-20 11:43:25",90,"Is there a rule-of thumb or even any way at all to tell how large a sample should be in order to estimate a model with a given number of parameters? \\n\\nSo, for example, if I want to estimate a least-squares regression with 5 parameters, how large should the sample be? \\n\\nDoes it matter what estimation technique you are using (e.g. maximum likelihood, least squares, GMM), or how many or what tests you are going to perform? Should the sample variability be taken into account when making the decision?",,
594,1,276,7b985e90-5544-4f51-9c19-3de8d9f83968,"2010-07-20 11:43:25",90,"How large should a sample be for a given estimation technique and parameters?",,
595,3,276,7b985e90-5544-4f51-9c19-3de8d9f83968,"2010-07-20 11:43:25",90,<sample-size><estimation><least-squares><maximum-likelihood>,,
596,2,277,d8cbe22f-ad70-487a-a0c2-69090041f56b,"2010-07-20 11:49:02",215,"When would one prefer to use a Conditional Autoregressive model over a Simultaneous Autoregressive model when modelling autocorrelated geo-referenced areal data?",,
597,1,277,d8cbe22f-ad70-487a-a0c2-69090041f56b,"2010-07-20 11:49:02",215,"Spatial Statistics Models - CAR v SAR",,
598,3,277,d8cbe22f-ad70-487a-a0c2-69090041f56b,"2010-07-20 11:49:02",215,<modeling><spatial>,,
599,2,278,880786c2-bddc-45fe-a04f-a3d739ee9f8f,"2010-07-20 11:49:27",221,"When a non-hierarchical cluster analysis is carried out, the order of observations in the data file determine the clustering results, especially if the data set is small (i.e, 5000 observations). To deal with this problem I usually performed a random reorder of data observations. My problem is that if I replicate the analysis n times, the results obtained are different and sometimes these differences are great. \\n\\nHow can I deal with this problem? Maybe I could run the analysis several times and after consider that one observation belong to the group in which more times was assigned. Has someone a better approach to this problem?\\n\\nManuel Ramon",,
600,1,278,880786c2-bddc-45fe-a04f-a3d739ee9f8f,"2010-07-20 11:49:27",221,"How to deal with the effect of the order of observations in a non hierarchical cluster analysis? ",,
601,3,278,880786c2-bddc-45fe-a04f-a3d739ee9f8f,"2010-07-20 11:49:27",221,<clustering><nonhierarchical><order><repeatability>,,
602,2,279,bad63b7f-9c81-40ef-80bb-e076053602e7,"2010-07-20 11:54:15",62,"When you're dealing with a Poisson distribution with large values of \\lambda (its parameter), it is common to use a normal approximation to the Poisson distribution. \\n\\nAs [this site][1] mentions, it's all right to use the normal approximation when \\lambda gets over 20, and the approximation improves as \\lambda gets even higher. \\n\\nThe Poisson distribution is defined only over the state space consisting of the non-negative integers, so rescaling and rounding is going to introduce odd things into your data. \\n\\nUsing the normal approx. for large Poisson statistics is VERY common.\\n\\n\\n  [1]: http://www.stat.ucla.edu/~dinov/courses_students.dir/Applets.dir/NormalApprox2PoissonApplet.html ",,
603,2,280,1c8a9441-a3fd-4012-b37a-0c20ac629776,"2010-07-20 11:54:50",221,"The [R project][1] website has lots of manuals to start, and I suggest you the [Nabble R forum][2] and the [R-bloggers][3] site as well. \\n\\n\\n  [1]: http://www.r-project.org/\\n  [2]: http://r.789695.n4.nabble.com/\\n  [3]: http://www.r-bloggers.com/",,
604,2,281,4029043d-567f-40f0-8239-329dbfd1ffe8,"2010-07-20 11:59:56",215,"\\nDay-to-day the most useful package must be "foreign" which has functions for reading and writing data for other statistical packages e.g. Stata, SPSS, Minitab, SAS, etc. Working in a field where R is not that commonplace means that this is a very important package.",,
605,16,281,4029043d-567f-40f0-8239-329dbfd1ffe8,"2010-07-20 11:59:56",-1,,,
606,2,282,6d08f421-4ebd-4186-9ee6-66a62986de24,"2010-07-20 12:02:26",81,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define Principal Component Analysis:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is form of multidimensional scaling. It ia linear transformation of the variables into a lower dimensional space which maintains all of the variances of the variables. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = TRUE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                   PC1        PC2   \\n    Maths   -0.5477226 -0.4082483 \\n    Science  0.6210590 -0.1543033  \\n    English  0.5477226 -0.4082483 \\n    Music    0.1195229  0.8017837 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                      y\\n    John -0.55*80 + 0.62*90 + 0.55*60 + 0.12*55  -0.41*80 + -0.15*85 + -0.41*60 + 0.80*55 \\n    Mike -0.55*90 + 0.62*85 + 0.55*70 + 0.12*45  -0.41*90 + -0.15*85 + -0.41*70 + 0.80*45\\n    Kate -0.55*95 + 0.62*80 + 0.55*40 + 0.12*50  -0.41*95 + -0.15*80 + -0.41*40 + 0.80*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  51.4    -26.15 \\n    Mike  47.1    -42.35\\n    Kate  25.35   -27.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n",,
607,5,282,74b8eeee-9d86-41fc-8f12-c9cf54cd3719,"2010-07-20 12:07:52",81,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define Principal Component Analysis:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which maintains all of the variances of the variables. For example, this would mean we could compare which person did best across all subjuects. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = TRUE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                   PC1        PC2   \\n    Maths   -0.5477226 -0.4082483 \\n    Science  0.6210590 -0.1543033  \\n    English  0.5477226 -0.4082483 \\n    Music    0.1195229  0.8017837 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                      y\\n    John -0.55*80 + 0.62*90 + 0.55*60 + 0.12*55  -0.41*80 + -0.15*85 + -0.41*60 + 0.80*55 \\n    Mike -0.55*90 + 0.62*85 + 0.55*70 + 0.12*45  -0.41*90 + -0.15*85 + -0.41*70 + 0.80*45\\n    Kate -0.55*95 + 0.62*80 + 0.55*40 + 0.12*50  -0.41*95 + -0.15*80 + -0.41*40 + 0.80*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  51.4    -26.15 \\n    Mike  47.1    -42.35\\n    Kate  25.35   -27.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, but I hope you get the idea.\\n","added 94 characters in body; added 94 characters in body",
608,2,283,81644edd-0912-4bc9-b8e0-fa8bde868bfe,"2010-07-20 12:09:08",215,"What is meant when we say we have a saturated model?",,
609,1,283,81644edd-0912-4bc9-b8e0-fa8bde868bfe,"2010-07-20 12:09:08",215,"What is a "saturated" model?",,
610,3,283,81644edd-0912-4bc9-b8e0-fa8bde868bfe,"2010-07-20 12:09:08",215,<modeling><regression><random-variables>,,
611,5,282,52ace550-9f77-4bb5-99a6-322aa036043b,"2010-07-20 12:14:11",81,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define Principal Component Analysis:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjuects. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = TRUE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                   PC1        PC2   \\n    Maths   -0.5477226 -0.4082483 \\n    Science  0.6210590 -0.1543033  \\n    English  0.5477226 -0.4082483 \\n    Music    0.1195229  0.8017837 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                      y\\n    John -0.55*80 + 0.62*90 + 0.55*60 + 0.12*55  -0.41*80 + -0.15*85 + -0.41*60 + 0.80*55 \\n    Mike -0.55*90 + 0.62*85 + 0.55*70 + 0.12*45  -0.41*90 + -0.15*85 + -0.41*70 + 0.80*45\\n    Kate -0.55*95 + 0.62*80 + 0.55*40 + 0.12*50  -0.41*95 + -0.15*80 + -0.41*40 + 0.80*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  51.4    -26.15 \\n    Mike  47.1    -42.35\\n    Kate  25.35   -27.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, but I hope you get the idea.\\n","added 9 characters in body",
612,2,284,a336071c-12a1-49db-8519-e4f3980ea013,"2010-07-20 12:25:30",215,"\\nA random variable, usually denoted X, is a variable where the outcome is uncertain. The observation of a particular outcome of this variable is called a realisation. More concretely, it is a function which maps a probability space into a measurable space, usually called a state space. Random variables are discrete (can take a number of distinct values) or continuous (can take an infinite number of values). \\n\\nConsider the random variable X which is the total obtained when rolling two dice. It can take any of the values 2-12 (with equal probability given fair dice) and the outcome is uncertain until the dice are rolled. ",,
613,2,285,0cd0436c-873e-400d-bd01-8e4b5732d46d,"2010-07-20 12:26:39",183,"You might consider transforming (perhaps a log) the positively skewed variables.\\n\\nIf after exploring various clustering algorithms you find that the four variables simply reflect varying intensity levels of usage, you might think about a theoretically based classification. Presumably this classification is going to be used for a purpose and that purpose could drive meaningful cut points on one or more of the variables.",,
614,5,275,8e397344-768c-453d-8c17-1dd0c5f87761,"2010-07-20 12:26:55",88,"In case of Poisson it is bad, since counts are counts -- their unit is an unity. On the other hand, if you'd use some advanced software like R, its Poisson handling functions will be aware of such large numbers and would use some numerical tricks to handle them.\\n\\nObviously I agree that normal approximation is another good approach.","added 73 characters in body",
615,2,286,010717b7-b5c3-46ba-8cc1-74b6628851d1,"2010-07-20 12:28:32",211,"There's a superb Probability book here:\\nhttp://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html\\nwhich you can also buy in hardcopy.;",,
616,2,287,ce653f04-8786-4f7b-8a2a-a62572d71626,"2010-07-20 12:29:17",90,"Can someone explain to me the difference between method of moments and GMM (general method of moments), their relationship, and when should one or the other be used?",,
617,1,287,ce653f04-8786-4f7b-8a2a-a62572d71626,"2010-07-20 12:29:17",90,"What is the difference/relationship between method of moments and GMM?",,
618,3,287,ce653f04-8786-4f7b-8a2a-a62572d71626,"2010-07-20 12:29:17",90,<estimation-technique><gmm><method-of-moments>,,
619,2,288,b859d50f-cb99-436b-9f68-2fe4377e04e2,"2010-07-20 12:29:34",220,"Suppose that I culture cancer cells in _n_ different dishes _g₁_, _g₂_, … , _g<sub>n</sub>_ and observe the number of cells _n<sub>i</sub>_ in each dish that look different than normal.  The total number of cells in dish _g<sub>i</sub>_ is _t<sub>i</sub>_.  There is individual differences between individual cells, but also differences between the populations in different dishes because each dish has a slightly different temperature, amount of liquid, and so on.\\n\\nI model this as a beta-binomial distribution: _n<sub>i</sub>_ ~ Binomial(_p<sub>i</sub>_, _t<sub>i</sub>_) where _p<sub>i</sub>_ ~ Beta(_α_, _β_).  Given a number of observations of _n<sub>i</sub>_ and _t<sub>i</sub>_, how can I estimate _α_ and _β_?",,
620,1,288,b859d50f-cb99-436b-9f68-2fe4377e04e2,"2010-07-20 12:29:34",220,"Estimating beta-binomial distribution",,
621,3,288,b859d50f-cb99-436b-9f68-2fe4377e04e2,"2010-07-20 12:29:34",220,<beta-binomial>,,
622,2,289,8a0441c7-e5aa-4c74-b2fd-68b429e2bdb3,"2010-07-20 12:32:41",80,"For visualizing graphs in a Java/SWT environment, check out Zest: http://eclipse.org/gef/zest",,
623,2,290,3d26e3c2-4e43-4d2f-bb63-3d5504168f53,"2010-07-20 12:33:30",189,"I know of Cameron and Trivedi's Microeconometrics Using Stata. \\n\\nWhat are other good text for learning the Stata? ",,
624,1,290,3d26e3c2-4e43-4d2f-bb63-3d5504168f53,"2010-07-20 12:33:30",189,"Resources for learning Stata",,
625,3,290,3d26e3c2-4e43-4d2f-bb63-3d5504168f53,"2010-07-20 12:33:30",189,<textbook>,,
626,2,291,8fba3f4f-f734-4b0b-be44-dc37ddbf67b9,"2010-07-20 12:35:43",211,http://lib.stat.cmu.edu/DASL/,,
627,2,292,1d599a2d-cdc1-4d54-97e4-02c8e7435783,"2010-07-20 12:38:38",56,"It should always be large enough! ;)\\n\\nAll parameter estimates come with an estimate uncertainty, which is determined by the sample size. If you carry out a regression analysis, it helps to remind yourself that the &Chi;<sup>2</sup> distribution is constructed from the input data set. If your model had 5 parameters and you had 5 data points, you would only be able to calculate a single point of the &Chi;<sup>2</sup> distribution. Since you will need to minimize it, you could only pick that one point as a guess for the minimum, but would have to assign infinite errors to your estimated parameters. Having more data points would allow you to map the parameter space better leading to a better estimate of the minimum of the &Chi;<sup>2</sup> distribution and thus smaller estimator errors.\\n\\nWould you be using a Maximum Likelihood estimator instead the situation would be similar: More data points leads to better estimate of the minimum.\\n\\nAs for point variance, you would need to model this as well. Having more data points would make clustering of points around the "true" value more obvious (due to the Central Limit Theorem) and the danger of interpreting a large, chance flucuation as the true value for that point would go down. And as for any other parameter your estimate for the point variance would become more stable the more data points you have.",,
628,2,293,9147a5db-6c69-4ffc-97db-0cd66fe3f855,"2010-07-20 12:43:02",211,"I think you need to rework this question.   It all depends on the problem/data which has generated the cross-tab.   ",,
629,2,294,1dc91fa3-07cd-4a01-b078-7d2383fb769d,"2010-07-20 12:47:39",211,"Bernard Flury, in his excellent book introducing multivariate analysis, described this as an anti-property of principal components.   It's actually worse than choosing between correlation or covariance.   If you changed the units (e.g. US style gallons, inches etc. and EU style litres, centimetres) you will get substantively different projections of the data.\\n\\nThe argument against automatically using correlation matrices is that it is quite a brutal way of standardising your data.   The problem with automatically using the covariance matrix, which is very apparent with that heptathalon data, is that the variables with the highest variance will dominate the first principal component (the variance maximising property).\\n\\nSo the "best" method to use is based on a subjective choice, careful thought and some experience.",,
630,5,282,0522d301-748f-4fb5-b863-f0fe78ff066c,"2010-07-20 12:52:16",81,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjects. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                PC1         PC2\\nMaths    0.27795606  0.76772853 \\nScience -0.17428077 -0.08162874 \\nEnglish -0.94200929  0.19632732 \\nMusic    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n","added 133 characters in body",
631,2,295,7b6012b4-1c0c-40fd-83b5-a868d0cf88d7,"2010-07-20 12:52:55",62,"A nice definition of p-value is "the probability of observing a test statistic at least as large as the one calculated assuming the null hypothesis is true". \\n\\nThe problem with that is that it requires an understanding of "test statistic" and "null hypothesis". But, that's easy to get across. If the null hypothesis is true, usually something like "parameter from population A is equal to parameter from population B", and you calculate statistics to estimate those parameters, what is the probability of seeing a test statistic that says, "they're this different"?\\n\\nE.g., If the coin is fair, what is the probability I'd see 60 heads out of 100 tosses? That's testing the null hypothesis, "the coin is fair", or "p = .5" where p is the probability of heads.\\n\\nThe test statistic in that case would be the number of heads. \\n\\nNow, I *assume* that what you're calling "t-value" is a generic "test statistic", not a value from a "t distribution". They're not the same thing, and the term "t-value" isn't (necessarily) widely used and could be confusing.\\n\\nWhat you're calling "t-value" is probably what I'm calling "test statistic". In order to calculate a p-value (remember, it's just a probability) you need a distribution, and a value to plug into that distribution which will return a probability. Once you do that, the probability you return is your p-value. You can see that they are related because under the same distribution, different test-statistics are going to return different p-values. More extreme test-statistics will return lower p-values giving greater indication that the null hypothesis is false. \\n\\nI've ignored the issue of one-sided and two-sided p-values here.  \\n",,
632,2,296,f3660aa9-db1c-4832-96c2-47b6956a13cb,"2010-07-20 12:54:28",127,"[MLComp][1] has quite a few interesting datasets, and as a bonus your algorithm will get ranked if you upload it.\\n\\n\\n  [1]: http://mlcomp.org/",,
633,2,297,a891f0fc-5407-4155-8c8c-add58bc55160,"2010-07-20 13:08:42",56,"I really enjoy working with [RooFit][1] for easy proper fitting of signal and background distributions and [TMVA][2] for quick principal component analyses and modelling of multivariate problems with some standard tools (like genetic algorithms and neural networks, also does BDTs). They are both part of the [ROOT][3] C++ libraries which have a pretty heavy bias towards particle physics problems though. \\n\\n[1]: http://roofit.sourceforge.net/\\n[2]: http://tmva.sourceforge.net/\\n[3]: http://root.cern.ch/drupal/",,
634,16,297,a891f0fc-5407-4155-8c8c-add58bc55160,"2010-07-20 13:08:42",-1,,,
635,2,298,c2be7b80-7399-44fc-8fe8-fc96ec7716d1,"2010-07-20 13:11:50",125,"Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?",,
636,1,298,c2be7b80-7399-44fc-8fe8-fc96ec7716d1,"2010-07-20 13:11:50",125,"In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?",,
637,3,298,c2be7b80-7399-44fc-8fe8-fc96ec7716d1,"2010-07-20 13:11:50",125,<distributions><regression><logarithm>,,
638,2,299,eb59ca47-6607-42b2-bc9b-6c8e7b4ca104,"2010-07-20 13:16:29",5,"One typically takes the log of an input variable to scale it and change the distribution (e.g. to make it normally distributed).  This is discussed in most introductory statistics texts.  \\n\\nYou can also read Andrew Gelman's paper on ["Scaling regression inputs by dividing by two standard deviations"][1] for a discussion on this.\\n\\nTaking the log is not an appropriate method for dealing with bad data/outliers.\\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf",,
639,2,300,4dd5a372-81ba-4191-9ac6-8bf91ae74749,"2010-07-20 13:19:31",56,"A "right" answer cannot depend on an arbitrary ordering of some method you are using.\\n\\nYou need to consider all possible orderings (or some representative sample) and estimate your parameters for every case. This will give you distributions for the parameters you are trying to estimate. Estimate the "true" parameter values from these distributions (this will also give you an estimate for your estimator error).\\n\\nAlternatively use a method that doesn't introduce an ordering.",,
640,5,299,a289dbc5-0ba1-410b-97f1-b011c707cfd4,"2010-07-20 13:22:18",5,"One typically takes the log of an input variable to scale it and change the distribution (e.g. to make it normally distributed).  It cannot be done blindly however; you need to be careful when making any scaling to ensure that the results are still interpretable.  \\n\\nThis is discussed in most introductory statistics texts.  You can also read Andrew Gelman's paper on ["Scaling regression inputs by dividing by two standard deviations"][1] for a discussion on this.  He also has a very nice discussion on this at the beginning of ["Data Analysis Using Regression and Multilevel/Hierarchical Models"][2].\\n\\nTaking the log is not an appropriate method for dealing with bad data/outliers.\\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf\\n  [2]: http://www.stat.columbia.edu/~gelman/arm/","added 323 characters in body",
641,2,301,f3f10e2c-a188-4ce3-83f9-0a5fc4cd5eee,"2010-07-20 13:22:40",8,"You tend to take logs of the data when there is a problem with the residuals. For example, if you plot the residuals against a particular covariate and observe an increasing/decreasing pattern (a funnel shape), then a transformation may be appropriate. Non-random residuals usually indicate that your model assumptions are wrong, i.e. non-normal data.\\n\\nSome data types automatically lend themselves to logarithmic transformations. For example, I usually take logs when dealing with concentrations or age. \\n\\nAlthough transformations aren't primarily used to deal outliers, they do help since taking logs squashes your data.",,
642,5,282,52b9a42b-5a2d-4b05-a068-126ddb3eb456,"2010-07-20 13:34:57",81,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjects. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n","added 20 characters in body",
643,4,242,678e8a67-4d1e-4a24-9288-3509ca6af74f,"2010-07-20 13:58:58",199,"Using time series analysis to analyze/predict violent behavior","edited title",
644,2,302,3fa50263-2bfd-46db-90c2-973e16b8bfa3,"2010-07-20 14:04:12",196,"I've heard two rules of thumb in this regard.  One holds that so long as there are enough observations in the error term to evoke the central limit theorem, e.g. 20 or 30, you are fine.  The other holds that for each estimated slope one should have at least 20 or 30 observations.  The difference between using 20 or 30 as the target number is based on different thoughts regarding when there are enough observations to reasonably evoke the Central Limit Theorem. ",,
645,2,303,b70a987b-72c8-422e-8116-ccc796cd7a73,"2010-07-20 14:08:42",NULL,"You have a hierarchical bayesian model. Brief details below:\\n\\nLikelihood Function: \\n\\nf(n<sub>i</sub> | p<sub>i</sub>, t<sub>i</sub>) = (t<sub>i</sub> n<sub>i</sub>) p<sub>i</sub><sup>n<sub>i</sub></sup> (1-p<sub>i</sub>)<sup>(t<sub>i</sub> - n<sub>i</sub>)</sup>\\n\\nPriors on p<sub>i</sub>, &alpha;, &beta;:\\n\\npi ~ Beta(&alpha;, &beta;)\\n\\n&alpha; ~ N(&alpha;_mean, &alpha;_var) I(&alpha; > 0)\\n\\n&beta; ~ N(&beta;_mean, &beta;_var) I(&beta; > 0)\\n\\nPosteriors are:\\n\\np<sub>i</sub> ~ Beta(&alpha; + ni, &beta; + ti-ni)\\n\\n&alpha; &prop; I(&alpha; > 0) &prod; p<sub>i</sub><sup>(&alpha;-1)</sup> exp(-(&alpha;-&alpha;_mean)<sup>2</sup>) / (2 &alpha;_var)\\n\\n&beta; &prop; I(&beta; > 0) &prod; (1-p<sub>i</sub>)^(&beta;-1) exp(-(&beta;-&beta;_mean)<sup>2</sup>) / (2 &beta;_var)\\n\\nYou can then use a combination of Gibbs and Metropolis-Hastings to draw from the posterior distributions.",,user28
646,2,304,7df5c293-9de3-408f-9668-6a63f1269b46,"2010-07-20 14:13:50",196,"Shane's point that taking the log to deal with bad data is well taken.  As is Colin's regarding the importance of normal residuals.  In practice I find that usually you can get normal residuals if the input and output variables are also relatively normal.  In practice this means eyeballing the distribution of the transformed and untransformed datasets and assuring oneself that they have become more normal and/or conducting tests of normality (e.g. Shapiro-Wilk or Kolmogorov-Smirnov tests) and determining whether the outcome is more normal.  Interpretablity and tradition are also important.  For example, in cognitive psychology log transforms of reaction time are often used, however, to me at least, the interpretation of a log RT is unclear.  Furthermore, one should be cautious using log transformed values as the shift in scale can change a main effect into an interaction and vice versa.",,
647,5,263,b1c38694-6f90-4d72-955a-0d670db1ea3d,"2010-07-20 14:14:17",190,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n- Full latex support for labels/legends etc. So same typesetting as in the rest of your document!\\n\\n\\nSpecifically, for different file formats like svg and eps, use the format parameter of [savefig][2]\\n\\nAn example:\\ninput.csv\\n<pre>"Line 1",0.5,0.8,1.0,0.9,0.9\\n"Line 2",0.2,0.7,1.2,1.1,1.1</pre>\\n\\nCode:\\n\\n    import csv\\n    import matplotlib.pyplot as plt\\n\\n    legends = []\\n    for row in csv.reader(open('input.csv')):\\n        legends.append(row[0])\\n        plt.plot(row[1:])\\n    \\n    plt.legend(legends)\\n    plt.savefig("out.svg", format='svg')\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/\\n  [2]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig","added 99 characters in body",
648,6,212,a4c1b0c1-43f0-4bbf-b757-8e698a343249,"2010-07-20 14:19:26",190,<statistical-significance>,"edited tags",
649,2,305,f78bfc90-da5a-45ad-91fa-6da337e423b1,"2010-07-20 14:19:41",196,"It seems like when the assumption of homogeneity of variance is met that the results from a Welch adjusted t-test and a standard t-test are approximately the same.  Why not simply always use the Welch adjusted t?",,
650,1,305,f78bfc90-da5a-45ad-91fa-6da337e423b1,"2010-07-20 14:19:41",196,"When conducting a t-test why would one prefer to assume (or test for) equal variances rather than always use a Welch approximation of the df?",,
651,3,305,f78bfc90-da5a-45ad-91fa-6da337e423b1,"2010-07-20 14:19:41",196,<variance><t-test><homogeneity><of>,,
652,2,306,5fe00df2-0a10-4696-82ba-1f17fe2250ab,"2010-07-20 14:21:37",5,"The model that fits the data doesn't have to be a time series model; I would advise thinking outside the box a little.  \\n\\nIf you have multiple variables (e.g. age, gender, diet, ethnicity, illness, medication) you can use these for a different model.  Maybe having certain patients in the same room is an important predictor?  Or perhaps it has to do with the attending staff?  Or consider using a multi-variate time series model (e.g. VECM) if you have other variables that you can use.  Look at the relationships between violence across patients: do certain patients act out together?  \\n\\nThe time series model is useful if time has some important role in the behavior.  For instance, there might be a clustering of violence.  Look at the volatility clustering literature.  As @Jonas suggests, with a lag order of 2, you may need to be on higher alert on the day following a burst in violence.  But that doesn't help you prevent the first day: there may be other information that you can link into the analysis to actually *understand* the cause of the violence, rather than simply forcasting it in a time series fashion.\\n\\nLastly, as a technical suggestion: if you're using R for the analysis, you might have a look at [the forecast package][1] from Rob Hyndman (the creator of this site).  This has many very nice features; see the paper ["Automatic Time Series Forecasting: The forecast Package for R"][2] in the Journal of Statistical Software.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/forecast/index.html\\n  [2]: http://www.jstatsoft.org/v27/i03",,
653,2,307,5f6e50cb-0598-4541-aa1c-792e4921ba06,"2010-07-20 14:23:30",229,"A saturated model is one in which there are as many estimated parameters as data points. By definition, this will lead to a perfect fit, but will be of little use statistically, as you have no data left to estimate variance.\\n\\nFor example, if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).",,
654,6,283,64482c84-ec66-420b-bd66-36b9e01f5319,"2010-07-20 14:26:17",NULL,<modeling><regression>,"edited tags",user28
655,5,163,1be5733c-1e25-4149-9572-7b21be461afe,"2010-07-20 14:37:39",81,"**Definition:**\\n\\nA random variable is a measurable function from a probability space into a measurable space  known as the state space.\\n\\n**Example:**\\n\\nLets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.\\n\\nWhichever number the die lands on is the number of free text-books I will give you.\\n\\nIn this case, the *final amount* of free text books that I give you is the **random variable** because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die) and is not known before I roll the die.","added 157 characters in body",
656,2,308,4f8bb450-e10e-401b-a4c2-b7fc0c7fcccd,"2010-07-20 14:38:07",22,"There are couple of good links with introductory material at Princton Uni Library [website][1].\\n\\n\\n  [1]: http://libguides.princeton.edu/content.php?pid=27916&sid=459449",,
657,2,309,f6f61d6b-57d9-4d0b-9a6d-eb1c8d51ec36,"2010-07-20 14:40:32",88,"The fact that something more complex reduces to something less complex when some assumption is checked is not enough to throw the simpler method away. ",,
658,2,310,6c4f6d01-b97d-405c-990e-429fb29df097,"2010-07-20 14:41:33",NULL,"You can simply ignore the 'factorial' when using maximum likelihood. Here is the reasoning for your suicides example. Let:\\n\\n&lambda; : Be the expected number of suicides per year\\n\\nk<sub>i</sub>: Be the number of suicides in year i.\\n\\nThen you would maximize the log-likelihood as:\\n\\nLL = &sum; ( k<sub>i</sub> log(&lambda;) - &lambda; - k<sub>i</sub>! )\\n\\nMaximizing the above is equivalent to maximizing the following as k<sub>i</sub>! is a constant :\\n\\nLL<sup>'</sup> = &sum; ( k<sub>i</sub> log(&lambda;) - &lambda; )\\n\\nCould explain why the factorial is an issue? Am I missing something?",,user28
659,2,311,78ac29d0-3df6-4e8c-bf7a-49be87d99441,"2010-07-20 14:47:31",89,"Rob Hyndman gave the easy exact answer for a fixed n.  If you're interested in asymptotic behavior for large n, this is handled in the field of [extreme value theory][1].  There is a small family of possible limiting distributions; see for example the first chapters of [this book][2].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Extreme_value_theory\\n  [2]: http://books.google.com/books?id=3ZKmAAAAIAAJ&q=leadbetter+and+lindgren&dq=leadbetter+and+lindgren&hl=en&ei=z7ZFTPPFH9T-nAfa58HaAw&sa=X&oi=book_result&ct=result&resnum=1&ved=0CC0Q6AEwAA",,
660,5,262,7f8929c1-dbb7-4835-886f-61c7815a51b0,"2010-07-20 14:48:45",8,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then use a combination of the `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or [lattice][2]. \\n\\nIn `ggplot2` the following commands should get you started.\\n    \\n    require(ggplot2)\\n    #You would use read.csv here\\n    N = 10\\n    d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y1, colour="Type 1"))\\n    p = p+geom_line(aes(x, y2, colour="Type 2"))\\n    p = p+geom_line(aes(x, y3, colour="Type 3"))\\n    #Add points\\n    p = p+geom_point(aes(x, y3, colour="Type 3"))\\n    print(p)   \\n\\nThis would give you the following plot:\\n\\n![Line plot][3]\\n\\n**Saving plots in R**\\n\\nSaving plots in R is straightforward:\\n\\n    #Look at ?jpeg to other different saving options\\n    jpeg("figure.jpg")\\n    print(p)#for ggplot2 graphics\\n    dev.off()\\n\\nInstead of `jpeg`'s you can also save as a `pdf` or postscript file:\\n\\n    #This example uses R base graphics\\n    #Just change to print(p) for ggplot2\\n    pdf("figure.pdf")\\n    plot(d$x,y1, type="l")\\n    lines(d$x, y2)\\n    dev.off()\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://lmdvr.r-forge.r-project.org/figures/figures.html\\n  [3]: http://img84.imageshack.us/img84/6393/tmpq.jpg","Added a link to lattice",
661,6,31,4c9e71d4-b0cb-4fa2-8dd6-25bdda1ab816,"2010-07-20 14:55:42",62,<hypothesis-testing><p-value><t-value>,"edited tags",
662,5,151,d99850c8-1e4e-4ae4-b59c-397f7354cae0,"2010-07-20 14:56:51",81,"If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.\\n\\nThe benefits of squaring include:\\n\\n - Squaring always gives a positive\\n   value, so the sum will not be zero.\\n - Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).\\n\\nSquaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.\\n\\nI suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)\\n\\n***It's important to note*** ***however*** that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.\\n\\nMy view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.\\n\\nAn much more indepth analysis can be read [here][1].\\n\\n \\n\\n\\n  [1]: http://www.leeds.ac.uk/educol/documents/00003759.htm","added 6 characters in body",
663,5,282,a71e6a62-f9d1-4fc1-93e9-d38ebf91f284,"2010-07-20 14:58:15",81,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 12 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjects. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n","edited body",
664,2,312,e72a8baa-2b07-4bb8-a08d-55707ff8e2fd,"2010-07-20 15:03:40",NULL,"I'm a physics graduate who ended up doing infosec so most of the statistics I ever learned is useful for thermodynamics. I'm currently trying to think of a model for working out how many of a population of computers are infected with viruses, though I assume the maths works out the same way for real-world diseases so references in or answers relevant to that field would be welcome too.\\n\\nHere's what I've come up with so far:\\n\\n - assume I know the total population of computers, N.\\n - I know the fraction D of computers that have virus-detection software (i.e. the amount of the population that is being screened)\\n - I know the fraction I of computers that have detection software _that has reported an infection_\\n - I don't know, but can find out or estimate, the probability of Type I and II errors in the detection software.\\n - I don't (yet) care about the time evolution of the population.\\n\\nSo where do I go from here? Would you model infection as a binomial distribution with probability like (I given D), or as a Poisson? Or is the distribution different?",,user209
665,1,312,e72a8baa-2b07-4bb8-a08d-55707ff8e2fd,"2010-07-20 15:03:40",NULL,"What approach could be used for modelling virus infections?",,user209
666,3,312,e72a8baa-2b07-4bb8-a08d-55707ff8e2fd,"2010-07-20 15:03:40",NULL,<distributions><modeling><poisson><binomial><disease>,,user209
667,6,10,2c0d7014-af9f-4f4d-9b2e-03cc17e82868,"2010-07-20 15:09:06",24,<scales><ordinal><interval><measurement>,"added relevant tags",
668,2,313,6bc4b6a8-72ba-4e62-aba2-6a56bd98eec8,"2010-07-20 15:10:15",226,"Imagine you have a bag containing 900 black marbles and 100 white, i.e. 10% of the marbles are white. Now imagine you take 1 marble out, look at it and record its colour, take out another, record its colour etc.. and do this 100 times. At the end of this process you will have a number for white marbles which, ideally, we would expect to be 10, i.e. 10% of 100, but in actual fact may be 8, or 13 or whatever simply due to randomness. If you repeat this 100 marble withdrawal experiment many, many times and then plot a histogram of the number of white marbles drawn per experiment, you'll find you will have a Bell Curve centred about 10. \\n\\nThis represents your 10% hypothesis: with any bag containing 1000 marbles of which 10% are white, if you randomly take out 100 marbles you will find 10 white marbles in the selection, give or take 4 or so. The p-value is all about this "give or take 4 or so." Let's say by referring to the Bell Curve created earlier you can determine that less than 5% of the time would you get 5 or fewer white marbles and another < 5% of the time accounts for 15 or more white marbles i.e. > 90% of the time your 100 marble selection will contain between 6 to 14 white marbles inclusive.\\n\\nNow assuming someone plonks down a bag of 1000 marbles with an unknown number of white marbles in it, we have the tools to answer these questions\\n\\ni) Are there fewer than 100 white marbles?\\n\\nii) Are there more than 100 white marbles?\\n\\niii) Does the bag contain 100 white marbles?\\n\\nSimply take out 100 marbles from the bag and count how many of this sample are white. \\n\\na) If there are 6 to 14 whites in the sample you cannot reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 6 through 14 will be > 0.05. \\n\\nb) If there are 5 or fewer whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 5 or fewer will be < 0.05. You would expect the bag to contain < 10% white marbles.\\n\\nc) If there are 15 or more whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 15 or more will be < 0.05. You would expect the bag to contain > 10% white marbles.",,
669,2,314,8ae52f0a-02db-4f6e-83ae-f72271ddc46b,"2010-07-20 15:10:34",5,"R is definitely the answer.  I would just add to what Rob and Colin already said:\\n\\nTo improve the quality of your plots, you should consider using [the **Cairo** package][1] for the output device.  That will greatly improve the *quality* of the final graphics.  You simply call the function before plotting and it redirects to Cairo as the output device.\\n\\n    Cairo(600, 600, file="plot.png", type="png", bg="white")\\n    plot(rnorm(4000),rnorm(4000),col="#ff000018",pch=19,cex=2) # semi-transparent red\\n    dev.off() # creates a file "plot.png" with the above plot\\n\\nLastly, in terms of putting it in a publication, that's the role that `Sweave` plays.  It makes combining plots with your paper a trivial operation (and has the added benefit of leaving you with something that is *reproducible* and understandable).  Use `cacheSweave` if you have long-running computations.\\n\\n  [1]: http://cran.r-project.org/web/packages/Cairo/index.html",,
670,6,305,4bf20bde-c38e-4c24-adc2-2e1dd43a3093,"2010-07-20 15:12:15",24,<variance><t-test><homogeneity><homogeneity-of-variance>,"edited tags",
671,5,117,44374837-e615-4d17-99b5-eb891333adb1,"2010-07-20 15:13:37",36,"http://www.r-bloggers.com/ is an aggregated blog from lots of blogs that talk about statistics using R, and the [#rstats][1] hashtag on twitter is also helpful. I write quite a bit about [statistics and R in genetics research][2].\\n\\n\\n  [1]: http://search.twitter.com/search?q=%23rstats\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/R","added 4 characters in body",
672,2,315,6f408f72-a5ba-428b-8821-8baff647f91d,"2010-07-20 15:15:57",36,"If you can use R try [ggplot2][1].\\n\\n\\n  [1]: http://had.co.nz/ggplot2/",,
673,2,316,2a4bd3d1-26d4-4a0c-81c6-e533acaa1b29,"2010-07-20 15:20:38",36,"**ggplot2** - hands down best visualization for R.\\n\\n**RMySQL/RSQLite/RODBC** - for connecting to a databases\\n\\n**sqldf** - manipulate data.frames with SQL queries\\n\\n**Hmisc/rms** - packages from Frank Harrell containing convenient miscellaneous functions and nice functions for regression analyses.\\n\\n**GenABEL** - nice package for genome-wide association studies\\n\\n**Rcmdr** - a decent GUI for R if you need one.",,
674,16,316,2a4bd3d1-26d4-4a0c-81c6-e533acaa1b29,"2010-07-20 15:20:38",-1,,,
675,2,317,91dafa9b-adf1-47f4-a02f-4d6fae744279,"2010-07-20 15:21:01",56,"[The Endeavour][1] sometimes features statistics posts. Otherwise it is mostly around the interplay of computer science and math.\\n\\n[1]: http://www.johndcook.com/blog/",,
676,16,317,91dafa9b-adf1-47f4-a02f-4d6fae744279,"2010-07-20 15:21:01",-1,,,
677,2,318,ad0ffa4f-7401-4aed-9b5b-03f1eab57017,"2010-07-20 15:23:53",NULL,"Computer virus propagation is structurally similar to infectious diseases propagation (vaccinations = anti-virus software, virus via email = getting a virus from someone etc).\\n\\nUse the following links http://en.wikipedia.org/wiki/Mathematical_modelling_in_epidemiology and http://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology as jumping points for your model.",,user28
678,5,316,0685016d-0b14-4439-8de6-8195d82e6255,"2010-07-20 15:28:42",36,"**ggplot2** - hands down best visualization for R.\\n\\n**RMySQL/RSQLite/RODBC** - for connecting to a databases\\n\\n**sqldf** - manipulate data.frames with SQL queries\\n\\n**Hmisc/rms** - packages from Frank Harrell containing convenient miscellaneous functions and nice functions for regression analyses.\\n\\n**GenABEL** - nice package for genome-wide association studies\\n\\n**Rcmdr** - a decent GUI for R if you need one.\\n\\n\\nAlso check out [CRANtastic - this link][1] has a list of the most popular R packages. Many on the top of the list have already been ment\\n\\n\\n  [1]: http://crantastic.org/popcon","added 183 characters in body",
679,2,319,3ed48ff3-13cc-4c78-bdb0-85735f01b008,"2010-07-20 15:33:42",71,"No amount of verbal explanation or calculations really helped me to understand *at a gut level* what p-values were, but it really snapped into focus for me once I took a course that involved simulation.  That gave me the ability to actually *see* data generated by the null hypothesis and to plot the means/etc. of simulated samples, then look at where my sample's statistic fell on that distribution.\\n\\nI think the key advantage to this is that it lets students forget about the math and the test statistic distributions for a minute and focus on the concepts at hand.  Granted, it required that I learn *how* to simulate that stuff, which will cause problems for an entirely different set of students.  But it worked for me, and I've used simulation countless times to help explain statistics to others with great success (e.g., "This is what your data looks like; this is what a Poisson distribution looks like overlaid.  Are you SURE you want to do a Poisson regression?").\\n\\nThis doesn't exactly answer the questions you posed, but for me, at least, it made them trivial.",,
680,2,320,1f357836-7bbc-4d6a-9eb7-a65535574ea4,"2010-07-20 15:47:04",61,"A saturated model is a model that is overparameterized to the point that it is basically just interpolating the data.  In some settings, such as image compression and reconstruction, this isn't necessarily a bad thing, but if you're trying to build a predictive model it's very problematic.\\n\\nIn short, saturated models lead to extremely high-variance predictors that are being pushed around by the noise more than the actual data.\\n\\nAs a thought experiment, imagine you've got a saturated model, and there is noise in the data, then imagine fitting the model a few hundred times, each time with a different realization of the noise, and then predicting a new point.  You're likely to get radically different results each time, both for your fit and your prediction (and polynomial models are especially egregious in this regard); in other words the variance of the fit and the predictor are extremely high.\\n\\nBy contrast a model that is not saturated will (if constructed reasonably) give fits that are more consistent with each other even under different noise realization, and the variance of the predictor will also be reduced.",,
681,6,134,eabdccd9-fcd9-47a7-a1aa-76e66eef254d,"2010-07-20 15:47:15",190,<algorithms><running-median>,"edited tags",
683,6,242,7c31e774-884a-45fd-b120-f7a2dbec5093,"2010-07-20 15:53:21",5,<time-series><forecasting>,"edited tags",
684,6,134,c541c63b-6843-45e8-aeb3-ff70f45b4d85,"2010-07-20 15:56:17",13,<algorithms><possibly-off-topic><running-median>,"edited tags",
685,2,321,4e355f03-d3bf-4665-a1e7-c971a64ccb82,"2010-07-20 16:01:25",220,"There is a variant of boosting called [gentleboost][1].  How does gentle boosting differ from other boosting variants?\\n\\n\\n  [1]: http://dx.doi.org/10.1214/aos/1016218223",,
686,1,321,4e355f03-d3bf-4665-a1e7-c971a64ccb82,"2010-07-20 16:01:25",220,"How does gentle boosting differ from other boosting variants?",,
687,3,321,4e355f03-d3bf-4665-a1e7-c971a64ccb82,"2010-07-20 16:01:25",220,<machine-learning><boosting>,,
688,2,322,445f6517-f06c-4eb1-a398-448ec97c2533,"2010-07-20 16:03:40",3807,"I'm looking for a book or online resource that explains different kinds of entropy such as Sample Entropy and Shannon Entropy and their advantages and disadvantages.\\nCan someone point me in the right direction?",,
689,1,322,445f6517-f06c-4eb1-a398-448ec97c2533,"2010-07-20 16:03:40",3807,"Good introduction into different kinds of entropy",,
690,3,322,445f6517-f06c-4eb1-a398-448ec97c2533,"2010-07-20 16:03:40",3807,<entropy>,,
691,6,2,64527c0d-9b5e-4694-9cd4-c127f0e18bf4,"2010-07-20 16:04:03",24,<distributions><fundamentals><normality>,"edited tags",
692,6,31,1bfa4ca1-480f-4c81-a17b-642b56a57965,"2010-07-20 16:04:36",24,<hypothesis-testing><fundamentals><p-value><t-value>,"edited tags",
693,6,26,2226f2d8-c323-4754-9710-e0b6a8f019d7,"2010-07-20 16:04:59",24,<standard-deviation><fundamentals><statistics>,"edited tags",
694,2,323,7f2579de-3756-4b87-8226-e2ecd0c469a6,"2010-07-20 16:10:16",3807,"[Statistical Modeling, Causal Inference, and Social Science][1] from Andrew Gelman is a good blog.\\n\\n\\n  [1]: http://www.stat.columbia.edu/~cook/movabletype/mlm/",,
695,16,323,7f2579de-3756-4b87-8226-e2ecd0c469a6,"2010-07-20 16:10:16",-1,,,
696,2,324,15489984-5ec1-4f0c-906a-8aa11cf7f203,"2010-07-20 16:12:08",3807,"[Darren Wilkinson's research blog][1]\\n\\n\\n  [1]: http://darrenjw.wordpress.com/",,
697,16,324,15489984-5ec1-4f0c-906a-8aa11cf7f203,"2010-07-20 16:12:08",-1,,,
701,2,326,b7997e40-1728-4a79-921b-50b6568fc9a1,"2010-07-20 16:13:03",3807,"[XI'AN'S OG][1]\\n\\n\\n  [1]: http://xianblog.wordpress.com/",,
702,16,326,b7997e40-1728-4a79-921b-50b6568fc9a1,"2010-07-20 16:13:03",-1,,,
704,2,327,35020ef9-a770-4808-9610-60f4f2c91b47,"2010-07-20 16:19:51",3807,"Malcom Gladewell analyses the problem in the book Outliers by analyzing Hockey Players.",,
705,2,328,4a99d03b-3a62-4d33-ad9e-868dd5497d53,"2010-07-20 16:27:08",75,"I realize that the statistical analysis of financial data is a huge topic, but that is exactly why it is necessary for me to ask my question as I try to break into the world of financial analysis.\\n\\nAs at this point I know next to nothing about the subject, the results of my google searches  are overwhelming.  Many of the matches advocate learning specialized tools or the R programming language. While I will learn these when they are necessary, I'm first interested in books, articles or any other resources that explain modern methods of statistical analysis specifically for financial data.  I assume there are a number of different wildly varied methods for analyzing data, so ideally I'm seeking an overview of the various methods that are practically applicable.  I'd like something that utilizes real world examples that a beginner is capable of grasping but that aren't overly simplistic.\\n\\nWhat are some good resources for learning bout the statistical analysis of financial data?",,
706,1,328,4a99d03b-3a62-4d33-ad9e-868dd5497d53,"2010-07-20 16:27:08",75,"Resources for learning about the Statistical Analysis of Financial Data",,
707,3,328,4a99d03b-3a62-4d33-ad9e-868dd5497d53,"2010-07-20 16:27:08",75,<data><finance><analysis><resources>,,
708,2,329,88a67037-f226-4f71-9457-2c0d33771472,"2010-07-20 16:31:26",5,"My favorite books on the subject:\\n\\n - I strongly recommend starting with [**Statistics and Finance**][7], by David Ruppert ([the R code for the book is available][8]).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.\\n - [Modeling Financial Time Series with S-Plus][6], by Eric Zivot\\n - [Analysis of Financial Time Series][9], by Ruey Tsay\\n - [Time Series Analysis][10], by Jonathan D. Cryer\\n\\nBeyond that, you may want some general resources, and the "bible" of finance is [Options, Futures, and Other Derivatives][11] by John Hull.  \\n\\n\\n  [6]: http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387279652/ref=sr_1_1?ie=UTF8&s=books&qid=1267799203&sr=1-1\\n  [7]: http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706\\n  [8]: http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm\\n  [9]: http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0471690740/ref=sr_1_1?ie=UTF8&s=books&qid=1267799281&sr=1-1\\n  [10]: http://www.amazon.com/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=pd_sim_b_2\\n  [11]: http://www.rotman.utoronto.ca/~hull/ofod/\\n",,
709,5,329,ebefd968-8212-4ac4-857b-ffce70ac5bfd,"2010-07-20 16:38:57",5,"My favorite books on the subject:\\n\\n - I strongly recommend starting with [**Statistics and Finance**][1], by David Ruppert ([the R code for the book is available][2]).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.\\n - [Modeling Financial Time Series with S-Plus][3], by Eric Zivot\\n - [Analysis of Financial Time Series][4], by Ruey Tsay\\n - [Time Series Analysis][5], by Jonathan D. Cryer\\n\\nBeyond that, you may want some general resources, and the "bible" of finance is [Options, Futures, and Other Derivatives][6] by John Hull.  \\n\\nLastly, in terms of some good general books, you might start with these two:\\n\\n - [A Random Walk Down Wall Street][7]\\n - [Against the Gods: The Remarkable Story of Risk][8] \\n\\n\\n  [1]: http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706\\n  [2]: http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm\\n  [3]: http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387279652/ref=sr_1_1?ie=UTF8&s=books&qid=1267799203&sr=1-1\\n  [4]: http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0471690740/ref=sr_1_1?ie=UTF8&s=books&qid=1267799281&sr=1-1\\n  [5]: http://www.amazon.com/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=pd_sim_b_2\\n  [6]: http://www.rotman.utoronto.ca/~hull/ofod/\\n  [7]: http://www.amazon.com/Random-Walk-Down-Wall-Street/dp/0393325350/ref=dp_ob_title_bk\\n  [8]: http://www.amazon.com/Against-Gods-Remarkable-Story-Risk/dp/0471295639/ref=sr_1_1?ie=UTF8&s=books&qid=1279643845&sr=1-1","added 393 characters in body",
714,2,331,f08499af-545b-4916-a7cb-e4f9433a5145,"2010-07-20 16:43:42",61,"Because exact results are preferable to approximations, and avoid odd edge cases where the approximation may lead to a different result than the exact method.  \\n\\nThe Welch method isn't a quicker way to do any old t-test, it's a tractable approximation to an otherwise very hard problem: how to construct a t-test under unequal variances.  The equal-variance case is well-understood, simple, and exact, and therefore should always be used when possible.",,
717,5,329,adc65f3e-b647-43cf-8887-81e0236843be,"2010-07-20 16:49:17",5,"You might start with this [series of lectures by Robert Shiller at Yale][1].  He gives a good overview of the field.\\n\\nMy favorite books on the subject:\\n\\n - I strongly recommend starting with [**Statistics and Finance**][2], by David Ruppert ([the R code for the book is available][3]).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.\\n - [Modeling Financial Time Series with S-Plus][4], by Eric Zivot\\n - [Analysis of Financial Time Series][5], by Ruey Tsay\\n - [Time Series Analysis][6], by Jonathan D. Cryer\\n\\nBeyond that, you may want some general resources, and the "bible" of finance is [Options, Futures, and Other Derivatives][7] by John Hull.  \\n\\nLastly, in terms of some good general books, you might start with these two:\\n\\n - [A Random Walk Down Wall Street][8]\\n - [Against the Gods: The Remarkable Story of Risk][9] \\n\\n\\n  [1]: http://oyc.yale.edu/economics/financial-markets/\\n  [2]: http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706\\n  [3]: http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm\\n  [4]: http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387279652/ref=sr_1_1?ie=UTF8&s=books&qid=1267799203&sr=1-1\\n  [5]: http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0471690740/ref=sr_1_1?ie=UTF8&s=books&qid=1267799281&sr=1-1\\n  [6]: http://www.amazon.com/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=pd_sim_b_2\\n  [7]: http://www.rotman.utoronto.ca/~hull/ofod/\\n  [8]: http://www.amazon.com/Random-Walk-Down-Wall-Street/dp/0393325350/ref=dp_ob_title_bk\\n  [9]: http://www.amazon.com/Against-Gods-Remarkable-Story-Risk/dp/0471295639/ref=sr_1_1?ie=UTF8&s=books&qid=1279643845&sr=1-1","added 177 characters in body",
719,2,333,6a10da1b-c826-4a06-b287-afdad005bed3,"2010-07-20 17:17:21",74,"Ed Thorpe started the whole thing (statistical arbitrage). He has a website, and some good articles.\\n\\nhttp://edwardothorp.com/\\n\\nYou should also read Nassim Taleb's "Fooled By Randomness".\\n\\nAlso, go on Google Scholar and read the top articles by Markowitz, Sharpe, Fama, Modigliani. ",,
726,2,337,dc31f588-ab91-4fa2-9eda-ac11a26f3469,"2010-07-20 17:20:41",88,"The entropy is only one (as a concept) -- there are only many its generalizations. Sample entropy is only some entropy-like descriptor used in heart rate analysis.",,
727,2,338,723debd0-610a-420d-a6d5-f2557e66bb78,"2010-07-20 17:22:01",89,"Cover and Thomas's book *Elements of Information Theory* is a good source on entropy and its applications, although I don't know that it addresses exactly the issues you have in mind.",,
728,2,339,0274c81f-7f90-4226-8ba6-12d519a51afd,"2010-07-20 17:31:49",218,"If the information required is the distribution of data about the mean, standard deviation comes in handy.\\n\\nThe sum of the difference of each value from the mean is zero (obviously, since the value are evenly spread around the mean), hence we square each difference so as to convert negative values to positive, sum them across the population, and take their square root. This value is then divided by the number of samples (or, the size of the population). This gives the standard deviation.",,
729,5,337,85be02a5-b620-4c62-9c85-c61f827c81fd,"2010-07-20 17:36:23",88,"The entropy is only one (as a concept) -- the amount of information needed to describe some system; there are only many its generalizations. Sample entropy is only some entropy-like descriptor used in heart rate analysis.","added 58 characters in body",
730,2,340,c154a027-379c-4e3b-aa03-9007accaf164,"2010-07-20 17:41:00",74,"The population is everything in the group of study. For example, if you are studying the price of Apple's shares, it is the historical, current, and even all future stock prices. Or, if you run an egg factory, it is all the eggs made by the factory.\\n\\nYou don't always have to sample, and do statistical tests. If your population is your immediate living family, you don't need to sample, as the population is small. \\n\\nSampling is popular for a variety of reasons:\\n\\n- it is cheaper than a census (sampling the whole population)\\n- you don't have access to future data, so must sample the past\\n- you have to destroy some items by testing them, and don't want to destroy them all (say, eggs)\\n\\n",,
731,5,86,739a4dac-d913-40df-9987-61ae77c5cd8c,"2010-07-20 17:46:55",13,"Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather **statistical properties** such as the **distribution** of the random variable may be stated.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.\\n\\nRandom variables may be classified as *discrete* if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is *continuous* and is used if the distribution covers values from an uncountable set such as the real numbers.","deleted 6 characters in body",
732,6,130,18998643-d455-49de-9ec6-23fb32d636d0,"2010-07-20 18:11:20",13,<r><subjective>,"edited tags",
733,6,138,55636248-1835-4bde-a691-79363f0540f8,"2010-07-20 18:11:56",13,<r><open-source><tutorials>,"edited tags",
734,2,341,75f3cc85-ecb3-4479-b73b-7c17cae3d0a3,"2010-07-20 18:12:29",88,"Do you think that unbalanced classes is a big problem for k-nearest neighbor? If so, do you know any smart way to handle this?",,
735,1,341,75f3cc85-ecb3-4479-b73b-7c17cae3d0a3,"2010-07-20 18:12:29",88,"kNN and unbalanced classes",,
736,3,341,75f3cc85-ecb3-4479-b73b-7c17cae3d0a3,"2010-07-20 18:12:29",88,<machine-learning><k-nearest-neighbour><unbalanced-classes>,,
737,6,134,91e1ac65-2a05-465e-8a14-176318e40a18,"2010-07-20 18:12:30",13,<algorithms><running-median>,"edited tags",
738,6,75,c1c5258c-0758-4b9d-99e0-64dff16b729c,"2010-07-20 18:13:33",13,<r><books><implementation>,"edited tags",
739,2,342,a7accaf9-ff88-4f2b-9684-75bebe8b7581,"2010-07-20 18:13:44",139,"If you're willing to tolerate an approximation, there are other methods. For example, one approximation is a value whose rank is within some (user specified) distance from the true median. For example, the median has (normalized) rank 0.5, and if you specify an error term of 10%, you'd want an answer that has rank between 0.45 and 0.55. \\n\\nIf such an answer is appropriate, then there are many solutions that can work on sliding windows of data. The basic idea is to maintain a sample of the data of a certain size (roughly 1/error term) and compute the median on this sample. It can be shown that with high probability, regardless of the nature of the input, the resulting median satisfies the properties I mentioned above.\\n\\nThus, the main question is how to maintain a running sample of the data of a certain size, and there are many approaches for that, including the technique known as reservoir sampling. For example, this paper: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7136",,
740,2,343,862fc056-2b44-48ca-9ae2-2675f56b6185,"2010-07-20 18:16:17",88,"[kNN][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",,
741,16,343,862fc056-2b44-48ca-9ae2-2675f56b6185,"2010-07-20 18:16:17",-1,,,
742,2,344,a2e44028-7a25-49fd-8c00-84c75e2389cc,"2010-07-20 18:17:25",88,"[Naive Bayes][1] and [Random Naive Bays][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Naive_bayes\\n  [2]: http://en.wikipedia.org/wiki/Random_naive_Bayes",,
743,16,344,a2e44028-7a25-49fd-8c00-84c75e2389cc,"2010-07-20 18:17:25",-1,,,
744,6,258,63d31190-33d6-4863-8d1a-355e43a602bb,"2010-07-20 18:18:08",88,<machine-learning><classification><application>,"edited tags",
745,6,269,8e72333e-d70a-4b7f-b8b4-239d7e05dc4b,"2010-07-20 18:51:26",24,<standard-deviation><fundamentals><variance><population><sample>,"edited tags",
746,10,134,27606156-98ee-43d6-992a-478139afd75a,"2010-07-20 18:54:42",-1,"{"Voters":[{"Id":88,"DisplayName":"mbq"},{"Id":190,"DisplayName":"Peter Smit"},{"Id":13,"DisplayName":"Sharpie"},{"Id":103,"DisplayName":"rcs"},{"Id":28,"DisplayName":"Srikant Vadali"}]}",2,
747,2,345,c5865774-11c0-4540-b523-c61dd34b6c18,"2010-07-20 19:08:50",247,"Also good is "Statistical Analysis of Financial Data in S-PLUS" by Rene A. Carmona",,
748,2,346,9d205183-645f-44ea-94c9-43732e6192c6,"2010-07-20 19:21:16",247,"I'm looking for a good algorithm (meaning minimal computation, minimal storage requirements) to estimate the median of a data set that is too large to store, such that each value can only be read once (unless you explicitly store that value). There are no bounds on the data that can be assumed.\\n\\nApproximations are fine, as long as the accuracy is known.\\n\\nAny pointers?",,
749,1,346,9d205183-645f-44ea-94c9-43732e6192c6,"2010-07-20 19:21:16",247,"What is a good algorithm for estimating the median of a huge read-once data set?",,
750,3,346,9d205183-645f-44ea-94c9-43732e6192c6,"2010-07-20 19:21:16",247,<algorithms><median>,,
751,6,50,3ed86ca4-48e1-4781-9731-d72349fcdec6,"2010-07-20 19:23:12",24,<fundamentals><random><variable><random-variable>,"edited tags",
752,5,321,061f7cb8-a528-4f36-83c4-0c8dffa02ada,"2010-07-20 19:25:13",220,"There is a variant of boosting called [gentleboost][1].  How does gentle boosting differ from the better-known [AdaBoost][2]?\\n\\n\\n  [1]: http://dx.doi.org/10.1214/aos/1016218223\\n  [2]: http://en.wikipedia.org/wiki/AdaBoost",capitalization,
753,4,321,061f7cb8-a528-4f36-83c4-0c8dffa02ada,"2010-07-20 19:25:13",220,"How does gentle boosting differ from AdaBoost?",capitalization,
754,2,347,a5d5e811-4c32-4a86-9720-83c64178bd1f,"2010-07-20 19:50:57",15,"I found this rather helpful: http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf",,
755,2,348,599d4cc5-a9d6-4011-aaae-8c02d2fa19ab,"2010-07-20 19:54:13",24,"A male cat and a female cat are penned up in a steel chamber, along with enough food and for 70 days.  \\n\\nA Frequentist would say the average gestation period for [felines][1] is 66 days, the female was in heat when the cats were penned up, and once in heat she will mate repeatedly for 4 to 7 days.  Since there were likely many acts of propagation and enough subsequent time for gestation, the odds are, when the box is opened on day 70, there's a litter of newborn kittens.\\n\\nA Bayesian would say, I heard some serious Marvin Gaye coming from the box on day 1 and then this morning I heard many kitten-like sounds coming from the box.  So without knowing much about cat reproduction, the odds are, when the box is opened on day 70, there's a litter of newborn kittens. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Cat#Reproduction",,
756,2,349,a2668c97-1881-47c0-9368-2849b00ee56e,"2010-07-20 19:59:33",54,"How about something like a binning procedure?  Assume you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] \\n\\nThen, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the number of bins. You're limited only by how much memory you have.",,
757,5,349,8fc79b22-4441-404b-8dfe-c105e285418d,"2010-07-20 20:04:37",54,"How about something like a binning procedure?  Assume (for illustration purposes) that you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] \\n\\nThen, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the size of the bins. You're limited only by how much memory you have.\\n\\nSince you don't know how big your values may get, just pick a bin size large enough that you aren't likely to run out of memory, using some quick back-of-the-envelope calculations. You might also store the bins sparsely, such that you only add a bin if it contains a value.","added 244 characters in body; added 68 characters in body",
758,2,350,7dd81642-0b68-425d-acff-28e9f7da21c4,"2010-07-20 20:12:26",178,"You can try to find a median based on grouped frequency distribution, [here is some details][1]\\n\\n\\n  [1]: http://www.statcan.gc.ca/edu/power-pouvoir/ch11/median-mediane/5214872-eng.htm",,
759,6,173,21682257-5d1b-4faf-9a80-1bd92a378cd3,"2010-07-20 20:16:38",71,<r><time-series><poisson><count-data><epidemiology>,"Tags have a little more definition now; adding "poisson" might help people find this.",
760,16,7,8a2b77e9-634d-408a-bfc7-a2cc5f0d8770,"2010-07-20 20:50:48",38,,,
761,6,7,8a2b77e9-634d-408a-bfc7-a2cc5f0d8770,"2010-07-20 20:50:48",38,<population><dataset><sample>,"Flagged as Community Wiki",
762,2,351,23c6767b-3ede-4793-b2f4-9d4ab33d855e,"2010-07-20 21:05:35",16,"[Timetric][1] provides a web interface to data and provide a list of the [publicly available data sets][2] they use\\n\\n\\n  [1]: http://timetric.com\\n  [2]: http://timetric.com/dataset/",,
763,16,351,23c6767b-3ede-4793-b2f4-9d4ab33d855e,"2010-07-20 21:05:35",-1,,,
764,2,352,c172ee5a-105f-408a-bfea-c7cb51889811,"2010-07-20 21:18:45",8,"I've never had to do this, so this is just a suggestion.\\n\\nI see two (other) possibilities. \\n\\n**Half data**\\n\\n1. Load in half the data and sort\\n1. Next read in the remaining values and compare against the your sorted list. \\n  1. If the new value is larger, discard it.\\n  1. else put the value in the sorted list and removing the largest value from that list.\\n\\n\\n**Sampling distribution**\\n\\nThe other option, is to use an approximation involving the sampling distribution. If your data is Normal, then the standard error for moderate n is:\\n\\n1.253 * sd / sqrt(n)\\n\\nJust determine the size of *n* that you would be happy with. To test this, I ran a quick monte-carlo simulation in R\\n\\n    n = 10000\\n    outsi\\n    de.ci.uni = 0\\n    outside.ci.nor = 0\\n    N=1000\\n    for(i in 1:N){\\n      #Theoretical median is 0\\n      uni = runif(n, -10, 10)\\n      nor  = rnorm(n, 0, 10)\\n      \\n      if(abs(median(uni)) > 1.96*1.253*sd(uni)/sqrt(n))\\n        outside.ci.uni = outside.ci.uni + 1\\n    \\n      if(abs(median(nor)) > 1.96*1.253*sd(nor)/sqrt(n))\\n        outside.ci.nor = outside.ci.nor + 1\\n    }\\n    \\n    outside.ci.uni/N\\n    outside.ci.nor/N\\n\\nFor n=10000, 15% of the uniform median estimates were outside the CI.\\n\\n\\n\\n",,
766,2,353,47c73b3d-7bf3-4e8f-881d-03e00b121a2b,"2010-07-20 22:11:54",61,"Fisher's scoring is just a version of Newton's method that happens to be identified with GLMs, there's nothing particularly special about it, other than the fact that the Fisher's information matrix happens to be rather easy to find for random variables in the exponential family.  It also ties in to a lot of other math-stat material that tends to come up about the same time, and gives a nice geometric intuition about what exactly Fisher information means.\\n\\nThere's absolutely no reason I can think of not to use some other optimizer if you prefer, other than that you might have to code it by hand rather than use a pre-existing package.  I suspect that any strong emphasis on Fisher scoring is a combination of (in order of decreasing weight) pedagogy, ease-of-derivation, historical bias, and "not-invented-here" syndrome.  ",,
767,2,354,9643afb3-0750-4c64-a2fa-509ef1830433,"2010-07-20 22:26:26",3807,"Why do we seek to minimize x^2 instead of minimizing x^1.95 or x^2.05.\\nAre there reasons why the number should be exactly two or is it simply a convention that has the advantage of simplifying the math?",,
768,1,354,9643afb3-0750-4c64-a2fa-509ef1830433,"2010-07-20 22:26:26",3807,"Bias towards natural numbers in the case of least squares.",,
769,3,354,9643afb3-0750-4c64-a2fa-509ef1830433,"2010-07-20 22:26:26",3807,<least-squares>,,
770,2,355,1b929a74-a3ce-4cbd-a131-840af5663896,"2010-07-20 22:27:42",158,"Check out [Wilmott.com][1] as well.  It's oriented toward more advanced practitioners, but if I had to choose one person from whom to learn financial math, it would be Paul Wilmott.  Brilliant but grounded.\\n\\n\\n  [1]: http://wilmott.com/",,
771,2,356,238266ae-6246-452d-ba44-821f4d033551,"2010-07-20 22:31:45",8,"To fit the model you can use [JAGS][1] or [Winbugs][2]. In fact if you look at the week 3 of the lecture notes at Paul Hewson's [webpage][3], the rats JAGS example is a beta binomial model. He puts gamma priors on alpha and beta.\\n\\n\\n  [1]: http://www-fis.iarc.fr/~martyn/software/jags/\\n  [2]: http://www.mrc-bsu.cam.ac.uk/bugs/\\n  [3]: http://users.aims.ac.za/~paulhewson/",,
772,5,349,4f0dd4df-76d6-40af-a1f5-6426f48cd93d,"2010-07-20 22:40:50",54,"How about something like a binning procedure?  Assume (for illustration purposes) that you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] \\n\\nThen, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the size of the bins. You're limited only by how much memory you have.\\n\\nSince you don't know how big your values may get, just pick a bin size large enough that you aren't likely to run out of memory, using some quick back-of-the-envelope calculations. You might also store the bins sparsely, such that you only add a bin if it contains a value.\\n\\nEdit:\\n\\nThe link ryfm provides gives an example of doing this, with the additional step of using the cumulative percentages to more accurately estimate the point within the median bin, instead of just using midpoints. This is a nice improvement.","added 250 characters in body",
773,5,348,a99dd212-e7c6-47d4-a57d-9e4ae38d53f1,"2010-07-20 22:52:03",24,"A male cat and a female cat are penned up in a steel chamber, along with enough food and water for 70 days.  \\n\\nA Frequentist would say the average gestation period for [felines][1] is 66 days, the female was in heat when the cats were penned up, and once in heat she will mate repeatedly for 4 to 7 days.  Since there were likely many acts of propagation and enough subsequent time for gestation, the odds are, when the box is opened on day 70, there's a litter of newborn kittens.\\n\\nA Bayesian would say, I heard some serious Marvin Gaye coming from the box on day 1 and then this morning I heard many kitten-like sounds coming from the box.  So without knowing much about cat reproduction, the odds are, when the box is opened on day 70, there's a litter of newborn kittens. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Cat#Reproduction","fixed grammar",
774,2,357,258da2e0-4d84-4699-adab-aa49f9bb41a2,"2010-07-20 23:07:48",3807,"You can't know whether there normality and that's why you have to make an assumption that's there.\\nYou can only prove the absence of normality with statistic tests.\\n\\nEven worse, when you work with real world data it's almost certain that there isn't true normality in your data.\\n\\nThat means that your statistical test is always a bit biased. The question is whether you can live with it's bias.\\nTo do that you have to understand your data and the kind of normality that your statistical tool assumes.\\n\\nIt's the reason why Frequentist tools are such as subjective as Bayesian tools. You can't determine based on the data that it's normally distributed. You have to assume normality.",,
775,2,358,ba7723d0-2792-4b26-8503-22106489cdc7,"2010-07-20 23:21:05",88,"We try to minimize the variance that is left within descriptors. Why variance? Read [this question][1]; this also comes together with the (mostly silent) assumption that errors are normally distributed.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/118/standard-deviation-why-square-the-difference-instead-of-taking-the-absolute-val",,
776,2,359,997b5499-97fb-44fd-b6fe-a74332eeeab7,"2010-07-20 23:28:49",90,"The Wald, Likelihood Ratio and Lagrange Multiplier tests in the context of maximum likelihood estimation are asymptotically equivalent. However, for small samples, they tend to diverge quite a bit, and in some cases they result in different conclusions.\\n\\nHow can they be ranked according to how likely they are to reject the null? What to do when the tests have conflicting answers? Can you just pick the one which gives the answer you want or is there a "rule" or "guideline" as to how to proceed?",,
777,1,359,997b5499-97fb-44fd-b6fe-a74332eeeab7,"2010-07-20 23:28:49",90,"The trinity of tests in maximum likelihood: what to do when faced with contradicting conclusions?",,
778,3,359,997b5499-97fb-44fd-b6fe-a74332eeeab7,"2010-07-20 23:28:49",90,<hypothesis-testing><maximum-likelihood>,,
779,2,360,66babed3-0f3a-40f6-b112-25e4593b4dba,"2010-07-20 23:56:33",88,"Does it really need some advanced model? Based on what I know about TB, in case there is no epidemy the infections are stochastic acts and so the count form month N shouldn't be correlated with count from month N-1. (You can check this assumption with autocorrelation). If so, analyzing just the distribution of monthly counts may be sufficient to decide if some count is significantly higher than normal.  \\nOn the other hand you can look for correlations with some other variables, like season, travel traffic, or anything that you can imagine that may be correlated. If you would found something like this, it could be then used for some data normalization.",,
780,2,361,9e2a94de-8a6b-45cd-b20a-b359a52d492a,"2010-07-21 00:09:09",170,"[Logistic Regression][1]:\\n\\n - fast and perform well on most datasets\\n - almost no parameters to tune\\n - handles both discrete/continuous features\\n - model is easily interpretable\\n - (not really restricted to binary classifications)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Logistic_regression",,
781,16,361,9e2a94de-8a6b-45cd-b20a-b359a52d492a,"2010-07-21 00:09:09",-1,,,
782,5,354,831ed5c9-7654-48a8-b4e9-968a297b8b2c,"2010-07-21 00:19:03",3807,"Why do we seek to minimize `x^2` instead of minimizing `|x|^1.95` or `|x|^2.05`.\\nAre there reasons why the number should be exactly two or is it simply a convention that has the advantage of simplifying the math?","added 10 characters in body",
783,2,362,47492cb7-f60a-4302-819f-86df9d13f334,"2010-07-21 00:24:35",196,"What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?  When will results from these two methods differ?",,
784,1,362,47492cb7-f60a-4302-819f-86df9d13f334,"2010-07-21 00:24:35",196,"What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?",,
785,3,362,47492cb7-f60a-4302-819f-86df9d13f334,"2010-07-21 00:24:35",196,<distributions><statistical-significance><normality>,,
786,5,173,9555b175-d6a7-4740-b450-6e53c0077f7b,"2010-07-21 00:25:04",71,"I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc.  I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not.  Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:\\n\\n![alt text][1]\\n\\n![alt text][2]\\n\\n\\nI've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.\\n\\n**EDIT:**  mbq's answer has forced me to think more carefully about what I'm asking here; I got too hung-up on the monthly counts and lost the actual focus of the question.  What I'd like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases?  It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable.  From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality.  How can I test if there's a real change in the process?  And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?\\n\\nWhew.  Thanks for bearing with me.\\n\\n\\n  [1]: http://img827.imageshack.us/img827/1927/activetbcases.png "Cases by month"\\n  [2]: http://img827.imageshack.us/img827/4348/tbcasedistribution.png "Distribution of counts"","added 975 characters in body; deleted 53 characters in body",
787,5,313,b25caff4-9276-4048-91e0-60b8bd958002,"2010-07-21 00:33:56",226,"Imagine you have a bag containing 900 black marbles and 100 white, i.e. 10% of the marbles are white. Now imagine you take 1 marble out, look at it and record its colour, take out another, record its colour etc.. and do this 100 times. At the end of this process you will have a number for white marbles which, ideally, we would expect to be 10, i.e. 10% of 100, but in actual fact may be 8, or 13 or whatever simply due to randomness. If you repeat this 100 marble withdrawal experiment many, many times and then plot a histogram of the number of white marbles drawn per experiment, you'll find you will have a Bell Curve centred about 10. \\n\\nThis represents your 10% hypothesis: with any bag containing 1000 marbles of which 10% are white, if you randomly take out 100 marbles you will find 10 white marbles in the selection, give or take 4 or so. The p-value is all about this "give or take 4 or so." Let's say by referring to the Bell Curve created earlier you can determine that less than 5% of the time would you get 5 or fewer white marbles and another < 5% of the time accounts for 15 or more white marbles i.e. > 90% of the time your 100 marble selection will contain between 6 to 14 white marbles inclusive.\\n\\nNow assuming someone plonks down a bag of 1000 marbles with an unknown number of white marbles in it, we have the tools to answer these questions\\n\\ni) Are there fewer than 100 white marbles?\\n\\nii) Are there more than 100 white marbles?\\n\\niii) Does the bag contain 100 white marbles?\\n\\nSimply take out 100 marbles from the bag and count how many of this sample are white. \\n\\na) If there are 6 to 14 whites in the sample you cannot reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 6 through 14 will be > 0.05. \\n\\nb) If there are 5 or fewer whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 5 or fewer will be < 0.05. You would expect the bag to contain < 10% white marbles.\\n\\nc) If there are 15 or more whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 15 or more will be < 0.05. You would expect the bag to contain > 10% white marbles.\\n\\n*In response to Baltimark's comment*\\n\\nGiven the example above, there is an approximately:-\\n\\n4.8% chance of getter 5 white balls or fewer\\n\\n1.85% chance of 4 or fewer\\n\\n0.55% chance of 3 or fewer\\n\\n0.1% chance of 2 or fewer \\n\\n6.25% chance of 15 or more\\n\\n3.25% chance of 16 or more\\n\\n1.5% chance of 17 or more\\n\\n0.65% chance of 18 or more\\n\\n0.25% chance of 19 or more\\n\\n0.1% chance of 20 or more\\n\\n0.05% chance of 21 or more\\n\\nThese numbers were estimated from an empirical distribution generated by a simple Monte Carlo routine run in R and the resultant quantiles of the sampling distribution. \\n\\nFor the purposes of answering the original question, suppose you draw 5 white balls, there is only an approximate 4.8% chance that if the 1000 marble bag really does contain 10% white balls you would pull out only 5 whites in a sample of 100. This equates to a p value < 0.05. You now have to choose between\\n\\ni) There really are 10% white balls in the bag and I have just been "unlucky" to draw so few\\n\\nor\\n\\nii) I have drawn so few white balls that there can't really be 10% white balls (reject the hypothesis of 10% white balls)\\n\\n\\n\\n","Response to comment; added 8 characters in body",
788,2,363,c9566a48-96b4-4723-82bc-a17aef5e67b8,"2010-07-21 00:44:08",74,"If you could go back in time and tell yourself to read a specific book at the beginning of your career as a statistician, which book would it be?",,
789,1,363,c9566a48-96b4-4723-82bc-a17aef5e67b8,"2010-07-21 00:44:08",74,"What is the single most influential book every statistician should read?",,
790,3,363,c9566a48-96b4-4723-82bc-a17aef5e67b8,"2010-07-21 00:44:08",74,<books>,,
794,16,363,f4d3ac15-1925-41a4-acf1-61fd4d5aa18b,"2010-07-21 00:54:29",74,,,
796,2,365,962d01c8-a675-491c-bb58-41df5432205d,"2010-07-21 01:04:08",226,"Another option is [Gnuplot][1]\\n\\n\\n  [1]: http://www.gnuplot.info/",,
797,2,366,eabd2fdf-e54e-4076-a2db-ab3a5b9decf0,"2010-07-21 01:23:00",226,"For a linear regression you could use a repeated median straight line fit.",,
798,2,367,ede43ea2-125c-4b68-93b6-eb0ee488a001,"2010-07-21 01:35:13",90,"I am no statistician, and I haven't read that much on the topic, but perhaps \\n\\n[Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century][1]\\n\\nshould be mentioned? It is no textbook, but still worth reading.\\n\\n\\n  [1]: http://www.amazon.com/Lady-Tasting-Tea-Statistics-Revolutionized/dp/0805071342",,
799,16,367,ede43ea2-125c-4b68-93b6-eb0ee488a001,"2010-07-21 01:35:13",-1,,,
800,2,368,7dc89390-3dcb-4d99-8edc-12411936cae1,"2010-07-21 03:01:00",NULL,"Suppose that the text has N words and that you require that an ASR should correctly predict at least 95% of words in the text. You currently have the observed error rate for the two methods. You can perform two type of tests.\\n\\nTest 1: Do the ASR models meet your criteria of 95% prediction?\\n\\nTest 2: Are the two ASR models equally good in speech recognition?\\n\\nYou could make different type of assumptions regarding the data generating mechanism for your ASR models. The simplest, although a bit naive, would assume that word detection of each word in the text is an iid bernoulli variable.\\n\\nUnder the above assumption you could do a test of proportions where you check if the error rate for each model is consistent with a true error rate of 5% (test 1) or a test of difference in proportions where you check if the error rates between the two models is the same (test 2). ",,user28
801,6,288,c44aa6fd-7660-4ccd-af2d-34c65915879c,"2010-07-21 03:10:58",NULL,<estimation><beta-binomial>,"edited tags",user28
802,6,50,69ea4103-b11a-43ff-a627-1245cd183866,"2010-07-21 03:11:45",NULL,<fundamentals><random-variable>,"edited tags",user28
803,6,75,a4fc2a77-67df-47c7-9ee3-773c7a647022,"2010-07-21 03:12:51",NULL,<r><books><code>,"edited tags",user28
804,2,369,90cc17b6-1510-44b7-bc48-30ddf7bbe962,"2010-07-21 03:47:17",191,"Say I've got a program that monitors a news feed and as I'm monitoring it I'd like to discover when a bunch of stories come out with a particular keyword in the title. Ideally I want to know when there are an unusual number of stories clustered around one another.\\n\\nI'm entirely new to statistical analysis and I'm wondering how you would approach this problem. How do you select what variables to consider? What characteristics of the problem affect your choice of an algorithm? Then, what algorithm do you choose and why?\\n\\nThanks, and if the problem needs clarification please let me know.",,
805,1,369,90cc17b6-1510-44b7-bc48-30ddf7bbe962,"2010-07-21 03:47:17",191,"Working through a clustering problem",,
806,3,369,90cc17b6-1510-44b7-bc48-30ddf7bbe962,"2010-07-21 03:47:17",191,<clustering><analysis>,,
807,2,370,eb88f174-996f-43b8-af6b-55d50c6e9bcc,"2010-07-21 03:48:55",159,"Here are two to put on the list:\\n\\n[Tufte. The visual display of quantitative information][1]<br>\\n[Tukey. Exploratory data analysis][2]\\n\\n\\n  [1]: http://www.amazon.com/dp/096139210X?tag=prorobjhyn-20&linkCode=ur2&camp=1789&creative=9325\\n  [2]: http://www.amazon.com/dp/0201076160?tag=prorobjhyn-20&linkCode=ur2&camp=1789&creative=9325",,
808,16,370,eb88f174-996f-43b8-af6b-55d50c6e9bcc,"2010-07-21 03:48:55",-1,,,
810,2,371,780ca46f-632c-499c-89f4-a6c304ce4f9b,"2010-07-21 04:19:00",34,"[Probability Theory: The Logic of Science][1]\\n\\n\\n  [1]: http://www.amazon.com/Probability-Theory-Logic-Science-Vol/dp/0521592712",,
811,16,371,780ca46f-632c-499c-89f4-a6c304ce4f9b,"2010-07-21 04:19:00",-1,,,
812,2,372,cf02cf90-0c32-483e-8916-f974ae8c29c2,"2010-07-21 04:26:07",252,"What topics in Statistics are most useful/relevant to Data Mining? ",,
813,1,372,cf02cf90-0c32-483e-8916-f974ae8c29c2,"2010-07-21 04:26:07",252,"What are the key statistical concepts that relate to data-mining?",,
814,3,372,cf02cf90-0c32-483e-8916-f974ae8c29c2,"2010-07-21 04:26:07",252,<data-mining><cart><probability>,,
815,2,373,b36bb39e-e988-4bb2-803a-ad9d38ad8143,"2010-07-21 04:30:50",252,"From Wikipedia :\\n\\n> Suppose you're on a game show, and\\n> you're given the choice of three\\n> doors: Behind one door is a car;\\n> behind the others, goats. You pick a\\n> door, say No. 1, and the host, who\\n> knows what's behind the doors, opens\\n> another door, say No. 3, which has a\\n> goat. He then says to you, "Do you\\n> want to pick door No. 2?" Is it to\\n> your advantage to switch your choice?\\n\\nThe answer is, of course, yes - but it's incredibly un-inituitive. What misunderstanding do most people have about probability that leads to us scratching our heads -- or better put; what general rule can we take away from this puzzle to better train our intuition in the future?\\n",,
816,1,373,b36bb39e-e988-4bb2-803a-ad9d38ad8143,"2010-07-21 04:30:50",252,"The Monty Hall Problem - where does our intuition fail us?",,
817,3,373,b36bb39e-e988-4bb2-803a-ad9d38ad8143,"2010-07-21 04:30:50",252,<probability><games>,,
818,2,374,a335d476-cfa7-4155-842f-048af06993a2,"2010-07-21 04:32:42",252,"What are pivot tables, and how can they be helpful in analyzing data?",,
819,1,374,a335d476-cfa7-4155-842f-048af06993a2,"2010-07-21 04:32:42",252,"What are pivot tables, and how can they be helpful in analyzing data?",,
820,3,374,a335d476-cfa7-4155-842f-048af06993a2,"2010-07-21 04:32:42",252,<analysis><pivot-table><pivots>,,
821,2,375,b3dfe5fb-66bc-4b9d-b483-9345aa7c9d5c,"2010-07-21 04:34:45",252,"What's a good way to test a series of numbers to see if they're random (or at least psuedo-random)? Is there a good statistical measure of randomness that can be used to determine how random a set is?\\n\\nMore importantly, how can one *prove* a method of generating numbers is psuedo-random?",,
822,1,375,b3dfe5fb-66bc-4b9d-b483-9345aa7c9d5c,"2010-07-21 04:34:45",252,"Testing (and proving) the randomness of numbers",,
823,3,375,b3dfe5fb-66bc-4b9d-b483-9345aa7c9d5c,"2010-07-21 04:34:45",252,<random><psuedo-random><proof>,,
824,2,376,3a8cfb81-9e65-4d6d-99cd-4837ef7d5361,"2010-07-21 04:48:50",61,"There's no reason you couldn't try to minimize norms other than x^2, there have been entire books written on quantile regression, for instance, which is more or less minimizing |x| if you're working with the median.  It's just generally harder to do and, depending on the error model, may not give good estimators (depending on whether that means low-variance or unbiased or low MSE estimators in the context).  \\n\\nAs for why we prefer integer moments over real-number-valued moments, the main reason is likely that while integer powers of real numbers always result in real numbers, non-integer powers of negative real numbers create complex numbers, thus requiring the use of an absolute value.  In other words, while the 3rd moment of a real-valued random variable is real, the 3.2nd moment is not necessarily real, and so causes interpretation problems.\\n\\nOther than that...\\n\\n 1. Analytical expressions for the integer moments of random variables are typically much easier to find than real-valued moments, be it by generating functions or some other method.  Methods to minimize them are thus easier to write.\\n 2. The use of integer moments leads to expressions that are more tractable than real-valued moments.\\n 3. I can't think of a compelling reason that (for instance) the 1.95th moment of the absolute value of X would provide better fitting properties than (for instance) the 2nd moment of X, although that could be interesting to investigate\\n 4. Specific to the L2 norm (or squared error), it can be written via dot products, which can lead to vast improvements in speed of computation.  It's also the only Lp space that's a Hilbert space, which is a nice feature to have.",,
825,2,377,eed2bbf0-5eb0-465e-a6f3-5854425ab88c,"2010-07-21 05:45:43",173,"This doesn't give a general rule, but I think that one reason why it's a challenging puzzle is that our intuition doesn't handle conditional probability very well. There are plenty of [other probability puzzles that play on the same phenomenon][1]. Since I'm linking to my blog, here's [a post specifically on Monty Hall][2].\\n\\n\\n  [1]: http://www.stubbornmule.net/2010/06/probability-paradoxes/\\n  [2]: http://www.stubbornmule.net/2010/06/monty-hall/",,
826,2,378,85da83da-5d09-4924-916f-30e31443ee02,"2010-07-21 05:54:25",68,"Consider two simple variations of the problem: \\n\\n 1. No doors are opened for the contestant. The host offers no help in picking a door. In this case it is obvious that the odds of picking the correct door are 1/3.\\n 2. Before the contestant is asked to venture a guess, the host opens a door and reveals a goat. After the host reveals a goat, the contestant has to pick the car from the two remaining doors. In this case it is obvious that the odds of picking the correct door is 1/2.\\n\\nFor a contestant to know the probability of his door choice being correct, he has to know how many positive outcomes are available to him and divide that number by the amount of possible outcomes. Because of the two simple cases outlined above, it is very natural to think of all the possible outcomes available as the number of doors to choose from, and the amount of positive outcomes as the number of doors that conceal a car. Given this intuitive assumption, even if the host opens a door to reveal a goat *after* the contestant makes a guess, the probability of either door containing a car remains 1/2.\\n\\nIn reality, probability recognizes a set of possible outcomes larger than the three doors and it recognizes a set of positive outcomes that is larger than the singular door with the car. In the correct analysis of the problem, the host provides the contestant with new information making a new question to be addressed: what is the probability that my original guess is such that the new information provided by the host is sufficient to inform me of the correct door? In answering this question, the set of positive outcomes and the set of possible outcomes are not tangible doors and cars but rather abstract arrangements of the goats and car. The three possible outcomes are the three possible arrangements of two goats and one car behind three doors. The two positive outcomes are the two possible arrangements where the first guess of the contestant is false. In each of these two arrangements, the information given by the host (one of the two remaining doors is empty) is sufficient for the contestant to determine the door that conceals the car.\\n\\n**In summation:**\\n\\nWe have a tendency to look for a simple mapping between physical manifestations of our choices (the doors and the cars) and the number of possible outcomes and desired outcomes in a question of probability. This works fine in cases where no new information is provided to the contestant. However, if the contestant is provided with more information (ie one of the doors you didn't choose is certainly not a car), this mapping breaks down and the correct question to be asked is found to be more abstract.",,
827,2,379,185268b0-8686-4623-a8b4-50f0a49661c4,"2010-07-21 06:09:43",190,"A pivot-table is a tool to dynamically show a slice and group multivariate data in tabular form.\\n\\nFor example, when we have the following data structure\\n\\n    Region  Year  Product  Sales \\n    US      2008  Phones   125 \\n    EU      2008  Phones   352 \\n    US      2008  Mouses   52 \\n    EU      2008  Mouses   65 \\n    US      2009  Phones   140 \\n    EU      2009  Phones   320 \\n    US      2009  Mouses   60 \\n    EU      2009  Mouses   100\\n\\nA pivot table can for example display a table with the sum of all products with in the rows the years and in the columns the regions. \\n\\nAll dimensions of the table can be switched easily. Also the data fields shown can be changed. This is called pivoting.\\n\\nThe tool is useful in exploratory data analyses. Because it is a dynamic tool, it can be used to visually detect patterns and outliers etc.\\n\\n\\nMost spreadsheet applications have support for this kind of tables.",,
828,6,374,0121e3d0-37de-49f4-a931-7c965c8f0fc6,"2010-07-21 06:10:08",190,<analysis><pivot-table>,"edited tags",
829,2,380,b03e85fd-8b4a-4de8-9253-e39771e1022a,"2010-07-21 06:15:36",88,"You cannot prove, because it is impossible; you can only check if there are no any embarrassing autocorrelations or distribution disturbances, and indeed [Diehard][1] is a standard for it. This is for statistics/physics, cryptographers will also mainly check (among other things) how hard is it to fit the generator to the data to obtain the future values.  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Diehard_tests",,
830,5,212,c81677f0-f635-4e03-a206-f6e1b8a48b54,"2010-07-21 06:19:29",190,"I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate.\\n\\nWhat methods do I have to test for statistical significance of my new results?\\n\\n**An example:**\\n\\nI have an experiment with 10 speaker, all having 100 (the same) sentences, total 900 words per speaker. Method A has an WER (word error rate) of 19.0%, Method B 18.5%.\\n\\nHow do I test whether Method B is significantly better?","Added an example",
831,6,212,c81677f0-f635-4e03-a206-f6e1b8a48b54,"2010-07-21 06:19:29",190,<statistical-significance>,"Added an example",
832,2,381,a0f69f88-9fb5-4f71-9fef-fcedf0b9554f,"2010-07-21 06:22:51",223,"Understanding **multivariate normal distribution** http://en.wikipedia.org/wiki/Multivariate_normal_distribution is important. \\n\\nThe concept of **correlation** and more generally (non linear) dependence is important. \\n\\n**Concentration of measure, asymptotic normality, convergence of random variables**.... how to make something from random to deterministic! http://en.wikipedia.org/wiki/Convergence_of_random_variables\\n\\n**maximum likelihood estimation** http://en.wikipedia.org/wiki/Maximum_likelihood and before that, statistical modeling :) and more generally minimum contrast estimation. \\n\\n**stationary process** http://en.wikipedia.org/wiki/Stationary_process and more generally stationnarity assumption and ergodic property. \\n\\nas Peter said, the question is so broad ... that the answer couldn't be given in a post ... ",,
833,6,375,c977ab3e-7b81-4b1c-a60e-a890e31a2f49,"2010-07-21 06:26:45",103,<random-generation><random><proof>,"edited tags",
834,2,382,a6c9ef59-b29c-47cc-8e68-93936f22c7e8,"2010-07-21 06:28:23",190,"Firstly I can recommend you the book [Foundations of statistical natural language processing][1] by Manning and Schütze.\\n\\nThe methods I would use are word-frequency distributions and ngram language models. The first works very well when you want to classify on topic and your topics are specific and expert (having keywords). Ngram modelling is the best way when you want to classify writing styles etc. \\n\\n\\n\\n\\n  [1]: http://nlp.stanford.edu/fsnlp/",,
835,2,383,5b483fbf-b2cb-48f8-97e6-e1f3f10b4b01,"2010-07-21 06:28:43",251,"You might be interested in applying relative distribution methods.  Call one group the reference group, and the other the comparison group.  In a way similar to constructing a probability-probability plot, you can construct a relative CDF/PDF, which is a ratio of the densities.  This relative density can be used for inference.  If the distributions are identical, you expect a uniform relative distribution.  There are tools, graphical and statistical, to explore and examine departures from uniformity.  \\n\\nA good starting point to get a better sense is [Applying Relative Distrbution Methods in R][1] and the [reldist][2] package in R.  For details, you'll need to refer to the book, [Relative Distribution Methods in the Social Sciences][3] by Handcock and Morris.  There's also a [paper][4] by the authors covering the relevant techniques.\\n\\n\\n  [1]: http://www.csss.washington.edu/Papers/wp27.pdf\\n  [2]: http://cran.r-project.org/web/packages/reldist/index.html\\n  [3]: http://csde.washington.edu/~handcock/RelDist/\\n  [4]: http://www.jstor.org/pss/270964\\n",,
836,6,124,3d32f10e-8abb-4c48-baa9-6c10cb98a958,"2010-07-21 06:29:15",190,<classification><text><information-retrieval>,"edited tags",
837,2,384,cfb0cfc5-a070-4fea-9dbb-44642d63a97f,"2010-07-21 06:33:36",190,"I would start with a frequency distribution. Collect for a big corpus the word-frequencies and select smartly the words that are keywords (not misspellings, with a very low frequency, and not stop words like "and", "or")\\n\\nThen when you have a number of new feeds, compare their distribution with the distribution that you build from your training data. Look to the big differences in frequencies and select so the important keywords of that moment.",,
838,2,385,07b1a40e-7643-4eca-90df-b03f71f0b055,"2010-07-21 06:39:38",190,"In principal unbalanced classes are not a problem at all for the k-nearest neighbor algorithm. \\n\\nBecause the algorithm is totally not influenced by the size of the class, it will not favor any on size. Try to run k-means with an obvious outlier and k+1 and you will see that most of the time the outlier will get it's own class.\\n\\nOff course, with hard datasets it is always advisable to run the algorithm multiple times. This to avoid trouble because of a bad initialization.",,
839,2,386,ca2fbaa3-853c-4658-b034-18a91f334437,"2010-07-21 06:46:11",223,"I would do some sort of "leave one out testing algorithm" (n is the number of data):\\n\\nfor i=1 to n\\n\\n 1. **compute a density estimation of the data set obtained by throwing Xi away**. (This density estimate should be done with some assumption if the dimension is high, for example, a gaussian assumption for which the density estimate is easy: mean and covariance)\\n 2. **Calculate the likelihood of Xi for the density estimated in step 1**. call it Li. \\n\\nend for \\n\\nsort the Li (for i=1,..,n) and use a multiple hypothesis testing procedure to say which are not good ... \\n \\nThis will work if n is sufficiently large... you can also use "leave k out strategy" which can be more relevent when you have "groups" of outliers ...",,
840,6,341,0ebae9d1-70b6-458c-81de-2d26da43ed16,"2010-07-21 06:50:32",190,<k-nearest-neighbour><unbalanced-classes>,"edited tags",
841,2,387,94a04ab7-911a-4c82-a9ec-38fdfe461271,"2010-07-21 07:04:26",69,"The following list contains many data sets you may be interested:\\n\\n - [America's Best Colleges - U.S. News & World Reports][1]\\n - [American FactFinder][2]\\n - [The Baseball Archive][3]\\n - [The Bureau of Justice Statistics][4]\\n - [The Bureau of Labor Statistics][5]\\n - [The Bureau of Transportation Statistics][6]\\n - [The Census Bureau][7]\\n - [Data and Story Library (DASL)][8]\\n - [Data Sets, UCLA Statistics Department][9]\\n - [DIG Stats][10]\\n - [Economic Research Service, US Department of Agriculture][11]\\n - [Energy Information Administration][12]\\n - [Eurostat][13]\\n - [Exploring Data][14]\\n - [FedStats][15]\\n - [The Gallop Organization][16]\\n - [International Fuel Prices][17]\\n - [Journal of Statistics Education Data Archive][18]\\n - [Kentucky Derby Race Statistics][19]\\n - [National Center for Education Statistics][20]\\n - [National Center for Health Statistics][21]\\n - [National Climatic Data Center][22]\\n - [National Geophysical Data Center][23]\\n - [National Oceanic and Atmospheric Administration][24]\\n - [Sports Data Resources][25]\\n - [Statistics Canada][26]\\n - [StatLib---Datasets Archive][27]\\n - [UK Government Statistical Service][28]\\n - [United Nations: Cyber SchoolBus Resources][29]\\n\\n\\n  [1]: http://www.usnews.com/usnews/edu/college/rankings/rankindex_brief.php\\n  [2]: http://factfinder.census.gov/servlet/BasicFactsServlet\\n  [3]: http://www.baseball1.com/\\n  [4]: http://www.albany.edu/sourcebook/\\n  [5]: http://www.bls.gov/\\n  [6]: http://www.bts.gov/\\n  [7]: http://www.census.gov/\\n  [8]: http://lib.stat.cmu.edu/DASL/\\n  [9]: http://www.stat.ucla.edu/data/\\n  [10]: http://www.cvgs.k12.va.us/DIGSTATS/\\n  [11]: http://www.ers.usda.gov/Briefing/\\n  [12]: http://www.eia.doe.gov/index.html\\n  [13]: http://epp.eurostat.ec.europa.eu/\\n  [14]: http://exploringdata.cqu.edu.au/\\n  [15]: http://www.fedstats.gov/\\n  [16]: http://www.gallup.com/\\n  [17]: http://www.gtz.de/en/themen/umwelt-infrastruktur/transport/10285.htm\\n  [18]: http://www.amstat.org/publications/jse/\\n  [19]: http://www.kentuckyderby.com/2003/derby_history/derby_statistics/\\n  [20]: http://www.ed.gov/NCES/\\n  [21]: http://www.cdc.gov/nchs/\\n  [22]: http://www.ncdc.noaa.gov/oa/ncdc.html\\n  [23]: http://www.ngdc.noaa.gov/\\n  [24]: http://www.noaa.gov/\\n  [25]: http://www.amstat.org/sections/SIS/Sports%20Data%20Resources/\\n  [26]: http://www.statcan.ca/start.html\\n  [27]: http://lib.stat.cmu.edu/datasets/\\n  [28]: http://www.statistics.gov.uk/\\n  [29]: http://www.un.org/cyberschoolbus/index.asp",,
842,16,387,94a04ab7-911a-4c82-a9ec-38fdfe461271,"2010-07-21 07:04:26",-1,,,
843,2,388,863bafc1-8aca-46e1-932c-a78d17815dc0,"2010-07-21 07:15:54",251,"On the math/foundations side: Harald Cramér's [Mathematical Methods of Statistics][1]. \\n\\n  [1]: http://www.amazon.com/Mathematical-Methods-Statistics-Harald-Cramer/dp/0691005478/\\n",,
844,16,388,863bafc1-8aca-46e1-932c-a78d17815dc0,"2010-07-21 07:15:54",-1,,,
845,2,389,9194ec8d-f652-4682-a8b3-4157cb6ed11b,"2010-07-21 07:51:34",199,"I do think there is something to be said for just excluding the outliers. A regression line is supposed to summarise the data. Because of leverage you can have a situation where 1% of your data points affects the slope by 50%.\\n\\nIt's only dangerous from a moral and scientific point of view if you don't tell anybody that you excluded the outliers. As long as you point them out you can say:\\n\\n"This regression line fits pretty well for most of the data. 1% of the time a value will come along that doesn't fit this trend, but hey, it's a crazy world, no system is perfect"",,
847,5,379,6ce6c95e-f1e7-44df-ab58-30ee7bac1ac9,"2010-07-21 08:00:10",190,"A pivot-table is a tool to dynamically show a slice and group multivariate data in tabular form.\\n\\nFor example, when we have the following data structure\\n\\n    Region  Year  Product  Sales \\n    US      2008  Phones   125 \\n    EU      2008  Phones   352 \\n    US      2008  Mouses   52 \\n    EU      2008  Mouses   65 \\n    US      2009  Phones   140 \\n    EU      2009  Phones   320 \\n    US      2009  Mouses   60 \\n    EU      2009  Mouses   100\\n\\nA pivot table can for example display a table with the sum of all products with in the rows the years and in the columns the regions. \\n\\nAll dimensions of the table can be switched easily. Also the data fields shown can be changed. This is called pivoting.\\n\\nThe tool is useful in exploratory data analyses. Because it is a dynamic tool, it can be used to visually detect patterns and outliers etc.\\n\\n\\nMost spreadsheet applications have support for this kind of tables.\\n\\nAn image from wikipedia: ![http://upload.wikimedia.org/wikipedia/commons/6/63/Pivottable-Pivoted.PNG][1]\\n\\n\\n  [1]: http://upload.wikimedia.org/wikipedia/commons/6/63/Pivottable-Pivoted.PNG","added 194 characters in body",
848,2,390,1c70b9fa-cacc-4754-adb3-371531da11ea,"2010-07-21 08:01:54",74,"Sweet, a good list so far.\\n\\nI would say Fooled By Randomness by Taleb, and Statistical Rules of Thumb by van Belle myself.",,
849,16,390,1c70b9fa-cacc-4754-adb3-371531da11ea,"2010-07-21 08:01:54",-1,,,
852,16,372,882037f9-679f-4db1-9ff0-1970ddac02f7,"2010-07-21 08:05:16",252,,,
853,6,372,882037f9-679f-4db1-9ff0-1970ddac02f7,"2010-07-21 08:05:16",252,<data-mining><probability><cart>,"edited tags",
854,2,392,9981dae4-bedd-459c-aea5-00ca5e2ccb15,"2010-07-21 08:19:36",251,"Take a look at the sample galleries for three popular visualization libraries:\\n\\n- [matplotlib gallery][1] (Python)\\n- [R graph gallery][2] (R) -- (also see [ggplot2][3], scroll down to reference)\\n- [prefuse visualization gallery][4] (Java)\\n\\nFor the first two, you can even view the associated source code -- the simple stuff is simple, not many lines of code.  The prefuse case will have the requisite Java boilerplate code.  All three support a number of backends/devices/renderers (pdf, ps, png, etc).  All three are clearly capable of high quality graphics.\\n\\nI think it pretty much boils down to which language are you most comfortable working in.  Go with that.\\n\\n  [1]: http://matplotlib.sourceforge.net/gallery.html\\n  [2]: http://addictedtor.free.fr/graphiques/\\n  [3]: http://had.co.nz/ggplot2/\\n  [4]: http://prefuse.org/gallery/\\n",,
855,5,154,fc15042b-738d-460b-a4dd-34b2e25c4312,"2010-07-21 08:37:04",108,"I am currently researching the *trial roulette method* for my masters thesis as an elicitation technique. This is a graphical method that allows an expert to represent her subjective probability distribution for an uncertain quantity.\\n\\nExperts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up to 1 - for example 20 chips of probability = 0.05 each. They are then instructed to arrange them on a pre-printed grid, with bins representing result intervals. Each column would represent their belief of the probability of getting the corresponding bin result.\\n\\nExample: A student is asked to predict the mark in a future exam. The\\nfigure below shows a completed grid for the elicitation of\\na subjective probability distribution. The horizontal axes of the\\ngrid shows the possible bins (or mark intervals) that the student was\\nasked to consider. The numbers in top row record the number of chips\\nper bin. The completed grid (using a total of 20 chips) shows that the\\nstudent believes there is a 30% chance that the mark will be between\\n60 and 64.9.\\n\\n![Eliciting priors from experts - Trial Roulette Method][1]\\n\\n\\n  [1]: http://img641.imageshack.us/img641/4716/chipsbinscrisp.png\\n\\n\\nSome reasons in favour of using this technique are:\\n\\n1. Many questions about the shape of the expert's subjective probability distribution can be answered without the need to pose a long series of questions to the expert - the statistician can simply read off density above or below any given point, or that between any two points. \\n\\n2. During the elicitation process, the experts can move around the chips if unsatisfied with the way they placed them initially - thus they can be sure of the final result to be submitted. \\n\\n3. It forces the expert to be coherent in the set of probabilities that are provided. If all the chips are used, the probabilities must sum to one.\\n\\n4. Graphical methods seem to provide more accurate results, especially for participants with modest levels of statistical sophistication.\\n","added 639 characters in body",
857,2,393,e87f1844-158c-4d49-a16c-ad10f34aded2,"2010-07-21 09:02:11",223,"The book from hastie, Tibshirani and Friedman http://www-stat.stanford.edu/~tibs/ElemStatLearn/ should be in any statistician's library ! ",,
858,16,393,e87f1844-158c-4d49-a16c-ad10f34aded2,"2010-07-21 09:02:11",-1,,,
860,5,358,afe6d715-a587-48af-8c8e-c1eb98141c83,"2010-07-21 09:31:29",88,"We try to minimize the variance that is left within descriptors. Why variance? Read [this question][1]; this also comes together with the (mostly silent) assumption that errors are normally distributed.\\n\\n**Extension:**  \\nTwo additional arguments:  \\n\\n 1. For variances, we have this nice "law" that the sum of variances is equal to the variance of sum, for uncorrelated samples. If we assume that the error is not correlated with the case, minimizing residual of squares will work straightforward to maximizing explained variance, what is maybe a not-so-good but still popular quality measure.  \\n\\n 2. If we assume normality of an error, least squares error estimator is a maximal likelihood one.\\n\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/118/standard-deviation-why-square-the-difference-instead-of-taking-the-absolute-val","added 501 characters in body",
861,2,394,dc24015f-3a84-4bc6-99ca-8ce58860ed61,"2010-07-21 09:53:40",210,"My understanding is that wbecause we are trying to mimimise errors, we need to find a way of not getting ourselves in a situation where the sum of the negative difference in errors is equal to the sum of the positive difference in errors but we haven't found a good fit. We do this by squaring the sum of the difference in errors which means the negative and positive difference in errors both become positive (-1*-1 = 1). If we raised x to the power of anything other than a positve integer we wouldn't address this problem because the erros would not have the same sign, or if we raised to the power of something that isn't an integer we'd enter the realms of complex numbers. ",,
862,2,395,6cd17769-43db-46e3-a0dc-d032daaf000b,"2010-07-21 10:13:25",210,"I have a data set where a series of measurements are being taken each week. In general the data set shows a +/- 1mm change each week with a mean measurement staying at about 0mm. In plotting the data this week it appears that some noticeable movement has cocured at two points and looking back at the data set, it is also possible that movement occured last week as well. \\nWhat is the best way of looking at this data set to see how likely it is that the movements that have been seen are real movements rather than just some effect caused by tthe natural tolerance in the readings.  ",,
863,1,395,6cd17769-43db-46e3-a0dc-d032daaf000b,"2010-07-21 10:13:25",210,"How to tell if something happened in a data set which monitors a value over time",,
864,3,395,6cd17769-43db-46e3-a0dc-d032daaf000b,"2010-07-21 10:13:25",210,<variance><monitoring>,,
865,2,396,b310dcf9-3ca0-4e10-b0e4-7de98e4cdafa,"2010-07-21 11:00:44",NULL,"I usually make my own idiosyncratic choices when preparing plots. However, I wonder if there are any best practices for generating plots. \\n\\nNote: [Rob's][1] comment to an answer to this [question][2] is very relevant here. \\n \\n\\n\\n  [1]: http://stats.stackexchange.com/users/159/rob-hyndman\\n  [2]: http://stats.stackexchange.com/questions/257/what-is-the-easiest-way-to-create-publication-quality-plots-under-linux",,user28
866,1,396,b310dcf9-3ca0-4e10-b0e4-7de98e4cdafa,"2010-07-21 11:00:44",NULL,"What best practices should I follow when preparing plots?",,user28
867,3,396,b310dcf9-3ca0-4e10-b0e4-7de98e4cdafa,"2010-07-21 11:00:44",NULL,<plotting><best-practices>,,user28
868,2,397,10cda2ca-6899-4347-a3c9-2f983426c816,"2010-07-21 11:11:47",8,"Just to add a bit to honk's answer, the [Diehard Test Suite][1] (developed by George Marsaglia) are the standard tests for PRNG.\\n\\nThere's a nice [Diehard C library][2] that gives you access to these tests. As well as the standard Diehard tests it also provides functions for a few other PRNG tests involving (amongst other things) checking bit order. There is also a facilty for testing the speed of the RNG and writing your own tests.\\n\\nThere is a R interface to the Diehard library, called [RDieHarder][3]. However, I couldn't get this to work.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Diehard_tests\\n  [2]: http://www.phy.duke.edu/~rgb/General/dieharder.php\\n  [3]: http://dirk.eddelbuettel.com/code/rdieharder.html",,
869,5,390,84042f91-3a99-4d03-9040-298a3f3494a7,"2010-07-21 11:15:14",190,"\\nI would say Fooled By Randomness by Taleb, and Statistical Rules of Thumb by van Belle myself.","deleted 28 characters in body",
870,2,398,cc17e688-715a-42fd-b25a-8460dff2c0c2,"2010-07-21 11:16:55",5,"We could stay here all day denoting best practices, but you should start by reading Tufte.  My primary recommendation:\\n\\n**Keep it simple.** \\n\\nOften people try to load up their charts with information. But you should really just have one main idea that you're trying to convey and if someone doesn't get your message almost immediately, then you should rethink how you have presented it.  So don't start working on your chart until the message itself is clear. Occam's razor applies here too. ",,
871,2,399,23b03306-3409-4181-a2e0-efd25569b909,"2010-07-21 11:18:56",190,"The Tufte principles are very good practices when preparing plots. See also his book [Beautiful Evidence][1]\\n\\nThe principles include:\\n\\n- Keep a high data-ink ratio\\n- Remove chart junk\\n- Give graphical element multiple functions\\n- Keep in mind the data density\\n\\nThe term to search for is Information Visualization\\n\\n\\n  [1]: http://www.amazon.com/Beautiful-Evidence-Edward-R-Tufte/dp/0961392177/ref=ntt_at_ep_dpt_2",,
872,2,400,6b7ab56d-24d2-4146-a731-96a8df2c6557,"2010-07-21 11:20:45",210,"One rule of thumb that I don't always follow but which is on occiasion useful is to take into accoutn that it is likely that your plot will be sent by fax at some point in it's future. You need to try and make your plots clear enough that even if they are sent by fax at some point in the future, the information the plot is trying to convey is still legible. ",,
873,2,401,104923de-7f0f-4660-acec-073787ce5590,"2010-07-21 12:22:21",256,"this problem you are asking about is known as test mining!\\n\\nthere are a few things you need to consider.. for example in your question you mentioned using keywords in titles.. one may ask why not the text in the article rather than just the title? which brings me to the first consideration.... what data do you limit yourself to?\\n\\nSecondly as the previous answer suggests using frequencies is a great start.... to take the analysis further you may start looking at what words occur frequently together! for example the word happy may occur very frequently... however if always accompanied with a "not" your conclusions would be very different!\\n\\nThere is a very nice Australian piece of software i have used in the past called Leximancer i would advise anybody nterested in text mining to have a look at their site and the examples they have.... from memory one of which analysed speaches by 2 US president candidant it makes for some very interesting reading!",,
874,2,402,5cd70f27-73fb-4958-8fe6-fe604cbbfff8,"2010-07-21 12:30:43",226,"You might consider applying a [Tukey Control chart][1] to the data.\\n\\n\\n  [1]: http://gunston.gmu.edu/708/frTukey.asp",,
875,2,403,51835900-5569-4c1e-9fa9-5058b94e3807,"2010-07-21 12:33:43",256,"I must agree.. there is no single best analysis!\\nnot just in cross tabulations or analysis of categorical data but in any data analysis... and thank god for that!\\nif there was just a single best way to address these analyses well many of us would not have a job to start with... not to mention the loss of the thrill of the hunt!\\n\\nthe joy of analysis is the unknown and the search for answers and evidence and how one question leads to another... that is what i love about statistics!\\n\\nSo back to the categorical data analysis... it really depends on what your doing. Are you looking to find how different variables affect each other as in drug tests for example we may look at treatment vs placebo crossed with disease and no disease... the question here is does treatment reduce disease.... chi square usually does well here (given a good sample size).\\nAnother context ihad today was looking at missing value trends... i was looking to find if missing values in one categorical variable relate to another... in some cases i knew the result should be missing and yet there were observations that had values... a completely different context to the drug test!",,
876,2,404,9e48bae4-a6bc-4cf2-a5d0-6785f71387ac,"2010-07-21 12:37:31",46,"You might want to have a look at [strucchange][1]: \\n\\n> Testing, monitoring and dating structural changes in (linear) regression models. strucchange features tests/methods from the generalized fluctuation test framework as well as from the F test (Chow test) framework. This includes methods to fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM, recursive/moving estimates) and F statistics, respectively. It is possible to monitor incoming data online using fluctuation processes. Finally, the breakpoints in regression models with structural changes can be estimated together with confidence intervals. Emphasis is always given to methods for visualizing the data."\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/strucchange/index.html\\n\\nPS. Nice graphics ;)",,
877,2,405,b9ed5129-899e-4601-b43d-ef2ce2907f75,"2010-07-21 12:38:51",190,"What kind of movement are we talking about?\\n\\nYou could of course fit a distribution over your data and see whether the new weeks fit in this distribution or are in the tail of it (which means it is likely something significant, real that you are observing)\\n\\nHowever, more information from your side would be helpful. Maybe you could provide a part of the dataset?",,
878,6,396,7f0055e9-f04b-4fca-9a2e-29bb5e9438a5,"2010-07-21 12:42:45",190,<plotting><best-practices><information-visualizatio>,"edited tags",
879,2,406,11d2e58a-e320-4863-94fa-e6ae221213d8,"2010-07-21 12:43:00",256,"I disagree with this question as it suggests that machine learning and statistics are different or conflicting sciences.... when the opposite is true!\\n\\nmachine learning makes extensive use of statistics... a quick survey of any Machine learning or data mining software package will reveal Clustering techniques such as k-means also found in statistics.... will also show dimension reduction techniques such as Principal components analysis also a statistical technique... even logistic regression yet another.\\n\\nIn my view the main difference is that traditionally statistics was used to proove a pre conceived theory and usually the analysis was design around that principal theory. Where with data mining or machine learning the opposite approach is usually the norm in that we have the outcome we just want to find a way to predict it rather than ask the question or form the theory is this the outcome!",,
880,2,407,b1eb98ed-96cd-4c13-9e01-87b58290f0f0,"2010-07-21 12:59:00",256,"One of the above answers touched in mahalanobis distances.... perhaps anpther step further and calculating simultaneous confidence intervals would help detect outliers!",,
881,2,408,2d2efe75-15b0-4e9f-804e-63348e4673cd,"2010-07-21 13:01:39",56,"In addition to conveying a clear message I always try to remember the plotsmanship:\\n\\n* font sizes for labels and legends should be big enough, preferably the same font size and font used in the final publication.\\n* linewidths should be big enough (1 pt lines tend to disappear if plots are shrunk only slightly). I try to go to linewidths of 3 to 5 pt.\\n* if plotting multiple datasets/curves with color make sure that they can be understood if printed in black-and-white, e.g. by using different symbols or linestyles in addition to color.\\n* always use a lossless (or close to lossless) format, e.g. a vector format like pdf, ps or svg or high resolution png or gif (jpeg doesn't work at all and was never designed for line art).\\n* prepare graphics in the final aspect ratio to be used in the publication. Changing the aspect ration later can give irritating font or symbol shapes.\\n* always remove useless clutter from the plotting program like unused histogram information, trend lines (hardly useful) or default titles.\\n\\nI have configured my plotting software (matplotlib, ROOT or root2matplotlib) to do most of this right by default. Before I was using `gnuplot` which needed extra care here.",,
882,2,409,f8a693cc-da1b-44c4-880b-671cfe44b986,"2010-07-21 13:11:33",219,"At the moment I use standard deviation of the mean to estimate uncertainty:\\n\\n![definition = stdev over sq.root of N](http://upload.wikimedia.org/math/8/a/e/8ae657bf495238caf69d82d754e70ed8.png)\\n\\nand then present it like this: ![mean plus-minus stdev about the mean](http://rogercortesi.com/eqn/tempimagedir/eqn1898.png) for each element in the time series.\\n\\nIs this valid? Is this appropriate for time series?",,
883,1,409,f8a693cc-da1b-44c4-880b-671cfe44b986,"2010-07-21 13:11:33",219,"How to approximate measurement uncertainty?",,
884,3,409,f8a693cc-da1b-44c4-880b-671cfe44b986,"2010-07-21 13:11:33",219,<time-series><standard-deviation><mean><uncertainty>,,
885,2,410,a4762071-bc71-4aa9-ab17-82fa382f7d49,"2010-07-21 13:20:53",56,"The answer to this question depends a lot on how your measurement uncertainty arises. If it is due to to uncorrelated normally distributed fluctuations in your measurement your measurement outcomes will also be normally distributed.\\n\\nIf this assumption is valid can be hard to prove, but plotting histograms of outcomes of independent measurement should give you a feeling for the shape (and size) of the uncertainty.\\n\\nSince the normal distribution works on a unlimited range of values, typical cases where measurements are not normally distributed are counting experiments where the number to be measured is small (e.g. less than 20) and/or fluctuations are large, or when the measured quantity is defined to be in a range, e.g. a fraction. One might still use normal approximation in a certain range though.",,
886,2,411,081c0a34-fcd2-4710-a4f3-7be4fe0015da,"2010-07-21 13:39:06",89,"There are many ways to measure how similar two probability distributions are.  Among methods which are popular (in different circles) are:\\n\\n1. the Kolmogorov distance: the sup-distance between the distribution functions;\\n\\n2. the Kantorovich-Rubinstein distance: the maximum difference between the expectations w.r.t. the two distributions of functions with Lipschitz constant 1, which also turns out to be the L^1 distance between the distribution functions;\\n\\n3. the bounded-Lipschitz distance: like the K-R distance but the functions are also required to have absolute value at most 1.\\n\\nThese have different advantages and disadvantages.  Only convergence in the sense of 3. actually corresponds precisely to convergence in distribution; convergence in the sense of 1. or 2. is slightly stronger in general.  (In particular, if X_n=1/n with probability 1, then X_n converges to 0 in distribution, but not in the Kolmogorov distance. However, if the limit distribution is continuous then this pathology doesn't occur.) \\n\\nFrom the perspective of elementary probability or measure theory, 1. is very natural because it compares the probabilities of being in some set. A more sophisticated probabilistic perspective, on the other hand, tends to focus more on expectations than probabilities. Also, from the perspective of functional analysis, distances like 2. or 3. based on duality with some function space are very appealing, because there is a large set of mathematical tools for working with such things.\\n\\nHowever, my impression (correct me if I'm wrong!) is that in statistics, the Kolmogorov distance is the usually preferred way of measuring similarity of distributions.  I can guess one reason: if one of the distributions is discrete with finite support -- in particular, if it is the distribution of some real-world data -- then the Kolmogorov distance to a model distribution is easy to compute.  (The K-R distance would be slightly harder to compute, and the B-L distance would probably be impossible in practical terms.)\\n\\nSo my question (finally) is, are there other reasons, either practical or theoretical, to favor the Kolmogorov distance (or some other distance) for statistical purposes?",,
887,1,411,081c0a34-fcd2-4710-a4f3-7be4fe0015da,"2010-07-21 13:39:06",89,"Motivation for Kolmogorov distance between distributions",,
888,3,411,081c0a34-fcd2-4710-a4f3-7be4fe0015da,"2010-07-21 13:39:06",89,<distributions><probability>,,
889,2,412,4f94620b-c1b9-4dc9-994d-9a71e44b6f34,"2010-07-21 13:40:02",8,"I think you need look at statistical [control charts][1]. The most common of which are cusum and Shewhart charts.\\n\\nBasically, data arrives sequentially and is tested against a number of rules. For example,\\n\\n1. Is the data far away from the cumulative mean - say 3 standard deviations \\n1. Has the data been increasing for the last few points.\\n1. Does the data alternate between positive and negative values.\\n\\nIn R you can use the [qcc][2] package.\\n\\nFor example,\\n \\n    #Taken from the documentation\\n    library(qcc)\\n    data(orangejuice)\\n    attach(orangejuice)\\n    plot(qcc(D[trial], sizes=size[trial], type="p"))\\n\\nGives the following plot, with possible problem points highlighted in red.\\n\\n![control chart][3]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Control_chart\\n  [2]: http://cran.r-project.org/web/packages/qcc/index.html\\n  [3]: http://img805.imageshack.us/img805/5858/tmp.jpg",,
890,5,409,96a49b7b-4935-43b4-849f-c2e05f060a02,"2010-07-21 13:40:27",219,"At the moment I use standard deviation of the mean to estimate uncertainty:\\n\\n![definition = stdev over sq.root of N](http://upload.wikimedia.org/math/8/a/e/8ae657bf495238caf69d82d754e70ed8.png)\\n\\nwhere *N* is in hundreds and mean is a time series (monthly) mean. I\\npresent it then like this: ![mean plus-minus stdev about the mean](http://rogercortesi.com/eqn/tempimagedir/eqn1898.png) for each element (month) in the (annual) time series.\\n\\nIs this valid? Is this appropriate for time series?","added 83 characters in body",
891,2,413,8fa466ad-5d0c-47c1-a341-bffa9222ee6f,"2010-07-21 13:48:15",223,"I don't really know what the conceptual/historical difference between machine learning and statistic is but I am sure it is not that obvious... and I am not really interest in knowing if I am a machine learner or a statistician, I think 10 years after Breiman's paper, lots of people are both...\\n\\nAnyway, I found  **interesting the question about predictive accuracy of models**. We have to remember that it is not always possible to measure the accuracy of a model and more precisely we are most often implicitly making some modeling when measuring errors.\\n\\nFor Example, mean absolute error in time series forecast is a mean over time and it measures the performance of a procedure to forecast the median with the assumption that performance is, in some sense, **stationary** and shows some **ergodic** property. If (for some reason) you need to forecast the mean temperature on earth for the next 50 years and if your modeling performs well for the last 50 years... it does not means that... \\n\\nMore generally, (if I remember, it is called no free lunch) you can't do anything without modeling... In addition, I think statistic is trying to find an answer to the question : "is something significant or not ", this is a very important question in science and can't be answered through a learning process. To state John Tukey (was he a statistician ?) :\\n\\n> *The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data* \\n\\n \\nHope this helps ! ",,
892,2,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,"2010-07-21 13:50:08",89,"Can anyone recommend a good introduction to statistics for a mathematician who is already well-versed in probability?  I have two distinct motivations for asking, which may well lead to different suggestions:\\n\\n1. I'd like to better understand the statistics motivation behind many problems considered by probabilists.\\n\\n2. I'd like to know how to better interpret the results of Monte Carlo simulations which I sometimes do to form mathematical conjectures.\\n\\nI'm open to the possibility that the best way to go is not to look for something like "Statistics for Probabilists" and just go to a more introductory source.",,
893,1,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,"2010-07-21 13:50:08",89,"Intro to statistics for mathematicians",,
894,3,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,"2010-07-21 13:50:08",89,<textbook>,,
895,16,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,"2010-07-21 13:50:08",89,,,
896,2,415,4e08983d-d95e-44dc-820a-837d9c07962d,"2010-07-21 13:53:13",223,"I think you should take a look to the similar post from mathoverflow at http://mathoverflow.net/questions/31655/statistics-for-mathematicians/31665#31665\\n\\nMy answer to this post was Asymptotic statistics from Van der Vaart http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521784504. ",,
897,16,415,4e08983d-d95e-44dc-820a-837d9c07962d,"2010-07-21 13:53:13",-1,,,
898,2,416,ca041f71-ff72-4b07-8705-cd28d919c434,"2010-07-21 14:00:03",215,"\\nThe population is the set of entities under study. For example, the mean height of men. This is a hypothetical population because it includes all men that have lived, are alive and will live in the future. I like this example because it drives home the point that we, as analysts, choose the population that we wish to study. Typically it is impossible to survey/measure the entire population because not all members are observable (e.g. men who will exist in the future). If it is possible to enumerate the entire population it is often costly to do so and would take a great deal of time. In the example above we have a population "men" and a parameter of interest, their height.\\n\\nInstead, we could take a subset of this population called a sample and use this sample to draw inferences about the population under study, given some conditions. Thus we could measure the mean height of men in a sample of the population which we call a statistic and use this to draw inferences about the parameter of interest in the population. It is an inference because there will be some uncertainty and inaccuracy involved in drawing conclusions about the population based upon a sample. This should be obvious - we have fewer members in our sample than our population therefore we have lost some information.\\n\\nThere are many ways to select a sample and the study of this is called sampling theory. A commonly used method is called Simple Random Sampling (SRS). In SRS each member of the population has an equal probability of being included in the sample, hence the term "random". There are many other sampling methods e.g. stratified sampling, cluster sampling, etc which all have their advantages and disadvantages.\\n\\nIt is important to remember that the sample we draw from the population is only one from a large number of potential samples. If ten researchers were all studying the same population, drawing their own samples then they may obtain different answers. Returning to our earlier example, each of the ten researchers may come up with a different mean height of men i.e. the statistic in question (mean height) varies of sample to sample -- it has a distribution called a sampling distribution. We can use this distribution to understand the uncertainty in our estimate of the population parameter.\\n\\nThe sampling distribution of the sample mean is known to be a normal distribution with a standard deviation equal to the sample standard deviation divided by the sample size. Because this could easily be confused with the standard deviation of the sample it more common to call the standard deviation of the sampling distribution the **standard error**.\\n\\n",,
899,2,417,3f950b8d-69ac-4f00-a8f1-3e4b3a78d0aa,"2010-07-21 14:01:46",251,"[Mathematical Methods of Statistics][1], Harald Cramér is really great if you're coming to Statistics from the mathematical side.  It's a bit dated, but still relevant for all the basic mathematical statistics.\\n\\nTwo other noteworthy books come to mind for inference and estimation theory:\\n\\n- [Theory of Point Estimation][2], E. L. Lehmann\\n- [Theory of Statistics][3], Schervish\\n\\nNot entirely sure if this is what you wanted, but you can check out the reviews and see if they meet your expectations.\\n\\n\\n  [1]: http://www.powells.com/biblio/61-9780691005478-1\\n  [2]: http://www.powells.com/biblio/9780387985022\\n  [3]: http://www.powells.com/biblio/0387945466\\n",,
900,16,417,3f950b8d-69ac-4f00-a8f1-3e4b3a78d0aa,"2010-07-21 14:01:46",-1,,,
901,2,418,74b8bd44-5d11-4b0b-a3d3-62a5c4cdccbd,"2010-07-21 14:30:42",77,"Coming from the field of computer vision, I've often used the [RANSAC][1] (Random Sample Consensus) method for fitting models to data with lots of outliers. \\n\\nHowever, I've never seen it used by statisticians, and I've always been under the impression that it wasn't considered a "statistically-sound" method. Why is that so? It is random in nature, which makes it harder to analyze, but so are bootstrapping methods. \\n\\nOr is simply a case of academic silos not talking to one another?\\n\\n  [1]: http://en.wikipedia.org/wiki/RANSAC",,
902,1,418,74b8bd44-5d11-4b0b-a3d3-62a5c4cdccbd,"2010-07-21 14:30:42",77,"Why isn't RANSAC most widely used in statistics?",,
903,3,418,74b8bd44-5d11-4b0b-a3d3-62a5c4cdccbd,"2010-07-21 14:30:42",77,<outliers><bootstrapping>,,
904,2,419,71d9bb14-10ea-4b0f-868a-79ce9f132ac9,"2010-07-21 14:43:31",215,"\\nI agree that students find this problem very difficult. The typical response I get is that after you've been shown a goat there's a 50:50 chance of getting the car so why does it matter? Students seem to divorce their first choice from the decision they're now being asked to make i.e. they view these two actions as independent. I then remind them that they were twice as likely to have chosen the wrong door initially hence why they're better off switching. \\n\\nIn recent years I've started actually playing the game in glass and it helps students to understand the problem much better. I use three cardboard toilet roll "middles" and in two of them are paper clips and in the third is a £5 note. ",,
905,5,395,c5f0c50b-d83a-4b5f-b400-cf2d6be2c005,"2010-07-21 14:46:28",210,"I have a data set where a series of measurements are being taken each week. In general the data set shows a +/- 1mm change each week with a mean measurement staying at about 0mm. In plotting the data this week it appears that some noticeable movement has occured at two points and looking back at the data set, it is also possible that movement occured last week as well. \\nWhat is the best way of looking at this data set to see how likely it is that the movements that have been seen are real movements rather than just some effect caused by \\nthe natural tolerance in the readings.\\n\\n**Edit**\\n\\nSome more information on the data set. Measurements have been taken at 39 locations which should behave in a similar way although only some of the points may show signs of movement. At each point the readings have now been taken 10 times on a bi-weekly basis and up until the most recent set of readings the measurements were between -1mm and 1mm. The measurements can only be taken with mm accuracy so we only recieve results to the nearest mm. The results for one of the points showing a movement is 0mm, 1mm, 0mm, -1mm, -1mm, 0mm, -1mm, -1mm, 1mm, 3mm. We are not looking for statisitically signficiant information, just an indicator of what might have occured. The reason is that if a measurement reaches 5mm in a subsequent week we have a problem and we'd like to be forwarned that this might occur. ","added additional measurement data",
906,2,420,e687de7c-faf1-44ab-acb7-2597d5c674d9,"2010-07-21 14:55:06",89,"I'd modify what Graham Cookson said slightly.  I think the really crucial thing that people overlook is not their first choice, but the *host's* choice, and the assumption that the host made sure *not* to reveal the car.  \\n\\nIn fact, when I discuss this problem in a class, I present it in part as a case study in being clear on your assumptions. It is to your advantage to switch *if the host is making sure only to reveal a goat*.  On the other hand, if the host picked randomly between doors 2 and 3, and happened to reveal a goat, then there is no advantage to switching.\\n\\n(Of course, the practical upshot is that if you don't know the host's strategy, you should switch anyway.)",,
907,2,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,"2010-07-21 15:01:21",219,"What book would you recommend for scientists who are not statisticians?\\n\\nClear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.",,
908,1,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,"2010-07-21 15:01:21",219,"What book would you recommend for non-statistician?",,
909,3,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,"2010-07-21 15:01:21",219,<books><science>,,
910,16,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,"2010-07-21 15:01:21",219,,,
911,2,422,f3f91749-49b8-4098-817a-1ff7ccc23ae3,"2010-07-21 15:09:11",247,"That'll depend very much on their background, but I found "Statistics in a Nutshell" to be pretty good.",,
912,16,422,f3f91749-49b8-4098-817a-1ff7ccc23ae3,"2010-07-21 15:09:11",-1,,,
913,2,423,f43ac356-f089-4201-b246-91cced19a08b,"2010-07-21 15:13:21",5,"This is one of my favorites:\\n\\n![alt text][1]\\n\\nOne entry per answer.  This is in the vein of [this StackOverflow question][2].\\n\\nP.S. Do not hotlink the cartoon without the site's permission please.\\n\\n\\n  [1]: http://imgs.xkcd.com/comics/correlation.png\\n  [2]: http://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon",,
914,1,423,f43ac356-f089-4201-b246-91cced19a08b,"2010-07-21 15:13:21",5,"What is your favorite "data analysis" cartoon?",,
915,3,423,f43ac356-f089-4201-b246-91cced19a08b,"2010-07-21 15:13:21",5,<humor>,,
916,16,423,f43ac356-f089-4201-b246-91cced19a08b,"2010-07-21 15:13:21",5,,,
917,2,424,02e9b914-9e84-4c77-b0df-a9f61419f7b3,"2010-07-21 15:21:33",88,"Was XKCD, so time for Dilbert:\\n\\n![alt text][1]\\n\\n\\n  [1]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/2000/300/2318/2318.strip.print.gif\\n\\n",,
918,16,424,02e9b914-9e84-4c77-b0df-a9f61419f7b3,"2010-07-21 15:21:33",-1,,,
919,2,425,511c25d7-dfba-43f7-8a3f-0224e22caced,"2010-07-21 15:23:53",13,"One of my favorites from [xckd](http://www.xkcd.com):\\n\\n![alt text][1]\\n\\n> RFC 1149.5 specifies 4 as the standard IEEE-vetted random number.\\n\\n  [1]: http://imgs.xkcd.com/comics/random_number.png",,
920,16,425,511c25d7-dfba-43f7-8a3f-0224e22caced,"2010-07-21 15:23:53",-1,,,
924,5,352,cc75cec1-2ad2-4729-8c20-9505d85df317,"2010-07-21 15:29:25",8,"I've never had to do this, so this is just a suggestion.\\n\\nI see two (other) possibilities. \\n\\n**Half data**\\n\\n1. Load in half the data and sort\\n1. Next read in the remaining values and compare against the your sorted list. \\n  1. If the new value is larger, discard it.\\n  1. else put the value in the sorted list and removing the largest value from that list.\\n\\n\\n**Sampling distribution**\\n\\nThe other option, is to use an approximation involving the sampling distribution. If your data is Normal, then the standard error for moderate *n* is:\\n\\n1.253 * sd / sqrt(n)\\n\\nTo determine the size of *n* that you would be happy with, I ran a quick Monte-Carlo simulation in R\\n\\n    n = 10000\\n    outside.ci.uni = 0\\n    outside.ci.nor = 0\\n    N=1000\\n    for(i in 1:N){\\n      #Theoretical median is 0\\n      uni = runif(n, -10, 10)\\n      nor  = rnorm(n, 0, 10)\\n      \\n      if(abs(median(uni)) > 1.96*1.253*sd(uni)/sqrt(n))\\n        outside.ci.uni = outside.ci.uni + 1\\n    \\n      if(abs(median(nor)) > 1.96*1.253*sd(nor)/sqrt(n))\\n        outside.ci.nor = outside.ci.nor + 1\\n    }\\n    \\n    outside.ci.uni/N\\n    outside.ci.nor/N\\n\\nFor n=10000, 15% of the uniform median estimates were outside the CI.\\n\\n\\n\\n","Fixed a typo; deleted 6 characters in body",
925,2,427,c488472b-5407-49df-a7da-22a4ba581195,"2010-07-21 15:33:11",215,"\\n*Before touching this topic, I always make sure that students are happy moving between percentages, decimals, odds and fractions. If they are not completely happy with this then they can get confused very quickly.*\\n\\nI like to explain hypothesis testing for the first time (and therefore p-values and test statistics) through Fisher's classic tea experiment. I have several reasons for this:\\n\\n(i) I think working through an experiment and defining the terms as we go along makes more sense that just defining all of these terms to begin with.\\n(ii) You don't need to rely explicitly on probability distributions, areas under the curve, etc to get over the key points of hypothesis testing.\\n(iii) It explains this ridiculous notion of "as or more extreme than those observed" in a fairly sensible manner\\n(iv) I find students like to understand the history, origins and back story of what they are studying as it makes it more real than some abstract theories.\\n(v) It doesn't matter what discipline or subject the students come from, they can relate to the example of tea (N.B. Some international students have difficulty with this peculiarly British institution of tea with milk.)\\n\\n[Note: I originally got this idea from Dennis Lindley's wonderful article "The Analysis of Experimental Data: The Appreciation of Tea & Wine" in which he demonstrates why Bayesian methods are superior to classical methods.]\\n\\nThe back story is that Muriel Bristol visits Fisher one afternoon in the 1920's at Rothamsted Experimental Station for a cup of tea. When Fisher put the milk in last she complained saying that she could also tell whether the milk was poured first (or last) and that she preferred the former. To put this to the test he designed his classic tea experiment where Muriel is presented with a pair of tea cups and she must identify which one had the milk added first. This is repeated with six pairs of tea cups. Her choices are either Right (R) or Wrong (W) and her results are: RRRRRW.\\n\\nSuppose that Muriel is actually just guessing and has no ability to discriminate whatsoever. This is called the **Null Hypothesis**. According to Fisher the purpose of the experiment is to discredit this null hypothesis. If Muriel is guessing she will identify the tea cup correctly with probability 0.5 on each turn and as they are independent the observed result has 0.5^6 = 0.016 (or 1/64). Fisher then argues that either:\\n\\n(a) the null hypothesis (Muriel is guessing) is true and an event of small probability has occurred or,\\n(b) the null hypothesis is false and Muriel has discriminatory powers.\\n\\nThe p-value (or probability value) is the probability of observing this outcome (RRRRRW) given the null hypothesis is true - it's the small probability referred to in (a), above. In this instance it's 0.016. Since events with small probabilities only occur rarely (by definition) situation (b) might be a more preferable explanation of what occurred than situation (a). When we reject the null hypothesis we're in fact accepting the opposite hypothesis which is we call the alternative hypothesis. In this example, Muriel has discriminatory powers is the alternative hypothesis.\\n\\nAn important consideration is what do we class as a "small" probability? What's the cutoff point at which we're willing to say that an event is unlikely? The standard benchmark is 5% (0.05) and this is called the significance level. When the p-value is smaller than the significance level we reject the null hypothesis as being false and accept our alternative hypothesis. It is common parlance to claim a result is "significant" when the p-value is smaller than the significance level i.e. when the probability of what we observed occurring given the null hypothesis is true is smaller than our cutoff point. It is important to be clear that using 5% is completely subjective (as is using the other common significance levels of 1% and 10%). \\n\\nFisher realised that this doesn't work; every possible outcome with one wrong pair was equally suggestive of discriminatory powers. The relevant probability for situation (a), above, is therefore 6(0.5)^6 = 0.094 (or 6/64) which now is **not significant** at a significance level of 5%. To overcome this Fisher argued that if 1 error in 6 is considered evidence of discriminatory powers then so is no errors i.e. outcomes that more strongly indicate discriminatory powers than the one observed should be included when calculating the p-value. This resulted in the following amendment to the reasoning, either:\\n\\n(a) the null hypothesis (Muriel is guessing) is true and the probability of events as, or more, extreme than that observed is small, or\\n(b) the null hypothesis is false and Muriel has discriminatory powers.\\n\\nBack to our tea experiment and we find that the p-value under this set-up is 7(0.5)^6 = 0.109 which still is not significant at the 5% threshold. \\n\\nI then get students to work with some other examples such as coin tossing to work out whether or not a coin is fair. This drills home the concepts of the null/alternative hypothesis, p-values and significance levels. We then move onto the case of a continuous variable and introduce the notion of a test-statistic. As we have already covered the normal distribution, standard normal distribution and the z-transformation in depth it's merely a matter of bolting together several concepts. \\n\\nAs well as calculating test-statistics, p-values and making a decision (significant/not significant) I get students to work through published papers in a fill in the missing blanks game. \\n\\n",,
926,2,428,28d9cebb-8e3b-4c41-9152-8a3fe0826813,"2010-07-21 15:35:45",210,"Could you group the data set into much smaller data sets (say 100 or 1000 or 10,000 data points) If you then calculated the median of each of the groups. If you did this with enough data sets you could plot something like the average of the results of each of the smaller sets and this woul, by running enough smaller data sets converge to an 'average' solution. ",,
927,2,429,1cfb2515-9716-453f-9e4c-99fceb61da31,"2010-07-21 15:36:34",39,"Briefly stated, the Shapiro-Wilk test is a specific test for normality, whereas the method used by [Kolmogorov-Smirnov test][1] is more general, but less powerful (meaning it correctly rejects the null hypothesis of normality less often). Both statistics take normality as the null and establishes a test statistic based on the sample, but how they do so is different from one another in ways that make them more or less sensitive to features of normal distributions.\\n\\nHow exactly W (the test statistic for Shapiro-Wilk) is calculated is [a bit involved][2], but conceptually, it involves arraying the sample values by size and measuring fit against expected means, variances and covariances.  These multiple comparisons against normality, as I understand it, give the test more power than the the Kolmogorov-Smirnov test, which is one way in which they may differ.\\n\\nBy contrast, the Kolmogorov-Smirnov test for normality is derived from a general approach for assessing goodness of fit by comparing the expected cumulative distribution against the empirical cumulative distribution, vis:\\n\\n![alt text][3] \\n\\nAs such, it is sensitive at the center of the distribution, and not the tails.  However, the K-S is test is convergent, in the sense that as n tends to infinity, the test converges to the true answer in probability (I believe that [Glivenko-Cantelli Theorem][4] applies here, but someone may correct me). These are two more ways in which these two tests might differ in their evaluation of normality.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\\n  [2]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/wilkshap.htm\\n  [3]: http://www.itl.nist.gov/div898/handbook/eda/section3/gif/ecdf.gif\\n  [4]: http://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem",,
928,2,430,cdaff380-5ac0-4bed-a45d-fee0352c0219,"2010-07-21 15:37:17",251,"I find that people find the solution a more intuitive if you change it to 100 doors, close first, second, to 98 doors.  Similarly for 50 doors, etc.  ",,
929,2,431,110b7955-7ec9-40c6-b4d4-4112e6798481,"2010-07-21 15:38:11",215,"\\nI think every statistician should read Stigler's *The History of Statistics: The Measurement of Uncertainty before 1900*\\n\\nIt is beautifully written, thorough and it isn't a historian's perspective but a mathematician's, hence it doesn't avoid the technical details. ",,
930,16,431,110b7955-7ec9-40c6-b4d4-4112e6798481,"2010-07-21 15:38:11",-1,,,
931,2,432,14ef398f-b785-48b8-b35a-84de0b4dccfe,"2010-07-21 15:38:46",8,"My favourite Dilbert cartoon:\\n\\n![alt text][1]\\n\\n\\n  [1]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/5000/600/5651/5651.strip.gif",,
932,16,432,14ef398f-b785-48b8-b35a-84de0b4dccfe,"2010-07-21 15:38:46",-1,,,
933,2,433,3b8730d2-6b08-440d-bfdf-998e713f91dd,"2010-07-21 15:43:44",5,"Here's [another one from Dilbert][1]:\\n\\n![alt text][2]\\n\\n\\n  [1]: http://dilbert.com/strips/comic/2010-07-02/\\n  [2]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/90000/3000/400/93464/93464.strip.gif",,
934,16,433,3b8730d2-6b08-440d-bfdf-998e713f91dd,"2010-07-21 15:43:44",-1,,,
935,5,113,ff22a260-1351-49d5-94bb-43af20ff29eb,"2010-07-21 15:44:07",39,"I have been looking into theoretical frameworks for method selection (note: not model selection) and have found very little systematic, mathematically-motivated work. By 'method selection', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type.\\n\\nWhat I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. [Inductive Policy: The Pragmatics of Bias Selection][1]). I may be unrealistic at this early stage of machine learning's development, but I was hoping to find something like what [measurement theory][2] does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems.\\n\\nAny suggestions?\\n\\n\\n  [1]: http://portal.acm.org/citation.cfm?id=218546\\n  [2]: ftp://ftp.sas.com/pub/neural/measurement.html","added 28 characters in body",
936,2,434,3d7037f2-062a-42bd-9ceb-3ec242a1f432,"2010-07-21 15:50:43",215,"\\n*Very* crudely I would say that:\\n\\n**Frequentist:** Sampling is infinite and decision rules can be sharp. Data are a repeatable random sample - there is a frequency. Underlying parameters are fixed i.e. they remain constant during this repeatable sampling process.\\n\\n**Bayesian:**  Unknown quantities are treated probabilistically and the state of the world can always be updated. Data are observed from the realised sample. Parameters are unknown and described probabilistically. It is the data which are fixed.\\n\\nThere is a brilliant [blog post][1] which gives an indepth example of how a Bayesian and Frequentist would tackle the same problem. Why not answer the problem for yourself and then check?\\n\\nThe problem (taken from Panos Ipeirotis' blog):\\n\\nYou have a coin that when flipped ends up head with probability p and ends up tail with probability 1-p. (The value of p is unknown.)\\n\\nTrying to estimate p, you flip the coin 100 times. It ends up head 71 times.\\n\\nThen you have to decide on the following event: "In the next two tosses we will get two heads in a row."\\n\\nWould you bet that the event will happen or that it will not happen?\\n\\n\\n  [1]: http://behind-the-enemy-lines.blogspot.com/2008/01/are-you-bayesian-or-frequentist-or.html\\n  ",,
937,2,435,2e9e6a78-c9e8-4ad9-a588-ad29b2d888d4,"2010-07-21 15:53:59",88,"One more [Dilbert][1]:\\n![alt text][2]\\n\\n\\n  [1]: http://dilbert.com/fast/2008-05-08/\\n  [2]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/5000/600/5652/5652.strip.print.gif",,
938,16,435,2e9e6a78-c9e8-4ad9-a588-ad29b2d888d4,"2010-07-21 15:53:59",-1,,,
939,5,207,65d9f33f-6c41-43e8-a605-2798d01277e0,"2010-07-21 16:00:43",NULL,"First, we need to understand what is a markov chain. Consider the following [weather][1] example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:\\n\\nProbability(Next day is sunny | Given today is rainy ) = 0.50\\n\\nSince, the next day's weather is either sunny or rainy it follows that:\\n\\nProbability(Next day is Rainy | Given today is rainy ) = 0.50 \\n\\nSimilarly, let:\\n\\nProbability(Next day is rainy | Given today is sunny ) = 0.10\\n\\nTherefore, it follows that:\\n\\nProbability(Next day is sunny | Given today is sunny ) = 0.90\\n\\nThe above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:\\n\\n             S   R\\n    P = S [ 0.9 0.1\\n        R   0.5 0.5]\\n\\nWe might ask several questions whose answers follow:\\n\\nQ1: If the weather is sunny today then what is the weather likely to be tomorrow?\\n\\nA1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. \\n\\nQ2: What about two days from today?\\n\\nA2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now:\\n\\nFirst day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9  0.9. \\n\\nOr\\n\\nFirst day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 * 0.5\\n\\nTherefore, the probability that the weather will be sunny in two days is:\\n\\nProb(Sunny two days from now) = 0.9  0.9 + 0.1  0.5 = 0.81 + 0.05 = 0.86 \\n\\nSimilarly, the probability that it will be rainy is:\\n\\nProb(Rainy two days from now) = 0.1 * 0.5 + 0.9 0.1 = 0.05 + 0.09 = 0.14\\n\\nIf you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities:\\n\\nProb(Sunny) = 0.833\\nProb(Rainy) = 0.167\\n\\nIn other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.\\n\\nThe above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions):\\n\\nIrrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.\\n\\nMarkov Chain Monte Carlo exploits the above feature as follows: \\n\\nWe want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. \\n\\nIf we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. \\n\\nWe then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monte carlo component.\\n\\nThere are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm).\\n\\n  [1]: http://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_very_simple_weather_model","fixed typo",user28
940,2,436,c839ffa2-6f6b-450a-8c47-62b42b870fda,"2010-07-21 16:04:18",215,"\\nThe answer would most definitely depend on their discipline, the methods/techniques that they would like to learn and their existing mathematical/statistical abilities.\\n\\nFor example, economists/social scientists who want to learn about cutting edge empirical econometrics could read Angrist and Pischke's Mostly Harmless Econometrics. This is a non-technical book covering the "natural experimental revolution" in economics. The book only presupposes that they know what regression is.\\n\\nBut I think the best book on applied regression is Gelman and Hill's Data Analysis Using Regression and Hierarchical/Multilevel Models. This covers basic regression, multilevel regression, and Bayesian methods in a clear and intuitive way. It would be good for any scientist with a basic background in statistics.\\n\\n",,
941,16,436,c839ffa2-6f6b-450a-8c47-62b42b870fda,"2010-07-21 16:04:18",-1,,,
942,2,437,26f52b9e-25b6-4bf8-9c66-10d07666844d,"2010-07-21 16:07:20",215,"\\nFor you I would suggest:\\n\\nIntroduction to the Mathematical and Statistical Foundations of Econometrics by Herman J. Bierens, CUP. The word "Introduction" in the title is a sick joke for most PhD econometrics students.\\n\\nMarkov Chain Monte Carlo by Dani Gamerman, Chapman & Hall is also concise.\\n\\n",,
943,16,437,26f52b9e-25b6-4bf8-9c66-10d07666844d,"2010-07-21 16:07:20",-1,,,
944,2,438,5db3168a-7a52-47ee-a568-f93b1a7c7054,"2010-07-21 16:42:43",215,"\\nOK here's my best attempt at an informal and crude explanation.\\n\\nA Markov Chain is a random process that has the property that the future depends only on the current state of the process and not the past i.e. it is memoryless. An example of a random process could be the stock exchange. An example of a Markov Chain would be a board game like Monopoly or Snakes and Ladders where your future position (after rolling the die) would depend only on where you started from before the roll, not any of your previous positions. A textbook example of a Markov Chain is the "drunkard's walk". Imagine somebody who is drunk and can move only left or right by one pace. The drunk moves left or right with equal probability. This is a Markov Chain where the drunk's future/next position depends only upon where he is at present.\\n\\nMonte Carlo methods are computational algorithms (simply sets of instructions) which randomly sample from some process under study. They are a way of estimating something which is too difficult or time consuming to find deterministically. They're basically a form of computer simulation of some mathematical or physical process. The Monte Carlo moniker comes from the analogy between a casino and random number generation. Returning to our board game example earlier, perhaps we want to know if some properties on the Monopoly board are visited more often than others. A Monte Carlo experiment would involve rolling the dice repeatedly and counting the number of times you land on each property. It can also be used for calculating numerical integrals. (Very informally, we can think of an integral as the area under the graph of some function.)  Monte Carlo integration works great on a high-dimensional functions by taking a random sample of points of the function and calculating some type of average at these various points. By increasing the sample size, the law of large numbers tells us we can increase the accuracy of our approximation by covering more and more of the function.\\n\\nThese two concepts can be put together to solve some difficult problems in areas such as Bayesian inference, computational biology, etc where multi-dimensional integrals need to be calculated to solve common problems. The idea is to construct a Markov Chain which converges to the desired probability distribution after a number of steps. The state of the chain after a large number of steps is then used as a sample from the desired distribution and the process is repeated. There many different MCMC algorithms which use different techniques for generating the Markov Chain. Common ones include the Metropolis-Hastings and the Gibbs Sampler.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n",,
945,2,439,099f72e6-7e89-4cbc-bf19-69484a45d0b1,"2010-07-21 16:50:35",30,"As you said, it's not necessarily the case that a mathematician may want a rigorous book. Maybe the goal is to get some intuition of the concepts quickly, and then fill in the details. I recommend two books from CMU professors, both published by Springer: "All of Statistics" by Larry Wasserman is quick and informal. "Theory of Statistics" by Mark Schervish is rigorous and relatively complete. It has decision theory, finite sample, some asymptotics and sequential analysis.",,
946,16,439,099f72e6-7e89-4cbc-bf19-69484a45d0b1,"2010-07-21 16:50:35",-1,,,
947,2,440,d6c19c35-bacd-4d4a-8881-41005bd8cf13,"2010-07-21 16:53:35",36,"[Peter Dalgaard's Introductory Statistics with R][1] is a great book for some introductory statistics with a focus on the R software for data analysis.\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0387790535/ref=nosim/gettgenedone-20",,
948,16,440,d6c19c35-bacd-4d4a-8881-41005bd8cf13,"2010-07-21 16:53:35",-1,,,
949,2,441,e2207497-bbe5-4cd1-99fc-0e13d244bdcf,"2010-07-21 16:54:38",36,"UCLA has the best free resources you'll find anywhere.\\n\\nhttp://www.ats.ucla.edu/stat/stata/",,
950,2,442,20724ac3-60e5-4a31-85be-3b7e1692a3fc,"2010-07-21 16:56:45",36,"I say the [visual display of quantitative information][1] by Tufte, and [Freakonomics][2] for something fun.\\n\\n\\n  [1]: http://amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20\\n  [2]: http://amazon.com/o/ASIN/0060731338/ref=nosim/gettgenedone-20",,
951,16,442,20724ac3-60e5-4a31-85be-3b7e1692a3fc,"2010-07-21 16:56:45",-1,,,
952,2,443,7d8cdf20-871d-4492-b1c6-3ea2d876b604,"2010-07-21 17:06:32",215,"\\nThere's two aspects to this *post hoc ergo propter hoc* problem that I like to cover: (i) reverse causality and (ii) endogeneity \\n\\nAn example of "possible" reverse causality:\\nSocial drinking and earnings - drinkers earn more money according to Bethany L. Peters & Edward Stringham, 2006. "No Booze? You May Lose: Why Drinkers Earn More Money Than Nondrinkers," Journal of Labor Research, Transaction Publishers, vol. 27(3), pages 411-421, June. Or do people who earn more money drink more either because they have a greater disposable income or due to stress? This is a great paper to discuss for all sorts of reasons including measurement error, response bias, causality, etc.\\n\\nAn example of "possible" endogeneity:\\nThe Mincer Equation explains log earnings by education, experience and experience squared. There is a long literature on this topic. Labour economists want to estimate the causal relationship of education on earnings but perhaps education is endogenous because "ability" could increase the amount of education an individual has (by lowering the cost of obtaining it) and could lead to an increase in earnings, irrespective of the level of education. A potential solution to this could be an instrumental variable. Angrist and Pischke's book, Mostly Harmless Econometrics covers this and relates topics in great detail and clarity.\\n\\n\\nOther silly examples that I have no support for include:\\n- Number of televisions per capita and the numbers of mortality rate. So let's send TVs to developing countries. Obviously both are endogenous to something like GDP.\\n- Number of shark attacks and ice cream sales. Both are endogenous to the temperature perhaps?\\n\\nI also like to tell the terrible joke about the lunatic and the spider. A lunatic is wandering the corridors of an asylum with a spider he's carrying in the palm of his hand. He sees the doctor and says, "Look Doc, I can talk to spiders. Watch this. Spider, go left!" The spider duly moves to the left. He continues, "Spider, go right." The spider shuffles to the right of his palm. The doctor replies, "Interesting, maybe we should talk about this in the next group session." The lunatic retorts, "That's nothing Doc. Watch this." He pulls of each of the spider's legs one by one and then shouts, "Spider, go left!" The spider lies motionless on his palm and the lunatic turns to the doctor and concludes, "If you pull of a spider's legs he'll go deaf." \\n\\n\\n\\n",,
956,2,445,1d756af2-9087-48fb-b0cb-f7ac3f6d9742,"2010-07-21 17:13:33",215,"\\nThe UCLA resource listed by Stephen Turner (above) are excellent if you just want to apply methods you're already familiar with using Stata.\\n\\nIf you're looking for textbooks which teach you statistics/econometrics while using Stata then these are solid recommendations (but it depends at what level you're looking at):\\n\\n**Introductory Methods**\\nAn Introduction to Modern Econometrics Using Stata by Chris Baum\\nIntroduction to Econometrics by Chris Dougherty\\n\\n**Advanced/Specialised Methods**\\nMultilevel and Longitudinal Modeling Using Stata by Rabe-Hesketh and Skrondal\\nRegression Models for Categorical Dependent Variables Using Stata by Long and Freese\\n",,
957,2,446,0a66e040-8908-41af-9142-ab9e249c1d85,"2010-07-21 17:17:45",215,"\\nThe Gelman books are all excellent but not necessarily introductory in that they assume that you know some statistics already. Therefore they are an introduction to the Bayesian way of doing statistics rather than to statistics in general. I would still give them the thumbs up, however.\\n\\nAs an introductory statistics/econometrics book which takes a Bayesian perspective, I would recommend Gary Koop's Bayesian Econometrics.",,
958,16,446,0a66e040-8908-41af-9142-ab9e249c1d85,"2010-07-21 17:17:45",-1,,,
959,2,447,43841708-f403-4cc5-9b94-b336e54e95fc,"2010-07-21 17:26:14",215,"\\nI really like these two books by Daniel McFadden of Berkeley:\\n\\n\\nhttp://elsa.berkeley.edu/users/mcfadden/e240a_sp98/e240a.html\\n\\nhttp://elsa.berkeley.edu/users/mcfadden/e240b_f01/e240b.html\\n\\n",,
960,2,448,643d2a93-597f-44a3-b521-355cd2b88b2e,"2010-07-21 17:26:15",77,"Here are my guidelines, based on the most common errors I see (in addition to all the other good points mentioned)\\n\\n- Use scatter graphs, not line plots, if element order is not relevant.\\n- When preparing plots that are meant to be compared, use the same scale factor for all of them. \\n- Even better - find a way to combine the data in a single graph (eg: boxplots are a better than several histograms to compare a large number of distributions).\\n- Do not forget to specify units\\n- Use a legend only if you must - it's generally clearer to label curves directly. \\n- If you must use a legend, move it inside the plot, in a blank area.\\n- For line graphs, aim for an aspect ratio which yields [lines that are roughly at 45o with the page][1].\\n\\n\\n  [1]: http://processtrends.com/pg_data_vis_bank_to_45.htm",,
961,2,449,cbe9113b-01b7-429c-9642-0faec97c8239,"2010-07-21 17:28:57",215,"\\nMore from an economics perspectives I think these two sets of lecture notes are very good:\\n\\nhttp://home.datacomm.ch/paulsoderlind/Courses/OldCourses/FinEcmtAll.pdf\\n\\nhttp://personal.lse.ac.uk/mele/files/fin_eco.pdf\\n\\nThe first provides econometric methods for analysing financial data whereas the second provides the financial economics theory behind the models being applied. They're both MSc level texts.",,
962,2,450,b1a1f180-ea20-4efa-9e39-ccd6f0497128,"2010-07-21 17:30:02",88,"For us, it is just one example of a robust regression -- I believe it is used by statisticians also, but maybe not so wide because it has some better known alternatives.",,
964,6,418,81ab183c-0cd2-489b-a32b-2896457dc560,"2010-07-21 17:30:38",88,<outliers><bootstrap><robust>,"edited tags",
965,2,451,0a1f4010-1e5c-4a4d-bbe8-c351d0030e36,"2010-07-21 17:38:08",215,"\\nWhen describing a variable we typically summarise it using two measures: a measure of centre and a measure of spread. Common measures of spread include the mean, median and mode. Common measure of spread include the variance and interquartile range.\\n\\nThe variance (represented by the Greek lowercase sigma) is commonly used when the mean is reported. The variance is the average squared deviation of variable. The deviation is calculated by subtracting the mean from each observation. This is squared because the sum would otherwise be zero and squaring removes this problem while maintaining the relative size of the deviations. The problem with using the variation as a measure of spread is that it is in squared units. For example if our variable of interest was height measured in inches then the variance would be reported in squared-inches which makes little sense. The standard deviation (represented by the Greek lowercase sigma raised to the power 2) is the square-root of the variance and returns the measure of spread to the original units. This is much more intuitive and is therefore more popular than the variance.\\n\\nWhen using the standard deviation, one has to be careful of outliers as they will skew the standard deviation (and the mean) as they are not resistant measures of spread. A simple example will illustrate this property. The mean of my terrible cricket batting scores of 13, 14, 16, 23, 26, 28, 33, 39, and 61 is 28.11. If we consider 61 to be an outlier and deleted it, the mean would be 24. \\n\\n\\n\\n\\n",,
966,2,452,362324e1-68ee-4fc4-b39f-67385c3f0a4f,"2010-07-21 17:45:01",215,"It has been suggested by Angrist and Pischke that Robust (i.e. robust to heteroskedasticity or unequal variances) Standard Errors are reported as a matter of course rather than testing for it. Two questions:\\n\\n(1) What is impact on the standard errors of doing so when there is homoskedasticity?\\n(2) Does anybody actually do this in their work?",,
967,1,452,362324e1-68ee-4fc4-b39f-67385c3f0a4f,"2010-07-21 17:45:01",215,"Always Report Robust (White) Standard Errors?",,
968,3,452,362324e1-68ee-4fc4-b39f-67385c3f0a4f,"2010-07-21 17:45:01",215,<regression><error><standard>,,
969,2,453,426606e1-99d5-45df-b069-e5f62de226b2,"2010-07-21 17:54:05",74,"[Statistics in Plain English](http://www.amazon.com/gp/product/041587291X?ie=UTF8&tag=httpvancouveb-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=041587291X") is pretty good.\\n\\n4.5 on Amazon, 11 reviews.\\n\\nExplains ANOVA pretty well too. ",,
970,16,453,426606e1-99d5-45df-b069-e5f62de226b2,"2010-07-21 17:54:05",-1,,,
971,2,454,18562440-a8ab-4735-afc9-be6e5616cadf,"2010-07-21 18:11:59",62,"It depends on the way in which the plots will be discussed. \\n\\nFor instance, if I'm sending out plots for a group meeting that will be done with callers from different locations, I prefer putting them together in Powerpoint as opposed to Excel, so it's easier to flip around. \\n\\nFor one-on-one technical calls, I'll put something in excel so that the client be able to move a plot aside, and view the raw data. Or, I can enter p-values into cells along side regression coefficients, e.g. \\n\\nKeep in mind: plots are cheap, especially for a slide show, or for emailing to a group. I'd rather make 10 clear plots that we can flip through than 5 plots where I try to put distinct cohorts (e.g. "males and females") on the same graph. \\n",,
972,2,455,a3cd71d3-e15e-4855-a429-ce270f242e7d,"2010-07-21 18:18:13",13,"Allright, I think this one is hilarious- but let's see if it passes the Statistical Analysis Miller test.\\n\\n## Fermirotica\\n\\n[![alt text][1]](http://xkcd.com/563/)\\n\\n> I love how Google handles dimensional analysis.  Stats are ballpark and vary wildly by time of day and whether your mom is in town.\\n\\n  [1]: http://imgs.xkcd.com/comics/fermirotica.png",,
973,16,455,a3cd71d3-e15e-4855-a429-ce270f242e7d,"2010-07-21 18:18:13",-1,,,
974,5,425,aa823d93-3479-431f-8f34-d42d546424df,"2010-07-21 18:19:37",13,"One of my favorites from [xckd](http://www.xkcd.com):\\n\\n##Random Number\\n[![alt text][1]](http://xkcd.com/221/)\\n\\n> RFC 1149.5 specifies 4 as the standard IEEE-vetted random number.\\n\\n  [1]: http://imgs.xkcd.com/comics/random_number.png","added 41 characters in body",
975,2,456,ea1d6b54-60db-45f7-af6c-b5f84dbfe3bc,"2010-07-21 18:26:02",39,"As the [Encyclopedia of GIS][1] states, the conditional autoregressive model (CAR) is appropriate for situations with first order dependency or relatively local spatial autocorrelation, and simultaneous autoregressive model (SAR ) is  more suitable where there are second order dependency or a more global spatial autocorrelation.\\n\\nThis is made clear by the fact that CAR obeys the spatial version of the [Markov property][2], namely that the state of neighbors is due only to their neighbors (i.e. spatially “memoryless”, instead of temporally), whereas SAR does not.  This is due to the different ways in which they specify their variance-covariance matrixes.  So, when the spatial Markov property obtains, CAR provides a simpler way to model autocorrelated geo-referenced areal data.\\n\\nSee [Gis And Spatial Data Analysis: Converging Perspectives][3] for more details.\\n\\n\\n  [1]: http://books.google.com/books?id=6q2lOfLnwkAC&dq=Encyclopedia+of+GIS\\n  [2]:  http://en.wikipedia.org/wiki/Markov_property\\n  [3]: http://www.geog.ucsb.edu/~good/papers/387.pdf",,
976,2,457,5acf72e6-36ca-4025-b375-581fc13e1e96,"2010-07-21 18:28:27",NULL,"I do not know the literature in the area well enough to offer a direct response. However, it seems to me that if the three tests differ then that is an indication that you need further research/data collection in order to definitively answer your question. \\n\\nYou may also want to look at [this][1] Google Scholar search\\n\\n\\n  [1]: http://scholar.google.com/scholar?q=small%20sample%20properties%20of%20wald%20likelihood%20ratio&um=1&ie=UTF-8&sa=N&hl=en&tab=ws",,user28
977,5,236,e07f6f8a-4c58-4086-a44b-6960022918f7,"2010-07-21 18:33:33",138,"Unfortunately, it only runs on macs, but otherwise a great application (basically *Processing* in python):\\n\\n* [http://nodebox.net/code/index.php/Home][1]\\n\\n> NodeBox is a Mac OS X application that lets you create 2D visuals (static, animated or interactive) using Python programming code and export them as a PDF or a QuickTime movie. NodeBox is free and well-documented.\\n\\n\\n  [1]: http://nodebox.net/code/index.php/Home","added 35 characters in body",
978,2,458,67ed99b0-ab92-49da-b60f-0292361da368,"2010-07-21 18:41:12",226,"One way to test patterns in stock market data is discussed [here][1]. A similar approach would be to randomise the stock market data and identify your patterns of interest, which would obviously be devoid of any meaning due to the deliberate randomising process. These randomly generated patterns and their returns would form your null hypothesis. By statistically comparing the pattern returns in the actual data with the returns from the null hypothesis randomised data patterns you may be able to distinguish patterns which actually have some meaning or predictive value. \\n\\n\\n  [1]: http://www.evidencebasedta.com/",,
979,2,459,1bd18628-2703-48b8-8784-feb2ae3893d6,"2010-07-21 19:59:48",259,"We're plotting time-series metrics in the context of network/server operations. The data has a 5-minute sample rate, and consists of things like CPU utilization, error rate, etc.\\n\\nWe're adding a horizontal "threshold" line to the graphs, to visually indicate a value threshold above which people should worry/take notice. For example, in the CPU utilization example, perhaps the "worry" threshold is 75%.\\n\\nMy team has some internal debate over what color this line should be:\\n\\n1. Something like a bright red that clearly stands out from the background grid and data lines, and indicates this is a warning condition\\n2. Something more subtle and definitely NOT red, since the "ink" for the line doesn't represent any actual data, and thus attention shouldn't be drawn to it unnecessarily.\\n\\nWould appreciate guidance / best practices...\\n\\nThanks,\\nMike",,
980,1,459,1bd18628-2703-48b8-8784-feb2ae3893d6,"2010-07-21 19:59:48",259,"Good line color for "threshold" line in a time-series graph?",,
981,3,459,1bd18628-2703-48b8-8784-feb2ae3893d6,"2010-07-21 19:59:48",259,<time-series><data-visualization>,,
982,2,460,fa0074cc-0f0a-41c5-8380-42984877e90e,"2010-07-21 20:03:40",61,"For moderate dimensions, like 3, then some sort of kernel cross-validation technique as suggested elsewhere seems reasonable and is the best I can come up with.\\n\\nFor higher dimensions, I'm not sure that the problem is solvable; it lands pretty squarely into 'curse-of-dimensionality' territory.  The issue is that distance functions tend to converge to very large values very quickly as you increase dimensionality, including distances derived from distributions.  If you're defining an outlier as "a point with a comparatively large distance function relative to the others", and all your distance functions are beginning to converge because you're in a high-dimensional space, well, you're in trouble.\\n\\nWithout some sort of distributional assumption that will let you turn it into a probabilistic classification problem, or at least some rotation that lets you separate your space into "noise dimensions" and "informative dimensions", I think that the geometry of high-dimensional spaces is going to prohibit any easy -- or at least robust -- identification of outliers.",,
983,2,461,25ea324c-4818-4cf2-a8b9-e9c96e2aabd4,"2010-07-21 20:07:32",71,"To me, whether or not the line represents actual data seems irrelevant.  What's the point of the plot?  If it's so that somebody will do something when utilization crosses a threshold, the line marking the threshold had better be very visible.  If the point of the plot is to give an overview of utilization over time, then why include the line at all?  Just put the major gridlines of your plot at intervals that will coincide with your threshold (25% in your example), and let the reader figure it out.\\n\\n... y'all been reading too much Tufte.",,
984,2,462,4866f182-901f-466d-bbea-d5553ba9e74f,"2010-07-21 20:11:02",88,"In the physics field there is a rule that the whole paper/report should be understandable only from quick look at the plots. So I would mainly advice that they should be self-explanatory.  \\nThis also implies that you must always check whether your audience is familiar with some kind of plot -- I had once made a big mistake assuming that every scientist knows what boxplots are, and then waisted an hour to explain it.",,
985,2,463,b183492f-95a6-4b81-a930-a10e06f9c3a2,"2010-07-21 20:29:12",90,"In addition to "The History of Statistics" suggested by Graham, another Stigler book worth reading is\\n\\n[Statistics on the Table: The History of Statistical Concepts and Methods][1]\\n\\n\\n  [1]: http://www.amazon.com/Statistics-Table-History-Statistical-Concepts/dp/0674009797/ref=sr_1_1?ie=UTF8&s=books&qid=1279743969&sr=8-1",,
986,16,463,b183492f-95a6-4b81-a930-a10e06f9c3a2,"2010-07-21 20:29:12",-1,,,
987,2,464,64952dbd-eef4-4537-a65a-3a3e4030ec15,"2010-07-21 20:34:43",88,"If this is about your "Qnotifier" I think that you should plot the threshold line in some darker gray so it is distinguishable but not disturbing. Then I would color the part of the  plot that reaches over the threshold in some alarmistic hue, like red. ",,
988,2,465,a56b0125-0379-468f-8014-407e90572478,"2010-07-21 20:45:08",90,"I thought that the White Standard Error and the Standard Error computed in the "normal" way (eg, Hessian and/or OPG in the case of maximum likelihood) were asymptotically equivalent in the case of homoskedasticity? \\n\\nOnly if there is heteroskedasticity will the "normal" standard error be inappropriate, which means that the White Standard Error is appropriate with or without heteroskedasticity, that is, even when your model is homoskedastic.\\n\\nI can't really talk about 2, but I don't see the why one wouldn't want to calculate the White SE and include in the results.",,
989,2,466,da844589-3519-4b9c-ac95-6c6919b44c43,"2010-07-21 20:53:54",115,"Not Statistics specific, but a good resource is:  http://www.reddit.com/r/mathbooks\\nAlso, George Cain at Georgia Tech maintains a list of freely available maths texts that includes some statistical texts.  http://people.math.gatech.edu/~cain/textbooks/onlinebooks.html\\n\\n",,
990,16,466,da844589-3519-4b9c-ac95-6c6919b44c43,"2010-07-21 20:53:54",115,,,
991,2,467,37ba9731-3538-4def-a649-6758f2228689,"2010-07-21 21:04:03",115,"http://www.reddit.com/r/datasets  and also, http://www.reddit.com/r/opendata both contain a constantly growing list of pointers to various datasets.\\n\\n",,
992,16,467,37ba9731-3538-4def-a649-6758f2228689,"2010-07-21 21:04:03",-1,,,
993,4,128,e46e23b6-13b3-4c2a-9ff0-d0450df6aaf5,"2010-07-21 21:29:10",132,"How does one interpret a Bland-Altman plot?","edited title",
994,16,170,ea59aefb-4e66-488b-b9ee-471a3512ad13,"2010-07-21 22:02:27",8,,,
995,6,170,ea59aefb-4e66-488b-b9ee-471a3512ad13,"2010-07-21 22:02:27",8,<textbook><teaching>,"Changing to community wiki",
996,2,468,6c23eca3-891a-405d-9119-c5e4201a0f5e,"2010-07-21 22:05:04",251,"If you're coming from the programming side, one option is to use the [Natural Language Toolkit][1] (NLTK) for Python.  There's an O'Reilly book, [available freely][2], which might be a less dense and more practical introduction to building classifiers for documents among other things.  \\n\\nIf you're interested in beefing up on the statistical side, Roger Levy's book in progress, [Probabilistic Models in Study of Language][3], might not be bad to peruse.  It's written for cogsci/compsci grad students starting out with statistical NLP techniques.\\n\\n\\n  [1]: http://www.nltrk.org\\n  [2]: http://www.nltk.org/book\\n  [3]: http://idiom.ucsd.edu/~rlevy/textbook/text.html",,
997,2,469,ff62c928-21c7-471d-b782-e9fefdd74aed,"2010-07-21 22:13:19",251,"The [Handbook of Computation Statistics][1] (Gentle, Härdle, Mori) is available online and quite good.  You'll also find a number of other books by Härdle on statistics in finance, nonparametrics, among other topics at the [same site][2].  \\n\\n\\n  [1]: http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/csa/\\n  [2]: http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/",,
998,16,469,ff62c928-21c7-471d-b782-e9fefdd74aed,"2010-07-21 22:13:19",-1,,,
999,6,124,b26c5640-62e6-4c5f-bbda-aec4ca5f9612,"2010-07-21 22:17:00",88,<classification><information-retrieval><text-mining>,"edited tags",
1000,2,470,a6aea7ba-a79a-4089-a043-2f2f3792a49b,"2010-07-21 22:35:38",36,"The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is a standard text for statistics and data mining, and is now free:\\n\\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn/\\n\\nWhy pay [$70 for it on amazon][1]?\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0387848576/ref=nosim/gettgenedone-20",,
1001,16,470,a6aea7ba-a79a-4089-a043-2f2f3792a49b,"2010-07-21 22:35:38",-1,,,
1002,2,471,bbf2be15-bc9d-4b23-8a27-f8f51ad48a84,"2010-07-21 22:57:58",168,"Darrell Huff -- How to Lie with Statistics",,
1003,16,471,bbf2be15-bc9d-4b23-8a27-f8f51ad48a84,"2010-07-21 22:57:58",-1,,,
1004,2,472,67c17969-9a61-4e2b-ad22-7f3f09e1a56a,"2010-07-21 23:00:34",61,"For getting into stochastic processes and SDEs, Tom Kurtz's [lecture notes][1] are hard to beat.  It starts with a decent review of probability and some convergence results, and then dives right into continuous time stochastic processes in fairly clear, comprehensible language.  In general it's one of the best books on the topic -- free or otherwise -- I've found.\\n\\n\\n  [1]: http://www.math.wisc.edu/~kurtz/m735.htm",,
1005,16,472,67c17969-9a61-4e2b-ad22-7f3f09e1a56a,"2010-07-21 23:00:34",-1,,,
1006,5,430,985d7cee-97ee-4980-ac45-1e40e6dd2637,"2010-07-22 00:37:06",251,"I find that people find the solution more intuitive if you change it to 100 doors, close first, second, to 98 doors.  Similarly for 50 doors, etc.  ","deleted 2 characters in body",
1007,6,396,162b01f4-fe50-46dc-8b4f-7d095c7f8533,"2010-07-22 00:39:48",159,<data-visualization><plotting><best-practices>,"edited tags",
1008,5,457,e4b364e1-369d-4828-99da-fc041fba76b1,"2010-07-22 01:21:24",NULL,"I do not know the literature in the area well enough to offer a direct response. However, it seems to me that if the three tests differ then that is an indication that you need further research/data collection in order to definitively answer your question. \\n\\nYou may also want to look at [this][1] Google Scholar search\\n\\n**Update in response to your comment:**\\n\\nIf collecting additional data is not possible then there is one workaround. Do a simulation which mirrors your data structure, sample size and your proposed model. You can set the parameters to some pre-specified values. Estimate the model using the data generated and then check which one of the three tests points you to the right model. Such a simulation would offer some guidance as to which test to use for your real data. Does that make sense?\\n\\n  [1]: http://scholar.google.com/scholar?q=small%20sample%20properties%20of%20wald%20likelihood%20ratio&um=1&ie=UTF-8&sa=N&hl=en&tab=ws","added 494 characters in body",user28
1009,2,473,c7d165c3-d32d-4735-bdfb-0edbda47109e,"2010-07-22 02:37:18",260,"The traditional solution to this problem is to use the [vector representation][1] for the news stories and then cluster the vectors.  The vectors are arrays where each entry represents a word or word class.  The value associated to each word will be the [tf-idf][2] weight.  This value goes up the more frequent the word in the document and down the more frequent the word is in the whole collection of documents.\\n\\nYou may think of the titles as the documents, but sticking to just the title for news stories may be a bit risky for clustering similar stories.  The problem is that by using word counts you are discarding all information on the order of the words. Longer texts compensate for that loss information by distinguishing documents by the vocabulary used (articles mentioning _finance_, _money_, ... are closer to each other than those mentioning _ergodic_, _Poincare_).\\n\\nIf you want to stick to titles, one idea is to think of word pairs as the words you use in the vector representation.  So for the title _The eagle has landed_, you would think of _the eagle_, _eagle has_, _has landed_. as the &ldquo;words.&rdquo;\\n\\nTo discover when a cluster has become much bigger or different from the others you will need to develop a decision procedure.\\n\\n[1]: http://dx.doi.org/10.1137/S0036144598347035\\n[2]: http://en.wikipedia.org/wiki/TF_IDF",,
1010,2,474,305a7e72-0754-4e2e-adb3-1c7bf6635a49,"2010-07-22 03:21:18",260,"For testing the numbers produced by random number generators the [Diehard tests][1] are a practical approach. But those tests seem kind of arbitrary and one is may be left wondering if more should be included or if there is any way to really check the randomness.  \\n\\nThe best candidate for a definition of a random sequence seems to be the [Martin-Löf randomness][2].   The main idea for this kind of randomness, is beautifully developed in [Knuth, section 3.5][3], is to test for uniformity for all types of sub-sequences of the sequence of random numbers.  Getting that _all type of subsequences_ definition right turned out to be be really hard even when one uses notions of computability.\\n\\nThe Diehard tests are just some of the possible subsequences one may consider and their failure would exclude Martin-Löf randomness.\\n\\n\\n[1]: http://en.wikipedia.org/wiki/Diehard_tests\\n[2]: http://en.wikipedia.org/wiki/Algorithmically_random_sequence\\n[3]: http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming",,
1011,2,475,643941cc-22e3-4d62-9f3f-3a7a1012442a,"2010-07-22 03:55:48",253,"There is also [Gephi][1] for plotting social networks.\\n\\n(p.s: Here is how to [connect it with R][2])\\n\\n\\n  [1]: http://gephi.org/\\n  [2]: http://www.r-bloggers.com/data-preparation-for-social-network-analysis-using-r-and-gephi/",,
1012,2,476,63bb4d01-3a1d-4955-91e7-457116b88b12,"2010-07-22 04:34:32",251,"I won't give a definitive answer in terms of ranking the three.  Build 95% CIs around your parameters based on each, and if they're radically different, then your first step should be to dig deeper.  Transform your data (though the LR will be invariant), regularize your likelihood, etc.  In a pinch though, I would probably opt for the LR test and associated CI.  A rough argument follows.\\n\\nThe LR is invariant under the choice of parametrization (e.g. T versus logit(T)).  The Wald statistic assumes normality of (T - T0)/SE(T).  If this fails, your CI is bad.  The nice thing about the LR is that you don't need to find a transform f(T) to satisfy normality.  The 95% CI based on T will be the same.  Also, if your likelihood isn't quadratic, the Wald 95% CI, which is symmetric, can be kooky since it may prefer values with lower likelihood to those with higher likelihood.\\n\\nAnother way to think about the LR is that it's using more information, loosely speaking, from the likelihood function.  The Wald is based on the MLE and the curvature of the likelihood at null.  The Score is based on the slope at null and curvature at null.  The LR evaluates the likelihood under the null, and the likelihood under the union of the null and alternative, and combines the two.  If you're forced to pick one, this may be intuitively satisfying for picking the LR.\\n\\nKeep in mind that there are other reasons, such as convenience or computational, to opt for the Wald or Score.  The Wald is the simplest and, given a multivariate parameter, if you're testing for setting many individual ones to 0, there are convenient ways to approximate the likelihood.  Or if you want to add a variable at a time from some set, you may not want to maximize the likelihood for each new model, and the implementation of Score tests offers some convenience here.  The Wald and Score become attractive as your models and likelihood become unattractive.  (But I don't think this is what you were questioning, since you have all three available ...)",,
1013,2,477,65ab28a8-9e4e-4fd1-9e88-db5729dd00d9,"2010-07-22 04:58:50",251,"I assume your friend prefers something that's biostatistics oriented.  Glantz's [Primer of Biostatistics][1] is a small book, an easy and quick read, and tends to get rave reviews from a similar audience.  If an online reference works, I like Gerard Dallal's [Handbook of Statitical Practice][2], which may do the trick if he's just refreshing previous knowledge.\\n\\n\\n  [1]: http://www.powells.com/biblio/62-9780071435093-1\\n  [2]: http://www.tufts.edu/~gdallal/LHSP.HTM\\n",,
1014,2,478,bdfd7861-2565-409b-ac5c-b78b60a5d017,"2010-07-22 05:13:17",251,"I'm going to assume some basic statistics knowledge and recommend:\\n\\n- [The Statistical Sleuth][1] (Ramsey, Schafer) which contain a good deal of mini case studies as they cover the basic statistical tools for data analysis.  \\n\\n- [A First Course in Multivariate Statistics][2] (Flury) which covers the essential statistics required for data mining and the like.  \\n\\n",,
1015,16,478,bdfd7861-2565-409b-ac5c-b78b60a5d017,"2010-07-22 05:13:17",-1,,,
1016,2,479,c0485573-7387-4e44-8de2-a9de97279f9d,"2010-07-22 07:28:42",128,"If it does not break your styleguide I would rather color the background of the plots red/(yellow/)green than just plotting a line. In my imagination this should make it pretty clear to a user that values are fine on green and to be checked on red. Just my 5&#xa2;.",,
1017,2,480,2adf0e2b-4013-4ff3-b104-e96328c95bcf,"2010-07-22 08:58:36",223,"I am not an expert of random forest but I clearly understand that the key issue with random forest is the (random) tree generation. Can you explain me how the trees are generated? (i.e. What is the used distribution for tree generation?)\\n\\nThanks in advance ! ",,
1018,1,480,2adf0e2b-4013-4ff3-b104-e96328c95bcf,"2010-07-22 08:58:36",223,"How does random forest generate the random forest ",,
1019,3,480,2adf0e2b-4013-4ff3-b104-e96328c95bcf,"2010-07-22 08:58:36",223,<r><machine-learning><algorithms><cart>,,
1020,2,481,3f588222-1d5b-4592-90fc-9dcba2cd7d8f,"2010-07-22 09:17:27",199,"Another question about time series from me.\\n\\nI have a dataset which gives daily records of violent incidents in a psychiatric hospital over three years. With the help from my previous question I have been fiddling with it and am a bit happier about it now.\\n\\nThe thing I have now is that the daily series is very noisy. It fluctuates wildly, up and down, from 0 at times up to 20. Using loess plots and the forecast package (which I can highly recommend for novices like me) I just get a totally flat line, with massive confidence intervals from the forecast.\\n\\nHowever, aggregating weekly or monthly the data make a lot more sense. They sweep down from the start of the series, and then increase again in the middle. Loess plotting and the forecast package both produce something that looks a lot more meaningful.\\n\\nIt does feel a bit like cheating though. Am I just preferring the aggregated versions because they look nice with no real validity to it?\\n\\nOr would it be better to compute a moving average and use that as the basis? I'm afraid I don't understand the theory behind all this well enough to be confident about what is acceptable",,
1021,1,481,3f588222-1d5b-4592-90fc-9dcba2cd7d8f,"2010-07-22 09:17:27",199,"Is it valid to aggregate a time series to make it look more meaningful?",,
1022,3,481,3f588222-1d5b-4592-90fc-9dcba2cd7d8f,"2010-07-22 09:17:27",199,<time-series><forecasting>,,
1023,2,482,ed04540b-8993-49d1-990f-3e0a05bd7aa3,"2010-07-22 09:24:52",190,"This totally depends on your time series and what effect you want to discover/proof etc.\\n\\nAn important thing here is, what kind of periods do you have in your data. Make a spectrum of you data and see what frequencies are common in you data.\\n\\nAnyway, you are not lying when you decide to display aggregated values. When you are looking to effects that are occurring over weeks (like, more violence in summer when it's hot weather) it is the right thing to do.\\n\\nMaybe you can also take a look at the Hilbert Huang Transform. This will give you Intrinsic Mode Functions that are very handy for visual analyses.",,
1024,2,483,f6e560f0-4f29-4f11-8883-de3a44d09c4f,"2010-07-22 09:53:52",88,"The main idea is the bagging procedure, not making trees random. In detail, each tree is build on a sample of objects drawn with replacement from the original set; thus each tree has some objects that it haven't seen, what makes it more generalizing.  \\nFurthermore, trees are being weekend in such a way that on the each split only M randomly selected attributes are considered; M is usually a square root of the number of attributes in the set. This makes the trees less overfitted, because they are not pruned.\\n\\nYou can find more details [here][1]. \\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm",,
1026,5,483,e7232a29-6bea-4ea0-ac51-858f870898a3,"2010-07-22 10:04:38",88,"The main idea is the bagging procedure, not making trees random. In detail, each tree is build on a sample of objects drawn with replacement from the original set; thus each tree has some objects that it haven't seen, what makes the whole ensemble more heterogeneous and thus better in generalizing.  \\nFurthermore, trees are being weekend in such a way that on the each split only M (or `mtry`) randomly selected attributes are considered; M is usually a square root of the number of attributes in the set. This makes the trees less overfitted, since they are not pruned.  \\nYou can find more details [here][1]. \\n\\nOn the other hand, there is a variant of RF called Extreme Random Forest, in which trees are made in more less random way (there is no optimization of splits) -- consult, I think, http://www.springerlink.com/index/10.1007/s10994-006-6226-1.\\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm","added 303 characters in body",
1027,2,485,a8357919-a99d-4836-a961-d6be46399262,"2010-07-22 10:08:10",183,"A question previously sought recommendations for [textbooks on mathematical statistics][1]\\n\\nDoes anyone know of any good online **video lectures** on mathematical statistics?\\nThe closest that I've found are:\\n\\n - [Machine Learning][2] \\n - [Econometrics][3] \\n\\n\\n  [1]: http://stats.stackexchange.com/questions/414/intro-to-statistics-for-mathematicians\\n  [2]: http://www.youtube.com/watch?v=UzxYlbK2c7E&feature=PlayList&p=A89DCFA6ADACE599&index=0\\n  [3]: http://economistsview.typepad.com/economics421/",,
1028,1,485,a8357919-a99d-4836-a961-d6be46399262,"2010-07-22 10:08:10",183,"Mathematical Statistics Videos",,
1029,3,485,a8357919-a99d-4836-a961-d6be46399262,"2010-07-22 10:08:10",183,<mathematics><books><mathematical-statistics>,,
1030,2,486,69a2e70a-37de-44de-872c-08adba50a5df,"2010-07-22 10:11:15",266,"I have calculated AIC and AICc to compare two general linear mixed models; The AICs are positive with model 1 having a lower AIC than model 2.  However, the values for AICc are both negative (model 1 is still < model 2).  Is it valid to use and compare negative AICc values? ",,
1031,1,486,69a2e70a-37de-44de-872c-08adba50a5df,"2010-07-22 10:11:15",266,"Negative values for AICc (corrected Akaike Information Criterion)",,
1032,3,486,69a2e70a-37de-44de-872c-08adba50a5df,"2010-07-22 10:11:15",266,<statistical-analysis><glm><model-choice><aic>,,
1033,2,487,79d39431-a9a7-4b45-b739-88fad43ca195,"2010-07-22 10:15:28",266,"Not a book, but I recently discovered an article by Jacob Cohen in American Psychologist entitled "Things I have learned (so far)."  It's available as a pdf [here][1].  \\n\\n\\n  [1]: http://www.uvm.edu/~bbeckage/Teaching/DataAnalysis/AssignedPapers/Cohen_1990.pdf",,
1034,16,487,79d39431-a9a7-4b45-b739-88fad43ca195,"2010-07-22 10:15:28",-1,,,
1035,2,488,99fbfee4-2256-46a5-949e-ada7185f3667,"2010-07-22 10:16:18",183,"A lot of Social Science / Psychology students with minimal mathematical background like Andy Field's book: Discovering Statistics Using SPSS. He also has a website that shares a [lot of material][1].\\n\\n\\n  [1]: http://www.statisticshell.com/woodofsuicides.html",,
1036,16,488,99fbfee4-2256-46a5-949e-ada7185f3667,"2010-07-22 10:16:18",-1,,,
1037,2,489,7d919442-3bf8-4669-9b1e-cbad4eee1d80,"2010-07-22 11:05:55",214,"It also depends on where you wan't to publish your plots. You'll save yourself a lot of trouble by consulting the guide for authors before making any plots for a journal. \\n\\nAlso save the plots in a format that is easy to modify or save the code you have used to create them. Chances are that you need to make corrections. ",,
1038,2,490,002db08a-661a-49c5-88e7-1cfb370dc1f5,"2010-07-22 11:10:29",223,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n",,
1039,1,490,002db08a-661a-49c5-88e7-1cfb370dc1f5,"2010-07-22 11:10:29",223,"Variable selection procedure for binary classification",,
1040,3,490,002db08a-661a-49c5-88e7-1cfb370dc1f5,"2010-07-22 11:10:29",223,<machine-learning><classification>,,
1041,2,491,e62bdd4c-1781-45a2-8f61-d640ea7c0f7b,"2010-07-22 11:19:10",268,"I recently found [Even You Can Learn Statistics][1] to be pretty useful.\\n\\n\\n  [1]: http://www.amazon.com/Even-You-Can-Learn-Statistics/dp/0131467573",,
1042,16,491,e62bdd4c-1781-45a2-8f61-d640ea7c0f7b,"2010-07-22 11:19:10",-1,,,
1043,2,492,bfbcf975-25b9-4a39-bbfb-986a2770f579,"2010-07-22 11:31:16",210,"I am proposing to try and find a trend in some very noisey long term data. The data is basically weekly measurements of something which moved about 5mm over a period of about 8 months. The data is to 1mm accuracey and is very noisey regularly changing +/-1 or 2mm in a week. We only have the data to the nearest mm. \\n\\nWe plan to use some basic singla processing with a fast fourier transform to seaparate out the noise from the raw data. The basic assumption is if we mirror our data set and add it to the end of our existing data set we can create a full wavelength of the data and therefore our data will show up in a fast fourier transform and we can hopefully then separate it out. \\n\\nGiven that this sounds a little dubious to me, is this a method worth purusing or is the method of mirroring and appending our data set somehow fundamentally flawed? We are looking at other approaches such as using a low pass filter as well. ",,
1044,1,492,bfbcf975-25b9-4a39-bbfb-986a2770f579,"2010-07-22 11:31:16",210,"Dubious use of signal processing principals to identify a trend",,
1045,3,492,bfbcf975-25b9-4a39-bbfb-986a2770f579,"2010-07-22 11:31:16",210,<data-mining>,,
1046,2,493,59273ad3-0641-4805-9660-2d5f757969d7,"2010-07-22 11:33:47",268,"Statsoft's [Electronic Statistics Handbook][1] ('The only Internet Resource about Statistics Recommended by Encyclopedia Britannica') is worth checking out.\\n\\n\\n  [1]: http://www.statsoft.com/textbook/",,
1047,16,493,59273ad3-0641-4805-9660-2d5f757969d7,"2010-07-22 11:33:47",-1,,,
1048,2,494,37404a21-781a-48b2-ae1e-b8e38c99f5da,"2010-07-22 11:42:42",88,"Generally, it is assumed that AIC is defined up to adding a constant, so the fact if it is negative or positive is not meaningful at all. So the answer is yes, it is valid.",,
1049,2,495,1ed0da2c-cb47-410b-9edd-80b9e7e2f050,"2010-07-22 11:43:42",214,"There is one called [Math and probability for life sciences][1], but I haven't followed it so I can't tell you if its good or not.\\n\\n\\n  [1]: http://www.academicearth.org/courses/math-and-proability-for-life-sciences",,
1050,2,496,3e2e451b-d3f8-4dee-a87f-2bc733979a0a,"2010-07-22 11:44:13",190,"I think you can get some distortion on the pasting point as not all the underlying waves will connect very well.\\n\\nI would suggest using a Hilbert Huang transform for this. Just do the split into intrinsic mode functions and see what is left over as residue when calculating them.",,
1051,2,497,05ba7635-4f1e-41d4-9bb5-575ac2d26c8d,"2010-07-22 11:53:51",190,"Greedy forward selection.\\n\\nThe steps for this method are:\\n\\n- Make sure you have a train and validation set\\n- Repeat the following\\n    - Train a classifier with each single feature separately that is not selected yet and with all the previously selected features\\n    - If the result improves, add the best performing feature, else stop procedure\\n",,
1052,16,490,f25f1565-c142-4ead-ae8f-52a2fa59c91a,"2010-07-22 11:58:13",223,,,
1053,2,498,ecb3fae1-79b8-4497-8673-27ed44117728,"2010-07-22 11:58:21",62,"Sometimes, I just want to do a copy & paste from the output window in SAS. I can highlight text with a mouse-drag, but only SOMETIMES does that get copied to the clipboard. It doesn't matter if I use "CTRL-C" or right click -> copy, or edit -> copy\\n\\nAny other SAS users experience this, and do you know a workaround/option/technique that can fix it? \\n\\nSometimes, I can fix it by clicking in another window, and coming back to the output window, but sometimes I just have to save the output window as a .lst and get the text from another editor. \\n\\n",,
1054,1,498,ecb3fae1-79b8-4497-8673-27ed44117728,"2010-07-22 11:58:21",62,"In PC SAS, how do you copy & paste from the output window?",,
1055,3,498,ecb3fae1-79b8-4497-8673-27ed44117728,"2010-07-22 11:58:21",62,<sas><pc-sas>,,
1056,2,499,51043432-6abf-4e50-bbab-4c36a7670267,"2010-07-22 12:06:28",267,"I've heard that when many regression model specifications (say, in OLS) are considered as possibilities for a dataset, this causes multiple comparison problems and the p-values and confidence intervals are no longer reliable. One extreme example of this is stepwise regression.\\n\\nWhen can I use the data itself to help specify the model, and when is this not a valid approach? Do you always need to have a subject-matter-based theory to form the model?\\n",,
1057,1,499,51043432-6abf-4e50-bbab-4c36a7670267,"2010-07-22 12:06:28",267,"When can you use data-based criteria to specify a regressionmodel?",,
1058,3,499,51043432-6abf-4e50-bbab-4c36a7670267,"2010-07-22 12:06:28",267,<regression><frequentist><multiple-comparisons>,,
1059,2,500,4e422acb-4676-4de7-be89-0dc690a56d55,"2010-07-22 12:16:29",190,"If I understand your question right, than the answer to your problem is to correct the p-values accordingly to the number of hypothesis.\\n\\nFor example Holm-Bonferoni corrections, where you sort the hypothesis (= your different models) by their p-value and reject those with a p samller than (desired p-value / index).\\n\\nMore about the topic can be found on [Wikipedia][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Multiple_comparisons",,
1060,2,501,969c7bea-636d-4f78-9151-cfd39cb5c8f9,"2010-07-22 12:17:20",88,"Backward elimination.\\n\\nStart with the full set, then iteratively train the classifier on the remaining features and remove the feature wit the smallest importance, stop when the classifier error rapidly increases/becomes unacceptable high.\\n\\nImportance can be even obtained by removing iteratively each feature and check the error increase or adapted from the classifier if it produces it (like in case of Random Forest). \\n",,
1061,16,501,969c7bea-636d-4f78-9151-cfd39cb5c8f9,"2010-07-22 12:17:20",-1,,,
1062,2,502,b0c56706-c7cf-4ec7-912d-9c2c4dfe8c19,"2010-07-22 12:25:42",NULL,"I do not know at what level you want the videos to be but I have heard good things about Khan's Academy: http://www.khanacademy.org/#Statistics",,user28
1063,2,503,8a60f4d9-8290-46b3-a0cb-f7092b33190b,"2010-07-22 12:25:48",256,"I have been using SAS a long time and have never had an issue with highlighting results from the output window.\\n\\nHowever since you are having an issue... there are alarge number of solutions!\\n\\nPerhaps the most i like... and probably a good habit to get into is to output your results into datasets... or into excel spread sheets directly (using the ODS) you can also output directly into pdf, rtf with 2 lines of the simplest code you can imagine!\\nif your code produces alot of output and you only have one table you want to copy you can specify the name of the table and it alone will be output using the ODS output.\\n\\nusually you just need to wrap your Procedure (like Proc Means for example)\\nwith \\n\\nods PDF;\\nProc Means Data = blah N NMISS MEAN STD;\\nclass upto you;\\nvar you name it;\\nrun;\\nods PDF close;\\n\\nof course there are many ways to get fancy with the way the output looks but that is a matter of trial and error and finding what you like or meets your needs.",,
1064,2,504,daf38661-3c55-4b6b-aa4a-e4bab9cab622,"2010-07-22 12:33:49",5,"Many of the **Berkeley** introductory statistics courses are available online (and on iTunes).  Here's an example: [**Stats 2**][1].  You can [find more here][2].\\n\\n\\n  [1]: http://webcast.berkeley.edu/course_details.php?seriesid=1906978493\\n  [2]: http://www.google.com/search?hl=en&rlz=1C1CHMP_en-USUS292US307&q=statistics+video+site:webcast.berkeley.edu+&aq=f&aqi=&aql=&oq=&gs_rfai=",,
1065,2,505,5585bce0-76c5-4f96-a957-3f86bd2b1bca,"2010-07-22 12:42:13",88,"Metropolis scanning / MCMC\\n\\n - Select few features randomly for a\\n   start, train classifier only on them\\n   and obtain the error. \\n - Make some\\n   random change to this working set --\\n   either remove one feature, add\\n   another at random or replace some\\n   feature with one not being currently\\n   used.\\n - Train new classifier and get\\n   its error; store in `dE` the difference\\n   the error on the new set minus the error on the previous set. \\n - With probability `min(1;exp(-beta*dE))` accept this change, otherwise reject it and try another random change.\\n - Repeat it for a long time and finally return the working set that has globally achieved the smallest error.\\n\\nYou may extend it with some wiser control of `beta` parameter. Simpler way is to use simulated annealing when you increase `beta` (lower the temperature in physical analogy) over the time to reduce fluctuations and drive the algorithm towards minimum. Harder is to use [replica exchange][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Replica_exchange",,
