Id,PostTypeId,AcceptedAnswerId,CreaionDate,Score,ViewCount,Body,OwnerUserId,LasActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,LastEditorUserId,LastEditDate,CommunityOwnedDate,ParentId,ClosedDate,OwnerDisplayName,LastEditorDisplayName
1,1,15,"2010-07-19 19:12:12",23,1278,"<p>How should I elicit prior distributions from experts when fitting a Bayesian model?</p>
",8,"2010-09-15 21:08:26","Eliciting priors from experts",<bayesian><prior><elicitation>,5,1,14,NULL,NULL,NULL,NULL,NULL,NULL,NULL
2,1,59,"2010-07-19 19:12:57",22,8198,"<p>In many different statistical methods there is an "assumption of normality".  What is "normality" and how do I know if there is normality?</p>
",24,"2012-11-12 09:21:54","What is normality?",<distributions><normality>,7,1,8,88,"2010-08-07 17:56:44",NULL,NULL,NULL,NULL,NULL
3,1,5,"2010-07-19 19:13:28",54,3613,"<p>What are some valuable Statistical Analysis open source projects available right now?</p>

<p>Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply.</p>
",18,"2013-05-27 14:48:36","What are some valuable Statistical Analysis open source projects?",<software><open-source>,19,4,36,183,"2011-02-12 05:50:03","2010-07-19 19:13:28",NULL,NULL,NULL,NULL
4,1,135,"2010-07-19 19:13:31",13,5224,"<p>I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  </p>

<p>What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?</p>
",23,"2010-09-08 03:00:19","Assessing the significance of differences in distributions",<distributions><statistical-significance>,5,2,2,NULL,NULL,NULL,NULL,NULL,NULL,NULL
5,2,NULL,"2010-07-19 19:14:43",81,NULL,"<p>The R-project</p>

<p><a href="http://www.r-project.org/">http://www.r-project.org/</a></p>

<p>R is valuable and significant because it was the first widely-accepted Open-Source alternative to big-box packages.  It's mature, well supported, and a standard within many scientific communities.</p>

<ul>
<li><a href="http://www.inside-r.org/why-use-r">Some reasons why it is useful and valuable</a> </li>
<li>There are some nice tutorials <a href="http://gettinggeneticsdone.blogspot.com/search/label/ggplot2">here</a>.</li>
</ul>
",23,"2010-07-19 19:21:15",NULL,NULL,NULL,3,NULL,23,"2010-07-19 19:21:15","2010-07-19 19:14:43",3,NULL,NULL,NULL
6,1,NULL,"2010-07-19 19:14:44",152,29229,"<p>Last year, I read a blog post from <a href="http://anyall.org/">Brendan O'Connor</a> entitled <a href="http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/">"Statistics vs. Machine Learning, fight!"</a> that discussed some of the differences between the two fields.  <a href="http://andrewgelman.com/2008/12/machine_learnin/">Andrew Gelman responded favorably to this</a>:</p>

<p>Simon Blomberg: </p>

<blockquote>
  <p>From R's fortunes
  package: To paraphrase provocatively,
  'machine learning is statistics minus
  any checking of models and
  assumptions'.
  -- Brian D. Ripley (about the difference between machine learning
  and statistics) useR! 2004, Vienna
  (May 2004) :-) Season's Greetings!</p>
</blockquote>

<p>Andrew Gelman:</p>

<blockquote>
  <p>In that case, maybe we should get rid
  of checking of models and assumptions
  more often. Then maybe we'd be able to
  solve some of the problems that the
  machine learning people can solve but
  we can't!</p>
</blockquote>

<p>There was also the <a href="http://projecteuclid.org/euclid.ss/1009213726"><strong>"Statistical Modeling: The Two Cultures"</strong> paper</a> by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the <em>predictive accuracy</em> of models.</p>

<p>Has the statistics field changed over the last decade in response to these critiques?  Do the <em>two cultures</em> still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?</p>
",5,"2014-05-29 03:54:31","The Two Cultures: statistics vs. machine learning?",<machine-learning>,15,5,137,22047,"2013-06-07 06:38:10","2010-08-09 13:05:50",NULL,NULL,NULL,NULL
7,1,18,"2010-07-19 19:15:59",76,5808,"<p>I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics.  While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data.</p>

<p>What I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source.  Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means.  Where would I go to find such information?</p>
",38,"2013-12-28 06:53:10","Locating freely available data samples",<dataset><sample><population><teaching>,24,3,79,253,"2013-09-26 21:50:36","2010-07-20 20:50:48",NULL,NULL,NULL,NULL
8,1,NULL,"2010-07-19 19:16:21",0,288,"<p>Sorry, but the emptyness was a bit overwhelming. And this has been stuck in my head since it got asked at Area51!</p>
",37,"2010-10-18 07:57:31","So how many staticians *does* it take to screw in a lightbulb?",<humor>,1,2,NULL,449,"2010-10-18 07:57:31",NULL,NULL,"2010-07-19 20:19:46",NULL,NULL
9,2,NULL,"2010-07-19 19:16:27",13,NULL,"<p><a href="http://incanter.org/">Incanter</a> is a Clojure-based, R-like platform (environment + libraries) for statistical computing and graphics. </p>
",50,"2010-07-19 19:16:27",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-07-19 19:16:27",3,NULL,NULL,NULL
10,1,1887,"2010-07-19 19:17:47",23,21925,"<p>Many studies in the social sciences use Likert scales.  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?</p>
",24,"2012-10-23 17:33:41","Under what conditions should Likert scales be used as ordinal or interval data?",<scales><measurement><ordinal><interval><likert>,4,4,12,919,"2011-03-30 15:31:46",NULL,NULL,NULL,NULL,NULL
11,1,1201,"2010-07-19 19:18:30",2,224,"<p>Is there a good, modern treatment covering the various methods of multivariate interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.</p>

<p>An example:</p>

<p><a href="http://en.wikipedia.org/wiki/Inverse_distance_weighting" rel="nofollow">Shepard's method</a></p>

<p>Say we're sampling from a multivariate normal distribution with unknown parameters. What can we say about the standard error of the interpolated estimates?</p>

<p>I was hoping for a pointer to a general survey addressing similar questions for the various types of multivariate interpolations in common use. </p>
",34,"2010-08-03 21:50:09","Multivariate Interpolation Approaches",<multivariable><interpolation>,1,2,1,34,"2010-07-28 07:58:52",NULL,NULL,NULL,NULL,NULL
12,2,NULL,"2010-07-19 19:18:41",20,NULL,"<p>See my response to <a href="http://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/2252450#2252450">"Datasets for Running Statistical Analysis on"</a> in reference to datasets in R.</p>
",5,"2010-07-19 19:18:41",NULL,NULL,NULL,1,NULL,NULL,NULL,"2011-08-12 20:29:33",7,NULL,NULL,NULL
13,2,NULL,"2010-07-19 19:18:56",14,NULL,"<p>Machine Learning seems to have its basis in the pragmatic - a Practical observation or simulation of reality.  Even within statistics, mindless "checking of models and assumptions" can lead to discarding methods that are useful.</p>

<p>For example, years ago, the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome.  Technically, that's a bad approach, but practically, it worked.</p>
",23,"2010-07-19 19:18:56",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,6,NULL,NULL,NULL
14,2,NULL,"2010-07-19 19:19:03",5,NULL,"<p>I second that Jay. Why is R valuable? Here's a short list of reasons. <a href="http://www.inside-r.org/why-use-r" rel="nofollow">http://www.inside-r.org/why-use-r</a>. Also check out <a href="http://had.co.nz/ggplot2/" rel="nofollow">ggplot2</a> - a very nice graphics package for R. Some nice tutorials <a href="http://gettinggeneticsdone.blogspot.com/search/label/ggplot2" rel="nofollow">here</a>.</p>
",36,"2010-07-19 19:19:03",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-19 19:19:03",3,NULL,NULL,NULL
15,2,NULL,"2010-07-19 19:19:46",13,NULL,"<p>John Cook gives some interesting recommendations. Basically, get percentiles/quantiles (not means or obscure scale parameters!) from the experts, and fit them with the appropriate distribution.</p>

<p><a href="http://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/">http://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/</a></p>
",6,"2010-07-19 19:19:46",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,1,NULL,NULL,NULL
16,2,NULL,"2010-07-19 19:22:31",16,NULL,"<p>Two projects spring to mind:</p>

<ol>
<li><a href="http://www.mrc-bsu.cam.ac.uk/bugs/">Bugs</a> - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC.</li>
<li><a href="http://www.bioconductor.org/">Bioconductor</a> - perhaps the most popular statistical tool in Bioinformatics. I know it's a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none.</li>
</ol>
",8,"2010-07-19 20:43:02",NULL,NULL,NULL,3,NULL,8,"2010-07-19 20:43:02","2010-07-19 19:22:31",3,NULL,NULL,NULL
17,1,29,"2010-07-19 19:24:12",9,1261,"<p>I have four competing models which I use to predict a binary outcome variable (say, employment status after graduating, 1 = employed, 0 = not-employed) for n subjects. A natural metric of model performance is hit rate which is the percentage of correct predictions for each one of the models. </p>

<p>It seems to me that I cannot use ANOVA in this setting as the data violates the assumptions underlying ANOVA. Is there an equivalent procedure I could use instead of ANOVA in the above setting to test for the hypothesis that all four models are equally effective?</p>
",NULL,"2012-01-22 23:34:51","How can I adapt ANOVA for binary data?",<anova><chi-squared><generalized-linear-model>,1,0,1,7972,"2012-01-22 23:34:51",NULL,NULL,NULL,user28,NULL
18,2,NULL,"2010-07-19 19:24:18",31,NULL,"<p>Also see the UCI machine learning Data Repository.</p>

<p><a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a></p>
",36,"2010-07-19 19:24:18",NULL,NULL,NULL,1,NULL,NULL,NULL,"2011-08-12 20:31:32",7,NULL,NULL,NULL
19,2,NULL,"2010-07-19 19:24:21",12,NULL,"<p><a href="http://www.gapminder.org/data/">Gapminder</a> has a number (430 at the last look) of datasets, which may or may not be of use to you.</p>
",55,"2010-07-19 19:24:21",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-08-12 20:29:45",7,NULL,NULL,NULL
20,2,NULL,"2010-07-19 19:24:35",3,NULL,"<p>The assumption of normality assumes your data is normally distributed (the bell curve, or gaussian distribution). You can check this by plotting the data or checking the measures for kurtosis (how sharp the peak is) and skewdness (?) (if more than half the data is on one side of the peak).</p>
",37,"2010-07-19 19:24:35",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,2,NULL,NULL,NULL
21,1,NULL,"2010-07-19 19:24:36",4,184,"<p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>

<p>Some of the concerns:</p>

<ul>
<li>Census blocks vary in sizes as rural
areas are a lot larger than condensed
urban areas. Is there a need to account for the area size difference?</li>
<li>if let's say I have census data
dating back to 4 - 5 census periods,
how far can i forecast it into the
future?</li>
<li>if some of the census zone change
lightly in boundaries, how can i
account for that change?</li>
<li>What are the methods to validate
census forecasts? for example, if i
have data for existing 5 census
periods, should I model the first 3
and test it on the latter two? or is
there another way?</li>
<li>what's the state of practice in
forecasting census data, and what are
some of the state of the art methods?</li>
</ul>
",59,"2010-09-30 21:14:45","Forecasting demographic census",<forecasting><population><census>,1,1,1,930,"2010-09-30 21:14:45",NULL,NULL,NULL,NULL,NULL
22,1,NULL,"2010-07-19 19:25:39",102,21916,"<p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>
",66,"2014-06-05 02:06:32","Bayesian and frequentist reasoning in plain English",<bayesian><frequentist>,15,1,67,930,"2011-10-04 07:05:14",NULL,NULL,NULL,NULL,NULL
23,1,91,"2010-07-19 19:26:04",10,7694,"<p>How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?</p>
",69,"2010-07-20 08:52:27","Finding the PDF given the CDF",<distributions><pdf><cdf>,2,1,2,NULL,NULL,NULL,NULL,NULL,NULL,NULL
24,2,NULL,"2010-07-19 19:26:13",17,NULL,"<p>For doing a variety of MCMC tasks in Python, there's <a href="http://code.google.com/p/pymc/">PyMC</a>, which I've gotten quite a bit of use out of.  I haven't run across anything that I can do in BUGS that I can't do in PyMC, and the way you specify models and bring in data seems to be a lot more intuitive to me.</p>
",61,"2010-07-19 19:26:13",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 19:26:13",3,NULL,NULL,NULL
25,1,32,"2010-07-19 19:27:13",7,2095,"<p>What modern tools (Windows-based) do you suggest for modeling financial time series?</p>
",69,"2014-08-13 20:29:48","Tools for modeling financial time series",<modeling><time-series><finance><software>,7,4,5,69,"2010-07-26 23:38:29","2010-07-26 23:38:29",NULL,NULL,NULL,NULL
26,1,61,"2010-07-19 19:27:43",18,3476,"<p>What is a standard deviation, how is it calculated and what is its use in statistics?</p>
",75,"2013-07-30 10:19:04","What is a standard deviation?",<standard-deviation><basic-concepts>,8,14,7,22047,"2013-07-30 10:19:04",NULL,NULL,NULL,NULL,NULL
27,2,NULL,"2010-07-19 19:28:12",4,NULL,"<p><a href="http://mathforum.org/workshops/sum96/data.collections/datalibrary/data.set6.html" rel="nofollow">http://mathforum.org/workshops/sum96/data.collections/datalibrary/data.set6.html</a></p>
",68,"2010-07-19 19:28:12",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-08-12 20:29:55",7,NULL,NULL,NULL
28,2,NULL,"2010-07-19 19:28:12",6,NULL,"<p><a href="http://www.gnu.org/software/gsl/" rel="nofollow">GSL</a> for those of you who wish to program in C / C++ is a valuable resource as it provides several routines for random generators, linear algebra etc. While GSL is primarily available for Linux there are also ports for Windows. (See: <a href="http://gladman.plushost.co.uk/oldsite/computing/gnu_scientific_library.php" rel="nofollow">http://gladman.plushost.co.uk/oldsite/computing/gnu_scientific_library.php</a> and <a href="http://david.geldreich.free.fr/dev.html" rel="nofollow">http://david.geldreich.free.fr/dev.html</a>)</p>
",NULL,"2010-07-19 19:28:12",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 19:28:12",3,NULL,user28,NULL
29,2,NULL,"2010-07-19 19:28:15",5,NULL,"<p>Contingency table (chi-square). Also Logistic Regression is your friend - use dummy variables. </p>
",36,"2010-07-19 19:28:15",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,17,NULL,NULL,NULL
30,1,55,"2010-07-19 19:28:34",7,705,"<p>Which methods are used for testing random variate generation algorithms?</p>
",69,"2011-05-12 18:38:27","Testing random variate generation algorithms",<algorithms><hypothesis-testing><random-variable><random-generation>,8,2,7,919,"2010-08-25 14:12:54",NULL,NULL,NULL,NULL,NULL
31,1,NULL,"2010-07-19 19:28:44",66,175495,"<p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results.  Many computerized tools report test results in terms of "p values" or "t values".</p>

<p>How would you explain the following points to college students taking their first course in statistics:</p>

<ul>
<li><p>What does a "p-value" mean in relation to the hypothesis being tested?  Are there cases when one should be looking for a high p-value or a low p-value?</p></li>
<li><p>What is the relationship between a p-value and a t-value?</p></li>
</ul>
",13,"2014-07-19 03:34:11","What is the meaning of p values and t values in statistical tests?",<hypothesis-testing><t-test><p-value><interpretation><intuition>,10,3,56,919,"2013-10-13 16:33:10",NULL,NULL,NULL,NULL,NULL
32,2,NULL,"2010-07-19 19:29:06",12,NULL,"<p>I recommend R (see <a href="http://cran.r-project.org/web/views/TimeSeries.html">the time series view on CRAN</a>).  </p>

<p>Some useful references:</p>

<ul>
<li><a href="http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf">Econometrics in R</a>, by Grant Farnsworth</li>
<li><a href="http://stackoverflow.com/questions/1714280/multivariate-time-series-modelling-in-r/1715488#1715488">Multivariate time series modelling in R</a></li>
</ul>
",5,"2010-07-19 19:29:06",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,25,NULL,NULL,NULL
33,1,49,"2010-07-19 19:30:03",5,1428,"<p>What R packages should I install for seasonality analysis?</p>
",69,"2012-11-08 00:40:55","R packages for seasonality analysis",<r><seasonality>,3,1,5,88,"2010-09-16 06:56:44",NULL,NULL,NULL,NULL,NULL
34,2,NULL,"2010-07-19 19:30:07",3,NULL,"<p><a href="http://scienceblogs.com/goodmath/2008/04/schools_of_thought_in_probabil.php" rel="nofollow">Schools of thought in Probability Theory</a></p>
",79,"2010-07-19 19:30:07",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,22,NULL,NULL,NULL
35,1,72,"2010-07-19 19:30:30",10,1223,"<p>I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.</p>

<pre><code>## assuming a median value of 1500
med = 1500
rawdist = rpois(1000000,med)
oDdist = rawDist + ((rawDist-med)*3)
</code></pre>

<p>Visually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a <a href="http://en.wikipedia.org/wiki/Overdispersion#Poisson">negative binomial distribution, as described here</a>? (If so, any pointers or links on doing so would be much appreciated).</p>

<p>Oh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.</p>

<hr>

<p><strong>Update:</strong>  For the sake of anyone else who searches and finds this question, here's a simple R function to model an overdispersed poisson using a negative binomial distribution. Set d to the desired mean/variance ratio:</p>

<pre><code>rpois.od&lt;-function (n, lambda,d=1) {
  if (d==1)
    rpois(n, lambda)
  else
     rnbinom(n, size=(lambda/(d-1)), mu=lambda)
}
</code></pre>

<p>(via the R mailing list: <a href="https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html">https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html</a>)</p>
",54,"2010-07-25 01:44:14","Modelling a Poisson distribution with overdispersion",<distributions><modeling><poisson><overdispersion>,2,0,4,54,"2010-07-25 01:44:14",NULL,NULL,NULL,NULL,NULL
36,1,NULL,"2010-07-19 19:31:47",41,67396,"<p>There is an old saying: "Correlation does not mean causation". When I teach, I tend to use the following standard examples to illustrate this point:</p>

<ol>
<li>number of storks and birth rate in Denmark;</li>
<li>number of priests in America and alcoholism;</li>
<li>in the start of the 20th century it was noted that there was a strong correlation between 'Number of radios' and 'Number of people in Insane Asylums'</li>
<li>and my favorite: <a href="http://en.wikipedia.org/wiki/File%3aPiratesVsTemp%28en%29.svg" rel="nofollow">pirates cause global warming</a>.</li>
</ol>

<p>However, I do not have any references for these examples and whilst amusing, they are obviously false.</p>

<p>Does anyone have any other good examples?</p>
",8,"2014-05-23 06:31:12","Examples for teaching: Correlation does not mean causation",<correlation><teaching>,27,7,21,43889,"2014-05-23 06:31:12","2010-08-16 13:01:42",NULL,NULL,NULL,NULL
38,2,NULL,"2010-07-19 19:32:28",2,NULL,"<p>If your mean value for the Poisson is 1500, then you're very close to a normal distribution; you might try using that as an approximation and then modelling the mean and variance separately.</p>
",61,"2010-07-19 19:32:28",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,35,NULL,NULL,NULL
39,1,721,"2010-07-19 19:32:29",4,210,"<p>I'm looking for worked out solutions using Bayesian and/or logit analysis similar to a workbook or an annal. </p>

<p>The worked out problems could be of any field; however, I'm interested in urban planning / transportation related fields. </p>
",59,"2010-07-27 05:40:44","Sample problems on logit modeling and Bayesian methods",<modeling><bayesian><logit><transportation>,1,0,1,190,"2010-07-27 04:57:47",NULL,NULL,NULL,NULL,NULL
40,1,111,"2010-07-19 19:32:47",8,491,"<p>What algorithms are used in modern and good-quality random number generators? </p>
",69,"2014-01-28 07:46:08","Pseudo-random number generation algorithms",<algorithms><random-variable><random-generation>,4,1,2,919,"2010-08-25 14:13:48",NULL,NULL,NULL,NULL,NULL
41,2,NULL,"2010-07-19 19:33:13",8,NULL,"<p>A quote from <a href="http://en.wikipedia.org/wiki/Standard_deviation" rel="nofollow">wiki</a>.</p>

<blockquote>
  <p>It shows how much variation there is from the "average" (mean, or expected/budgeted value). A low standard deviation indicates that the data points tend to be very close to the mean, whereas high standard deviation indicates that the data is spread out over a large range of values.</p>
</blockquote>
",83,"2010-07-19 19:33:13",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,26,NULL,NULL,NULL
42,2,NULL,"2010-07-19 19:33:19",12,NULL,"<p><a href="http://www.cs.waikato.ac.nz/ml/weka">Weka</a> for data mining - contains many classification and clustering algorithms in Java.</p>
",80,"2010-07-19 19:33:19",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-07-19 19:33:19",3,NULL,NULL,NULL
43,2,NULL,"2010-07-19 19:33:37",7,NULL,"<p>R is great, but I wouldn't really call it "windows based" :) That's like saying the cmd prompt is windows based. I guess it is technically in a window...</p>

<p>RapidMiner is far easier to use [1]. It's a free, open-source, multi-platform, GUI. Here's a video on time series forecasting:</p>

<p><a href="http://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1">http://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1</a></p>

<p>Also, don't forget to read:</p>

<p><a href="http://www.forecastingprinciples.com/">http://www.forecastingprinciples.com/</a></p>

<p>[1] No, I don't work for them. </p>
",74,"2010-07-19 20:24:23",NULL,NULL,NULL,5,NULL,74,"2010-07-19 20:24:23",NULL,25,NULL,NULL,NULL
44,1,89,"2010-07-19 19:34:42",8,453,"<p>How would you explain data visualization and why it is important to a layman?</p>
",68,"2013-07-27 00:28:49","Explain data visualization",<data-visualization><intuition>,5,1,5,12786,"2013-07-27 00:28:49",NULL,NULL,NULL,NULL,NULL
45,2,NULL,"2010-07-19 19:34:44",5,NULL,"<p>The <a href="http://en.wikipedia.org/wiki/Mersenne_twister" rel="nofollow">Mersenne Twister</a> is one I've come across and used before now.</p>
",55,"2010-07-19 19:34:44",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,40,NULL,NULL,NULL
46,2,NULL,"2010-07-19 19:35:04",0,NULL,"<p>A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. </p>

<p>To put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. </p>

<p>Standard deviation is a property of a population. It measures how much average "dispersion" there is to that population. Are all the obsrvations clustered around the mean, or are they widely spread out? </p>

<p>To estimate the standard deviation of a population, we often calculate the standard deviation of a "sample" from that population. To do this, you take observations from that population, calculate a mean of those observations, and then calculate the square root of the average squared deviation from that "sample mean". </p>

<p>To get an unbiased estimator of the variance, you don't actually calculate the average squared deviation from the sample mean, but instead, you divide by (N-1) where N is the number of observations in your sample. Note that this "sample standard deviation" is not an unbiased estimator of the standard deviation, but the square of the "sample standard deviation" is an unbiased estimator of the variance of the population. </p>
",62,"2010-07-20 02:13:12",NULL,NULL,NULL,2,NULL,62,"2010-07-20 02:13:12",NULL,26,NULL,NULL,NULL
47,1,268,"2010-07-19 19:36:12",7,312,"<p>I have a dataset of 130k internet users characterized by 4 variables describing users' number of sessions, locations visited, avg data download and session time aggregated from four months of activity.</p>

<p>Dataset is very heavy-tailed. For example third of users logged only once during four months, whereas six users had more than 1000 sessions.</p>

<p>I wanted to come up with a simple classification of users, preferably with indication of the most appropriate number of clusters.</p>

<p>Is there anything you could recomend as a soultion?</p>
",22,"2010-11-23 18:59:58","Clustering of large, heavy-tailed dataset",<clustering><large-data>,4,2,3,8,"2010-10-08 16:05:59",NULL,NULL,NULL,NULL,NULL
48,2,NULL,"2010-07-19 19:36:20",3,NULL,"<p>Flip through Freakonomics for some great examples. Their bibliography is chock full of references.</p>
",36,"2010-07-19 19:36:20",NULL,NULL,NULL,4,NULL,NULL,NULL,"2010-08-16 13:01:42",36,NULL,NULL,NULL
49,2,NULL,"2010-07-19 19:36:52",6,NULL,"<p>You don't need to install any packages because this is possible with base-R functions.  Have a look at <a href="http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/ts/html/arima.html">the arima function</a>.  </p>

<p>This is a basic function of <a href="http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins">Box-Jenkins analysis</a>, so you should consider reading one of the R time series text-books for an overview; my favorite is Shumway and Stoffer. "<a href="http://rads.stackoverflow.com/amzn/click/0387293175">Time Series Analysis and Its Applications: With R Examples</a>".</p>
",5,"2010-07-19 19:36:52",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,33,NULL,NULL,NULL
50,1,85,"2010-07-19 19:37:31",18,2414,"<p>What do they mean when they say "random variable"? </p>
",62,"2014-07-18 11:34:37","What is meant by a "random variable"?",<random-variable><intuition><definition>,7,4,9,919,"2013-04-01 19:24:52",NULL,NULL,NULL,NULL,NULL
51,1,64,"2010-07-19 19:37:55",1,120,"<p>Are there any objective methods of assessment or standardized tests available to measure the effectiveness of a software that does pattern recognition?</p>
",68,"2010-07-19 20:12:16","Measuring the effectiveness of a pattern recognition software",<pattern-recognition>,1,0,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
52,2,NULL,"2010-07-19 19:39:06",7,NULL,"<p>Usually one (p=.042), but it also depends on power.</p>
",36,"2010-07-19 19:39:06",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,8,NULL,NULL,NULL
53,1,78,"2010-07-19 19:39:08",40,25962,"<p>What are the main differences between performing Principal Components Analysis on a correlation and covariance matrix? Do they give the same results?</p>
",17,"2013-06-27 07:10:58","PCA on correlation or covariance?",<pca>,4,4,20,6029,"2013-06-27 07:10:58",NULL,NULL,NULL,NULL,NULL
54,1,65,"2010-07-19 19:41:19",11,497,"<p>As I understand UK Schools teach that the Standard Deviation is found using:</p>

<p><img src="http://upload.wikimedia.org/math/e/e/4/ee485814ab9e19908f2b39d5d70406d5.png" alt="alt text"></p>

<p>whereas US Schools teach:</p>

<p><img src="http://upload.wikimedia.org/math/8/3/a/83a7338b851dcaafaf5b64353af56596.png" alt="alt text"></p>

<p>(at a basic level anyway).</p>

<p>This has caused a number of my students problems in the past as they have searched on the Internet, but found the wrong explanation.</p>

<p>Why the difference?</p>

<p>With simple datasets say 10 values, what degree of error will there be if the wrong method is applied (eg in an exam)?</p>
",55,"2012-12-26 19:02:47","Why do US and UK Schools Teach Different methods of Calculating the Standard Deviation?",<standard-deviation><error><teaching><unbiased-estimator>,5,5,2,919,"2012-12-26 19:02:47",NULL,NULL,NULL,NULL,NULL
55,2,NULL,"2010-07-19 19:41:39",7,NULL,"<p>The <a href="http://en.wikipedia.org/wiki/Diehard_tests" rel="nofollow">Diehard Test Suite</a> is something close to a Golden Standard for testing random number generators. It includes a number of tests where a good random number generator should produce result distributed according to some know distribution against which the outcome using the tested generator can then be compared.</p>

<p><strong>EDIT</strong></p>

<p>I have to update this since I was not exactly right:
Diehard might still be used a lot, but it is no longer maintained and not state-of-the-art anymore. NIST has come up with <a href="http://csrc.nist.gov/groups/ST/toolkit/rng/index.html" rel="nofollow">a set of improved tests</a> since.</p>
",56,"2011-05-12 18:38:27",NULL,NULL,NULL,0,NULL,56,"2011-05-12 18:38:27",NULL,30,NULL,NULL,NULL
56,2,NULL,"2010-07-19 19:42:28",56,NULL,"<p>Here is how I would explain the basic difference to my grandma:</p>

<p>I have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping.</p>

<p>Problem: Which area of my home should I search?</p>

<p>Frequentist Reasoning: </p>

<p>I can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming from. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone.</p>

<p>Bayesian Reasoning:</p>

<p>I can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone.</p>
",NULL,"2010-07-19 19:42:28",NULL,NULL,NULL,7,NULL,NULL,NULL,NULL,22,NULL,user28,NULL
57,2,NULL,"2010-07-19 19:42:34",2,NULL,"<p>From <a href="http://en.wikipedia.org/wiki/Random_variable" rel="nofollow">Wikipedia</a>:</p>

<blockquote>
  <p>In mathematics (especially probability
  theory and statistics), a random
  variable (or stochastic variable) is
  (in general) a measurable function 
  that maps a probability space into a
  measurable space. Random variables
  mapping all possible outcomes of an
  event into the real numbers are
  frequently studied in elementary
  statistics and used in the sciences to
  make predictions based on data
  obtained from scientific experiments.
  In addition to scientific
  applications, random variables were
  developed for the analysis of games of
  chance and stochastic  events. The
  utility of random variables comes from
  their ability to capture only the
  mathematical properties necessary to
  answer probabilistic  questions.</p>
</blockquote>

<p>From <a href="http://cnx.org/content/m13418/latest/" rel="nofollow">cnx.org</a>:</p>

<blockquote>
  <p>A random variable is a function, which assigns unique numerical values to all possible
  outcomes of a random experiment under fixed conditions. A random variable is not a 
  variable but rather a function that maps events to numbers.</p>
</blockquote>
",69,"2010-07-19 19:47:54",NULL,NULL,NULL,4,NULL,69,"2010-07-19 19:47:54",NULL,50,NULL,NULL,NULL
58,1,6988,"2010-07-19 19:42:57",10,1667,"<p>What is the back-propagation algorithm and how does it work?</p>
",68,"2013-04-10 12:09:31","Can someone please explain the back-propagation algorithm?",<algorithms><optimization><neural-networks>,3,0,1,3911,"2011-04-29 00:38:41",NULL,NULL,NULL,NULL,NULL
59,2,NULL,"2010-07-19 19:43:20",24,NULL,"<p>The assumption of normality is just the supposition that the underlying <a href="http://en.wikipedia.org/wiki/Random_variable">random variable</a> of interest is distributed <a href="http://en.wikipedia.org/wiki/Normal_distribution">normally</a>, or approximately so.  Intuitively, normality may be understood as the result of the sum of a large number of independent random events.</p>

<p>More specifically, normal distributions are defined by the following function:</p>

<p><img src="http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png" alt="alt text"></p>

<p>where $\\mu$ and $\\sigma^2$ are the mean and the variance, respectively, and which appears as follows:</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png" alt="alt text"></p>

<p>This can be checked in <a href="http://en.wikipedia.org/wiki/Normality_test">multiple ways</a>, that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected <a href="http://en.wikipedia.org/wiki/Q-Q_plot">quantile distribution</a>).</p>
",39,"2012-10-06 10:39:27",NULL,NULL,NULL,1,NULL,805,"2012-10-06 10:39:27",NULL,2,NULL,NULL,NULL
60,2,NULL,"2010-07-19 19:43:20",1,NULL,"<p><a href="http://en.wikipedia.org/wiki/K-means_algorithm" rel="nofollow">K-Means clustering</a> should work well for this type of problem.  However, it does require that you specify the number of clusters in advance.</p>

<p>Given the nature of this data, however, you may be able to work with a <a href="http://en.wikipedia.org/wiki/Cluster_analysis#Hierarchical_clustering" rel="nofollow">hierarchical clustering algorithm</a> instead.  Since all 4 variables are most likely fairly highly correlated, you can most likely break out clusters, and stop when you reach a small enough distance between clusters.  This may be a much simpler approach in this specific case, and allows you to determine "how many clusters" by just stopping as soon as you've broken your set into fine enough clusters.</p>
",41,"2010-07-19 19:43:20",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,47,NULL,NULL,NULL
61,2,NULL,"2010-07-19 19:44:35",22,NULL,"<p>Standard deviation is a number that represents the "spread" or "dispersion" of a set of data. There are other measures for spread, such as range and variance. </p>

<p>Here are some example sets of data, and their standard deviations:</p>

<pre><code>[1,1,1]     standard deviation = 0   (there's no spread)  
[-1,1,3]    standard deviation = 1.6 (some spread) 
[-99,1,101] standard deviation = 82  (big spead)
</code></pre>

<p>The above data sets have the same mean. </p>

<p>Deviation means "distance from the mean".</p>

<p>"Standard" here means "standardized", meaning the standard deviation and mean are in the same units, unlike variance. </p>

<p>For example, if the mean height is 2 <strong>meters</strong>, the standard deviation might be 0.3 <strong>meters</strong>, whereas the variance would be 0.09 <strong>meters squared</strong>. </p>

<p>It is convenient to know that <a href="http://en.wikipedia.org/wiki/Chebyshev%27s_inequality" rel="nofollow">at least 75%</a> of the data points <em>always</em> lie within 2 standard deviations of the mean (or <a href="http://en.wikipedia.org/wiki/68-95-99.7_rule" rel="nofollow">around 95%</a> if the distribution is Normal).</p>

<p>For example, if the mean is 100, and the standard deviation is 15, then at least 75% of the values are between 70 and 130. </p>

<p>If the distribution happens to be Normal, then 95% of the values are between 70 and 130. </p>

<p>Generally speaking, IQ test scores are normally distributed and have an average of 100. Someone who is "very bright" is two standard deviations above the mean, meaning an IQ test score of 130.</p>
",74,"2013-01-23 09:37:19",NULL,NULL,NULL,5,NULL,74,"2013-01-23 09:37:19","2013-01-23 09:37:19",26,NULL,NULL,NULL
62,1,80,"2010-07-19 19:44:58",4,124,"<p>With the recent FIFA world cup, I decided to have some fun and determine which months produced world cup football players. Turned out, most footballers in the 2010 world cup were born in the first half of the year.</p>

<p>Someone pointed out, that children born in the first half of the year had a physical advantage over others and hence "survivorship bias" was involved in the equation. Is this an accurate observation? Can someone please explain why he says that?</p>

<p>Also, when trying to understand the concept, I found most examples revolved around the financial sector. Are they any other everyday life examples explaining it?</p>

<p>Thanks!</p>
",58,"2010-07-20 16:19:51","A case of survivorship bias?",<statistical-bias>,2,0,NULL,58,"2010-07-19 19:50:45",NULL,NULL,NULL,NULL,NULL
63,2,NULL,"2010-07-19 19:45:19",25,NULL,"<p>It might be useful to explain that "causes" is an asymmetric relation (X causes Y is different from Y causes X), whereas "is correlated with" is a symmetric relation.</p>

<p>For instance, homeless population and crime rate might be correlated, in that both tend to be high or low in the same locations.  It is equally valid to say that homelesss population is correlated with crime rate, or crime rate is correlated with homeless population.  To say that crime causes homelessness, or homeless populations cause crime are different statements.  And correlation does not imply that either is true.  For instance, the underlying cause could be a 3rd variable such as drug abuse, or unemployment.  </p>

<p>The mathematics of statistics is not good at identifying underlying causes, which requires some other form of judgement.</p>
",87,"2010-07-19 19:45:19",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-08-16 13:01:42",36,NULL,NULL,NULL
64,2,NULL,"2010-07-19 19:46:08",6,NULL,"<p>Yes, there are many methods.  You would need to specify which model you're using, because it can vary.  </p>

<p>For instance, Some models will be compared based on the <a href="http://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a> or <a href="http://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a> criteria.  In other cases, one would look at the <a href="http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">MSE from cross validation</a> (as, for instance, with a support vector machine).</p>

<ol>
<li>I recommend reading <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a> by Christopher Bishop.</li>
<li>This is also discussed in Chapter 5 on Credibility, and particularly section 5.5 "Comparing data mining methods" of <a href="http://rads.stackoverflow.com/amzn/click/0120884070">Data Mining: Practical Machine Learning Tools and Techniques</a> by Witten and Frank (which discusses Weka in detail).</li>
<li>Lastly, you should also have a look at <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a> by Hastie, Tibshirani and Friedman which is available for free online.</li>
</ol>
",5,"2010-07-19 20:12:16",NULL,NULL,NULL,0,NULL,5,"2010-07-19 20:12:16",NULL,51,NULL,NULL,NULL
65,2,NULL,"2010-07-19 19:46:11",14,NULL,"<p>The first formula is the <em>population</em> standard deviation and the second formula is the the <em>sample</em> standard deviation. The second formula is also related to the unbiased estimator of the variance - see <a href="http://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance">wikipedia</a> for further details.</p>

<p>I suppose (here) in the UK they don't make the distinction between sample and population at high school. They certainly don't touch concepts such as biased estimators. </p>
",8,"2010-07-19 20:44:53",NULL,NULL,NULL,3,NULL,8,"2010-07-19 20:44:53",NULL,54,NULL,NULL,NULL
66,2,NULL,"2010-07-19 19:46:11",5,NULL,"<p>This is <a href="http://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel's Correction</a>.  The US version is showing the formula for the <em>sample standard deviation</em>, where the UK version above is the <em>standard deviation of the sample</em>.</p>
",41,"2010-07-19 19:46:11",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,54,NULL,NULL,NULL
67,2,NULL,"2010-07-19 19:47:16",2,NULL,"<p>From <a href="http://en.wikipedia.org/wiki/Data_visualization" rel="nofollow">Wikipedia</a>: Data visualization is the study of the visual representation of data, meaning "information which has been abstracted in some schematic form, including attributes or variables for the units of information"</p>

<p>Data viz is important for visualizing trends in data, telling a story - See <a href="http://www.edwardtufte.com/tufte/posters" rel="nofollow">Minard's map of Napoleon's march</a> - possibly one of the best data graphics ever printed.</p>

<p>Also see any of Edward Tufte's books - especially <a href="http://rads.stackoverflow.com/amzn/click/0961392142" rel="nofollow">Visual Display of Quantitative Information.</a></p>
",36,"2010-07-19 19:47:16",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,44,NULL,NULL,NULL
69,2,NULL,"2010-07-19 19:47:49",3,NULL,"<p>Since N is the number of points in the data set, one could argue that by calculating the mean one has reduced the degree of freedom in the data set by one (since one introduced a dependency into the data set), so one should use N-1 when estimating the standard deviation from a data set for which one had to estimate the mean before.</p>
",56,"2010-07-19 19:47:49",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,54,NULL,NULL,NULL
70,2,NULL,"2010-07-19 19:48:45",15,NULL,"<p><a href="http://data.worldbank.org/data-catalog"><strong>World Bank</strong></a> offers quite a lot of interesting data and has been recently very active in developing nice <a href="http://data.worldbank.org/developers/api-overview">API</a> for it.</p>

<p>Also, <a href="http://www.cs.purdue.edu/commugrate/data_access/all_data_sets.php"><strong>commugrate</strong></a> project has an interesting list available.</p>

<p>For US health related data head for <a href="http://www.healthindicators.gov/"><strong>Health Indicators Warehouse</strong></a>. </p>

<p>Daniel Lemire's blog <a href="http://lemire.me/blog/archives/2012/03/27/publicly-available-large-data-sets-for-database-research/">points</a> to few interesting examples (mostly tailored towards DB research) including <strong>Canadian Census 1880</strong> and <strong>synoptic cloud reports</strong>.</p>

<p>And as for today (03/04/2012) <a href="http://1940census.archives.gov/"><strong>US 1940 census records</strong></a> are also available to download.</p>
",22,"2012-04-03 09:51:07",NULL,NULL,NULL,1,NULL,22,"2012-04-03 09:51:07","2011-08-12 20:30:59",7,NULL,NULL,NULL
71,2,NULL,"2010-07-19 19:50:33",2,NULL,"<p>It's an algorithm for training feedforward multilayer neural networks (multilayer perceptrons). There are several nice java applets around the web that illustrate what's happening, like this one: <a href="http://neuron.eng.wayne.edu/bpFunctionApprox/bpFunctionApprox.html" rel="nofollow">http://neuron.eng.wayne.edu/bpFunctionApprox/bpFunctionApprox.html</a>. Also, <a href="http://rads.stackoverflow.com/amzn/click/0198538642" rel="nofollow">Bishop's book on NNs</a> is the standard desk reference for anything to do with NNs.</p>
",36,"2010-07-19 19:50:33",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,58,NULL,NULL,NULL
72,2,NULL,"2010-07-19 19:51:05",6,NULL,"<p>for overdispersed poisson, use the negative binomial, which allows you to parameterize the variance as a function of the mean precisely.  rnbinom(), etc. in R.</p>
",96,"2010-07-19 19:51:05",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,35,NULL,NULL,NULL
73,1,104362,"2010-07-19 19:51:32",24,5717,"<p><strong>Duplicate thread:</strong> <a href="http://stats.stackexchange.com/questions/1676/i-just-installed-the-latest-version-of-r-what-packages-should-i-obtain">I just installed the latest version of R. What packages should I obtain?</a></p>

<p>What are the <a href="http://www.r-project.org/">R</a> packages you couldn't imagine your daily work with data?
Please list both general and specific tools.</p>

<p>UPDATE:
As for 24.10.10 <code>ggplot2</code> seems to be the winer with 7 votes.</p>

<p>Other packages mentioned more than one are:</p>

<ul>
<li><code>plyr</code> - 4</li>
<li><code>RODBC</code>, <code>RMySQL</code> - 4</li>
<li><code>sqldf</code> - 3</li>
<li><code>lattice</code> - 2</li>
<li><code>zoo</code> - 2</li>
<li><code>Hmisc/rms</code> - 2</li>
<li><code>Rcurl</code> - 2</li>
<li><code>XML</code> - 2</li>
</ul>

<p>Thanks all for your answers!</p>
",22,"2014-06-23 08:58:48","What R packages do you find most useful in your daily work?",<r>,24,5,26,88,"2011-05-11 12:29:47","2010-07-19 20:07:49",NULL,NULL,NULL,NULL
74,2,NULL,"2010-07-19 19:51:34",25,NULL,"<p>In such a discussion, I always recall the famous Ken Thompson quote </p>

<blockquote>
  <p>When in doubt, use brute force.</p>
</blockquote>

<p>In this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. </p>
",88,"2010-07-19 19:51:34",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,6,NULL,NULL,NULL
75,1,94,"2010-07-19 19:52:31",5,754,"<p>I'm using <a href="http://www.r-project.org/" rel="nofollow"><strong>R</strong></a> and the manuals on the R site are really informative. However, I'd like to see some more examples and implementations with R which can help me develop my knowledge faster. Any suggestions?</p>
",69,"2010-07-21 03:12:51","Where can I find useful R tutorials with various implementations?",<r><books><code>,5,4,7,NULL,"2010-07-21 03:12:51","2010-10-19 11:13:59",NULL,NULL,NULL,user28
76,2,NULL,"2010-07-19 19:52:49",22,NULL,"<p>I use <a href="http://cran.r-project.org/web/packages/plyr/index.html"><strong>plyr</strong></a> and <a href="http://cran.r-project.org/web/packages/ggplot2/index.html"><strong>ggplot2</strong></a> the most on a daily basis.</p>

<p>I also rely heavily on time series packages; most especially, the <a href="http://cran.r-project.org/web/packages/zoo/index.html"><strong>zoo</strong></a> package.</p>
",5,"2010-07-19 19:52:49",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-08-09 13:16:41",73,NULL,NULL,NULL
77,2,NULL,"2010-07-19 19:54:03",16,NULL,"<ol>
<li><p>Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally. Google has made hundreds of billions of dollars not caring about causation. </p></li>
<li><p>To find causation, you generally need experimental data, not observational data. Though, in economics, they often use observed "shocks" to the system to test for causation, like if a CEO dies suddenly and the stock price goes up, you can assume causation. </p></li>
<li><p>Correlation is a necessary but not sufficient condition for causation. To show causation requires a counter-factual.</p></li>
</ol>
",74,"2010-09-09 18:16:59",NULL,NULL,NULL,7,NULL,74,"2010-09-09 18:16:59","2010-08-16 13:01:42",36,NULL,NULL,NULL
78,2,NULL,"2010-07-19 19:54:38",33,NULL,"<p>You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales. Using the correlation matrix <em>standardises</em> the data.</p>

<p>In general they give different results. Especially when the scales are different.</p>

<p>As example, take a look a look at this R heptathlon data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (200m) are around 20.</p>

<pre><code>library(HSAUR)
# look at heptathlon data
heptathlon

# correlations
round(cor(heptathlon[,-8]),2)   # ignoring "score" 
# covariance
round(cov(heptathlon[,-8]),2)

# PCA
# scale=T bases the PCA on the correlation matrix
hep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)
hep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)

# PC scores per competitor
hep.scores.cor = predict(hep.PC.cor)
hep.scores.cov = predict(hep.PC.cov)

# Plot of PC1 vs PC2
par(mfrow = c(2, 1))
plot(hep.scores.cov[,1],hep.scores.cov[,2],
     xlab="PC 1",ylab="PC 2", pch=NA, main="Covariance")
text(hep.scores.cov[,1],hep.scores.cov[,2],labels=1:25) 

plot(hep.scores.cor[,1],hep.scores.cor[,2],
     xlab="PC 1",ylab="PC 2", pch=NA, main="Correlation")
text(hep.scores.cor[,1],hep.scores.cor[,2],labels=1:25) 
</code></pre>

<p>Notice that the outlying individuals (in <em>this</em> data set) are outliers regardless of whether the covariance or correlation matrix is used.</p>
",8,"2010-11-06 21:56:17",NULL,NULL,NULL,4,NULL,8,"2010-11-06 21:56:17",NULL,53,NULL,NULL,NULL
79,2,NULL,"2010-07-19 19:56:04",4,NULL,"<p>I am not sure this is purely a US vs. British issue. Here is a brief page I wrote <a href="http://www.graphpad.com/faq/viewfaq.cfm?faq=1382" rel="nofollow">explaining the difference between using n vs. n-1 when computing a Standard Deviation</a>.</p>
",25,"2010-07-19 19:56:04",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,54,NULL,NULL,NULL
80,2,NULL,"2010-07-19 19:56:43",8,NULL,"<p>The basic idea behind this is that football clubs have an age cut-off when determining teams.  In the league my children participate in the age restrictions states that children born after July 31st are placed on the younger team.  This means that two children that are effectively the same age can be playing with two different age groups.  The child born July 31st will be playing on the older team and theoretically be the youngest and smallest on the team and in the league.  The child born on August 1st will be the oldest and largest child in the league and will be able to benefit from that.</p>

<p>The survivorship bias comes because competitive leagues will select the best players for their teams.  The best players in childhood are often the older players since they have additional time for their bodies to mature.  This means that otherwise acceptable younger players are not selected simply because of their age.  Since they are not given the same opportunities as the older kids, they don’t develop the same skills and eventually drop out of competitive soccer.</p>

<p>If the cut-off for competitive soccer in enough countries is January 1st, that would support the phenomena you see.  A similar phenomena has been observed in several other sports including baseball and ice hockey.</p>
",93,"2010-07-19 19:56:43",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,62,NULL,NULL,NULL
81,2,NULL,"2010-07-19 19:58:56",7,NULL,"<p>I use the <strong><a href="http://cran.r-project.org/web/packages/xtable/index.html">xtable</a></strong> package. The xtable package turns tables produced by R (in particular, the tables displaying the anova results) into LaTeX tables, to be included in an article. </p>
",69,"2010-07-19 19:58:56",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-19 11:07:08",73,NULL,NULL,NULL
83,2,NULL,"2010-07-19 20:02:51",2,NULL,"<p>R is designed around ideas such as "reproducible research" and "trustworthy software", as John Chambers says <a href="http://books.google.com/books?id=UXneuOIvhEAC&amp;printsec=frontcover" rel="nofollow">in his excellent book "Software for Data Analysis: Programming with R"</a>.  </p>

<p>One of the best ways to learn R is to look at the wealth of source code that available on <a href="http://cran.r-project.org/" rel="nofollow">CRAN</a> (with 2461 packages and counting).  Simple <code>install.packages</code>, load a <code>library()</code>, and start browsing the code.</p>
",5,"2010-07-19 20:02:51",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-19 11:13:59",75,NULL,NULL,NULL
84,2,NULL,"2010-07-19 20:03:34",6,NULL,"<p>I would explain it to a layman as:</p>

<blockquote>
  <p>Data visualization is taking data, and making a picture out of it.  This allows you to easily see and understand relationships within the data much more easily than just looking at the numbers.</p>
</blockquote>
",41,"2010-07-19 20:03:34",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,44,NULL,NULL,NULL
85,2,NULL,"2010-07-19 20:08:00",15,NULL,"<p>A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as "state", and then the random variable is a function of the state.</p>

<p>Example:  Suppose we have three dice rolls (D1,D2,D3).  Then the state S=(D1,D2,D3).  One random variable X is the number of 5s. X=(D1==5?)+(D2==5?)+(D3==5?).   Another random variable Y is the sum of the dice rolls Y=D1+D2+D3.  </p>
",87,"2010-07-28 14:54:34",NULL,NULL,NULL,2,NULL,87,"2010-07-28 14:54:34",NULL,50,NULL,NULL,NULL
86,2,NULL,"2010-07-19 20:08:37",8,NULL,"<p>Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather <strong>statistical properties</strong> such as the <strong>distribution</strong> of the random variable may be stated.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.</p>

<p>Random variables may be classified as <em>discrete</em> if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is <em>continuous</em> and is used if the distribution covers values from an uncountable set such as the real numbers.</p>
",13,"2010-07-20 17:46:55",NULL,NULL,NULL,5,NULL,13,"2010-07-20 17:46:55",NULL,50,NULL,NULL,NULL
89,2,NULL,"2010-07-19 20:11:47",5,NULL,"<p>When I teach very basic statistics to Secondary School Students I talk about evolution and how we have evolved to spot patterns in pictures rather than lists of numbers and that data visualisation is one of the techniques we use to take advantage of this fact. </p>

<p>Plus I try to talk about recent news stories where statistical insight contradicts what the press is implying, making use of sites like <a href="http://www.gapminder.org/">Gapminder</a> to find the representation before choosing the story.</p>
",55,"2010-07-19 20:11:47",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,44,NULL,NULL,NULL
90,2,NULL,"2010-07-19 20:12:24",3,NULL,"<p><a href="http://www.r-bloggers.com/" rel="nofollow">R bloggers</a> has been steadily supplying me with a lot of good pragmatic content.<br>
From the author:</p>

<pre><code>R-Bloggers.com is a central hub (e.g: A blog aggregator) of content 
collected from bloggers who write about R (in English). 
The site will help R bloggers and users to connect and follow 
the “R blogosphere”.
</code></pre>
",22,"2010-07-19 20:12:24",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-19 11:13:59",75,NULL,NULL,NULL
91,2,NULL,"2010-07-19 20:15:54",10,NULL,"<p>As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. </p>

<p>In the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta "functions" can be used to represent these atoms.    </p>
",87,"2010-07-20 08:52:27",NULL,NULL,NULL,0,NULL,87,"2010-07-20 08:52:27",NULL,23,NULL,NULL,NULL
93,1,NULL,"2010-07-19 20:17:07",5,412,"<p>We're trying to use a Gaussian process to model h(t) -- the hazard function -- for a very small initial population, and then fit that using the available data.  While this gives us nice plots for credible sets for h(t) and so on, it unfortunately is also just pushing the inference problem from h(t) to the covariance function of our process.  Perhaps predictably, we have several reasonable and equally defensible guesses for this that all produce different result.  </p>

<p>Has anyone run across any good approaches for addressing such a problem?  Gaussian-process related or otherwise?</p>
",61,"2010-09-16 12:33:50","Robust nonparametric estimation of hazard/survival functions based on low count data",<nonparametric><survival><hazard>,1,3,3,88,"2010-09-16 12:33:50",NULL,NULL,NULL,NULL,NULL
94,2,NULL,"2010-07-19 20:18:24",5,NULL,"<p>Quick R site is basic, but quite nice for start <a href="http://www.statmethods.net/index.html">http://www.statmethods.net/index.html</a> . </p>
",88,"2010-07-19 20:18:24",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-19 11:13:59",75,NULL,NULL,NULL
95,1,NULL,"2010-07-19 20:21:35",5,446,"<p>I have been using various GARCH-based models to forecast volatility for various North American equities using historical daily data as inputs.</p>

<p>Asymmetric GARCH models are often cited as a modification of the basic GARCH model to account for the 'leverage effect' i.e. volatility tends to increase more after a negative return than a similarly sized positive return.</p>

<p>What kind of a difference would you expect to see between a standard GARCH and an asymmetric GARCH forecast for a broad-based equity index like the S&amp;P 500 or the NASDAQ-100?</p>

<p>There is nothing particularly special about these two indices, but I think it is helpful to give something concrete to focus the discussion, as I am sure the effect would be different depending on the equities used.</p>
",57,"2012-07-26 19:47:59","How Large a Difference Can Be Expected Between Standard GARCH and Asymmetric GARCH Volatility Forecasts?",<time-series><garch><volatility-forecasting><finance>,2,0,2,154,"2010-07-20 06:14:40",NULL,NULL,NULL,NULL,NULL
96,2,NULL,"2010-07-19 20:22:44",4,NULL,"<p>Another great resource is <a href="http://learnr.wordpress.com/" rel="nofollow">the LearnR blog</a>, which went through an extensive study of visualizations with lattice and ggplot2.</p>
",5,"2010-07-19 20:22:44",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-19 11:13:59",75,NULL,NULL,NULL
97,1,104,"2010-07-19 20:23:22",35,14608,"<p>I have some <a href="http://en.wikipedia.org/wiki/Ordinal_scale#Ordinal_scale">ordinal data</a> gained from survey questions.  In my case they are <a href="http://en.wikipedia.org/wiki/Likert_scale">Likert style</a> responses (Strongly Disagree-Disagree-Neutral-Agree-Strongly Agree).  In my data they are coded as 1-5.</p>

<p>I  don't think means would mean much here, so what basic summary statistics are considered usefull?</p>
",114,"2014-07-31 01:06:31","What are good basic statistics to use for ordinal data?",<summary-statistics><likert><ordinal>,8,1,14,88,"2011-03-31 07:30:20",NULL,NULL,NULL,NULL,NULL
98,2,NULL,"2010-07-19 20:23:57",10,NULL,"<p>Eliciting priors is a tricky business. </p>

<p><a href="http://www.stat.cmu.edu/tr/tr808/tr808.pdf">Statistical Methods for Eliciting Probability Distributions</a> and <a href="http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf">Eliciting Probability Distributions</a> are quite good practical guides for prior elicitation.  The process in both papers is outlined as follows:</p>

<ol>
<li>background and preparation;</li>
<li>identifying and recruiting the expert(s);</li>
<li>motivation and training the expert(s);</li>
<li>structuring and decomposition (typically deciding precisely what variables should
be elicited, and how to elicit joint distributions in the multivariate case);</li>
<li>the elicitation itself.</li>
</ol>

<p>Of course, they also review how the elicitation results in information that may be fit to or otherwise define distributions (for instance, in the Bayesian context, <a href="http://en.wikipedia.org/wiki/Beta_distribution">Beta distributions</a>), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow and small-tailed distributions, etc.).</p>
",39,"2010-07-19 20:56:56",NULL,NULL,NULL,0,NULL,39,"2010-07-19 20:56:56",NULL,1,NULL,NULL,NULL
99,2,NULL,"2010-07-19 20:25:08",7,NULL,"<p><a href="http://cran.r-project.org/web/packages/multicore/index.html" rel="nofollow">multicore</a> is quite nice for tool for making faster scripts faster.<br>
<a href="http://cran.r-project.org/web/packages/cacheSweave/index.html" rel="nofollow">cacheSweave</a> saves a lot of time when using <code>Sweave</code>.</p>
",88,"2010-07-19 20:25:08",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-19 20:25:08",73,NULL,NULL,NULL
100,1,110,"2010-07-19 20:30:23",4,3918,"<p>I'd like to see the answer with qualitative view on the problem, not just definition. Examples and analogous from other areas of applied math also would be good.</p>

<p>I understand, my question is silly, but I can't find good and intuitive introduction textbook on signal processing — if someone would suggest one, I will be happy.</p>

<p>Thanks.</p>
",117,"2011-06-07 12:25:23","What's the purpose of window function in spectral analysis?",<signal-processing>,2,0,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
101,2,NULL,"2010-07-19 20:32:08",40,NULL,"<p>Understanding p-value</p>

<p>Suppose, that you want to test the hypothesis that the average height of male students at your University is 5 ft 7 inches. You collect heights of 100 students selected at random and compute the sample mean (say it turns out to be 5 ft 9 inches). Using an appropriate formula/statistical routine you compute the p-value for your hypothesis and say it turns out to be 0.06. </p>

<p>In order to interpret p=0.06 appropriately, we should keep several things in mind:</p>

<ol>
<li><p>The first step under classical hypothesis testing is the assumption that the hypothesis under consideration is true. (In our context, we assume that the <strong>true</strong> average height is 5 ft 7 inches.</p></li>
<li><p>Imagine doing the following calculation: Compute the probability that the sample mean is greater than 5 ft 9 inches assuming that our hypothesis is in fact correct (see point 1). </p></li>
</ol>

<p>In other words, we want to know P(Sample mean >= 5 ft 9 inches | Given true value = 5 ft 7 inches). </p>

<p>The calculation in step 2 is what is called the p-value. Therefore, a p-value of 0.06 would mean that if we were to repeat our experiment many, many times (each time we select 100 students at random and compute the sample mean) then 6 times out of 100 we can expect to see a sample mean greater than 5 ft 9 inches.</p>

<p>Given the above understanding, should we still retain our assumption that our hypothesis is true (see step 1). Well, a p=0.06 indicates that one of two things have happened: </p>

<p>(A) Either our hypothesis is correct and an extremely unlikely event has occurred (e.g., all 100 students are student athletes)</p>

<p>or</p>

<p>(B) Our assumption is incorrect and the sample we have obtained is not that unusual. </p>

<p>The traditional way to choose between (A) and (B) is to choose an arbitrary cut-off for p. We choose (A) if p > 0.05 and (B) if p &lt; 0.05.</p>

<p>PS: Need to run now. Will add about the t-test when I find some more time.</p>
",NULL,"2010-07-19 20:32:08",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,31,NULL,user28,NULL
102,2,NULL,"2010-07-19 20:32:14",1,NULL,"<p>For me <a href="http://nvac.pnl.gov/agenda.stm">Illuminating the Path</a> report has been always good point of reference.<br>
For more recent overview you can also have a look at good <a href="http://queue.acm.org/detail.cfm?id=1805128">article</a> by Heer and colleagues.</p>

<p>But what would explain better than visualization itself?</p>

<p><img src="http://i.stack.imgur.com/uFvje.jpg" alt="alt text"></p>

<p>(<a href="http://blog.ffctn.com/what-is-data-visualization">Source</a>)</p>
",22,"2010-11-23 20:42:01",NULL,NULL,NULL,0,NULL,22,"2010-11-23 20:42:01",NULL,44,NULL,NULL,NULL
103,1,NULL,"2010-07-19 20:33:26",28,1990,"<p>What is the best blog on data visualization?</p>

<p>I'm making this question a community wiki since it is highly subjective.  Please limit each answer to one link.</p>
",5,"2012-10-24 15:02:27","What is your favorite data visualization blog?",<data-visualization><blog>,16,6,37,88,"2012-01-22 20:18:39","2010-07-19 20:33:26",NULL,NULL,NULL,NULL
104,2,NULL,"2010-07-19 20:34:46",16,NULL,"<p>A frequency table is a good place to start. You can do the count, and relative frequency for each level. Also, the total count, and number of missing values may be of use. </p>

<p>You can also use a contingency table to compare two variables at once. Can display using a mosaic plot too.</p>
",74,"2010-07-19 20:34:46",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,97,NULL,NULL,NULL
105,2,NULL,"2010-07-19 20:35:34",19,NULL,"<p><a href="http://flowingdata.com/" rel="nofollow">FlowingData</a> | Data Visualization, Infographics, and Statistics</p>
",46,"2012-10-24 14:50:25",NULL,NULL,NULL,1,NULL,615,"2012-10-24 14:50:25","2010-07-19 20:35:34",103,NULL,NULL,NULL
107,2,NULL,"2010-07-19 20:36:01",11,NULL,"<p><a href="http://infosthetics.com/" rel="nofollow">information aesthetics</a> - Data Visualization &amp; Information Design</p>
",46,"2012-10-24 14:52:13",NULL,NULL,NULL,0,NULL,615,"2012-10-24 14:52:13","2010-07-19 20:36:01",103,NULL,NULL,NULL
108,2,NULL,"2010-07-19 20:36:47",14,NULL,"<p><a href="http://www.informationisbeautiful.net/" rel="nofollow">Information Is Beautiful</a> | Ideas, issues, knowledge, data - visualized!</p>
",5,"2012-10-24 14:51:28",NULL,NULL,NULL,1,NULL,615,"2012-10-24 14:51:28","2010-07-19 20:36:47",103,NULL,NULL,NULL
109,1,NULL,"2010-07-19 20:37:21",2,3980,"<p>Following one-way ANOVA, there are many possible follow-up multiple comparison tests. Holm's test (or better, the Holm-Sidak) test has lots of power, but because it works in a stepwise manner, it cannot compute confidence intervals. Its advantage over the tests than can compute confidence intervals (Tukey, Dunnett) is that is has more power. But is it fair to say that the Holm method <em>always</em> has more power than the methods of Tukey and Dunnet? Or does it depend...?</p>
",25,"2010-08-04 14:38:22","Power of Holm's multiple comparison testing compared to others",<multiple-comparisons><power>,1,2,1,NULL,NULL,NULL,NULL,"2010-08-05 13:06:12",NULL,NULL
110,2,NULL,"2010-07-19 20:40:36",3,NULL,"<p>It depends on where you apply the window function.  If you do it in the time domain, it's because you only want to analyze the periodic behavior of the function in a short duration.  You do this when you don't believe that your data is from a stationary process.  </p>

<p>If you do it in the frequency domain, then you do it to isolate a specific set of frequencies for further analysis; you do this when you believe that (for instance) high-frequency components are spurious.</p>

<p>The first three chapters of "A Wavelet Tour of Signal Processing" by Stephane Mallat have an excellent introduction to signal processing in general, and chapter 4 goes into a very good discussion of windowing and time-frequency representations in both continuous and discrete time, along with a few worked-out examples.</p>
",61,"2010-07-19 20:40:36",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,100,NULL,NULL,NULL
111,2,NULL,"2010-07-19 20:41:24",8,NULL,"<p>In R, the default setting for random number generation are:</p>

<ol>
<li>For U(0,1), use the Mersenne-Twister algorithm</li>
<li>For Guassian numbers use  the numerical inversion of the standard normal distribution function. </li>
</ol>

<p>You can easily check this, viz.</p>

<pre><code>&gt; RNGkind()
[1] "Mersenne-Twister" "Inversion"
</code></pre>

<p>It is possible to change the default generator to other PRNGs, such as Super-Duper,Wichmann-Hill, Marsaglia-Multicarry or even a user-supplied PRNG. See the ?RNGkind for further details. I have never needed to change the default PRNG.</p>

<p>The <a href="http://www.gnu.org/software/gsl/manual/html_node/Random-number-environment-variables.html">C GSL</a> library also uses the <a href="http://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html">Mersenne-Twister</a> by default.</p>
",8,"2010-07-19 20:41:24",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,40,NULL,NULL,NULL
112,2,NULL,"2010-07-19 20:52:27",2,NULL,"<p>I see all my favorite blogs have been listed. So I'll give you this one:</p>

<p><a href="http://ilovecharts.tumblr.com/" rel="nofollow">I Love Charts</a></p>

<p>It's a bit light hearted.</p>
",12,"2012-10-24 14:59:02",NULL,NULL,NULL,0,NULL,615,"2012-10-24 14:59:02","2010-07-19 20:52:27",103,NULL,NULL,NULL
113,1,NULL,"2010-07-19 20:54:23",9,413,"<p>I have been looking into theoretical frameworks for method selection (note: not model selection) and have found very little systematic, mathematically-motivated work. By 'method selection', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type.</p>

<p>What I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. <a href="http://portal.acm.org/citation.cfm?id=218546">Inductive Policy: The Pragmatics of Bias Selection</a>). I may be unrealistic at this early stage of machine learning's development, but I was hoping to find something like what <a href="ftp://ftp.sas.com/pub/neural/measurement.html">measurement theory</a> does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems.</p>

<p>Any suggestions?</p>
",39,"2010-10-08 23:57:02","What are some good frameworks for method selection?",<machine-learning><methodology><theory>,1,5,6,39,"2010-07-21 15:44:07",NULL,NULL,NULL,NULL,NULL
114,1,NULL,"2010-07-19 21:00:53",30,1220,"<p>What statistical research blogs would you recommend, and why?</p>
",8,"2014-06-02 12:33:11","What statistical blogs would you recommend?",<blog>,13,1,22,32036,"2014-06-01 21:15:49","2010-07-19 21:18:51",NULL,NULL,NULL,NULL
115,2,NULL,"2010-07-19 21:01:35",1,NULL,"<p><a href="http://datavis.tumblr.com/" rel="nofollow">We Love Datavis</a>, a data visualization tumblog.</p>
",127,"2012-10-24 15:02:27",NULL,NULL,NULL,0,NULL,615,"2012-10-24 15:02:27","2010-07-19 21:01:35",103,NULL,NULL,NULL
116,2,NULL,"2010-07-19 21:04:16",12,NULL,"<p>Cosma Shalizi's <a href="http://www.cscs.umich.edu/~crshalizi/weblog/">blog</a>, often talks about statistics, and is always interesting.</p>
",72,"2010-07-19 21:04:16",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,114,NULL,NULL,NULL
117,2,NULL,"2010-07-19 21:04:24",25,NULL,"<p><a href="http://www.r-bloggers.com/">http://www.r-bloggers.com/</a> is an aggregated blog from lots of blogs that talk about statistics using R, and the <a href="http://search.twitter.com/search?q=%23rstats">#rstats</a> hashtag on twitter is also helpful. I write quite a bit about <a href="http://gettinggeneticsdone.blogspot.com/search/label/R">statistics and R in genetics research</a>.</p>
",36,"2010-07-20 15:13:37",NULL,NULL,NULL,1,NULL,36,"2010-07-20 15:13:37",NULL,114,NULL,NULL,NULL
118,1,151,"2010-07-19 21:04:39",122,39118,"<p>In the definition of standard deviation, why do we have to <strong>square</strong> the difference from the mean to get the mean (E) and take the <strong>square root back</strong> at the end? Can't we just simply take <strong>the absolute value</strong> of the difference instead and get the expected value (mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method (the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard?</p>

<p>The definition of standard deviation:</p>

<p>$\\sigma = \\sqrt{E\\left[\\left(X - \\mu\\right)^2\\right]}.$</p>

<p>Can't we just take the absolute value instead and still be a good measurement?</p>

<p>$\\sigma = E\\left[|X - \\mu|\\right]$</p>
",83,"2014-07-31 17:00:06","Why square the difference instead of taking the absolute value in standard deviation?",<standard-deviation><definition>,19,11,69,88,"2011-07-28 16:42:05",NULL,NULL,NULL,NULL,NULL
119,2,NULL,"2010-07-19 21:11:44",6,NULL,"<p>There are many reasons; probably the main is that it works well as parameter of normal distribution.</p>
",88,"2013-04-27 14:09:42",NULL,NULL,NULL,3,NULL,88,"2013-04-27 14:09:42",NULL,118,NULL,NULL,NULL
120,2,NULL,"2010-07-19 21:14:07",39,NULL,"<p>One way you can think of this is that standard deviation is similar to a "distance from the mean".  </p>

<p>Compare this to distances in euclidean space - this gives you the true distance, where what you suggested (which, btw, is the <a href="http://en.wikipedia.org/wiki/Average_absolute_deviation">absolute deviation</a>) is more like a <a href="http://en.wikipedia.org/wiki/Manhattan_distance_transform">manhattan distance</a> calculation.</p>
",41,"2010-07-19 21:14:07",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,118,NULL,NULL,NULL
121,2,NULL,"2010-07-19 21:14:25",71,NULL,"<p>The squared difference has nicer mathematical properties; it's continuously differentiable (nice when you want to minimize it), it's a sufficient statistic for the Gaussian distribution, and it's (a version of) the L2 norm which comes in handy for proving convergence and so on.</p>

<p>The mean absolute deviation (the absolute value notation you suggest) is also used as a measure of dispersion, but it's not as "well-behaved" as the squared error.</p>
",61,"2010-07-19 21:14:25",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,118,NULL,NULL,NULL
123,2,NULL,"2010-07-19 21:15:20",13,NULL,"<p>Squaring the difference from the mean has a couple of reasons.</p>

<ul>
<li><p>Variance is defined as the 2nd moment of the deviation (the R.V here is (x-$\\mu$) ) and thus the square as moments are simply the expectations of higher powers of the random variable.</p></li>
<li><p>Having a square as opposed to the absolute value function gives a nice continuous and differentiable function (absolute value is not differentiable at 0) - which makes it the natural choice, especially in the context of estimation and regression analysis.</p></li>
<li><p>The squared formulation also naturally falls out of parameters of the Normal Distribution. </p></li>
</ul>
",130,"2010-07-19 21:15:20",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,118,NULL,NULL,NULL
124,1,NULL,"2010-07-19 21:17:30",23,1131,"<p>I'm a programmer without statistical background, and I'm currently looking at different classification methods for a large number of different documents that I want to classify into pre-defined categories. I've been reading about kNN, SVM and NN. However, I have some trouble getting started. What resources do you recommend? I do know single variable and multi variable calculus quite well, so my math should be strong enough. I also own Bishop's book on Neural Networks, but it has proven to be a bit dense as an introduction.</p>
",131,"2013-10-09 16:22:48","Statistical classification of text",<classification><information-retrieval><text-mining>,7,0,22,88,"2010-07-21 22:17:00",NULL,NULL,NULL,NULL,NULL
125,1,NULL,"2010-07-19 21:18:12",75,29261,"<p>Which is the best introductory textbook for Bayesian statistics?</p>

<p>One book per answer, please.</p>
",5,"2014-07-18 21:06:01","What is the best introductory Bayesian statistics textbook?",<bayesian><books>,26,2,78,88,"2012-01-22 20:18:28","2010-07-19 21:18:12",NULL,NULL,NULL,NULL
126,2,NULL,"2010-07-19 21:19:43",26,NULL,"<p>My favorite is <a href="http://www.amazon.com/exec/obidos/ISBN=158488388X/">"Bayesian Data Analysis"</a> by Gelman, et al.</p>
",5,"2010-07-19 21:19:43",NULL,NULL,NULL,4,NULL,NULL,NULL,"2010-07-19 21:19:43",125,NULL,NULL,NULL
127,2,NULL,"2010-07-19 21:23:20",18,NULL,"<p>Another vote for Gelman et al., but a close second for me -- being of the learn-by-doing persuasion -- is <a href="http://bayes.bgsu.edu/bcwr/">Bayesian Computation with R</a>.</p>
",61,"2010-07-19 21:23:20",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-19 21:23:20",125,NULL,NULL,NULL
128,1,191,"2010-07-19 21:23:57",3,12626,"<p>In Plain English, how does one interpret a Bland-Altman plot?  </p>

<p>What are the advantages of using a Bland-Altman plot over other methods of comparing two different measurement methods?</p>
",132,"2011-04-13 08:21:54","How does one interpret a Bland-Altman plot?",<data-visualization>,1,0,4,88,"2011-04-13 08:21:54",NULL,NULL,NULL,NULL,NULL
129,2,NULL,"2010-07-19 21:24:58",4,NULL,"<p>I quite like <a href="http://rads.stackoverflow.com/amzn/click/1584885874" rel="nofollow">Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference</a> by Gamerman and Lopes.</p>
",8,"2010-10-05 13:56:15",NULL,NULL,NULL,0,NULL,8,"2010-10-05 13:56:15","2010-07-19 21:24:58",125,NULL,NULL,NULL
130,1,131,"2010-07-19 21:26:27",30,7111,"<p>I had a plan of learning R in the near future. Reading <a href="http://stats.stackexchange.com/questions/3/what-are-some-valuable-statistical-analysis-open-source-projects" rel="nofollow">another question</a> I found out about Clojure. Now I don't know what to do.</p>

<p>I think a big <strong>advantage of R</strong> for me is that some people in Economics use it, including one of my supervisors (though the other said: stay away from R!). One <strong>advantage of Clojure</strong> is that it is Lisp-based, and as I have started learning Emacs and I am keen on writing my own customisations, it would be helpful (yeah, I know Clojure and Elisp are different dialects of Lisp, but they are both Lisp and thus similar I would imagine).</p>

<p>I can't ask which one is better, because I know this is very personal, but could someone give me the advantages (or advantages) of Clojure x R, especially in practical terms? For example, which one should be easier to learn, which one is more flexible or more powerful, which one has more libraries, more support, more users, etc?</p>

<p><strong>My intended use</strong>: The bulk of my estimation should be done using Matlab, so I am not looking for anything too deep in terms of statistical analysis, but rather a software to substitute Excel for the initial data manipulation and visualisation, summary statistics and charting, but also some basic statistical analysis or the initial attempts at my estimation.</p>
",90,"2014-08-01 17:38:23","Clojure versus R: advantages and disadvantages for data analysis",<r>,3,10,12,88,"2010-08-07 18:00:44",NULL,NULL,NULL,NULL,NULL
131,2,NULL,"2010-07-19 21:28:41",23,NULL,"<p>Let me start by saying that I love both languages: you can't go wrong with either, and they are certainly better than something like C++ or Java for doing data analysis.</p>

<p>For basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as <a href="http://mitpress.mit.edu/sicp/">SICP</a>).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.  Also, keep in mind that R (or S) is largely derived from Scheme, so you would benefit from Lisp knowledge when using it.</p>

<p>In general:</p>

<p>The main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.</p>

<p>Clojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.</p>

<p>I would add that I gave <a href="http://bit.ly/dhDZkp">a talk relating Clojure/Incanter to R</a> a while ago, so you may find it of interest.  In my experience around creating this, Clojure was generally slower than R for simple operations.</p>
",5,"2010-07-26 14:06:09",NULL,NULL,NULL,0,NULL,5,"2010-07-26 14:06:09",NULL,130,NULL,NULL,NULL
132,2,NULL,"2010-07-19 21:29:37",6,NULL,"<p>Coming from non-statistical background I found <a href="http://www.princeton.edu/~slynch/bayesbook/bookinfo.html">Introduction to Applied Bayesian Statistics and Estimation for Social Scientists</a> quite informative and easy to follow.</p>
",22,"2010-07-19 21:29:37",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 21:29:37",125,NULL,NULL,NULL
133,2,NULL,"2010-07-19 21:31:53",0,NULL,"<p>I don't know how to use SAS/R/Orange, but it sounds like the kind of test you need is a <a href="http://en.wikipedia.org/wiki/Chi-square_test" rel="nofollow">chi-square test</a>. </p>
",139,"2010-07-19 21:31:53",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,4,NULL,NULL,NULL
134,1,3449,"2010-07-19 21:32:38",10,4572,"<p>On smaller window sizes, <code>n log n</code> sorting might work. Are there any better algorithms to achieve this?</p>
",138,"2013-10-31 23:55:13","Algorithms to compute the running median?",<algorithms><median>,8,6,7,8,"2010-08-03 12:14:50",NULL,NULL,NULL,NULL,NULL
135,2,NULL,"2010-07-19 21:36:12",7,NULL,"<p>I believe that this calls for a <a href="http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm">two-sample Kolmogorov–Smirnov test</a>, or the like.  The two-sample Kolmogorov–Smirnov test is based on comparing differences in the <a href="http://en.wikipedia.org/wiki/Empirical_distribution_function">empirical distribution functions</a> (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples.  It also generalizes out to a multivariate form.</p>

<p>This test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them (e.g. <a href="http://cran.r-project.org/web/packages/fBasics/fBasics.pdf">fBasics</a>), and run it on your sample data.</p>
",39,"2010-07-19 21:52:08",NULL,NULL,NULL,3,NULL,39,"2010-07-19 21:52:08",NULL,4,NULL,NULL,NULL
137,2,NULL,"2010-07-19 21:38:09",15,NULL,"<p>I recommend these books - they are highly rated on Amazon too:</p>

<p>"Text Mining" by Weiss</p>

<p>"Text Mining Application Programming", by Konchady</p>

<p>For software, I recommend RapidMiner (with the text plugin), free and open-source.</p>

<p>This is my "text mining process":</p>

<pre><code>* collect the documents (usually a web crawl)
      o [sample if too large]
      o timestamp
      o strip out markup
* tokenize: break into characters, words, n-grams, or sliding windows
* stemming (aka lemmatization)
      o [include synonyms]
      o see porter or snowflake algorithm
      o pronouns and articles are usually bad predictors
* remove stopwords
* feature vectorization
      o binary (appears or doesn’t)
      o word count
      o relative frequency: tf-idf
      o information gain, chi square
      o [have a minimum value for inclusion]
* weighting
      o weight words at top of document higher?
</code></pre>

<p>Then you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. </p>

<p>You can see my series of text mining videos <a href="http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html" rel="nofollow">here</a></p>
",74,"2013-01-17 18:54:28",NULL,NULL,NULL,2,NULL,74,"2013-01-17 18:54:28",NULL,124,NULL,NULL,NULL
138,1,1213,"2010-07-19 21:38:10",67,14326,"<p>I'm interested in learning <a href="http://en.wikipedia.org/wiki/R_%28programming_language%29">R</a> on the cheap. What's the best free resource/book/tutorial for learning R?</p>
",142,"2014-08-11 11:14:36","Resources for learning R",<r>,21,4,111,142,"2013-03-31 01:01:08","2010-08-01 18:56:25",NULL,"2014-02-12 01:23:48",NULL,NULL
139,2,NULL,"2010-07-19 21:39:17",21,NULL,"<p>If I had to choose one thing, make sure that you read <a href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">"The R Inferno"</a>.</p>

<p>There are many good resources on <a href="http://www.r-project.org">the R homepage</a>, but in particular, read <a href="http://cran.r-project.org/doc/manuals/R-intro.pdf">"An Introduction to R"</a> and <a href="http://cran.r-project.org/doc/manuals/R-lang.pdf">"The R Language Definition"</a>.</p>
",5,"2010-07-19 21:39:17",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
140,2,NULL,"2010-07-19 21:39:35",7,NULL,"<p>The official guides are pretty nice; check out <a href="http://cran.r-project.org/manuals.html">http://cran.r-project.org/manuals.html</a> . There is also a lot of contributed documentation there.</p>
",88,"2010-07-19 21:39:35",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
141,2,NULL,"2010-07-19 21:40:02",2,NULL,"<p>Light-hearted: <a href="http://thisisindexed.com/" rel="nofollow">Indexed</a></p>

<p>Also, see older visualizations from the same creator at the original <a href="http://indexed.blogspot.com/" rel="nofollow">Indexed Blog</a>.</p>
",142,"2012-10-24 14:58:17",NULL,NULL,NULL,0,NULL,615,"2012-10-24 14:58:17","2010-07-19 21:40:02",103,NULL,NULL,NULL
142,2,NULL,"2010-07-19 21:42:57",5,NULL,"<p>After you learn the basics, I find the following sites very useful:</p>

<ol>
<li><a href="http://www.r-bloggers.com/">R-bloggers</a>. </li>
<li>Subscribing to the <a href="http://stackoverflow.com/questions/tagged/R">Stack overflow R tag</a>.</li>
</ol>
",8,"2010-07-19 21:42:57",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
143,2,NULL,"2010-07-19 21:48:28",5,NULL,"<p>Neural network may be to slow for a large number of documents (also this is now pretty much obsolete).<br>
And you may also check Random Forest among classifiers; it is quite fast, scales nice and does not need complex tuning.</p>
",88,"2010-07-19 21:48:28",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,124,NULL,NULL,NULL
144,2,NULL,"2010-07-19 21:48:52",17,NULL,"<p><a href="http://www.statmethods.net/index.html">Quick-R</a> can be a good place to start.</p>

<p>A little bit data mining oriented <a href="http://www.rdatamining.com">R and Data Mining</a> resources: <a href="http://www.rdatamining.com/docs/Rdatamining.pdf">Examples and Case Studies</a> and <a href="http://www.rdatamining.com/docs/R-refcard-data-mining.pdf">R Reference Card for Data Mining</a>. </p>
",22,"2011-04-01 15:12:01",NULL,NULL,NULL,0,NULL,22,"2011-04-01 15:12:01","2010-08-01 18:56:25",138,NULL,NULL,NULL
145,1,147,"2010-07-19 21:50:16",4,2287,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href="http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples">Locating freely available data samples</a>  </p>
</blockquote>



<p>Where can I find freely accessible data sources?</p>

<p>I'm thinking of sites like</p>

<ul>
<li><a href="http://www2.census.gov/census_2000/datasets/" rel="nofollow">http://www2.census.gov/census_2000/datasets/</a>?</li>
</ul>
",138,"2010-08-30 15:02:00","Free Dataset Resources?",<dataset>,5,4,2,442,"2010-08-30 15:02:00","2010-07-19 21:50:44",NULL,"2010-07-20 00:21:48",NULL,NULL
146,1,149,"2010-07-19 21:52:51",12,4419,"<p>A while ago a user on R-help mailing list asked about the soundness of using PCA scores in a regression. The user is trying to use some PC scores to explain variation in another PC (see full discussion <a href="http://r.789695.n4.nabble.com/PCA-and-Regression-td2280038.html" rel="nofollow">here</a>). The answer was that no, this is not sound because PCs are orthogonal to each other. Can someone explain in a bit more detail why this is so?</p>
",144,"2012-01-04 06:55:32","PCA scores in multiple regression",<r><pca><scores><regression>,6,3,6,919,"2011-12-19 18:45:44",NULL,NULL,NULL,NULL,NULL
147,2,NULL,"2010-07-19 21:53:02",4,NULL,"<p>Amazon has free Public Data sets for use with EC2. </p>

<p><a href="http://aws.amazon.com/publicdatasets/" rel="nofollow">http://aws.amazon.com/publicdatasets/</a></p>

<p>Here's a list: <a href="http://developer.amazonwebservices.com/connect/kbcategory.jspa?categoryID=243" rel="nofollow">http://developer.amazonwebservices.com/connect/kbcategory.jspa?categoryID=243</a></p>
",142,"2010-07-19 21:53:02",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 21:53:02",145,NULL,NULL,NULL
148,2,NULL,"2010-07-19 21:58:51",4,NULL,"<p><a href="http://infochimps.org/" rel="nofollow">http://infochimps.org/</a> - is a good resource for free data sets.</p>
",130,"2010-07-19 21:58:51",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 21:58:51",145,NULL,NULL,NULL
149,2,NULL,"2010-07-19 22:02:10",8,NULL,"<p>A principal component is a weighted linear combination of all your factors (X's).</p>

<p>example: PC1 = 0.1X1 + 0.3X2</p>

<p>There will be one component for each factor (though in general a small number are selected). </p>

<p>The components are created such that they have zero correlation (are orthogonal), by design.</p>

<p>Therefore, component PC1 should not explain any variation in component PC2.</p>

<p>You may want to do regression on your Y variable and the PCA representation of your X's, as they will not have multi-collinearity. However, this could be hard to interpret.</p>

<p>If you have more X's than observations, which breaks OLS, you can regress on your components, and simply select a smaller number of the highest variation components. </p>

<p><a href="http://rads.stackoverflow.com/amzn/click/0387954422" rel="nofollow">Principal Component Analysis</a> by Jollife a very in-depth and highly cited book on the subject</p>

<p>This is also good: <a href="http://www.statsoft.com/textbook/principal-components-factor-analysis/" rel="nofollow">http://www.statsoft.com/textbook/principal-components-factor-analysis/</a></p>
",74,"2012-01-04 06:55:32",NULL,NULL,NULL,0,NULL,74,"2012-01-04 06:55:32",NULL,146,NULL,NULL,NULL
150,2,NULL,"2010-07-19 22:13:29",3,NULL,"<p>For complete beginners, try William Briggs <a href="http://rads.stackoverflow.com/amzn/click/0557019907" rel="nofollow">Breaking the Law of Averages: Real-Life Probability and Statistics in Plain English</a></p>
",25,"2010-07-19 22:13:29",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-19 22:13:29",125,NULL,NULL,NULL
151,2,NULL,"2010-07-19 22:31:12",60,NULL,"<p>If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.</p>

<p>The benefits of squaring include:</p>

<ul>
<li>Squaring always gives a positive
value, so the sum will not be zero.</li>
<li>Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).</li>
</ul>

<p>Squaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.</p>

<p>I suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)</p>

<p><em><strong>It's important to note</em></strong> <em><strong>however</em></strong> that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.</p>

<p>My view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.</p>

<p>An much more indepth analysis can be read <a href="http://www.leeds.ac.uk/educol/documents/00003759.htm">here</a>.</p>
",81,"2010-07-20 14:56:51",NULL,NULL,NULL,4,NULL,81,"2010-07-20 14:56:51",NULL,118,NULL,NULL,NULL
152,1,1087,"2010-07-19 22:37:38",9,1008,"<p>Label switching (i.e., the posterior distribution is invariant to switching component labels) is a problematic issue when using MCMC to estimate mixture models. </p>

<ol>
<li><p>Is there a standard (as in widely accepted) methodology to deal with the issue?</p></li>
<li><p>If there is no standard approach then what are the pros and cons of the leading approaches to solve the label switching problem?</p></li>
</ol>
",NULL,"2011-03-27 16:03:35","Is there a standard method to deal with label switching problem in MCMC estimation of mixture models?",<bayesian><mcmc><mixture>,2,2,1,919,"2011-03-27 16:03:35",NULL,NULL,NULL,user28,NULL
153,2,NULL,"2010-07-19 22:39:27",6,NULL,"<p>The simple answer is that Likert scales are always ordinal. The intervals between positions on the scale are monotonic but never so well-defined as to be numerically uniform increments.</p>

<p>That said, the distinction between ordinal and interval is based on the specific demands of the analysis being performed. Under special circumstances, you may be able to treat the responses as if they fell on an interval scale. To do this, typically the respondents need to be in close agreement regarding the meaning of the scale responses and the analysis (or the decisions made based on the analysis) should be relatively insensitive to problems that may arise.</p>
",145,"2010-07-19 22:39:27",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,10,NULL,NULL,NULL
154,2,NULL,"2010-07-19 22:40:47",20,NULL,"<p>I am currently researching the <em>trial roulette method</em> for my masters thesis as an elicitation technique. This is a graphical method that allows an expert to represent her subjective probability distribution for an uncertain quantity.</p>

<p>Experts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up to 1 - for example 20 chips of probability = 0.05 each. They are then instructed to arrange them on a pre-printed grid, with bins representing result intervals. Each column would represent their belief of the probability of getting the corresponding bin result.</p>

<p>Example: A student is asked to predict the mark in a future exam. The
figure below shows a completed grid for the elicitation of
a subjective probability distribution. The horizontal axis of the
grid shows the possible bins (or mark intervals) that the student was
asked to consider. The numbers in top row record the number of chips
per bin. The completed grid (using a total of 20 chips) shows that the
student believes there is a 30% chance that the mark will be between
60 and 64.9.</p>

<p><img src="http://img641.imageshack.us/img641/4716/chipsbinscrisp.png" alt="Eliciting priors from experts - Trial Roulette Method"></p>

<p>Some reasons in favour of using this technique are:</p>

<ol>
<li><p>Many questions about the shape of the expert's subjective probability distribution can be answered without the need to pose a long series of questions to the expert - the statistician can simply read off density above or below any given point, or that between any two points. </p></li>
<li><p>During the elicitation process, the experts can move around the chips if unsatisfied with the way they placed them initially - thus they can be sure of the final result to be submitted. </p></li>
<li><p>It forces the expert to be coherent in the set of probabilities that are provided. If all the chips are used, the probabilities must sum to one.</p></li>
<li><p>Graphical methods seem to provide more accurate results, especially for participants with modest levels of statistical sophistication.</p></li>
</ol>
",108,"2010-09-03 17:46:44",NULL,NULL,NULL,3,NULL,108,"2010-09-03 17:46:44",NULL,1,NULL,NULL,NULL
155,1,NULL,"2010-07-19 22:43:50",27,3293,"<p>I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept?</p>

<p>My favorite is <a href="http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Co-integration/Murray93DrunkAndDog.pdf">Murray's</a> explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their joint first differences are stationary.</p>

<blockquote>
  <p>The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But
  periodically she intones "Oliver, where are you?", and Oliver interrupts his aimless
  wandering to bark. He hears her; she hears him. He thinks, "Oh, I can't let her get too far
  off; she'll lock me out." She thinks, "Oh, I can't let him get too far off; he'll wake
  me up in the middle of the night with his barking." Each assesses how far
  away the other is and moves to partially close that gap.</p>
</blockquote>
",154,"2013-10-23 15:29:05","What is your favorite layman's explanation for a difficult statistical concept?",<teaching><communication>,10,2,17,8489,"2012-04-04 16:22:03","2012-08-21 15:17:25",NULL,NULL,NULL,NULL
156,1,198,"2010-07-19 22:50:13",2,233,"<p>I know this must be standard material, but I had difficulty in finding a proof in this form.</p>

<p>Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.</p>

<p>Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let</p>

<p>$$\\left\\{\\begin{align}\\n\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\\n\\bar e &amp;= v - X\\bar y\\n\\end{align}\\right.\\quad.$$</p>

<p>If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and </p>

<p>$$\\left\\{\\begin{align}\\nu &amp;= c^T\\bar y\\\\\\ns^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\\n\\end{align}\\right.\\quad,$$</p>

<p>then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.</p>

<p>I would be grateful if you could provide an outline for its proof.</p>
",148,"2012-05-15 04:52:05","How to get to a t variable from linear regression",<regression>,1,0,1,10515,"2012-05-14 21:49:03",NULL,NULL,NULL,NULL,NULL
157,2,NULL,"2010-07-19 22:52:22",7,NULL,"<p>Definitely the Monty Hall Problem. <a href="http://en.wikipedia.org/wiki/Monty_Hall_problem">http://en.wikipedia.org/wiki/Monty_Hall_problem</a></p>
",36,"2010-07-19 22:52:22",NULL,NULL,NULL,3,NULL,NULL,NULL,"2012-08-21 15:17:25",155,NULL,NULL,NULL
159,2,NULL,"2010-07-19 23:00:30",8,NULL,"<p><a href="http://junkcharts.typepad.com/">Junk Charts</a> is always interesting and thought-provoking, usually providing both criticism of visualizations in the popular media and suggestions for improvements.</p>
",145,"2010-07-19 23:00:30",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 23:00:30",103,NULL,NULL,NULL
160,2,NULL,"2010-07-19 23:06:43",1,NULL,"<p><a href="http://dataspora.com/blog/" rel="nofollow">Dataspora</a>, a data science blog.</p>
",158,"2012-10-24 15:01:09",NULL,NULL,NULL,0,NULL,615,"2012-10-24 15:01:09","2010-07-19 23:06:43",103,NULL,NULL,NULL
161,1,NULL,"2010-07-19 23:11:36",9,2304,"<p>Econometricians often talk about a time series being <em>integrated with order k, I(k)</em>. <em>k</em> being the minimum number of differences required to obtain a stationary time series.</p>

<p>What methods or statistical tests can be used to determine, given a level of confidence, the <em>order of integration</em> of a time series?</p>
",154,"2010-07-20 11:14:41","What methods can be used to determine the Order of Integration of a time series?",<time-series>,2,0,2,159,"2010-07-19 23:39:49",NULL,NULL,NULL,NULL,NULL
162,2,NULL,"2010-07-19 23:13:32",14,NULL,"<ol>
<li><p>If you carved your distribution (histogram) out
of wood, and tried to balance it on
your finger, the balance point would
be the mean, no matter the shape of the distribution.</p></li>
<li><p>If you put a stick in the middle of
your scatter plot, and attached the
stick to each data point with a
spring, the resting point of the
stick would be your regression line. [1]</p></li>
</ol>

<p>[1] this would technically be principal components regression. you would have to force the springs to move only "vertically" to be least squares, but the example is illustrative either way. </p>
",74,"2012-04-09 08:18:37",NULL,NULL,NULL,4,NULL,74,"2012-04-09 08:18:37","2012-08-21 15:17:25",155,NULL,NULL,NULL
163,2,NULL,"2010-07-19 23:17:53",5,NULL,"<p><strong>Definition:</strong></p>

<p>A random variable is a measurable function from a probability space into a measurable space  known as the state space.</p>

<p><strong>Example:</strong></p>

<p>Lets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.</p>

<p>Whichever number the die lands on is the number of free text-books I will give you.</p>

<p>In this case, the <em>final amount</em> of free text books that I give you is the <strong>random variable</strong> because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die) and is not known before I roll the die.</p>
",81,"2010-07-20 14:37:39",NULL,NULL,NULL,6,NULL,81,"2010-07-20 14:37:39",NULL,50,NULL,NULL,NULL
164,2,NULL,"2010-07-19 23:19:44",2,NULL,"<p>For governmental data:</p>

<p>US: <a href="http://www.data.gov/" rel="nofollow">http://www.data.gov/</a></p>

<p>World: <a href="http://www.guardian.co.uk/world-government-data" rel="nofollow">http://www.guardian.co.uk/world-government-data</a></p>
",158,"2010-07-19 23:19:44",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 23:19:44",145,NULL,NULL,NULL
165,1,207,"2010-07-19 23:21:05",56,22890,"<p>Maybe the concept, why it's used, and an example.</p>
",74,"2013-10-02 12:17:36","How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?",<bayesian><mcmc><teaching>,8,3,47,2910,"2013-03-06 12:36:00","2010-08-16 06:37:30",NULL,NULL,NULL,NULL
166,1,NULL,"2010-07-19 23:21:35",10,12304,"<p>Australia is currently having an election and understandably the media reports new political poll results daily. In a country of 22 million what percentage of the population would need to be sampled to get a statistically valid result?</p>

<p>Is it possible that using too large a sample could affect the results, or does statistical validity monotonically increase with sample size?</p>
",154,"2012-09-07 18:25:29","How do you decide the sample size when polling a large population?",<sample-size><polling>,3,0,5,442,"2010-09-17 13:20:58",NULL,NULL,NULL,NULL,NULL
167,2,NULL,"2010-07-19 23:26:31",10,NULL,"<p>Principal components are orthogonal by definition, so any pair of PCs will have zero correlation.</p>

<p>However, PCA can be used in regression if there are a large number of explanatory variables. These can be reduced to a small number of principal components and used as predictors in a regression.</p>
",159,"2010-07-19 23:26:31",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,146,NULL,NULL,NULL
168,1,179,"2010-07-19 23:26:44",17,1022,"<p>For univariate kernel density estimators (KDE), I use Silverman's rule for calculating $h$:</p>

<p>\\begin{equation}
0.9 \\min(sd, IQR/1.34)\\times n^{-0.2}
\\end{equation}</p>

<p>What are the standard rules for multivariate KDE (assuming a Normal kernel).</p>
",8,"2011-03-02 12:11:19","Choosing a bandwidth for kernel density estimators",<smoothing><kde><kernel>,2,1,6,8,"2010-11-05 13:06:26",NULL,NULL,NULL,NULL,NULL
169,2,NULL,"2010-07-19 23:27:36",2,NULL,"<p>For time series data, try the <a href="http://robjhyndman.com/TSDL" rel="nofollow">Time Series Data Library</a>.</p>
",159,"2010-07-19 23:27:36",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-19 23:27:36",145,NULL,NULL,NULL
170,1,174,"2010-07-19 23:29:54",56,7733,"<p>Are there any free statistical textbooks available? </p>
",8,"2014-06-20 00:16:54","Free statistical textbooks",<teaching><books>,19,1,75,69,"2010-10-27 09:23:26","2010-07-21 22:02:27",NULL,NULL,NULL,NULL
171,2,NULL,"2010-07-19 23:32:30",8,NULL,"<p>There are a number of statistical tests (known as "unit root tests") for dealing with this problem. The most popular is probably the "Augmented Dickey-Fuller" (ADF) test, although the Phillips-Perron (PP) test and the KPSS test are also widely used. </p>

<p>Both the ADF and PP tests are based on a null hypothesis of a unit root (i.e., an I(1) series). The KPSS test is based on a null hypothesis of stationarity (i.e., an I(0) series). Consequently, the KPSS test can give quite different results from the ADF or PP tests.</p>
",159,"2010-07-19 23:32:30",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,161,NULL,NULL,NULL
172,2,NULL,"2010-07-19 23:34:18",10,NULL,"<p>Sample size doesn't much depend on the population size, which is counter-intuitive to many.</p>

<p>Most polling companies use 400 or 1000 people in their samples.</p>

<p>There is a reason for this:</p>

<p>A sample size of 400 will give you a confidence interval of +/-5% 19 times out of 20 (95%)</p>

<p>A sample size of 1000 will give you a confidence interval of +/-3% 19 times out of 20 (95%)</p>

<p>When you are measuring a proportion near 50% anyways. </p>

<p>This calculator isn't bad:</p>

<p><a href="http://www.raosoft.com/samplesize.html">http://www.raosoft.com/samplesize.html</a></p>
",74,"2010-09-11 18:48:28",NULL,NULL,NULL,1,NULL,74,"2010-09-11 18:48:28",NULL,166,NULL,NULL,NULL
173,1,NULL,"2010-07-19 23:37:22",15,2519,"<p>I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc.  I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not.  Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:</p>

<p><img src="http://img827.imageshack.us/img827/1927/activetbcases.png" alt="alt text" title="Cases by month"></p>

<p><img src="http://img827.imageshack.us/img827/4348/tbcasedistribution.png" alt="alt text" title="Distribution of counts"></p>

<p>I've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.</p>

<p><strong>EDIT:</strong>  mbq's answer has forced me to think more carefully about what I'm asking here; I got too hung-up on the monthly counts and lost the actual focus of the question.  What I'd like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases?  It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable.  From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality.  How can I test if there's a real change in the process?  And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?</p>

<p>Whew.  Thanks for bearing with me.</p>
",71,"2013-02-21 20:25:38","Time series for count data, with counts < 20",<r><time-series><poisson><count-data><epidemiology>,9,0,11,71,"2010-07-21 00:25:04",NULL,NULL,NULL,NULL,NULL
174,2,NULL,"2010-07-19 23:37:43",35,NULL,"<p>The most widely used and probably the best of what is available is
<a href="http://www.statsoft.com/textbook/">http://www.statsoft.com/textbook/</a></p>

<p>Other online stats books include</p>

<ul>
<li><a href="http://davidmlane.com/hyperstat/">http://davidmlane.com/hyperstat/</a></li>
<li><a href="http://faculty.vassar.edu/lowry/webtext.html">http://faculty.vassar.edu/lowry/webtext.html</a></li>
<li><a href="http://www.psychstat.missouristate.edu/multibook2/mlt.htm">http://www.psychstat.missouristate.edu/multibook2/mlt.htm</a></li>
<li><a href="http://bookboon.com/uk/student/statistics">http://bookboon.com/uk/student/statistics</a></li>
<li><a href="http://www.freebookcentre.net/SpecialCat/Free-Statistics-Books-Download.html">http://www.freebookcentre.net/SpecialCat/Free-Statistics-Books-Download.html</a></li>
</ul>

<p>Update: I can now add my own forecasting textbook</p>

<ul>
<li><a href="http://otexts.com/fpp/">Forecasting: principles and practice (Hyndman &amp; Athanasopoulos, 2012)</a></li>
</ul>
",159,"2012-05-23 04:01:40",NULL,NULL,NULL,0,NULL,159,"2012-05-23 04:01:40",NULL,170,NULL,NULL,NULL
175,1,NULL,"2010-07-19 23:39:49",29,28291,"<p>Often times a statistical analyst is handed a set dataset and asked to fit a model using a technique such as linear regression.  Very frequently the dataset is accompanied with a disclaimer similar to "Oh yeah, we messed up collecting some of these data points -- do what you can".</p>

<p>This situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following:</p>

<ul>
<li><p>It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it "makes the fit look bad".</p></li>
<li><p>In real life, the people who collected the data are frequently not available to answer questions such as "when generating this data set, which of the points did you mess up, exactly?"</p></li>
</ul>

<p>What statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis?</p>

<p>Are there any special considerations for multilinear regression?</p>
",13,"2011-10-10 09:02:51","How should outliers be dealt with in linear regression analysis?",<regression><outliers>,8,1,24,159,"2010-08-13 12:59:06",NULL,NULL,NULL,NULL,NULL
176,2,NULL,"2010-07-19 23:40:01",18,NULL,"<p>Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book.</p>

<p>Then informally:</p>

<p>The <strong><em>Frequentist</em></strong> would say that each outcome has an equal 1 in 6 chance of occurring. She views probability as being derived from long run frequency distributions.</p>

<p>The <strong><em>Bayesian</em></strong> however would say hang on a second, I know that man, he's David Blaine, a famous trickster! I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on a 3  <em>BUT</em>  I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll iteratively increase the chance from 1% to something slightly higher, otherwise I'll reduce it even further. She views probability as degrees of belief in a proposition.</p>
",81,"2011-09-18 10:09:48",NULL,NULL,NULL,2,NULL,81,"2011-09-18 10:09:48",NULL,22,NULL,NULL,NULL
177,2,NULL,"2010-07-19 23:45:44",22,NULL,"<p>Rather than exclude outliers, you can use a robust method of regression. In R, for example, the <a href="http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/rlm.html"><code>rlm()</code> function from the MASS package</a> can be used instead of the <code>lm()</code> function. The method of estimation can be tuned to be more or less robust to outliers.</p>
",159,"2011-10-10 09:02:51",NULL,NULL,NULL,2,NULL,159,"2011-10-10 09:02:51",NULL,175,NULL,NULL,NULL
178,2,NULL,"2010-07-19 23:48:50",9,NULL,"<p><a href="http://rapid-i.com/" rel="nofollow">RapidMiner</a> for data and text mining</p>
",74,"2013-04-20 07:21:26",NULL,NULL,NULL,0,NULL,74,"2013-04-20 07:21:26","2010-07-19 23:48:50",3,NULL,NULL,NULL
179,2,NULL,"2010-07-19 23:59:29",12,NULL,"<p>For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example,</p>

<pre><code>plot(density(precip, bw="SJ"))
</code></pre>

<p>The situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The <a href="http://cran.r-project.org/web/packages/ks/">ks package in R</a> provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix.</p>
",159,"2010-07-19 23:59:29",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,168,NULL,NULL,NULL
180,2,NULL,"2010-07-20 00:06:20",4,NULL,"<p>I really like the <a href="http://research.stlouisfed.org/fred2/" rel="nofollow">FRED</a>, from the St. Louis Fed (economics data). You can chart the series or more than one series, you can do some transformations to your data and chart it, and the NBER recessions are shaded.</p>
",90,"2010-07-20 00:06:20",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 00:06:20",145,NULL,NULL,NULL
181,1,1097,"2010-07-20 00:15:02",50,39097,"<p>Is there a standard and accepted method for selecting the number of layers, and the number of nodes in each layer, in a FF NN? I'm interested in automated ways of building neural networks.</p>
",159,"2013-08-30 02:48:10","How to choose the number of hidden layers and nodes in a feedforward neural network?",<model-selection><neural-networks>,4,0,44,28988,"2013-08-30 01:49:40",NULL,NULL,NULL,NULL,NULL
182,2,NULL,"2010-07-20 00:15:47",16,NULL,"<p>Sometimes outliers are bad data, and should be excluded, such as typos. Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept. </p>

<p>Outlier detection methods include:</p>

<p>Univariate -> boxplot. outside of 1.5 times inter-quartile range is an outlier.</p>

<p>Bivariate -> scatterplot with confidence ellipse. outside of, say, 95% confidence ellipse is an outlier. </p>

<p>Multivariate -> Mahalanobis D2 distance</p>

<p>Mark those observations as outliers.</p>

<p>Run a logistic regression (on Y=IsOutlier) to see if there are any systematic patterns. </p>

<p>Remove ones that you can demonstrate they are not representative of any sub-population. </p>
",74,"2010-09-09 00:10:56",NULL,NULL,NULL,0,NULL,74,"2010-09-09 00:10:56",NULL,175,NULL,NULL,NULL
183,1,518,"2010-07-20 00:20:51",3,753,"<p>I need to analyze the 100k MovieLens dataset for clustering with two algorithms of my choice, between the likes of k-means, agnes, diana, dbscan, and several others. What tools (like Rattle, or Weka) would be best suited to help me make some simple clustering analysis over this dataset?</p>
",166,"2013-07-15 11:25:42","What tools could be used for applying clustering algorithms on MovieLens?",<clustering>,4,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
184,2,NULL,"2010-07-20 00:21:58",6,NULL,"<p>Try using the <code>stl()</code> function for time series decomposition. It provides a very flexible method for extracting a seasonal component from a time series.</p>
",159,"2010-07-20 00:21:58",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,33,NULL,NULL,NULL
185,2,NULL,"2010-07-20 00:30:00",10,NULL,"<p>A great introductory text covering the topics you mentioned is <a href="http://www.informationretrieval.org">Introduction to Information Retrieval</a>, which is available online in full text for free.</p>

<p><img src="http://nlp.stanford.edu/IR-book/iir.jpg" alt="Introduction to Information Retrieval"></p>
",80,"2010-07-20 00:30:00",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,124,NULL,NULL,NULL
187,2,NULL,"2010-07-20 00:47:45",3,NULL,"<p>As far as I know there is no way to select automatically the number of layers and neurons in each layer. But there are networks that can build automatically their topology, like EANN (Evolutionary Artificial Neural Networks, which use Genetic Algorithms to evolved the topology).</p>

<p>There are several approaches, a more or less modern one that seemed to give good results was NEAT (Neuro Evolution of Augmented Topologies). You can get more info:</p>

<p><a href="http://nn.cs.utexas.edu/?neat" rel="nofollow">http://nn.cs.utexas.edu/?neat</a></p>
",119,"2010-07-20 00:47:45",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,181,NULL,NULL,NULL
188,2,NULL,"2010-07-20 00:52:13",21,NULL,"<p>I'd probably say something like this:</p>

<p>"Anytime we want to talk about probabilities, we're really integrating a density.  In Bayesian analysis, a lot of the densities we come up with aren't analytically tractable: you can only integrate them -- if you can integrate them at all -- with a great deal of suffering.  So what we do instead is simulate the random variable a lot, and then figure out probabilities from our simulated random numbers.  If we want to know the probability that X is less than 10, we count the proportion of simulated random variable results less than 10 and use that as our estimate.  That's the "Monte Carlo" part, it's an estimate of probability based off of random numbers.  With enough simulated random numbers, the estimate is very good, but it's still inherently random.</p>

<p>"So why "Markov Chain"?  Because under certain technical conditions, you can generate a memoryless process (aka a Markovian one) that has the same limiting distribution as the random variable that you're trying to simulate.  You can iterate any of a number of different kinds of simulation processes that generate correlated random numbers (based only on the current value of those numbers), and you're guaranteed that once you pool enough of the results, you will end up with a pile of numbers that looks "as if" you had somehow managed to take independent samples from the complicated distribution you wanted to know about.</p>

<p>"So for example, if I want to estimate the probability that a standard normal random variable was less than 0.5, I could generate ten thousand independent realizations from a standard normal distribution and count up the number less than 0.5; say I got 6905 that were less than 10000; my estimate for P(Z&lt;0.5) would be 0.6905, which isn't that far off from the actual value.  That'd be a Monte Carlo estimate.</p>

<p>"Now imagine I couldn't draw independent normal random variables, instead I'd start at 0, and then with every step add some uniform random number between -0.5 and 0.5 to my current value, and then decide based on a particular test whether I liked that new value or not; if I liked it, I'd use the new value as my current one, and if not, I'd reject it and stick with my old value.  Because I only look at the new and current values, this is a Markov chain.  If I set up the test to decide whether or not I keep the new value correctly (it'd be a random walk Metropolis-Hastings, and the details get a bit complex), then even though I never generate a single normal random variable, if I do this procedure for long enough, the list of numbers I get from the procedure will be distributed like a large number of draws from something that generates normal random variables.  This would give me a Markov Chain Monte Carlo simulation for a standard normal random variable.  If I used this to estimate probabilities, that would be a MCMC estimate."</p>
",61,"2010-07-20 00:52:13",NULL,NULL,NULL,4,NULL,NULL,NULL,"2010-08-16 06:37:30",165,NULL,NULL,NULL
189,2,NULL,"2010-07-20 00:59:34",4,NULL,"<p>For a continuous random variable you can always approximate the pdf by calculating (CDF(x2) - CDF(x1))/(x2 - x1) where x1 and x2 are on either side of the point where you want to know the pdf and the distance |x2 - x1| is small.</p>
",173,"2010-07-20 00:59:34",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,23,NULL,NULL,NULL
190,2,NULL,"2010-07-20 01:07:38",7,NULL,"<p><a href="http://www.sportsci.org/resource/stats/">A New View of Statistics</a> by Will G. Hopkins is great!</p>
",25,"2010-07-20 01:07:38",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,170,NULL,NULL,NULL
191,2,NULL,"2010-07-20 01:17:17",3,NULL,"<p>The Bland-Altman plot is more widely known as the <strong>Tukey Mean-Difference Plot</strong> (one of many charts devised by John Tukey <a href="http://en.wikipedia.org/wiki/John_Tukey" rel="nofollow">http://en.wikipedia.org/wiki/John_Tukey</a>).</p>

<p>The idea is that x-axis is the mean of your two measurements, which is your best guess as to the "correct" result and the y-axis is the difference between the two measurement differences. The chart can then highlight certain types of anomalies in the measurements. For example, if one method always gives too high a result, then you'll get all of your points above or all below the zero line. It can also reveal, for example, that one method over-estimates high values and under-estimates low values.</p>

<p>If you see the points on the Bland-Altman plot scattered all over the place, above and below zero, then the suggests that there is no consistent bias of one approach versus the other (of course, there could be hidden biases that this plot does not show up).</p>

<p>Essentially, it is a good first step for exploring the data. Other techniques can be used to dig into more particular sorts of behaviour of the measurements.</p>
",173,"2010-07-20 01:17:17",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,128,NULL,NULL,NULL
192,1,293,"2010-07-20 01:18:11",3,1597,"<p>I'm aware that this one is far from <em>yes or no</em> question, but I'd like to know which techniques do you prefer in categorical data analysis - i.e. cross tabulation with two categorical variables.</p>

<p>I've come up with: </p>

<ul>
<li>&chi;<sup>2</sup> test - well, this is quite self-explanatory
<ul>
<li>Fisher's exact test - when n &lt; 40,</li>
<li>Yates' continuity correction - when n > 40,</li>
</ul></li>
<li>Cramer's V - measure of association for tables which have more than <em>2 x 2</em> cells,</li>
<li>&Phi; coefficient - measure of association for <em>2 x 2</em> tables,</li>
<li>contingency coefficient (C) - measure of association for <em>n x n</em> tables,</li>
<li>odds ratio - independence of two categorical variables,</li>
<li>McNemar marginal homogeniety test,</li>
</ul>

<p>And my question here is: Which statistical techniques for cross-tabulated data (two categorical variables) do you consider relevant (and why)?</p>
",1356,"2010-11-03 15:54:57","Cross tabulation of two categorical variables: recommended techniques",<categorical-data><association-measure>,4,1,4,930,"2010-11-03 15:54:57",NULL,NULL,NULL,NULL,NULL
193,2,NULL,"2010-07-20 01:45:12",6,NULL,"<p>Suppose that you want to know what percentage of people would vote for a particular candidate (say, $\\pi$, note that by definition $\\pi$ is between 0 and 100). You sample $N$ voters at random to find out how they would vote and your survey of these $N$ voters tells you that the percentage is $p$. So, you would like to establish a confidence interval for the true percentage. </p>

<p>If you assume that $p$ is normally distributed (an assumption that may or may not be justified depending on how 'big' $N$ is) then your confidence interval for $\\pi$ would be of the following form:
$$
CI = [ p - k * sd(p),~~ p + k * sd(p)]
$$
where $k$ is a constant that depends on the extent of confidence you want (i.e., 95% or 99% etc).</p>

<p>From a polling perspective, you want the width of your confidence interval to be 'low'. Usually, pollsters work with the margin of error which is basically one-half of the CI. In other words, $\\text{MoE} = k * sd(p)$.  </p>

<p>Here is how we would go about calculating $sd(p)$: By definition, $p = \\sum X_i / N$ where, $X_i = 1$ if voter $i$ votes for candidate and $0$ otherwise.</p>

<p>Since, we sampled the voters at random, we could assume that $X_i$ is a i.i.d Bernoulli random variable. Therefore, 
$$
Var(P) =  V\\left( \\sum\\frac{X_i}{N}\\right) = \\frac{\\sum V(X_i)}{N^2} = \\frac{N \\pi (1-\\pi)}{N^2} = \\frac{\\pi  (1-\\pi)}{N}.
$$
Thus,
$$
sd(p) = \\sqrt{\\frac{\\pi * (1-\\pi)}{N}}
$$
Now to estimate margin of error we need to know $\\pi$ which we do not know obviously. But, an inspection of the numerator suggests that the 'worst' estimate for $sd(p)$ in the sense that we get the 'largest' standard deviation is when $\\pi = 50$. Therefore, the worst possible standard deviation is:
$$
sd(p) = \\sqrt{50 * 50 / N } = 50 / \\sqrt{N}
$$
So, you see that the margin of error falls off exponentially with $N$ and thus you really do not need very big samples to reduce your margin of error, or in other words $N$ need not be very large for you to obtain a narrow confidence interval.</p>

<p>For example, for a 95 % confidence interval (i.e., $k= 1.96$) and $N = 1000$, the confidence interval is: 
$$
\\left[p - 1.96 \\frac{50}{\\sqrt{1000}},~~ p + 1.96 \\frac{50}{\\sqrt{1000}}\\right] = [p - 3,~~ p + 3] 
$$
As we increase $N$ the costs of polling go up linearly but the gains go down exponentially. That is the reason why pollsters usually cap $N$ at 1000 as that gives them a reasonable error of margin under the worst possible assumption of $\\pi = 50\\%$.  </p>
",NULL,"2012-09-07 18:25:29",NULL,NULL,NULL,0,NULL,7290,"2012-09-07 18:25:29",NULL,166,NULL,user28,NULL
194,1,200,"2010-07-20 01:47:36",6,838,"<p>I am sure that everyone who's trying to find patterns in historical stock market data or betting history would like to know about this. Given a huge sets of data, and thousands of random variables that may or may not affect it, it makes sense to ask any patterns that you extract out from the data are indeed true patterns, not statistical fluke.</p>

<p>A lot of patterns are only valid when they are tested in the samples. And even those that are patterns that are valid out of samples may cease to become valid when you apply it in the real world. </p>

<p>I understand that it is not possible to completely 100% make sure a pattern is valid all the time, but besides in and out of samples tests, are their any tests that could establish the validness of a pattern?</p>
",175,"2012-08-20 10:05:15","Data Mining-- How to Tell Whether the Pattern Extracted is Meaningful?",<data-mining>,3,2,3,NULL,NULL,NULL,NULL,NULL,NULL,NULL
195,1,2872,"2010-07-20 02:01:05",5,506,"<p>I am looking at fitting distributions to data (with a particular focus on the tail) and am leaning towards Anderson-Darling tests rather than Kolmogorov-Smirnov. What do you think are the relative merits of these or other tests for fit (e.g. Cramer-von Mises)?</p>
",173,"2010-09-20 00:29:57","What do you think is the best goodness of fit test?",<hypothesis-testing><fitting>,3,0,3,NULL,NULL,NULL,NULL,NULL,NULL,NULL
196,1,232,"2010-07-20 02:17:24",19,4090,"<p>Besides <a href="http://en.wikipedia.org/wiki/Gnuplot">gnuplot</a> and <a href="http://www.ggobi.org/">ggobi</a>, what open source tools are people using for visualizing multi-dimensional data?</p>

<p>Gnuplot is more or less a basic plotting package. </p>

<p>Ggobi can do a number of nifty things, such as:</p>

<ul>
<li>animate data along a dimension or among discrete collections</li>
<li>animate linear combinations varying the coefficients</li>
<li>compute principal components and other transformations</li>
<li>visualize and rotate 3 dimensional data clusters</li>
<li>use colors to represent a different dimension</li>
</ul>

<p>What other useful approaches are based in open source and thus freely reusable or customizable?</p>

<p><strong>Please provide a brief description of the package's abilities in the answer.</strong></p>
",87,"2012-11-21 06:38:02","Open source tools for visualizing multi-dimensional data?",<data-visualization><open-source>,6,2,12,9007,"2012-11-21 06:25:07","2010-07-20 02:35:32",NULL,NULL,NULL,NULL
197,2,NULL,"2010-07-20 02:24:38",9,NULL,"<p>How about R with <a href="http://had.co.nz/ggplot2/">ggplot2</a>?</p>

<p>Other tools that I really like:</p>

<ul>
<li><a href="http://www.processing.org/">Processing</a></li>
<li><a href="http://prefuse.org/">Prefuse</a>  </li>
<li><a href="http://vis.stanford.edu/protovis/">Protovis</a></li>
</ul>
",5,"2010-07-20 02:42:01",NULL,NULL,NULL,1,NULL,5,"2010-07-20 02:42:01","2010-08-16 13:45:34",196,NULL,NULL,NULL
198,2,NULL,"2010-07-20 02:32:43",3,NULL,"<p>Start with the distribution of $\\bar{y}$, show that since $v$ is normal, $\\bar{y}$ is multivariate normal and that consequently $u$ must also be a multivariate normal; also show that the covariance matrix of $\\bar{y}$ is of the form $\\sigma^2\\cdot(X^T X)^{-1}$ and thus -- if $\\sigma^2$ were known -- the variance of $u$ would be $\\sigma^2 c^T (X^T X)^{-1} c$.  Show that the distribution of $\\bar{e}^T  \\bar{e}$ must be chi-squared and (<em>carefully</em>) find the degrees of freedom.  Think about how what the operation $\\bar{e}^T \\bar{e} c^T (X^T X)^{-1} c$ must therefore produce, and what it's distribution and degrees of freedom are.</p>

<p>The result follows (almost) immediately from the definition of the t-distribution.</p>
",61,"2012-05-15 04:52:05",NULL,NULL,NULL,0,NULL,183,"2012-05-15 04:52:05",NULL,156,NULL,NULL,NULL
199,2,NULL,"2010-07-20 02:32:53",4,NULL,"<p>You could try:</p>

<ul>
<li>Bagging <a href="http://en.m.wikipedia.org/wiki/Bootstrap_aggregating" rel="nofollow">http://en.m.wikipedia.org/wiki/Bootstrap_aggregating</a></li>
<li>Boosting <a href="http://en.m.wikipedia.org/wiki/Boosting" rel="nofollow">http://en.m.wikipedia.org/wiki/Boosting</a></li>
<li>Cross validation <a href="http://en.m.wikipedia.org/wiki/Cross-validation_(statistics" rel="nofollow">http://en.m.wikipedia.org/wiki/Cross-validation_(statistics</a>)</li>
</ul>
",5,"2010-07-20 02:32:53",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,194,NULL,NULL,NULL
200,2,NULL,"2010-07-20 02:48:45",13,NULL,"<p>If you want to know that a pattern is meaningful, you need to show what it actually <em>means</em>. Statistical tests do not do this. Unless your data can be said to be in some sense "complete", inferences draw from the data will always be provisional.</p>

<p>You can increase your <em>confidence</em> in the validity of a pattern by testing against more and more out of sample data, but that doesn't protect you from it turning out to be an artefact. The broader your range of out of sample data -- eg, in terms of how it is acquired and what sort of systematic confounding factors might exist within it -- the better the validation.</p>

<p>Ideally, though, you need to go beyond identifying patterns and come up with a persuasive theoretical framework that <em>explains</em> the patterns you've found, and then test <em>that</em> by other, independent means. (This is called "science".)</p>
",174,"2012-08-20 10:05:15",NULL,NULL,NULL,3,NULL,174,"2012-08-20 10:05:15",NULL,194,NULL,NULL,NULL
201,2,NULL,"2010-07-20 03:11:36",8,NULL,"<p>Start R and type <code>data()</code>. This will show all datasets in the search path.
Many additional datasets are available in add-on packages.
For example, there are some interesting real-world social science datasets in the <code>AER</code> package.</p>
",183,"2010-07-20 03:11:36",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-08-12 20:30:02",7,NULL,NULL,NULL
202,2,NULL,"2010-07-20 03:13:22",11,NULL,"<ul>
<li>If you like learning through videos, I collated a <a href="http://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html">list of R training videos</a>.</li>
<li>I also prepared a <a href="http://jeromyanglim.blogspot.com/2009/06/learning-r-for-researchers-in.html">general post on learning R</a> with  suggestions on books, online manuals, blogs, videos, user interfaces, and more.</li>
</ul>
",183,"2011-05-27 03:38:47",NULL,NULL,NULL,0,NULL,183,"2011-05-27 03:38:47","2010-08-01 18:56:25",138,NULL,NULL,NULL
203,1,NULL,"2010-07-20 03:31:45",16,6033,"<p>Following on from <a href="http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data" rel="nofollow">this question</a>:
Imagine that you want to test for differences in central tendency between two groups (e.g., males and females)
on a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied).
I think a t-test would be sufficiently accurate for most purposes,
 but that a bootstrap test of differences between group means would often provide more accurate estimate of confidence intervals.
What statistical test would you use?</p>
",183,"2011-12-13 08:28:12","Group differences on a five point Likert item",<t-test><scales><ordinal><likert><interval>,5,2,2,930,"2011-03-30 18:36:01",NULL,NULL,NULL,NULL,NULL
204,2,NULL,"2010-07-20 03:35:58",7,NULL,"<p>The lattice package in R.</p>

<blockquote>
  <p>Lattice is a powerful and elegant high-level data visualization
  system, with an emphasis on multivariate data,that is sufﬁcient for
  typical graphics needs, and is also ﬂexible enough to handle most
  nonstandard requirements.</p>
</blockquote>

<p><a href="http://www.statmethods.net/advgraphs/trellis.html" rel="nofollow">Quick-R has a quick introduction</a>.</p>
",183,"2012-11-21 06:38:02",NULL,NULL,NULL,2,NULL,183,"2012-11-21 06:38:02","2010-07-20 03:35:58",196,NULL,NULL,NULL
205,1,353,"2010-07-20 03:51:24",12,543,"<p>I'm curious about why we treat fitting GLMS as though they were some special optimization problem.  Are they?  It seems to me that they're just maximum likelihood, and that we write down the likelihood and then ... we maximize it!  So why do we use Fisher scoring instead of any of the myriad of optimization schemes that has been developed in the applied math literature? </p>
",187,"2011-11-23 02:36:20","Why do we make a big fuss about using Fisher scoring when we fit a GLM?",<generalized-linear-model><optimization>,2,0,2,159,"2010-11-16 23:36:33",NULL,NULL,NULL,NULL,NULL
206,1,209,"2010-07-20 03:53:54",13,70255,"<p>What is the difference between discrete data and continuous data?</p>
",188,"2014-06-18 07:12:57","What is the difference between discrete data and continuous data?",<continuous-data><discrete-data>,8,1,4,183,"2011-05-27 03:35:28",NULL,NULL,NULL,NULL,NULL
207,2,NULL,"2010-07-20 04:00:14",48,NULL,"<p>First, we need to understand what is a markov chain. Consider the following <a href="http://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_very_simple_weather_model">weather</a> example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:</p>

<p>Probability(Next day is sunny | Given today is rainy ) = 0.50</p>

<p>Since, the next day's weather is either sunny or rainy it follows that:</p>

<p>Probability(Next day is Rainy | Given today is rainy ) = 0.50 </p>

<p>Similarly, let:</p>

<p>Probability(Next day is rainy | Given today is sunny ) = 0.10</p>

<p>Therefore, it follows that:</p>

<p>Probability(Next day is sunny | Given today is sunny ) = 0.90</p>

<p>The above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:</p>

<pre><code>         S   R
P = S [ 0.9 0.1
    R   0.5 0.5]
</code></pre>

<p>We might ask several questions whose answers follow:</p>

<p>Q1: If the weather is sunny today then what is the weather likely to be tomorrow?</p>

<p>A1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. </p>

<p>Q2: What about two days from today?</p>

<p>A2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now:</p>

<p>First day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9  0.9. </p>

<p>Or</p>

<p>First day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 * 0.5</p>

<p>Therefore, the probability that the weather will be sunny in two days is:</p>

<p>Prob(Sunny two days from now) = 0.9  0.9 + 0.1  0.5 = 0.81 + 0.05 = 0.86 </p>

<p>Similarly, the probability that it will be rainy is:</p>

<p>Prob(Rainy two days from now) = 0.1 * 0.5 + 0.9 0.1 = 0.05 + 0.09 = 0.14</p>

<p>If you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities:</p>

<p>Prob(Sunny) = 0.833
Prob(Rainy) = 0.167</p>

<p>In other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.</p>

<p>The above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions):</p>

<p>Irrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.</p>

<p>Markov Chain Monte Carlo exploits the above feature as follows: </p>

<p>We want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. </p>

<p>If we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. </p>

<p>We then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monte carlo component.</p>

<p>There are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm).</p>
",NULL,"2010-07-21 16:00:43",NULL,NULL,NULL,0,NULL,NULL,"2010-07-21 16:00:43","2010-08-16 06:37:30",165,NULL,user28,user28
208,2,NULL,"2010-07-20 04:07:11",4,NULL,"<p>Temperatures are continuous. It can be 23 degrees, 23.1 degrees, 23.100004 degrees. </p>

<p>Gender is discrete. You can only be male or female (insert san francisco joke here). Something you would represent with a whole number like 0, 1, 2, etc</p>

<p>The difference is important as many statistical and data mining algorithms can handle one type but not the other. For example in regular regression, the Y must be continuous. In logistic regression the Y is discrete. </p>
",74,"2010-07-20 04:07:11",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,206,NULL,NULL,NULL
209,2,NULL,"2010-07-20 04:16:52",19,NULL,"<p>Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.</p>

<p>Continuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric.</p>

<p>It sometimes makes sense to treat numeric data that is properly of one type as being of the other. For example, something like <em>height</em> is continuous, but often we don't really care too much about tiny differences and instead group heights into a number of discrete <strong>bins</strong>. Conversely, if we're counting large amounts of some discrete entity -- grains of rice, or termites, or pennies in the economy -- we may choose not to think of 2,000,006 and 2,000,008 as crucially different values but instead as nearby points on an approximate continuum.</p>

<p>It can also sometimes be useful to treat numeric data as categorical, eg: underweight, normal, obese. This is usually just another kind of binning.</p>

<p>It seldom makes sense to consider categorical data as continuous.</p>
",174,"2010-07-20 05:25:22",NULL,NULL,NULL,4,NULL,174,"2010-07-20 05:25:22",NULL,206,NULL,NULL,NULL
210,2,NULL,"2010-07-20 04:19:12",9,NULL,"<p>Data is always discrete. Given a sample of <code>n</code> values on a variable, the maximum number of distinct values the variable can take is equal to  <code>n</code>. See this quote </p>

<blockquote>
  <p>All actual sample spaces are discrete, and all observable random
  variables have discrete distributions. The continuous distribution is
  a mathematical construction, suitable for mathematical treatment,
  but not practically observable. E.J.G. Pitman (1979, p. 1).</p>
</blockquote>

<p>Data on a variable are typically assumed to be drawn from a random variable.
The random variable is continuous over a range if there is an infinite number of possible values that the variable can take between any two different points in the range.
For example, height, weight, and time are typically assumed to be continuous.
Of course, any measurement of these variables will be finitely accurate and in some
 sense discrete.</p>

<p>It is useful to distinguish between ordered (i.e., ordinal), unordered (i.e., nominal),<br>
and binary discrete variables.</p>

<p>Some introductory textbooks confuse a continuous variable with a numeric variable.
For example, a score on a computer game is discrete even though it is numeric.</p>

<p>Some introductory textbooks confuse a ratio variable with continuous variables. A count variable is a ratio variable, but it is not continuous.</p>

<p>In actual practice, a variable is often treated as continuous when it can take on a sufficiently large number of different values.</p>

<h3>References</h3>

<ul>
<li>Pitman, E. J. G. 1979. Some basic theory for statistical inference. London: Chapman and Hall. <em>Note:</em>  I found the quote in the introduction of Chapter 2 of Murray Aitkin's book <em>Statistical Inference: An Integrated Bayesian/Likelihood Approach</em></li>
</ul>
",183,"2012-03-30 05:31:05",NULL,NULL,NULL,3,NULL,183,"2012-03-30 05:31:05",NULL,206,NULL,NULL,NULL
211,2,NULL,"2010-07-20 04:49:07",6,NULL,"<p>I have written a document that is freely available at my website and on CRAN. See the linked page:</p>

<p><a href="http://www.ms.unimelb.edu.au/~andrewpr/r-users/">icebreakeR</a></p>

<p>The datasets that are used in the document are also linked from that page.  Feedback is welcome and appreciated!</p>

<p>Andrew</p>
",187,"2010-07-20 04:49:07",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
212,1,5001,"2010-07-20 04:54:20",3,430,"<p>I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate.</p>

<p>What methods do I have to test for statistical significance of my new results?</p>

<p><strong>An example:</strong></p>

<p>I have an experiment with 10 speaker, all having 100 (the same) sentences, total 900 words per speaker. Method A has an WER (word error rate) of 19.0%, Method B 18.5%.</p>

<p>How do I test whether Method B is significantly better?</p>
",190,"2010-11-29 18:25:11","What method to use to test Statistical Significance of ASR results",<statistical-significance>,3,0,NULL,190,"2010-07-21 06:19:29",NULL,NULL,NULL,NULL,NULL
213,1,532,"2010-07-20 05:02:33",51,9772,"<p>Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.</p>

<p>I am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.</p>

<p>One possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches?</p>
",159,"2013-12-11 18:58:18","What is the best way to identify outliers in multivariate data?",<multivariable><outliers>,13,3,42,159,"2010-07-20 05:28:56",NULL,NULL,NULL,NULL,NULL
214,2,NULL,"2010-07-20 05:02:42",4,NULL,"<p>Some free Stats textbooks are also available <a href="http://www.e-booksdirectory.com/mathematics.php" rel="nofollow">here</a>.</p>
",40,"2010-07-20 05:02:42",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,170,NULL,NULL,NULL
215,2,NULL,"2010-07-20 05:03:12",1,NULL,"<p>I'm not sure about these tests, so this answer may be off-topic.  Apologies if so.  But, are you sure that you want a test?  It really depends on what the purpose of the exercise is.  Why are you fitting the distributions to the data, and what will you do with the fitted distributions  afterward?  </p>

<p>If you want to know what distribution fits best just because you're interested, then a test may help.  </p>

<p>On the other hand, if you want to actually do something with the distribution, then you'd be better off developing a loss function based on your intentions, and using the distribution that gives you the most satisfactory value for the loss function.  </p>

<p>It sounds to me from your description (particular focus on the tail) that you want to actually do something with the distribution.  If so, it's hard for me to imagine a situation where an existing test will provide better guidance than comparing the effects of the fitted distributions in situ, somehow.</p>
",187,"2010-07-20 05:03:12",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,195,NULL,NULL,NULL
216,1,217,"2010-07-20 05:04:40",9,483,"<p>What are some good visualization libraries for online use? Are they easy to use and is there good documentation?</p>

<p>Thanks.</p>
",191,"2010-07-20 08:45:07","Web visualization libraries",<data-visualization><library><protovis>,3,0,7,NULL,NULL,NULL,NULL,NULL,NULL,NULL
217,2,NULL,"2010-07-20 05:10:08",6,NULL,"<p>IMO, <strong><a href="http://vis.stanford.edu/protovis/">Protovis</a></strong> is the best and is very well documented and supported.  It is the basis for my <a href="http://cran.r-project.org/web/packages/webvis/index.html">webvis</a> R package.  </p>

<p>These are also very good, although they have more of a learning curve:</p>

<ul>
<li><a href="http://www.processing.org/">Processing</a></li>
<li><a href="http://prefuse.org/">Prefuse</a>  </li>
</ul>
",5,"2010-07-20 05:15:51",NULL,NULL,NULL,4,NULL,5,"2010-07-20 05:15:51",NULL,216,NULL,NULL,NULL
218,2,NULL,"2010-07-20 05:13:21",2,NULL,"<p>This interesting question is the subject of some research in <a href="http://www.acera.unimelb.edu.au/" rel="nofollow">ACERA</a>.  The lead researcher is Andrew  Speirs-Bridge, and his work is eminently google-able :)</p>
",187,"2010-07-20 05:13:21",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,1,NULL,NULL,NULL
219,2,NULL,"2010-07-20 05:21:14",3,NULL,"<p>The Xorshift PNG designed by George Marsaglia. Its period (2^128-1) is much shorter than the Mersenne-Twister but the algorithm is very simple to implement and lends itself to parallelization. Performs well on many-core architectures such as DSP chips and Nvidia's Tesla.</p>
",154,"2010-07-20 05:33:00",NULL,NULL,NULL,3,NULL,154,"2010-07-20 05:33:00",NULL,40,NULL,NULL,NULL
220,1,221,"2010-07-20 05:23:04",14,9481,"<p>If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?</p>
",85,"2012-04-12 23:26:05","How is the minimum of a set of random variables distributed?",<distributions><random-variable><minimum>,3,2,6,3911,"2011-04-29 00:37:42",NULL,NULL,NULL,NULL,NULL
221,2,NULL,"2010-07-20 05:35:48",15,NULL,"<p>If the cdf of $X_i$ is denoted by $F(x)$, then the cdf of the minimum is given by $1-[1-F(x)]^n$.</p>
",159,"2012-04-12 23:26:05",NULL,NULL,NULL,2,NULL,159,"2012-04-12 23:26:05",NULL,220,NULL,NULL,NULL
222,1,282,"2010-07-20 05:37:46",26,36801,"<p>I know this is probably simplistic but what are Principal component scores?</p>

<p>This question originates from my attempt to understand this question <a href="http://stats.stackexchange.com/questions/213/what-is-the-best-way-to-identify-outliers-in-multivariate-data" rel="nofollow">here</a>.</p>
",191,"2014-01-13 11:11:25","What are principal component scores?",<pca><scores>,7,1,22,88,"2010-08-07 17:55:46",NULL,NULL,NULL,NULL,NULL
223,1,NULL,"2010-07-20 05:54:15",5,291,"<p>I have a friend who is an MD and wants to refresh his Statistics. So is there any recommended resource online (or offline) ? He did stats ~20 years ago.</p>
",79,"2012-02-01 18:51:11","Intro to statistics for an MD?",<books>,3,5,5,4872,"2012-02-01 18:51:11",NULL,NULL,NULL,NULL,NULL
224,1,NULL,"2010-07-20 06:03:59",8,1418,"<p>Which visualization libraries (plots, graphs, ...) would you suggest to use in a standalone application (Linux, .Net, Windows, whatever). Reasonable performance would be nice as well.</p>
",128,"2011-05-27 03:36:21","Recommended visualization libraries for standalone applications",<data-visualization><software>,10,3,13,183,"2011-05-27 03:36:21","2010-08-30 05:10:29",NULL,NULL,NULL,NULL
225,1,229,"2010-07-20 06:07:37",7,617,"<p>Why is the average of the highest value from 100 draws from a normal distribution different from the 98% percentile of the normal distribution?  It seems that by definition that they should be the same.  But...</p>

<p>Code in R:</p>

<pre><code>NSIM &lt;- 10000
x &lt;- rep(NA,NSIM)
for (i in 1:NSIM)
{
    x[i] &lt;- max(rnorm(100))
}
qnorm(.98)
qnorm(.99)
mean(x)
median(x)
hist(x)
</code></pre>

<p>I imagine that I'm misunderstanding something about what the maximum of a 100 draws from the normal distribution should be.  As is demonstrated by an unexpectedly asymetrical distribution of maximum values.</p>
",196,"2012-04-01 04:18:55","Why is the average of the highest value from 100 draws from a normal distribution different from the 98th percentile of the normal distribution?",<r><distributions><maximum>,4,2,NULL,196,"2010-07-20 08:47:33",NULL,NULL,NULL,NULL,NULL
226,2,NULL,"2010-07-20 06:23:21",12,NULL,"<p>Principal component analysis (PCA) is one popular approach analyzing variance when you are dealing with multivariate data. You have random variables X1, X2,...Xn which are all correlated (positively or negatively) to varying degrees, and you want to get a better understanding of what's going on. PCA can help.</p>

<p>What PCA gives you is a change of variable into Y1, Y2,..., Yn (i.e. the same number of variables) which are linear combinations of the Xs. For example, you might have Y1 = 2.1 X1 - 1.76 X2 + 0.2 X3...</p>

<p>The Ys the nice property that each of these have zero correlation with each other. Better still, you get them in decreasing order of variance. So, Y1 "explains" a big chunk of the variance of the original variables, Y2 a bit less and so on. Usually after the first few Ys, the variables become somewhat meaningless. The PCA score for any of the Xi is just it's coefficient in each of the Ys. In my earlier example, the score for X2 in the first principal component (Y1) is 1.76.</p>

<p>The way PCA does this magic is by computing eigenvectors of the covariance matrix.</p>

<p>To give a concrete example, imagine X1,...X10 are changes in 1 year, 2 year, ..., 10 year Treasury bond yields over some time period. When you compute PCA you generally find that the first component has scores for each bond of the same sign and about the same sign. This tells you that most of the variance in bond yields comes from everything moving the same way: "parallel shifts" up or down. The second component typically shows "steepening" and "flattening" of the curve and has opposite signs for X1 and X10.</p>
",173,"2010-07-20 06:23:21",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,222,NULL,NULL,NULL
227,2,NULL,"2010-07-20 06:24:32",2,NULL,"<p>Let $i=1,\\dots,N$ index the rows and $j=1,\\dots,M$ index the columns. Suppose you linearize the combination of variables (columns):</p>

<p>$$Z_{i,1} = c_{i,1}\\cdot Y_{i,1} + c_{i,2}\\cdot Y_{i,2} + ... + c_{i,M}\\cdot Y_{i,M}$$</p>

<p>The above formula basically says to multiply row elements with a certain value $c$ (loadings) and sum them by columns. Resulting values ($Y$ values times the loading) are scores.</p>

<p>A principal component (PC) is a linear combination $Z_1 = (Z_{1,1}, ..., Z_{N,1}$) (values by columns which are called scores). In essence, the PC should present the most important features of variables (columns). Ergo, you can extract as many PC as there are variables (or less).</p>

<p>An output from <a href="http://cran.r-project.org/" rel="nofollow">R</a> on PCA (a fake example) looks like this. PC1, PC2... are principal components 1, 2... The example below is showing only the first 8 principal components (out of 17). You can also extract other elements from PCA, like loadings and scores.</p>

<pre><code>Importance of components:
                          PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8
Standard deviation     1.0889 1.0642 1.0550 1.0475 1.0387 1.0277 1.0169 1.0105
Proportion of Variance 0.0697 0.0666 0.0655 0.0645 0.0635 0.0621 0.0608 0.0601
Cumulative Proportion  0.0697 0.1364 0.2018 0.2664 0.3298 0.3920 0.4528 0.5129
</code></pre>
",144,"2012-07-02 02:33:13",NULL,NULL,NULL,1,NULL,159,"2012-07-02 02:33:13",NULL,222,NULL,NULL,NULL
228,2,NULL,"2010-07-20 06:27:16",2,NULL,"<ul>
<li><a href="http://insideria.com/2009/12/28-rich-data-visualization-too.html" rel="nofollow">http://insideria.com/2009/12/28-rich-data-visualization-too.html</a> 28 Rich Data Visualization Tools</li>
<li><a href="http://www.rgraph.net/" rel="nofollow">http://www.rgraph.net/</a> R graph</li>
<li><a href="http://vis.stanford.edu/protovis/" rel="nofollow">http://vis.stanford.edu/protovis/</a></li>
</ul>
",10,"2010-07-20 06:27:16",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,216,NULL,NULL,NULL
229,2,NULL,"2010-07-20 06:28:47",10,NULL,"<p>The maximum does not have a normal distribution. Its cdf is $\\Phi(x)^{100}$ where $\\Phi(x)$ is the standard normal cdf. In general the moments of this distribution are tricky to obtain analytically. There is an ancient paper on this by <a href="http://www.jstor.org/stable/2332087" rel="nofollow">Tippett (<em>Biometrika</em>, 1925)</a>.</p>
",159,"2012-03-31 07:26:34",NULL,NULL,NULL,2,NULL,4856,"2012-03-31 07:26:34",NULL,225,NULL,NULL,NULL
230,2,NULL,"2010-07-20 06:35:21",5,NULL,"<p>You could have a look at Processing: <a href="http://processing.org/" rel="nofollow">http://processing.org/</a></p>
",173,"2010-07-20 06:35:21",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-12 08:48:39",224,NULL,NULL,NULL
231,2,NULL,"2010-07-20 06:41:13",4,NULL,"<p>This is one I've used successfully:</p>

<p><a href="http://rads.stackoverflow.com/amzn/click/013124941X" rel="nofollow">http://www.amazon.co.uk/Statistics-without-Psychology-Christine-Dancey/dp/013124941X</a></p>

<p>I just stumlbed on this too, this might be useful:</p>

<p><a href="http://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm" rel="nofollow">http://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm</a></p>

<p>I'm sure I knew of a free pdf that some doctors I know use, but I can't seem to find it at the moment. I will try to dig it out.</p>
",199,"2010-07-20 06:41:13",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,223,NULL,NULL,NULL
232,2,NULL,"2010-07-20 06:41:25",12,NULL,"<ul>
<li><a href="http://rosuda.org/mondrian/">Mondrian</a>: Exploratory data analysis with focus on large data and databases.</li>
<li><a href="http://rosuda.org/iplots/">iPlots</a>: a package for the R statistical environment which provides high interaction statistical graphics, written in Java.</li>
</ul>
",103,"2010-07-20 07:26:11",NULL,NULL,NULL,2,NULL,103,"2010-07-20 07:26:11","2010-07-20 06:41:25",196,NULL,NULL,NULL
234,2,NULL,"2010-07-20 06:47:14",5,NULL,"<p>Say you have a cloud of N points in, say, 3D (which can be listed in a 100x3 array). Then, the principal components analysis (PCA) fits an arbitrarily oriented ellipsoid into the data. The principal component score is the length of the diameters of the ellipsoid. </p>

<p>In the direction in which the diameter is large, the data varies a lot, while in the direction in which the diameter is small, the data varies litte. If you wanted to project N-d data into a 2-d scatter plot, you plot them along the two largest principal components, because with that approach you display most of the variance in the data.</p>
",198,"2010-07-20 06:47:14",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,222,NULL,NULL,NULL
235,2,NULL,"2010-07-20 07:17:58",4,NULL,"<p><a href="http://raphaeljs.com/" rel="nofollow">RaphaelJS</a> can do some pretty amazing stuff and it just got some major backing from <a href="http://www.sencha.com/" rel="nofollow">Sencha</a> (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, <a href="http://g.raphaeljs.com/" rel="nofollow">gRaphael</a>, that focuses on drawing charts and graphs.</p>

<p>The <a href="http://simile.mit.edu/" rel="nofollow">MIT SIMILE Project</a> also has some interesting JavaScript libraries:</p>

<ul>
<li><a href="http://www.simile-widgets.org/timeplot/" rel="nofollow">Timeplot</a></li>
<li><a href="http://www.simile-widgets.org/timeline/" rel="nofollow">Timeline</a></li>
</ul>

<p>There is also a project to port Processing to JavaScript: <a href="http://processingjs.org/" rel="nofollow">ProcessingJS</a></p>

<p><a href="http://jmol.sourceforge.net/" rel="nofollow">Jmol</a> is a Java applet for viewing chemical structures, but it is used as the display engine for 3D graphics in the <a href="http://www.sagemath.org/" rel="nofollow">SAGE</a> system, which has a completely browser-based GUI.</p>

<p>And for an open source alternative to Google Maps, there is the excellent <a href="http://www.openlayers.org" rel="nofollow">OpenLayers</a> JavaScript library which powers the frontend of the equally excellent <a href="http://www.openstreetmap.org/" rel="nofollow">OpenStreetMap</a>.</p>
",13,"2010-07-20 08:45:07",NULL,NULL,NULL,0,NULL,13,"2010-07-20 08:45:07",NULL,216,NULL,NULL,NULL
236,2,NULL,"2010-07-20 07:30:34",0,NULL,"<p>Unfortunately, it only runs on macs, but otherwise a great application (basically <em>Processing</em> in python):</p>

<ul>
<li><a href="http://nodebox.net/code/index.php/Home" rel="nofollow">http://nodebox.net/code/index.php/Home</a></li>
</ul>

<blockquote>
  <p>NodeBox is a Mac OS X application that lets you create 2D visuals (static, animated or interactive) using Python programming code and export them as a PDF or a QuickTime movie. NodeBox is free and well-documented.</p>
</blockquote>
",138,"2010-07-21 18:33:33",NULL,NULL,NULL,0,NULL,138,"2010-07-21 18:33:33","2010-10-12 08:48:39",224,NULL,NULL,NULL
237,2,NULL,"2010-07-20 07:31:08",1,NULL,"<p>I'm going to leave the main question alone, because I think I will get it wrong (although I too analyse data for a healthcare provider, and to be honest, if I had these data, I would just analyse them using standard techniques and hope for the best, they look pretty okay to me).</p>

<p>As for R packages, I have found the TSA library and it's accompanying <a href="http://rads.stackoverflow.com/amzn/click/0387759581" rel="nofollow">book</a> very useful indeed. The <code>armasubsets</code> command, particularly, I think is a great time saver.</p>
",199,"2010-08-09 12:18:10",NULL,NULL,NULL,0,NULL,8,"2010-08-09 12:18:10",NULL,173,NULL,NULL,NULL
238,2,NULL,"2010-07-20 07:33:09",8,NULL,"<p>There is always lovely gnuplot:</p>

<ul>
<li><a href="http://www.gnuplot.info/" rel="nofollow">http://www.gnuplot.info/</a></li>
</ul>

<blockquote>
  <p>Gnuplot is a portable command-line driven graphing utility for linux, OS/2, MS Windows, OSX, VMS, and many other platforms. The source code is copyrighted but freely distributed (i.e., you don't have to pay for it). It was originally created to allow scientists and students to visualize mathematical functions and data interactively, but has grown to support many non-interactive uses such as web scripting. It is also used as a plotting engine by third-party applications like Octave. Gnuplot has been supported and under active development since 1986.</p>
  
  <p>Gnuplot supports many types of plots in either 2D and 3D. It can draw using lines, points, boxes, contours, vector fields, surfaces, and various associated text. It also supports various specialized plot types. </p>
</blockquote>
",138,"2010-07-20 07:33:09",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-12 08:48:39",224,NULL,NULL,NULL
240,2,NULL,"2010-07-20 07:55:30",4,NULL,"<p>My first response would be that if you can do multivariate regression on the data, then to use the residuals from that regression to spot outliers. (I know you said it's not a regression problem, so this might not help you, sorry !)</p>

<p>I'm copying some of this from a <a href="http://stackoverflow.com/questions/1444306/how-to-use-outlier-tests-in-r-code/1444548#1444548">Stackoverflow question I've previously answered</a> which has some example <a href="http://en.wikipedia.org/wiki/R_%28programming_language%29" rel="nofollow">R</a> code</p>

<p>First, we'll create some data, and then taint it with an outlier;</p>

<pre><code>&gt; testout&lt;-data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5),Y=rnorm(50,mean=200,sd=25)) 
&gt; #Taint the Data 
&gt; testout$X1[10]&lt;-5 
&gt; testout$X2[10]&lt;-5 
&gt; testout$Y[10]&lt;-530 

&gt; testout 
         X1         X2        Y 
1  44.20043  1.5259458 169.3296 
2  40.46721  5.8437076 200.9038 
3  48.20571  3.8243373 189.4652 
4  60.09808  4.6609190 177.5159 
5  50.23627  2.6193455 210.4360 
6  43.50972  5.8212863 203.8361 
7  44.95626  7.8368405 236.5821 
8  66.14391  3.6828843 171.9624 
9  45.53040  4.8311616 187.0553 
10  5.00000  5.0000000 530.0000 
11 64.71719  6.4007245 164.8052 
12 54.43665  7.8695891 192.8824 
13 45.78278  4.9921489 182.2957 
14 49.59998  4.7716099 146.3090 
&lt;snip&gt; 
48 26.55487  5.8082497 189.7901 
49 45.28317  5.0219647 208.1318 
50 44.84145  3.6252663 251.5620 
</code></pre>

<p>It's often most usefull to examine the data graphically (you're brain is much better at spotting outliers than maths is)</p>

<pre><code>&gt; #Use Boxplot to Review the Data 
&gt; boxplot(testout$X1, ylab="X1") 
&gt; boxplot(testout$X2, ylab="X2") 
&gt; boxplot(testout$Y, ylab="Y") 
</code></pre>

<p>You can then use stats to calculate critical cut off values, here using the Lund Test (See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476. and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.)</p>

<pre><code>&gt; #Alternative approach using Lund Test 
&gt; lundcrit&lt;-function(a, n, q) { 
+ # Calculates a Critical value for Outlier Test according to Lund 
+ # See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476. 
+ # and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132. 
+ # a = alpha 
+ # n = Number of data elements 
+ # q = Number of independent Variables (including intercept) 
+ F&lt;-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE) 
+ crit&lt;-((n-q)*F/(n-q-1+F))^0.5 
+ crit 
+ } 

&gt; testoutlm&lt;-lm(Y~X1+X2,data=testout) 

&gt; testout$fitted&lt;-fitted(testoutlm) 

&gt; testout$residual&lt;-residuals(testoutlm) 

&gt; testout$standardresid&lt;-rstandard(testoutlm) 

&gt; n&lt;-nrow(testout) 

&gt; q&lt;-length(testoutlm$coefficients) 

&gt; crit&lt;-lundcrit(0.1,n,q) 

&gt; testout$Ynew&lt;-ifelse(testout$standardresid&gt;crit,NA,testout$Y) 

&gt; testout 
         X1         X2        Y    newX1   fitted    residual standardresid 
1  44.20043  1.5259458 169.3296 44.20043 209.8467 -40.5171222  -1.009507695 
2  40.46721  5.8437076 200.9038 40.46721 231.9221 -31.0183107  -0.747624895 
3  48.20571  3.8243373 189.4652 48.20571 203.4786 -14.0134646  -0.335955648 
4  60.09808  4.6609190 177.5159 60.09808 169.6108   7.9050960   0.190908291 
5  50.23627  2.6193455 210.4360 50.23627 194.3285  16.1075799   0.391537883 
6  43.50972  5.8212863 203.8361 43.50972 222.6667 -18.8306252  -0.452070155 
7  44.95626  7.8368405 236.5821 44.95626 223.3287  13.2534226   0.326339981 
8  66.14391  3.6828843 171.9624 66.14391 148.8870  23.0754677   0.568829360 
9  45.53040  4.8311616 187.0553 45.53040 214.0832 -27.0279262  -0.646090667 
10  5.00000  5.0000000 530.0000       NA 337.0535 192.9465135   5.714275585 
11 64.71719  6.4007245 164.8052 64.71719 159.9911   4.8141018   0.118618011 
12 54.43665  7.8695891 192.8824 54.43665 194.7454  -1.8630426  -0.046004311 
13 45.78278  4.9921489 182.2957 45.78278 213.7223 -31.4266180  -0.751115595 
14 49.59998  4.7716099 146.3090 49.59998 201.6296 -55.3205552  -1.321042392 
15 45.07720  4.2355525 192.9041 45.07720 213.9655 -21.0613819  -0.504406009 
16 62.27717  7.1518606 186.6482 62.27717 169.2455  17.4027250   0.430262983 
17 48.50446  3.0712422 228.3253 48.50446 200.6938  27.6314695   0.667366651 
18 65.49983  5.4609713 184.8983 65.49983 155.2768  29.6214506   0.726319931 
19 44.38387  4.9305222 213.9378 44.38387 217.7981  -3.8603382  -0.092354925 
20 43.52883  8.3777627 203.5657 43.52883 228.9961 -25.4303732  -0.634725264 
&lt;snip&gt; 
49 45.28317  5.0219647 208.1318 45.28317 215.3075  -7.1756966  -0.171560291 
50 44.84145  3.6252663 251.5620 44.84145 213.1535  38.4084869   0.923804784 
       Ynew 
1  169.3296 
2  200.9038 
3  189.4652 
4  177.5159 
5  210.4360 
6  203.8361 
7  236.5821 
8  171.9624 
9  187.0553 
10       NA 
11 164.8052 
12 192.8824 
13 182.2957 
14 146.3090 
15 192.9041 
16 186.6482 
17 228.3253 
18 184.8983 
19 213.9378 
20 203.5657 
&lt;snip&gt; 
49 208.1318 
50 251.5620 
</code></pre>

<p>Obviosuly there are other outlier tests than the Lund test (Grubbs springs to mind), but I'm not sure which are better suited to multivariate data.</p>
",114,"2010-07-20 07:55:30",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,213,NULL,NULL,NULL
241,2,NULL,"2010-07-20 07:56:06",5,NULL,"<p>I'm not sure what you mean when you say you aren't thinking of a regression problem but of "true multivariate data".  My initial response would be to calculate the Mahalanobis distance since it doesn't require that you specify a particular IV or DV, but at its core (as far as I understand it) it is related to a leverage statistic.</p>
",196,"2010-07-20 07:56:06",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,213,NULL,NULL,NULL
242,1,306,"2010-07-20 07:56:16",12,470,"<p>This is a bit of a flippant question, but I have a serious interest in the answer. I work in a psychiatric hospital and I have three years' of data, collected every day across each ward regarding the level of violence on that ward.</p>

<p>Clearly the model which fits these data is a time series model. I had to difference the scores in order to make them more normal. I fit an ARMA model with the differenced data, and the best fit I think was a model with one degree of differencing and first order auto-correlation at lag 2.</p>

<p>My question is, what on earth can I use this model for? Time series always seems so useful in the textbooks when it's about hare populations and oil prices, but now I've done my own the result seems so abstract as to be completely opaque. The differenced scores correlate with each other at lag two, but I can't really advise everyone to be on high alert two days after a serious incident in all seriousness.</p>

<p>Or can I?</p>
",199,"2010-07-20 15:53:21","Using time series analysis to analyze/predict violent behavior",<time-series><forecasting>,2,5,3,5,"2010-07-20 15:53:21",NULL,NULL,NULL,NULL,NULL
243,2,NULL,"2010-07-20 08:05:04",7,NULL,"<p>Depending on the size of the dataset in question, a permutation test might be preferable to a bootstrap in that it may be able to provide an exact test of the hypothesis (and an exact CI).</p>
",196,"2010-07-20 09:09:01",NULL,NULL,NULL,0,NULL,196,"2010-07-20 09:09:01",NULL,203,NULL,NULL,NULL
244,2,NULL,"2010-07-20 08:13:31",17,NULL,"<p>The Visualization Tool Kit <a href="http://www.vtk.org" rel="nofollow">VTK</a> is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.</p>

<p><a href="http://graphviz.org/" rel="nofollow">Graphviz</a> is used pretty extensively for visualizing graphs and other tree-like data structures.</p>

<p><a href="http://igraph.sourceforge.net/" rel="nofollow">igraph</a> can also be used for visualization of tree-like data structures.  Contains nice interfaces to scripting languages such as R and Python along with a stand-alone C library.</p>

<p>The <a href="http://www.ncl.ucar.edu/" rel="nofollow">NCL</a> (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.</p>

<p>If you are willing to relax the executable requirement, or try a tool like <a href="http://www.py2exe.org/" rel="nofollow">py2exe</a>, there is the possibility of leveraging some neat Python libraries and applications such as:</p>

<ul>
<li><p><a href="http://code.enthought.com/projects/mayavi/" rel="nofollow">MayaVi</a>: A higher level front-end to VTK developed by Enthought.</p></li>
<li><p><a href="http://code.enthought.com/chaco/" rel="nofollow">Chaco</a>: Another Enthought library focused on 2D graphs.</p></li>
<li><p><a href="http://matplotlib.sourceforge.net/" rel="nofollow">Matplotlib</a>: Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.</p></li>
<li><p><a href="http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/" rel="nofollow">Basemap</a>: An add-on to Matplotlib for drawing maps and displaying geographic data (<a href="http://www.scipy.org/Cookbook/Matplotlib/Maps" rel="nofollow">sexy examples here</a>).</p></li>
</ul>

<p>If we were to bend the concept of "standalone application" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:</p>

<ul>
<li><p><a href="http://asymptote.sourceforge.net/" rel="nofollow">Asymptote</a> can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader (<a href="http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf" rel="nofollow">example</a>).</p></li>
<li><p><a href="http://sourceforge.net/projects/pgf/" rel="nofollow">PGF/TikZ</a> provides a wonderful vector drawing language to TeX documents.  The <a href="http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf" rel="nofollow">manual</a> is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  <a href="http://sourceforge.net/projects/pgfplots/" rel="nofollow">PGFPlots</a> provides an abstraction layer for drawing plots.  A wondeful showcase can be found at <a href="http://www.texample.net/tikz/examples/all/" rel="nofollow">TeXample</a>.</p></li>
<li><p><a href="http://www.tug.org/PSTricks/main.cgi/" rel="nofollow">PSTricks</a> served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.</p></li>
</ul>

<p>And for kicks, there's <a href="http://www.mps.mpg.de/dislin/" rel="nofollow">DISLIN</a>, which has a native interface for <code>Fortran</code>!  Not open source or free for commercial use though.</p>
",13,"2010-07-20 08:47:44",NULL,NULL,NULL,4,NULL,13,"2010-07-20 08:47:44","2010-10-12 08:48:39",224,NULL,NULL,NULL
245,2,NULL,"2010-07-20 08:20:36",0,NULL,"<p>Principal component scores are a group of scores that are obtained following a Principle Components Analysis (PCA).  In PCA the relationships between a group of scores is analyzed such that an equal number of new "imaginary" variables (aka principle components) are created.  The first of these new imaginary variables is maximally correlated with all of the original group of variables.  The next is somewhat less correlated, and so forth until the point that if you used all of the principal components scores to predict any given variable from the initial group you would be able to explain all of its variance.  The way in which PCA proceeds is complex and has certain restrictions.  Among these is the restriction that the correlation between any two principal components (i.e. imaginary variables) is zero; thus it doesn't make sense to try to predict one principal component with another.</p>
",196,"2010-07-20 08:20:36",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,222,NULL,NULL,NULL
246,2,NULL,"2010-07-20 08:28:52",6,NULL,"<p>You fitted the model to the differences, which means that you're describing the change in levels of violence. You get a lag of 2 days. A lag is indicative of the memory of the process. In other words, the change in levels of violence today has some dependency on the change in levels of violence in the last two days. For longer time-scales, the contribution of random influences becomes strong enough so that there is no clear link anymore.</p>

<p>Is the auto-correlation positive? Then a change of levels of violence today suggests a similar change in levels of violence in two days. Is it negative? Then violence might stay higher for two days.</p>

<p>Of course, you may want to have to control for confounding effects. For example, after a serious incident, people may be more likely to report minor incidents, but this "sensitization" would be going away after two days.</p>
",198,"2010-07-20 08:28:52",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,242,NULL,NULL,NULL
247,2,NULL,"2010-07-20 08:36:36",2,NULL,"<p>Might be a bit narrow in scope, but if you're doing any work in Clojure on the JVM there's the excellent <a href="http://incanter.org" rel="nofollow">Incanter</a>:</p>

<blockquote>
  <p>Incanter is a Clojure-based, R-like platform for statistical computing and graphics.</p>
</blockquote>
",171,"2010-07-20 08:36:36",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-10-12 08:48:39",224,NULL,NULL,NULL
248,2,NULL,"2010-07-20 08:38:38",4,NULL,"<p>ggobi and the R links to Ggobi are really rather good for this.   There are simpler visualisations (iPlots is very nice, also interactive, as mentioned).</p>

<p>But it depends whether you are doing something more specialised.   For example TreeView lets you visualise the kind of cluster dendrograms you get out of microarrays.</p>
",211,"2010-07-20 08:38:38",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 08:38:38",196,NULL,NULL,NULL
249,1,NULL,"2010-07-20 08:49:13",4,454,"<p>I have a set of $N$ bodies, which is a random sample from a population whose mean and variance I want to estimate. A property of each body is being measured $m_i$ times ($m_i&gt;1$) and different for each body index $i$  identifies which body it is; the property is expected to be distributed around zero). I would like to describe the resulting measurement. Particularly I'm interested in average property value and in the variance.</p>

<p>The average value is simple. First calculate the mean values for each body and then calculate the mean of means.</p>

<p>The variance is more tricky. There are two variances: the variance of measurement and the variance of property values. In order to have an idea on the confidence we have in any single measurement, we need to account for both the sources. Unfortunately, I can't think of a good method. It is obvious that putting all the numbers in a single pool and calculating the stdev of this pool isn't a good idea.</p>

<p>Any suggestion?</p>

<p><strong>EDIT</strong>
Colin Gillespie suggests applying Random Effects Model. This model seems to be the right solution for my case, except for the fact that it is described (in Wikipedia) for the cases where each group (body in my case) is sampled equally ($m_i$ is constant for all the bodies), which is not correct in my case</p>
",213,"2012-12-05 18:20:34","Variance components",<standard-deviation><variance><anova><random-effects-model>,1,1,NULL,17230,"2012-12-05 18:20:34",NULL,NULL,NULL,NULL,NULL
250,2,NULL,"2010-07-20 08:53:15",5,NULL,"<p>Update (August 2014): as @gappy comments below, as of R version 3.0.0 the limits are higher and means R is capable of handling larger datasets.</p>

<p>Here's a data point: R has a <a href="http://www.bytemining.com/2010/05/hitting-the-big-data-ceiling-in-r/" rel="nofollow">"big data ceiling"</a>, useful to know if you plan on working with huge data sets.</p>

<p>I'm unsure whether the same limitations apply to Clojure/Incanter, whether it outperforms R or is actually worse. I imagine the JVM can probably handle large datasets, especially if you manage to harness the power of Clojure's lazy features.</p>
",171,"2014-08-01 17:38:23",NULL,NULL,NULL,5,NULL,171,"2014-08-01 17:38:23",NULL,130,NULL,NULL,NULL
252,2,NULL,"2010-07-20 09:13:29",5,NULL,"<p>For me personally, I use the following three packages the most, all available from the awesome <a href="http://www.omegahat.org" rel="nofollow">Omega Project for Statistical Computing</a> (I do not claim to be an expert, but for my purposes they are very easy to use):</p>

<ul>
<li><p><strong>RCurl</strong>: It has lots of options which allows access to websites that the default functions in base R would have difficulty with I think it's fair to say. It is an R-interface to the libcurl library, which has the <em>added</em> benefit of a whole community outside of R developing it. Also available on <a href="http://cran.r-project.org/web/packages/RCurl/index.html" rel="nofollow">CRAN</a>.</p></li>
<li><p><strong>XML</strong>: It is very forgiving of parsing malformed XML/HTML. It is an R-interface to the libxml2 library and again has the <em>added</em> benefit of a whole community outside of R developing it Also available on <a href="http://cran.r-project.org/web/packages/XML/index.html" rel="nofollow">CRAN</a>.</p></li>
<li><strong>RJSONIO</strong>: It allows one to parse the text returned from a json call and organise it into a list structure for further analysis.The competitor to this package is rjson but this one has the advantage of being vectorised, readily extensible through S3/S4, fast and scalable to large data.  </li>
</ul>
",81,"2010-07-28 10:02:05",NULL,NULL,NULL,3,NULL,81,"2010-07-28 10:02:05","2010-07-20 09:13:29",73,NULL,NULL,NULL
253,2,NULL,"2010-07-20 09:26:05",7,NULL,"<p>If you're an economist/econometrician then Grant Farnworth's paper on using R is indispensable and is available on CRAN at:
<a href="http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf">http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf</a></p>
",215,"2010-07-20 11:37:25",NULL,NULL,NULL,0,NULL,215,"2010-07-20 11:37:25","2010-08-01 18:56:25",138,NULL,NULL,NULL
254,2,NULL,"2010-07-20 09:30:18",4,NULL,"<p>I think if I understand your description correctly, you need to use a <a href="http://en.wikipedia.org/wiki/Random_effects_model" rel="nofollow">linear mixed model</a>. However, this maybe overkill, since these models are used to find differences between groups. For example, if you have two types of bodies and you wish to determine if they are different.</p>

<p>Basically, you have <em>between</em> subject variation and <em>within</em> subject variation.</p>

<p>To fit these models in R, you can use the <code>lmer</code> function from the <code>lme4</code> library. So if I understand you correctly, your function will look something like this:</p>

<pre><code>#Load the R library
library(lme4)

#data is a R data frame that contains your data
#measurement and Subject are variables
fm1 = lmer(measurement ~ (1|Subject), data)
</code></pre>

<p>If you are looking for differences between bodies, then it will look something like:</p>

<pre><code>fm2 = lmer(measurement ~ body + (body|Subject), data)
</code></pre>

<p>The command <code>summary(fm1)</code> should give the values you are after.</p>

<p>Here are some resources that will help you get started:</p>

<ol>
<li><a href="http://lme4.r-forge.r-project.org/" rel="nofollow">Documentation</a> for the lme4 package</li>
<li><a href="http://zoonek2.free.fr/UNIX/48_R/all.html" rel="nofollow">Statistics with R</a></li>
</ol>

<p>Most statistical software will be able to fit models of this type.</p>

<p>BTW, the subject part is usually called the random effect. However, there a many different  views on what a random effect is. See Ch11.4 of <a href="http://rads.stackoverflow.com/amzn/click/0521867061" rel="nofollow">Data analysis using regression</a> by Gelman and Hill for more details.</p>
",8,"2010-07-20 10:50:24",NULL,NULL,NULL,3,NULL,8,"2010-07-20 10:50:24",NULL,249,NULL,NULL,NULL
255,2,NULL,"2010-07-20 09:30:51",8,NULL,"<p>It may be an overshoot, but you may train an unsupervised Random Forest on the data and use the object proximity measure to detect outliers. More details <a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#outliers" rel="nofollow">here</a>.</p>
",88,"2010-07-20 09:30:51",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,213,NULL,NULL,NULL
256,1,265,"2010-07-20 09:34:22",14,1422,"<p>What is the easiest way to understand boosting?</p>

<p>Why doesn't it boost very weak classifiers "to infinity" (perfection)?</p>
",217,"2013-08-16 15:55:48","How does boosting work?",<machine-learning><boosting>,3,0,7,7290,"2013-08-16 15:55:48",NULL,NULL,NULL,NULL,NULL
257,1,262,"2010-07-20 09:38:44",14,6700,"<p>We may assume that we have CSV file and we want a very basic line plot with several lines on one plot and a simple legend.</p>
",217,"2012-11-03 15:43:54","What is the easiest way to create publication-quality plots under Linux?",<data-visualization><csv-file>,7,1,9,159,"2010-08-17 05:27:15",NULL,NULL,NULL,NULL,NULL
258,1,NULL,"2010-07-20 09:43:23",14,780,"<p>Rules:</p>

<ul>
<li>one classifier per answer</li>
<li>vote up if you agree </li>
<li>downvote/remove duplicates.</li>
<li>put your application in the comment</li>
</ul>
",217,"2012-10-22 13:41:54","What is the best out-of-the-box 2-class classifier for your application?",<machine-learning><classification><application>,10,0,10,88,"2011-11-17 00:45:57","2010-07-20 09:43:23",NULL,NULL,NULL,NULL
259,2,NULL,"2010-07-20 09:44:11",12,NULL,"<p><a href="http://en.wikipedia.org/wiki/Support_vector_machine">Support vector machine</a></p>
",217,"2010-07-20 09:44:11",NULL,NULL,NULL,5,NULL,NULL,NULL,"2010-07-20 09:44:11",258,NULL,NULL,NULL
260,2,NULL,"2010-07-20 09:45:06",12,NULL,"<p><a href="http://en.wikipedia.org/wiki/Random_forest">Random forest</a></p>
",217,"2010-07-20 09:45:06",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-20 09:45:06",258,NULL,NULL,NULL
261,2,NULL,"2010-07-20 09:45:18",12,NULL,"<p>It's hard to go past R for graphics. You could do what you want in 3 lines. For example, assuming the csv file has four columns:</p>

<pre><code>x &lt;- read.csv("file.csv")
matplot(x[,1],x[,2:4],type="l",col=1:3)
legend("topleft",legend=c("A","B","C"),lty=1,col=1:3)
</code></pre>
",159,"2010-07-20 09:45:18",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,257,NULL,NULL,NULL
262,2,NULL,"2010-07-20 09:50:45",13,NULL,"<p>The easiest way is to use R </p>

<p>Use <code>read.csv</code> to enter the data into R, then use a combination of the <code>plot</code> and <code>line</code> commands</p>

<p>If you want something really special, then look at the libraries <a href="http://had.co.nz/ggplot2/">ggplot2</a> or <a href="http://lmdvr.r-forge.r-project.org/figures/figures.html">lattice</a>. </p>

<p>In <code>ggplot2</code> the following commands should get you started.</p>

<pre><code>require(ggplot2)
#You would use read.csv here
N = 10
d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))
p = ggplot(d)

p = p+geom_line(aes(x, y1, colour="Type 1"))
p = p+geom_line(aes(x, y2, colour="Type 2"))
p = p+geom_line(aes(x, y3, colour="Type 3"))
#Add points
p = p+geom_point(aes(x, y3, colour="Type 3"))
print(p)   
</code></pre>

<p>This would give you the following plot:</p>

<p><img src="http://img84.imageshack.us/img84/6393/tmpq.jpg" alt="Line plot"></p>

<p><strong>Saving plots in R</strong></p>

<p>Saving plots in R is straightforward:</p>

<pre><code>#Look at ?jpeg to other different saving options
jpeg("figure.jpg")
print(p)#for ggplot2 graphics
dev.off()
</code></pre>

<p>Instead of <code>jpeg</code>'s you can also save as a <code>pdf</code> or postscript file:</p>

<pre><code>#This example uses R base graphics
#Just change to print(p) for ggplot2
pdf("figure.pdf")
plot(d$x,y1, type="l")
lines(d$x, y2)
dev.off()
</code></pre>
",8,"2010-07-20 14:48:45",NULL,NULL,NULL,7,NULL,8,"2010-07-20 14:48:45",NULL,257,NULL,NULL,NULL
263,2,NULL,"2010-07-20 09:53:11",8,NULL,"<p>My favorite tool is Python with <a href="http://matplotlib.sourceforge.net/" rel="nofollow">mathplotlib</a></p>

<p>The advantages:</p>

<ul>
<li>Immediate export from the environment where I do my experiments in</li>
<li>Support for the scipy/numpy data structures</li>
<li>Familiar syntax/options (matlab background)</li>
<li>Full latex support for labels/legends etc. So same typesetting as in the rest of your document!</li>
</ul>

<p>Specifically, for different file formats like svg and eps, use the format parameter of <a href="http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig" rel="nofollow">savefig</a></p>

<p>An example:
input.csv</p>

<pre>"Line 1",0.5,0.8,1.0,0.9,0.9
"Line 2",0.2,0.7,1.2,1.1,1.1</pre>

<p>Code:</p>

<pre><code>import csv
import matplotlib.pyplot as plt

legends = []
for row in csv.reader(open('input.csv')):
    legends.append(row[0])
    plt.plot(row[1:])

plt.legend(legends)
plt.savefig("out.svg", format='svg')
</code></pre>
",190,"2010-07-20 14:14:17",NULL,NULL,NULL,6,NULL,190,"2010-07-20 14:14:17",NULL,257,NULL,NULL,NULL
264,2,NULL,"2010-07-20 09:59:00",5,NULL,"<p><a href="http://www.ligo.caltech.edu/docs/T/T030168-00.pdf">Here</a> is an article describing one possible algorith. Source code included and a quite serious application (gravitational wave detection based on laser interferometry), so you can expect it to be well tested.</p>
",217,"2010-07-20 09:59:00",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,134,NULL,NULL,NULL
265,2,NULL,"2010-07-20 10:05:48",14,NULL,"<p>In plain English: If your classifier misclassifies some data, train another copy of it mainly on this misclassified part with hope that it will discover something subtle. And then, as usual, iterate. On the way there are some voting schemes that allow to combine all those classifiers' predictions in sensible way.</p>

<p>Because sometimes it is impossible (the noise is just hiding some of the information, or it is not even present in the data); on the other hand, boosting too much may lead to overfitting.</p>
",88,"2010-07-20 10:17:16",NULL,NULL,NULL,0,NULL,88,"2010-07-20 10:17:16",NULL,256,NULL,NULL,NULL
266,2,NULL,"2010-07-20 10:23:56",3,NULL,"<p>As you mentioned sorting would be <code>O(n·log n)</code> for a window of length <code>n</code>. Doing this moving adds another <code>l=vectorlength</code> making the total cost <code>O(l·n·log n)</code>.</p>

<p>The simplest way to push this is by keeping an ordered list of the last n elements in memory when moving from one window to the next one. As removing/inserting one element from/into an ordered list are both <code>O(n)</code> this would result in costs of <code>O(l·n)</code>.</p>

<p>Pseudocode:</p>

<pre><code>l = length(input)
aidvector = sort(input(1:n))
output(i) = aid(n/2)
for i = n+1:l
    remove input(i-n) from aidvector
    sort aid(n) into aidvector
    output(i) = aid(n/2)
</code></pre>
",128,"2010-07-20 10:23:56",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,134,NULL,NULL,NULL
267,1,271,"2010-07-20 10:35:55",7,4254,"<p>If I have two lists A and B, both of which are subsets of a much larger list C, how can I determine if the degree of overlap of A and B is greater than I would expect by chance?</p>

<p>Should I just randomly select elements from C of the same lengths as lists A and B and determine that random overlap, and do this many times to determine some kind or empirical p-value? Is there a better way to test this?</p>
",194,"2013-07-17 02:30:07","How do I calculate if the degree of overlap between two lists is significant?",<statistical-significance>,2,1,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
268,2,NULL,"2010-07-20 11:01:06",3,NULL,"<p>You could of for a supervised self-organizing map (e.g. with <a href="http://cran.r-project.org/web/packages/kohonen/index.html" rel="nofollow">kohonen</a> package for R), and use the login frequency as dependent variable. That way, the clustering will focus on separating the frequent visitors from the rare visitors. By plotting the number of users on each map unit, you may get an idea in clusters present in your data.</p>

<p>Because SOMs are non-linear mapping methods, this approach is particularly interesting for tailed data.</p>
",107,"2010-07-20 11:01:06",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,47,NULL,NULL,NULL
269,1,416,"2010-07-20 11:07:42",11,53644,"<p>What is the difference between a population and a sample? What common variables and statistics are used for each one, and how do those relate to each other? </p>
",62,"2014-02-16 09:59:06","What is the difference between a population and a sample?",<standard-deviation><variance><sample><population>,4,0,5,88,"2010-08-07 17:55:39",NULL,NULL,NULL,NULL,NULL
270,1,279,"2010-07-20 11:08:47",12,1291,"<p>Due to the factorial in a poisson distribution, it becomes unpractical to estimate poisson models (for example, using maximum likelihood) when the observations are large. So, for example, if I am trying to estimate a model to explain the number of suicides in a given year (only annual data are available), and say, there are thousands of suicides every year, is it wrong to express suicides in hundreds, so that 2998 would be 29.98 ~= 30? In other words, is it wrong to change the unit of measurement to make the data manageable? </p>
",90,"2012-02-16 15:40:08","Poisson regression with large data: is it wrong to change the unit of measurement?",<modeling><poisson><large-data>,5,0,4,8,"2010-10-08 16:05:24",NULL,NULL,NULL,NULL,NULL
271,2,NULL,"2010-07-20 11:10:42",7,NULL,"<p>If I understand your question correctly, you need to use the <a href="http://en.wikipedia.org/wiki/Hypergeometric_distribution" rel="nofollow">Hypergeometric distribution</a>. This distribution is usually associated with urn models, i.e there are $n$ balls in an urn, $y$ are painted red, and you draw $m$ balls from the urn. Then if $X$ is the number of balls in your sample of $m$ that are red, $X$ has a hyper-geometric distribution.</p>

<p>For your specific example, let $n_A$, $n_B$ and $n_C$ denote the lengths of your three lists and let $n_{AB}$ denote the overlap between $A$ and $B$. Then </p>

<p>$$n_{AB} \\sim \\text{HG}(n_A, n_C, n_B)$$</p>

<p>To calculate a p-value, you could use this R command: </p>

<pre><code>#Some example values
n_A = 100;n_B = 200; n_C = 500; n_A_B = 50
1-phyper(n_A_B, n_B, n_C-n_B, n_A)
[1] 0.008626697
</code></pre>

<p>Word of caution. Remember multiple testing, i.e. if you have lots of <em>A</em> and <em>B</em> lists, then you will need to adjust your p-values with a correction. For the example the FDR or Bonferroni corrections.</p>
",8,"2013-07-17 02:30:07",NULL,NULL,NULL,0,NULL,805,"2013-07-17 02:30:07",NULL,267,NULL,NULL,NULL
272,2,NULL,"2010-07-20 11:14:41",2,NULL,"<p>Also, for some elaborate discussion (including bashing of ADF / PP / KPSS :) you might want to have a look at the book by Maddala and Kim:</p>

<p><a href="http://rads.stackoverflow.com/amzn/click/0521587824" rel="nofollow">http://www.amazon.com/Cointegration-Structural-Change-Themes-Econometrics/dp/0521587824</a></p>

<p>Quite extensive and not very easy to read sometimes, but a useful reference.</p>
",216,"2010-07-20 11:14:41",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,161,NULL,NULL,NULL
273,2,NULL,"2010-07-20 11:18:01",1,NULL,"<ul>
<li><p>Clearly R</p></li>
<li><p>RadidMiner is nice, but switching to thinking in terms of operators takes a moment</p></li>
<li><p>Matlab / Octave</p></li>
</ul>

<p>If you describe a specific problem, I may be able to get more specific.</p>
",216,"2010-07-20 11:18:01",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,25,NULL,NULL,NULL
274,2,NULL,"2010-07-20 11:21:59",10,NULL,"<p>The population is the whole set of values, or individuals, you are interested in. The sample is a subset of the population, and is the set of values you actually use in your estimation.</p>

<p>So, for example, if you want to know the average height of the residents of China, that is your population, ie, the population of China. The thing is, this is quite large a number, and you wouldn't be able to get data for everyone there. So you draw a sample, that is, you get some observations, or the height of some of the people in China (a subset of the population, the sample) and do your inference based on that. </p>
",90,"2010-07-20 11:21:59",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,269,NULL,NULL,NULL
275,2,NULL,"2010-07-20 11:29:53",5,NULL,"<p>In case of Poisson it is bad, since counts are counts -- their unit is an unity. On the other hand, if you'd use some advanced software like R, its Poisson handling functions will be aware of such large numbers and would use some numerical tricks to handle them.</p>

<p>Obviously I agree that normal approximation is another good approach.</p>
",88,"2010-07-20 12:26:55",NULL,NULL,NULL,0,NULL,88,"2010-07-20 12:26:55",NULL,270,NULL,NULL,NULL
276,1,553,"2010-07-20 11:43:25",8,1517,"<p>Is there a rule-of thumb or even any way at all to tell how large a sample should be in order to estimate a model with a given number of parameters? </p>

<p>So, for example, if I want to estimate a least-squares regression with 5 parameters, how large should the sample be? </p>

<p>Does it matter what estimation technique you are using (e.g. maximum likelihood, least squares, GMM), or how many or what tests you are going to perform? Should the sample variability be taken into account when making the decision?</p>
",90,"2010-09-16 22:25:36","How large should a sample be for a given estimation technique and parameters?",<sample-size><estimation><least-squares><maximum-likelihood>,4,0,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
277,1,NULL,"2010-07-20 11:49:02",12,3033,"<p>When would one prefer to use a Conditional Autoregressive model over a Simultaneous Autoregressive model when modelling autocorrelated geo-referenced areal data?</p>
",215,"2011-04-26 08:51:44","Spatial statistics models -- CAR vs SAR",<modeling><spatial>,2,0,4,88,"2011-04-26 08:51:44",NULL,NULL,NULL,NULL,NULL
278,1,NULL,"2010-07-20 11:49:27",6,120,"<p>When a non-hierarchical cluster analysis is carried out, the order of observations in the data file determine the clustering results, especially if the data set is small (i.e, 5000 observations). To deal with this problem I usually performed a random reorder of data observations. My problem is that if I replicate the analysis n times, the results obtained are different and sometimes these differences are great. </p>

<p>How can I deal with this problem? Maybe I could run the analysis several times and after consider that one observation belong to the group in which more times was assigned. Has someone a better approach to this problem?</p>

<p>Manuel Ramon</p>
",221,"2010-09-17 20:36:37","How to deal with the effect of the order of observations in a non hierarchical cluster analysis?",<clustering>,3,1,1,88,"2010-09-17 20:36:37",NULL,NULL,NULL,NULL,NULL
279,2,NULL,"2010-07-20 11:54:15",9,NULL,"<p>When you're dealing with a Poisson distribution with large values of \\lambda (its parameter), it is common to use a normal approximation to the Poisson distribution. </p>

<p>As <a href="http://www.stat.ucla.edu/~dinov/courses_students.dir/Applets.dir/NormalApprox2PoissonApplet.html" rel="nofollow">this site</a> mentions, it's all right to use the normal approximation when \\lambda gets over 20, and the approximation improves as \\lambda gets even higher. </p>

<p>The Poisson distribution is defined only over the state space consisting of the non-negative integers, so rescaling and rounding is going to introduce odd things into your data. </p>

<p>Using the normal approx. for large Poisson statistics is VERY common.</p>
",62,"2010-07-20 11:54:15",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,270,NULL,NULL,NULL
280,2,NULL,"2010-07-20 11:54:50",4,NULL,"<p>The <a href="http://www.r-project.org/" rel="nofollow">R project</a> website has lots of manuals to start, and I suggest you the <a href="http://r.789695.n4.nabble.com/" rel="nofollow">Nabble R forum</a> and the <a href="http://www.r-bloggers.com/" rel="nofollow">R-bloggers</a> site as well. </p>
",221,"2010-07-20 11:54:50",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
281,2,NULL,"2010-07-20 11:59:56",2,NULL,"<p>Day-to-day the most useful package must be "foreign" which has functions for reading and writing data for other statistical packages e.g. Stata, SPSS, Minitab, SAS, etc. Working in a field where R is not that commonplace means that this is a very important package.</p>
",215,"2012-08-27 17:52:32",NULL,NULL,NULL,0,NULL,919,"2012-08-27 17:52:32","2010-07-20 11:59:56",73,NULL,NULL,NULL
282,2,NULL,"2010-07-20 12:02:26",23,NULL,"<p><strong>First, lets define a score:</strong></p>

<p>John, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:</p>

<pre><code>      Maths    Science    English    Music    
John  80        85          60       55  
Mike  90        85          70       45
Kate  95        80          40       50
</code></pre>

<p>In this case there are 12 scores in total. Each <strong>score</strong> represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.</p>

<p><strong>Now lets informally define a Principal Component:</strong></p>

<p>In the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:</p>

<ul>
<li>You could plot two subjects in the exact same way you would with x &amp; y co-ordinates in a 2D graph. </li>
<li>You could even plot three subjects in the same way you would plot x, y &amp; z in a 3D graph (though this is generally bad practice, because some distortion is inevitable in the 2D representation of 3D data). </li>
</ul>

<p>But how would you plot 4 subjects?</p>

<p>At the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as <em>Multidimensional scaling</em>.</p>

<p>Principal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could look at the types of subjects each student is maybe more suited to.</p>

<p>A principal component is therefore a combination of the original variables after a linear transformation. In <strong>R</strong>, this is:</p>

<pre><code>DF&lt;-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))
prcomp(DF, scale = FALSE)
</code></pre>

<p>Which will give you something like this (first two Principal Components only for sake of simplicity):</p>

<pre><code>                PC1         PC2
Maths    0.27795606  0.76772853 
Science -0.17428077 -0.08162874 
English -0.94200929  0.19632732 
Music    0.07060547 -0.60447104 
</code></pre>

<p><strong>So what is a Principal Component Score?</strong></p>

<p>It's a score from the table at the end of this post.</p>

<p>The output from <strong>R</strong> means we can now plot each person's score across all subjects in a 2D graph as follows:</p>

<pre><code>      x                                       y
John 0.28*80 + -0.17*85 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 
Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45
Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50
</code></pre>

<p>Which simplifies to:</p>

<pre><code>      x       y
John  -44.6  33.2
Mike  -51.9   48.8
Kate  -21.1   44.35
</code></pre>

<p>There are <strong><em>six principal component scores</em></strong> in the table above. You can now plot the scores in a 2D graph to get a sense of the type of subjects each student is perhaps more suited to.</p>

<p>EDIT 1: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.</p>

<p>EDIT 2: full credit to @drpaulbrewer for his comment in improving this answer.</p>
",81,"2014-01-13 11:11:25",NULL,NULL,NULL,3,NULL,29367,"2014-01-13 11:11:25",NULL,222,NULL,NULL,NULL
283,1,307,"2010-07-20 12:09:08",22,13685,"<p>What is meant when we say we have a saturated model?</p>
",215,"2011-09-21 13:17:58","What is a "saturated" model?",<modeling><regression>,5,0,6,NULL,"2010-07-20 14:26:17",NULL,NULL,NULL,NULL,user28
284,2,NULL,"2010-07-20 12:25:30",1,NULL,"<p>A random variable, usually denoted X, is a variable where the outcome is uncertain. The observation of a particular outcome of this variable is called a realisation. More concretely, it is a function which maps a probability space into a measurable space, usually called a state space. Random variables are discrete (can take a number of distinct values) or continuous (can take an infinite number of values). </p>

<p>Consider the random variable X which is the total obtained when rolling two dice. It can take any of the values 2-12 (with equal probability given fair dice) and the outcome is uncertain until the dice are rolled. </p>
",215,"2010-07-20 12:25:30",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,50,NULL,NULL,NULL
285,2,NULL,"2010-07-20 12:26:39",1,NULL,"<p>You might consider transforming (perhaps a log) the positively skewed variables.</p>

<p>If after exploring various clustering algorithms you find that the four variables simply reflect varying intensity levels of usage, you might think about a theoretically based classification. Presumably this classification is going to be used for a purpose and that purpose could drive meaningful cut points on one or more of the variables.</p>
",183,"2010-07-20 12:26:39",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,47,NULL,NULL,NULL
286,2,NULL,"2010-07-20 12:28:32",12,NULL,"<p>There's a superb Probability book here:
<a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html">http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html</a>
which you can also buy in hardcopy.;</p>
",211,"2010-07-20 12:28:32",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,170,NULL,NULL,NULL
287,1,506,"2010-07-20 12:29:17",7,1068,"<p>Can someone explain to me the difference between method of moments and GMM (general method of moments), their relationship, and when should one or the other be used?</p>
",90,"2013-10-22 14:58:47","What is the difference/relationship between method of moments and GMM?",<estimation><method-of-moments><generalized-moments>,1,1,3,5739,"2013-10-22 14:58:47",NULL,NULL,NULL,NULL,NULL
288,1,NULL,"2010-07-20 12:29:34",5,1093,"<p>Suppose that I culture cancer cells in <em>n</em> different dishes <em>g₁</em>, <em>g₂</em>, … , <em>g<sub>n</sub></em> and observe the number of cells <em>n<sub>i</sub></em> in each dish that look different than normal.  The total number of cells in dish <em>g<sub>i</sub></em> is <em>t<sub>i</sub></em>.  There is individual differences between individual cells, but also differences between the populations in different dishes because each dish has a slightly different temperature, amount of liquid, and so on.</p>

<p>I model this as a beta-binomial distribution: <em>n<sub>i</sub></em> ~ Binomial(<em>p<sub>i</sub></em>, <em>t<sub>i</sub></em>) where <em>p<sub>i</sub></em> ~ Beta(<em>α</em>, <em>β</em>).  Given a number of observations of <em>n<sub>i</sub></em> and <em>t<sub>i</sub></em>, how can I estimate <em>α</em> and <em>β</em>?</p>
",220,"2012-11-15 07:31:15","Estimating beta-binomial distribution",<estimation><beta-binomial>,3,0,1,NULL,"2010-07-21 03:10:58",NULL,NULL,NULL,NULL,user28
289,2,NULL,"2010-07-20 12:32:41",4,NULL,"<p>For visualizing graphs in a Java/SWT environment, check out Zest: <a href="http://eclipse.org/gef/zest" rel="nofollow">http://eclipse.org/gef/zest</a></p>
",80,"2010-07-20 12:32:41",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-12 08:48:39",224,NULL,NULL,NULL
290,1,445,"2010-07-20 12:33:30",6,811,"<p>I know of Cameron and Trivedi's Microeconometrics Using Stata. </p>

<p>What are other good texts for learning Stata? </p>
",189,"2011-11-17 02:52:41","Resources for learning Stata",<books><stata><big-list>,7,1,2,919,"2011-11-16 20:27:35","2011-11-16 20:17:12",NULL,NULL,NULL,NULL
291,2,NULL,"2010-07-20 12:35:43",7,NULL,"<p><a href="http://lib.stat.cmu.edu/DASL/" rel="nofollow">http://lib.stat.cmu.edu/DASL/</a></p>
",211,"2010-07-20 12:35:43",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-08-12 20:30:10",7,NULL,NULL,NULL
292,2,NULL,"2010-07-20 12:38:38",2,NULL,"<p>It should always be large enough! ;)</p>

<p>All parameter estimates come with an estimate uncertainty, which is determined by the sample size. If you carry out a regression analysis, it helps to remind yourself that the &Chi;<sup>2</sup> distribution is constructed from the input data set. If your model had 5 parameters and you had 5 data points, you would only be able to calculate a single point of the &Chi;<sup>2</sup> distribution. Since you will need to minimize it, you could only pick that one point as a guess for the minimum, but would have to assign infinite errors to your estimated parameters. Having more data points would allow you to map the parameter space better leading to a better estimate of the minimum of the &Chi;<sup>2</sup> distribution and thus smaller estimator errors.</p>

<p>Would you be using a Maximum Likelihood estimator instead the situation would be similar: More data points leads to better estimate of the minimum.</p>

<p>As for point variance, you would need to model this as well. Having more data points would make clustering of points around the "true" value more obvious (due to the Central Limit Theorem) and the danger of interpreting a large, chance flucuation as the true value for that point would go down. And as for any other parameter your estimate for the point variance would become more stable the more data points you have.</p>
",56,"2010-07-20 12:38:38",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,276,NULL,NULL,NULL
293,2,NULL,"2010-07-20 12:43:02",4,NULL,"<p>I think you need to rework this question.   It all depends on the problem/data which has generated the cross-tab.   </p>
",211,"2010-07-20 12:43:02",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,192,NULL,NULL,NULL
294,2,NULL,"2010-07-20 12:47:39",25,NULL,"<p>Bernard Flury, in his excellent book introducing multivariate analysis, described this as an anti-property of principal components.   It's actually worse than choosing between correlation or covariance.   If you changed the units (e.g. US style gallons, inches etc. and EU style litres, centimetres) you will get substantively different projections of the data.</p>

<p>The argument against automatically using correlation matrices is that it is quite a brutal way of standardising your data.   The problem with automatically using the covariance matrix, which is very apparent with that heptathalon data, is that the variables with the highest variance will dominate the first principal component (the variance maximising property).</p>

<p>So the "best" method to use is based on a subjective choice, careful thought and some experience.</p>
",211,"2010-07-20 12:47:39",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,53,NULL,NULL,NULL
295,2,NULL,"2010-07-20 12:52:55",8,NULL,"<p>A nice definition of p-value is "the probability of observing a test statistic at least as large as the one calculated assuming the null hypothesis is true". </p>

<p>The problem with that is that it requires an understanding of "test statistic" and "null hypothesis". But, that's easy to get across. If the null hypothesis is true, usually something like "parameter from population A is equal to parameter from population B", and you calculate statistics to estimate those parameters, what is the probability of seeing a test statistic that says, "they're this different"?</p>

<p>E.g., If the coin is fair, what is the probability I'd see 60 heads out of 100 tosses? That's testing the null hypothesis, "the coin is fair", or "p = .5" where p is the probability of heads.</p>

<p>The test statistic in that case would be the number of heads. </p>

<p>Now, I <em>assume</em> that what you're calling "t-value" is a generic "test statistic", not a value from a "t distribution". They're not the same thing, and the term "t-value" isn't (necessarily) widely used and could be confusing.</p>

<p>What you're calling "t-value" is probably what I'm calling "test statistic". In order to calculate a p-value (remember, it's just a probability) you need a distribution, and a value to plug into that distribution which will return a probability. Once you do that, the probability you return is your p-value. You can see that they are related because under the same distribution, different test-statistics are going to return different p-values. More extreme test-statistics will return lower p-values giving greater indication that the null hypothesis is false. </p>

<p>I've ignored the issue of one-sided and two-sided p-values here.  </p>
",62,"2010-07-20 12:52:55",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,31,NULL,NULL,NULL
296,2,NULL,"2010-07-20 12:54:28",10,NULL,"<p><a href="http://mlcomp.org/">MLComp</a> has quite a few interesting datasets, and as a bonus your algorithm will get ranked if you upload it.</p>
",127,"2010-07-20 12:54:28",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-08-12 20:30:18",7,NULL,NULL,NULL
297,2,NULL,"2010-07-20 13:08:42",5,NULL,"<p>I really enjoy working with <a href="http://roofit.sourceforge.net/">RooFit</a> for easy proper fitting of signal and background distributions and <a href="http://tmva.sourceforge.net/">TMVA</a> for quick principal component analyses and modelling of multivariate problems with some standard tools (like genetic algorithms and neural networks, also does BDTs). They are both part of the <a href="http://root.cern.ch/drupal/">ROOT</a> C++ libraries which have a pretty heavy bias towards particle physics problems though. </p>
",56,"2010-07-20 13:08:42",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 13:08:42",3,NULL,NULL,NULL
298,1,NULL,"2010-07-20 13:11:50",55,67592,"<p>Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?</p>
",125,"2013-04-25 08:25:41","In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?",<distributions><regression><data-transformation><logarithm>,6,4,55,919,"2010-10-12 19:04:00",NULL,NULL,NULL,NULL,NULL
299,2,NULL,"2010-07-20 13:16:29",8,NULL,"<p>One typically takes the log of an input variable to scale it and change the distribution (e.g. to make it normally distributed).  It cannot be done blindly however; you need to be careful when making any scaling to ensure that the results are still interpretable.  </p>

<p>This is discussed in most introductory statistics texts.  You can also read Andrew Gelman's paper on <a href="http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf">"Scaling regression inputs by dividing by two standard deviations"</a> for a discussion on this.  He also has a very nice discussion on this at the beginning of <a href="http://www.stat.columbia.edu/~gelman/arm/">"Data Analysis Using Regression and Multilevel/Hierarchical Models"</a>.</p>

<p>Taking the log is not an appropriate method for dealing with bad data/outliers.</p>
",5,"2010-07-20 13:22:18",NULL,NULL,NULL,0,NULL,5,"2010-07-20 13:22:18",NULL,298,NULL,NULL,NULL
300,2,NULL,"2010-07-20 13:19:31",4,NULL,"<p>A "right" answer cannot depend on an arbitrary ordering of some method you are using.</p>

<p>You need to consider all possible orderings (or some representative sample) and estimate your parameters for every case. This will give you distributions for the parameters you are trying to estimate. Estimate the "true" parameter values from these distributions (this will also give you an estimate for your estimator error).</p>

<p>Alternatively use a method that doesn't introduce an ordering.</p>
",56,"2010-07-20 13:19:31",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,278,NULL,NULL,NULL
301,2,NULL,"2010-07-20 13:22:40",6,NULL,"<p>You tend to take logs of the data when there is a problem with the residuals. For example, if you plot the residuals against a particular covariate and observe an increasing/decreasing pattern (a funnel shape), then a transformation may be appropriate. Non-random residuals usually indicate that your model assumptions are wrong, i.e. non-normal data.</p>

<p>Some data types automatically lend themselves to logarithmic transformations. For example, I usually take logs when dealing with concentrations or age. </p>

<p>Although transformations aren't primarily used to deal outliers, they do help since taking logs squashes your data.</p>
",8,"2010-07-20 13:22:40",NULL,NULL,NULL,7,NULL,NULL,NULL,NULL,298,NULL,NULL,NULL
302,2,NULL,"2010-07-20 14:04:12",1,NULL,"<p>I've heard two rules of thumb in this regard.  One holds that so long as there are enough observations in the error term to evoke the central limit theorem, e.g. 20 or 30, you are fine.  The other holds that for each estimated slope one should have at least 20 or 30 observations.  The difference between using 20 or 30 as the target number is based on different thoughts regarding when there are enough observations to reasonably evoke the Central Limit Theorem. </p>
",196,"2010-07-20 14:04:12",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,276,NULL,NULL,NULL
303,2,NULL,"2010-07-20 14:08:42",2,NULL,"<p>You have a hierarchical bayesian model. Brief details below:</p>

<p>Likelihood Function: </p>

<p>$$f(n_i | p_i, t_i) = (t_i n_i) p_i^{n_i} (1-p_i)^{(t_i - n_i)}$$</p>

<p>Priors on $p_i, \\alpha, \\beta$:</p>

<p>$$pi \\sim Beta(\\alpha, \\beta)$$</p>

<p>$$\\alpha ~ N(\\alpha_{mean}, \\alpha_{var}) I(\\alpha &gt; 0)$$</p>

<p>$$\\beta ~ N(\\beta_{mean}, \\beta_{var}) I(\\beta &gt; 0)$$</p>

<p>Posteriors are:</p>

<p>$$p_i \\sim Beta(\\alpha + n_i, \\beta + t_i-n_i)$$</p>

<p>$$\\alpha \\propto I(\\alpha &gt; 0) \\prod p_i^{(\\alpha-1)} exp(-(\\alpha-\\alpha_{mean})^2) / (2 \\alpha_{var})$$</p>

<p>$$\\beta \\propto I(\\beta &gt; 0) \\prod (1-p_i)^{(\\beta-1)} exp(-(\\beta-\\beta_{mean})^2) / (2 \\beta_{var})$$</p>

<p>You can then use a combination of Gibbs and Metropolis-Hastings to draw from the posterior distributions.</p>
",NULL,"2012-11-15 07:31:15",NULL,NULL,NULL,2,NULL,9007,"2012-11-15 07:31:15",NULL,288,NULL,user28,NULL
304,2,NULL,"2010-07-20 14:13:50",0,NULL,"<p>Shane's point that taking the log to deal with bad data is well taken.  As is Colin's regarding the importance of normal residuals.  In practice I find that usually you can get normal residuals if the input and output variables are also relatively normal.  In practice this means eyeballing the distribution of the transformed and untransformed datasets and assuring oneself that they have become more normal and/or conducting tests of normality (e.g. Shapiro-Wilk or Kolmogorov-Smirnov tests) and determining whether the outcome is more normal.  Interpretablity and tradition are also important.  For example, in cognitive psychology log transforms of reaction time are often used, however, to me at least, the interpretation of a log RT is unclear.  Furthermore, one should be cautious using log transformed values as the shift in scale can change a main effect into an interaction and vice versa.</p>
",196,"2010-07-20 14:13:50",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,298,NULL,NULL,NULL
305,1,796,"2010-07-20 14:19:41",27,2939,"<p>It seems like when the assumption of homogeneity of variance is met that the results from a Welch adjusted t-test and a standard t-test are approximately the same.  Why not simply always use the Welch adjusted t?</p>
",196,"2013-10-06 07:30:23","When conducting a t-test why would one prefer to assume (or test for) equal variances rather than always use a Welch approximation of the df?",<variance><t-test><homogeneity>,6,0,16,159,"2010-07-26 13:05:38",NULL,NULL,NULL,NULL,NULL
306,2,NULL,"2010-07-20 14:21:37",8,NULL,"<p>The model that fits the data doesn't have to be a time series model; I would advise thinking outside the box a little.  </p>

<p>If you have multiple variables (e.g. age, gender, diet, ethnicity, illness, medication) you can use these for a different model.  Maybe having certain patients in the same room is an important predictor?  Or perhaps it has to do with the attending staff?  Or consider using a multi-variate time series model (e.g. VECM) if you have other variables that you can use.  Look at the relationships between violence across patients: do certain patients act out together?  </p>

<p>The time series model is useful if time has some important role in the behavior.  For instance, there might be a clustering of violence.  Look at the volatility clustering literature.  As @Jonas suggests, with a lag order of 2, you may need to be on higher alert on the day following a burst in violence.  But that doesn't help you prevent the first day: there may be other information that you can link into the analysis to actually <em>understand</em> the cause of the violence, rather than simply forcasting it in a time series fashion.</p>

<p>Lastly, as a technical suggestion: if you're using R for the analysis, you might have a look at <a href="http://cran.r-project.org/web/packages/forecast/index.html">the forecast package</a> from Rob Hyndman (the creator of this site).  This has many very nice features; see the paper <a href="http://www.jstatsoft.org/v27/i03">"Automatic Time Series Forecasting: The forecast Package for R"</a> in the Journal of Statistical Software.</p>
",5,"2010-07-20 14:21:37",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,242,NULL,NULL,NULL
307,2,NULL,"2010-07-20 14:23:30",13,NULL,"<p>A saturated model is one in which there are as many estimated parameters as data points. By definition, this will lead to a perfect fit, but will be of little use statistically, as you have no data left to estimate variance.</p>

<p>For example, if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).</p>
",229,"2010-07-20 14:23:30",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,283,NULL,NULL,NULL
308,2,NULL,"2010-07-20 14:38:07",1,NULL,"<p>There are couple of good links with introductory material at Princton Uni Library <a href="http://libguides.princeton.edu/content.php?pid=27916&amp;sid=459449" rel="nofollow">website</a>.</p>
",22,"2010-07-20 14:38:07",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-11-16 20:17:12",290,NULL,NULL,NULL
309,2,NULL,"2010-07-20 14:40:32",2,NULL,"<p>The fact that something more complex reduces to something less complex when some assumption is checked is not enough to throw the simpler method away. </p>
",88,"2010-07-20 14:40:32",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,305,NULL,NULL,NULL
310,2,NULL,"2010-07-20 14:41:33",1,NULL,"<p>You can simply ignore the 'factorial' when using maximum likelihood. Here is the reasoning for your suicides example. Let:</p>

<p>&lambda; : Be the expected number of suicides per year</p>

<p>k<sub>i</sub>: Be the number of suicides in year i.</p>

<p>Then you would maximize the log-likelihood as:</p>

<p>LL = &sum; ( k<sub>i</sub> log(&lambda;) - &lambda; - k<sub>i</sub>! )</p>

<p>Maximizing the above is equivalent to maximizing the following as k<sub>i</sub>! is a constant :</p>

<p>LL<sup>'</sup> = &sum; ( k<sub>i</sub> log(&lambda;) - &lambda; )</p>

<p>Could explain why the factorial is an issue? Am I missing something?</p>
",NULL,"2010-07-20 14:41:33",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,270,NULL,user28,NULL
311,2,NULL,"2010-07-20 14:47:31",7,NULL,"<p>Rob Hyndman gave the easy exact answer for a fixed n.  If you're interested in asymptotic behavior for large n, this is handled in the field of <a href="http://en.wikipedia.org/wiki/Extreme_value_theory">extreme value theory</a>.  There is a small family of possible limiting distributions; see for example the first chapters of <a href="http://books.google.com/books?id=3ZKmAAAAIAAJ&amp;q=leadbetter+and+lindgren&amp;dq=leadbetter+and+lindgren&amp;hl=en&amp;ei=z7ZFTPPFH9T-nAfa58HaAw&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CC0Q6AEwAA">this book</a>.</p>
",89,"2010-07-20 14:47:31",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,220,NULL,NULL,NULL
312,1,318,"2010-07-20 15:03:40",4,118,"<p>I'm a physics graduate who ended up doing infosec so most of the statistics I ever learned is useful for thermodynamics. I'm currently trying to think of a model for working out how many of a population of computers are infected with viruses, though I assume the maths works out the same way for real-world diseases so references in or answers relevant to that field would be welcome too.</p>

<p>Here's what I've come up with so far:</p>

<ul>
<li>assume I know the total population of computers, N.</li>
<li>I know the fraction D of computers that have virus-detection software (i.e. the amount of the population that is being screened)</li>
<li>I know the fraction I of computers that have detection software <em>that has reported an infection</em></li>
<li>I don't know, but can find out or estimate, the probability of Type I and II errors in the detection software.</li>
<li>I don't (yet) care about the time evolution of the population.</li>
</ul>

<p>So where do I go from here? Would you model infection as a binomial distribution with probability like (I given D), or as a Poisson? Or is the distribution different?</p>
",NULL,"2010-07-20 15:23:53","What approach could be used for modelling virus infections?",<distributions><modeling><poisson><binomial><disease>,1,0,NULL,NULL,NULL,NULL,NULL,NULL,user209,NULL
313,2,NULL,"2010-07-20 15:10:15",3,NULL,"<p>Imagine you have a bag containing 900 black marbles and 100 white, i.e. 10% of the marbles are white. Now imagine you take 1 marble out, look at it and record its colour, take out another, record its colour etc.. and do this 100 times. At the end of this process you will have a number for white marbles which, ideally, we would expect to be 10, i.e. 10% of 100, but in actual fact may be 8, or 13 or whatever simply due to randomness. If you repeat this 100 marble withdrawal experiment many, many times and then plot a histogram of the number of white marbles drawn per experiment, you'll find you will have a Bell Curve centred about 10. </p>

<p>This represents your 10% hypothesis: with any bag containing 1000 marbles of which 10% are white, if you randomly take out 100 marbles you will find 10 white marbles in the selection, give or take 4 or so. The p-value is all about this "give or take 4 or so." Let's say by referring to the Bell Curve created earlier you can determine that less than 5% of the time would you get 5 or fewer white marbles and another &lt; 5% of the time accounts for 15 or more white marbles i.e. > 90% of the time your 100 marble selection will contain between 6 to 14 white marbles inclusive.</p>

<p>Now assuming someone plonks down a bag of 1000 marbles with an unknown number of white marbles in it, we have the tools to answer these questions</p>

<p>i) Are there fewer than 100 white marbles?</p>

<p>ii) Are there more than 100 white marbles?</p>

<p>iii) Does the bag contain 100 white marbles?</p>

<p>Simply take out 100 marbles from the bag and count how many of this sample are white. </p>

<p>a) If there are 6 to 14 whites in the sample you cannot reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 6 through 14 will be > 0.05. </p>

<p>b) If there are 5 or fewer whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 5 or fewer will be &lt; 0.05. You would expect the bag to contain &lt; 10% white marbles.</p>

<p>c) If there are 15 or more whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 15 or more will be &lt; 0.05. You would expect the bag to contain > 10% white marbles.</p>

<p><em>In response to Baltimark's comment</em></p>

<p>Given the example above, there is an approximately:-</p>

<p>4.8% chance of getter 5 white balls or fewer</p>

<p>1.85% chance of 4 or fewer</p>

<p>0.55% chance of 3 or fewer</p>

<p>0.1% chance of 2 or fewer </p>

<p>6.25% chance of 15 or more</p>

<p>3.25% chance of 16 or more</p>

<p>1.5% chance of 17 or more</p>

<p>0.65% chance of 18 or more</p>

<p>0.25% chance of 19 or more</p>

<p>0.1% chance of 20 or more</p>

<p>0.05% chance of 21 or more</p>

<p>These numbers were estimated from an empirical distribution generated by a simple Monte Carlo routine run in R and the resultant quantiles of the sampling distribution. </p>

<p>For the purposes of answering the original question, suppose you draw 5 white balls, there is only an approximate 4.8% chance that if the 1000 marble bag really does contain 10% white balls you would pull out only 5 whites in a sample of 100. This equates to a p value &lt; 0.05. You now have to choose between</p>

<p>i) There really are 10% white balls in the bag and I have just been "unlucky" to draw so few</p>

<p>or</p>

<p>ii) I have drawn so few white balls that there can't really be 10% white balls (reject the hypothesis of 10% white balls)</p>
",226,"2010-07-21 00:33:56",NULL,NULL,NULL,2,NULL,226,"2010-07-21 00:33:56",NULL,31,NULL,NULL,NULL
314,2,NULL,"2010-07-20 15:10:34",11,NULL,"<p>R is definitely the answer.  I would just add to what Rob and Colin already said:</p>

<p>To improve the quality of your plots, you should consider using <a href="http://cran.r-project.org/web/packages/Cairo/index.html">the <strong>Cairo</strong> package</a> for the output device.  That will greatly improve the <em>quality</em> of the final graphics.  You simply call the function before plotting and it redirects to Cairo as the output device.</p>

<pre><code>Cairo(600, 600, file="plot.png", type="png", bg="white")
plot(rnorm(4000),rnorm(4000),col="#ff000018",pch=19,cex=2) # semi-transparent red
dev.off() # creates a file "plot.png" with the above plot
</code></pre>

<p>Lastly, in terms of putting it in a publication, that's the role that <code>Sweave</code> plays.  It makes combining plots with your paper a trivial operation (and has the added benefit of leaving you with something that is <em>reproducible</em> and understandable).  Use <code>cacheSweave</code> if you have long-running computations.</p>
",5,"2010-07-20 15:10:34",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,257,NULL,NULL,NULL
315,2,NULL,"2010-07-20 15:15:57",5,NULL,"<p>If you can use R try <a href="http://had.co.nz/ggplot2/" rel="nofollow">ggplot2</a>.</p>
",36,"2010-07-20 15:15:57",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-12 08:48:39",224,NULL,NULL,NULL
316,2,NULL,"2010-07-20 15:20:38",7,NULL,"<p><strong>ggplot2</strong> - hands down best visualization for R.</p>

<p><strong>RMySQL/RSQLite/RODBC</strong> - for connecting to a databases</p>

<p><strong>sqldf</strong> - manipulate data.frames with SQL queries</p>

<p><strong>Hmisc/rms</strong> - packages from Frank Harrell containing convenient miscellaneous functions and nice functions for regression analyses.</p>

<p><strong>GenABEL</strong> - nice package for genome-wide association studies</p>

<p><strong>Rcmdr</strong> - a decent GUI for R if you need one.</p>

<p>Also check out <a href="http://crantastic.org/popcon" rel="nofollow">CRANtastic - this link</a> has a list of the most popular R packages. Many on the top of the list have already been ment</p>
",36,"2010-07-20 15:28:42",NULL,NULL,NULL,1,NULL,36,"2010-07-20 15:28:42","2010-07-20 15:20:38",73,NULL,NULL,NULL
317,2,NULL,"2010-07-20 15:21:01",18,NULL,"<p><a href="http://www.johndcook.com/blog/">The Endeavour</a> sometimes features statistics posts. Otherwise it is mostly around the interplay of computer science and math.</p>
",56,"2010-07-20 15:21:01",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 15:21:01",114,NULL,NULL,NULL
318,2,NULL,"2010-07-20 15:23:53",4,NULL,"<p>Computer virus propagation is structurally similar to infectious diseases propagation (vaccinations = anti-virus software, virus via email = getting a virus from someone etc).</p>

<p>Use the following links <a href="http://en.wikipedia.org/wiki/Mathematical_modelling_in_epidemiology">http://en.wikipedia.org/wiki/Mathematical_modelling_in_epidemiology</a> and <a href="http://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology">http://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology</a> as jumping points for your model.</p>
",NULL,"2010-07-20 15:23:53",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,312,NULL,user28,NULL
319,2,NULL,"2010-07-20 15:33:42",15,NULL,"<p>No amount of verbal explanation or calculations really helped me to understand <em>at a gut level</em> what p-values were, but it really snapped into focus for me once I took a course that involved simulation.  That gave me the ability to actually <em>see</em> data generated by the null hypothesis and to plot the means/etc. of simulated samples, then look at where my sample's statistic fell on that distribution.</p>

<p>I think the key advantage to this is that it lets students forget about the math and the test statistic distributions for a minute and focus on the concepts at hand.  Granted, it required that I learn <em>how</em> to simulate that stuff, which will cause problems for an entirely different set of students.  But it worked for me, and I've used simulation countless times to help explain statistics to others with great success (e.g., "This is what your data looks like; this is what a Poisson distribution looks like overlaid.  Are you SURE you want to do a Poisson regression?").</p>

<p>This doesn't exactly answer the questions you posed, but for me, at least, it made them trivial.</p>
",71,"2010-07-20 15:33:42",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,31,NULL,NULL,NULL
320,2,NULL,"2010-07-20 15:47:04",14,NULL,"<p>A saturated model is a model that is overparameterized to the point that it is basically just interpolating the data.  In some settings, such as image compression and reconstruction, this isn't necessarily a bad thing, but if you're trying to build a predictive model it's very problematic.</p>

<p>In short, saturated models lead to extremely high-variance predictors that are being pushed around by the noise more than the actual data.</p>

<p>As a thought experiment, imagine you've got a saturated model, and there is noise in the data, then imagine fitting the model a few hundred times, each time with a different realization of the noise, and then predicting a new point.  You're likely to get radically different results each time, both for your fit and your prediction (and polynomial models are especially egregious in this regard); in other words the variance of the fit and the predictor are extremely high.</p>

<p>By contrast a model that is not saturated will (if constructed reasonably) give fits that are more consistent with each other even under different noise realization, and the variance of the predictor will also be reduced.</p>
",61,"2010-07-20 15:47:04",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,283,NULL,NULL,NULL
321,1,NULL,"2010-07-20 16:01:25",10,1950,"<p>There is a variant of boosting called <a href="http://dx.doi.org/10.1214/aos/1016218223">gentleboost</a>.  How does gentle boosting differ from the better-known <a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>?</p>
",220,"2010-09-08 17:30:41","How does gentle boosting differ from AdaBoost?",<machine-learning><boosting>,1,4,4,220,"2010-07-20 19:25:13",NULL,NULL,NULL,NULL,NULL
322,1,338,"2010-07-20 16:03:40",11,1449,"<p>I'm looking for a book or online resource that explains different kinds of entropy such as Sample Entropy and Shannon Entropy and their advantages and disadvantages.
Can someone point me in the right direction?</p>
",3807,"2010-09-05 23:52:45","Good introduction into different kinds of entropy",<entropy>,6,0,6,NULL,NULL,NULL,NULL,NULL,NULL,NULL
323,2,NULL,"2010-07-20 16:10:16",22,NULL,"<p><a href="http://www.stat.columbia.edu/~cook/movabletype/mlm/">Statistical Modeling, Causal Inference, and Social Science</a> from Andrew Gelman is a good blog.</p>
",3807,"2010-07-20 16:10:16",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-20 16:10:16",114,NULL,NULL,NULL
324,2,NULL,"2010-07-20 16:12:08",9,NULL,"<p><a href="http://darrenjw.wordpress.com/">Darren Wilkinson's research blog</a></p>
",3807,"2010-07-20 16:12:08",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 16:12:08",114,NULL,NULL,NULL
326,2,NULL,"2010-07-20 16:13:03",12,NULL,"<p><a href="http://xianblog.wordpress.com/">XI'AN'S OG</a></p>
",3807,"2010-07-20 16:13:03",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-20 16:13:03",114,NULL,NULL,NULL
327,2,NULL,"2010-07-20 16:19:51",1,NULL,"<p>Malcom Gladewell analyses the problem in the book Outliers by analyzing Hockey Players.</p>
",3807,"2010-07-20 16:19:51",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,62,NULL,NULL,NULL
328,1,329,"2010-07-20 16:27:08",5,2388,"<p>I realize that the statistical analysis of financial data is a huge topic, but that is exactly why it is necessary for me to ask my question as I try to break into the world of financial analysis.</p>

<p>As at this point I know next to nothing about the subject, the results of my google searches  are overwhelming.  Many of the matches advocate learning specialized tools or the R programming language. While I will learn these when they are necessary, I'm first interested in books, articles or any other resources that explain modern methods of statistical analysis specifically for financial data.  I assume there are a number of different wildly varied methods for analyzing data, so ideally I'm seeking an overview of the various methods that are practically applicable.  I'd like something that utilizes real world examples that a beginner is capable of grasping but that aren't overly simplistic.</p>

<p>What are some good resources for learning bout the statistical analysis of financial data?</p>
",75,"2012-08-27 15:59:23","Resources for learning about the Statistical Analysis of Financial Data",<books><finance>,9,3,15,88,"2010-09-17 20:53:00",NULL,NULL,NULL,NULL,NULL
329,2,NULL,"2010-07-20 16:31:26",11,NULL,"<p>You might start with this <a href="http://oyc.yale.edu/economics/financial-markets/">series of lectures by Robert Shiller at Yale</a>.  He gives a good overview of the field.</p>

<p>My favorite books on the subject:</p>

<ul>
<li>I strongly recommend starting with <a href="http://rads.stackoverflow.com/amzn/click/0387202706"><strong>Statistics and Finance</strong></a>, by David Ruppert (<a href="http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm">the R code for the book is available</a>).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.</li>
<li><a href="http://rads.stackoverflow.com/amzn/click/0387279652">Modeling Financial Time Series with S-Plus</a>, by Eric Zivot</li>
<li><a href="http://rads.stackoverflow.com/amzn/click/0471690740">Analysis of Financial Time Series</a>, by Ruey Tsay</li>
<li><a href="http://rads.stackoverflow.com/amzn/click/0387759581">Time Series Analysis</a>, by Jonathan D. Cryer</li>
</ul>

<p>Beyond that, you may want some general resources, and the "bible" of finance is <a href="http://www.rotman.utoronto.ca/~hull/ofod/">Options, Futures, and Other Derivatives</a> by John Hull.  </p>

<p>Lastly, in terms of some good general books, you might start with these two:</p>

<ul>
<li><a href="http://rads.stackoverflow.com/amzn/click/0393325350">A Random Walk Down Wall Street</a></li>
<li><a href="http://rads.stackoverflow.com/amzn/click/0471295639">Against the Gods: The Remarkable Story of Risk</a> </li>
</ul>
",5,"2010-07-20 16:49:17",NULL,NULL,NULL,0,NULL,5,"2010-07-20 16:49:17",NULL,328,NULL,NULL,NULL
331,2,NULL,"2010-07-20 16:43:42",5,NULL,"<p>Because exact results are preferable to approximations, and avoid odd edge cases where the approximation may lead to a different result than the exact method.  </p>

<p>The Welch method isn't a quicker way to do any old t-test, it's a tractable approximation to an otherwise very hard problem: how to construct a t-test under unequal variances.  The equal-variance case is well-understood, simple, and exact, and therefore should always be used when possible.</p>
",61,"2010-07-20 16:43:42",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,305,NULL,NULL,NULL
333,2,NULL,"2010-07-20 17:17:21",3,NULL,"<p>Ed Thorpe started the whole statistical arbitrage thing. He has a website, and some good articles.</p>

<p><a href="http://edwardothorp.com/" rel="nofollow">http://edwardothorp.com/</a></p>

<p>You should also read Nassim Taleb's "Fooled By Randomness".</p>

<p>Also, go on Google Scholar and read the top articles by Markowitz, Sharpe, Fama, Modigliani. If you don't have full access,  go to the nearest college and get a community library card. </p>
",74,"2010-08-13 18:01:09",NULL,NULL,NULL,3,NULL,74,"2010-08-13 18:01:09",NULL,328,NULL,NULL,NULL
337,2,NULL,"2010-07-20 17:20:41",1,NULL,"<p>The entropy is only one (as a concept) -- the amount of information needed to describe some system; there are only many its generalizations. Sample entropy is only some entropy-like descriptor used in heart rate analysis.</p>
",88,"2010-07-20 17:36:23",NULL,NULL,NULL,4,NULL,88,"2010-07-20 17:36:23",NULL,322,NULL,NULL,NULL
338,2,NULL,"2010-07-20 17:22:01",7,NULL,"<p>Cover and Thomas's book <a href="http://rads.stackoverflow.com/amzn/click/0471062596" rel="nofollow">Elements of Information Theory</a> is a good source on entropy and its applications, although I don't know that it addresses exactly the issues you have in mind.</p>
",89,"2010-09-02 09:52:44",NULL,NULL,NULL,2,NULL,8,"2010-09-02 09:52:44",NULL,322,NULL,NULL,NULL
339,2,NULL,"2010-07-20 17:31:49",1,NULL,"<p>If the information required is the distribution of data about the mean, standard deviation comes in handy.</p>

<p>The sum of the difference of each value from the mean is zero (obviously, since the value are evenly spread around the mean), hence we square each difference so as to convert negative values to positive, sum them across the population, and take their square root. This value is then divided by the number of samples (or, the size of the population). This gives the standard deviation.</p>
",218,"2010-07-20 17:31:49",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,26,NULL,NULL,NULL
340,2,NULL,"2010-07-20 17:41:00",5,NULL,"<p>The population is everything in the group of study. For example, if you are studying the price of Apple's shares, it is the historical, current, and even all future stock prices. Or, if you run an egg factory, it is all the eggs made by the factory.</p>

<p>You don't always have to sample, and do statistical tests. If your population is your immediate living family, you don't need to sample, as the population is small. </p>

<p>Sampling is popular for a variety of reasons:</p>

<ul>
<li>it is cheaper than a census (sampling the whole population)</li>
<li>you don't have access to future data, so must sample the past</li>
<li>you have to destroy some items by testing them, and don't want to destroy them all (say, eggs)</li>
</ul>
",74,"2010-07-20 17:41:00",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,269,NULL,NULL,NULL
341,1,385,"2010-07-20 18:12:29",2,435,"<p>Do you think that unbalanced classes is a big problem for k-nearest neighbor? If so, do you know any smart way to handle this?</p>
",88,"2012-04-24 22:17:17","kNN and unbalanced classes",<k-nearest-neighbour><unbalanced-classes>,2,0,NULL,190,"2010-07-21 06:50:32",NULL,NULL,NULL,NULL,NULL
342,2,NULL,"2010-07-20 18:13:44",3,NULL,"<p>If you're willing to tolerate an approximation, there are other methods. For example, one approximation is a value whose rank is within some (user specified) distance from the true median. For example, the median has (normalized) rank 0.5, and if you specify an error term of 10%, you'd want an answer that has rank between 0.45 and 0.55. </p>

<p>If such an answer is appropriate, then there are many solutions that can work on sliding windows of data. The basic idea is to maintain a sample of the data of a certain size (roughly 1/error term) and compute the median on this sample. It can be shown that with high probability, regardless of the nature of the input, the resulting median satisfies the properties I mentioned above.</p>

<p>Thus, the main question is how to maintain a running sample of the data of a certain size, and there are many approaches for that, including the technique known as reservoir sampling. For example, this paper: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7136" rel="nofollow">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7136</a></p>
",139,"2010-07-20 18:13:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,134,NULL,NULL,NULL
343,2,NULL,"2010-07-20 18:16:17",3,NULL,"<p><a href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm" rel="nofollow">kNN</a></p>
",88,"2010-07-20 18:16:17",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 18:16:17",258,NULL,NULL,NULL
344,2,NULL,"2010-07-20 18:17:25",3,NULL,"<p><a href="http://en.wikipedia.org/wiki/Naive_bayes" rel="nofollow">Naive Bayes</a> and <a href="http://en.wikipedia.org/wiki/Random_naive_Bayes" rel="nofollow">Random Naive Bays</a></p>
",88,"2010-07-20 18:17:25",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-20 18:17:25",258,NULL,NULL,NULL
345,2,NULL,"2010-07-20 19:08:50",1,NULL,"<p>Also good is "Statistical Analysis of Financial Data in S-PLUS" by Rene A. Carmona</p>
",247,"2010-07-20 19:08:50",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,328,NULL,NULL,NULL
346,1,428,"2010-07-20 19:21:16",26,4873,"<p>I'm looking for a good algorithm (meaning minimal computation, minimal storage requirements) to estimate the median of a data set that is too large to store, such that each value can only be read once (unless you explicitly store that value). There are no bounds on the data that can be assumed.</p>

<p>Approximations are fine, as long as the accuracy is known.</p>

<p>Any pointers?</p>
",247,"2014-08-30 16:14:38","What is a good algorithm for estimating the median of a huge read-once data set?",<algorithms><median><large-data>,10,2,9,8,"2010-10-08 16:06:48",NULL,NULL,NULL,NULL,NULL
347,2,NULL,"2010-07-20 19:50:57",2,NULL,"<p>I found this rather helpful: <a href="http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf" rel="nofollow">http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf</a></p>
",15,"2010-07-20 19:50:57",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-19 11:13:59",75,NULL,NULL,NULL
348,2,NULL,"2010-07-20 19:54:13",-4,NULL,"<p>A male cat and a female cat are penned up in a steel chamber, along with enough food and water for 70 days.  </p>

<p>A Frequentist would say the average gestation period for <a href="http://en.wikipedia.org/wiki/Cat#Reproduction" rel="nofollow">felines</a> is 66 days, the female was in heat when the cats were penned up, and once in heat she will mate repeatedly for 4 to 7 days.  Since there were likely many acts of propagation and enough subsequent time for gestation, the odds are, when the box is opened on day 70, there's a litter of newborn kittens.</p>

<p>A Bayesian would say, I heard some serious Marvin Gaye coming from the box on day 1 and then this morning I heard many kitten-like sounds coming from the box.  So without knowing much about cat reproduction, the odds are, when the box is opened on day 70, there's a litter of newborn kittens. </p>
",24,"2010-07-20 22:52:03",NULL,NULL,NULL,2,NULL,24,"2010-07-20 22:52:03",NULL,22,NULL,NULL,NULL
349,2,NULL,"2010-07-20 19:59:33",8,NULL,"<p>How about something like a binning procedure?  Assume (for illustration purposes) that you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] </p>

<p>Then, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the size of the bins. You're limited only by how much memory you have.</p>

<p>Since you don't know how big your values may get, just pick a bin size large enough that you aren't likely to run out of memory, using some quick back-of-the-envelope calculations. You might also store the bins sparsely, such that you only add a bin if it contains a value.</p>

<p>Edit:</p>

<p>The link ryfm provides gives an example of doing this, with the additional step of using the cumulative percentages to more accurately estimate the point within the median bin, instead of just using midpoints. This is a nice improvement.</p>
",54,"2010-07-20 22:40:50",NULL,NULL,NULL,5,NULL,54,"2010-07-20 22:40:50",NULL,346,NULL,NULL,NULL
350,2,NULL,"2010-07-20 20:12:26",1,NULL,"<p>You can try to find a median based on grouped frequency distribution, <a href="http://www.statcan.gc.ca/edu/power-pouvoir/ch11/median-mediane/5214872-eng.htm" rel="nofollow">here is some details</a></p>
",178,"2010-07-20 20:12:26",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,346,NULL,NULL,NULL
351,2,NULL,"2010-07-20 21:05:35",4,NULL,"<p><a href="http://timetric.com" rel="nofollow">Timetric</a> provides a web interface to data and provide a list of the <a href="http://timetric.com/dataset/" rel="nofollow">publicly available data sets</a> they use</p>
",16,"2010-07-20 21:05:35",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-20 21:05:35",7,NULL,NULL,NULL
352,2,NULL,"2010-07-20 21:18:45",1,NULL,"<p>I've never had to do this, so this is just a suggestion.</p>

<p>I see two (other) possibilities. </p>

<p><strong>Half data</strong></p>

<ol>
<li>Load in half the data and sort</li>
<li>Next read in the remaining values and compare against the your sorted list. 
<ol>
<li>If the new value is larger, discard it.</li>
<li>else put the value in the sorted list and removing the largest value from that list.</li>
</ol></li>
</ol>

<p><strong>Sampling distribution</strong></p>

<p>The other option, is to use an approximation involving the sampling distribution. If your data is Normal, then the standard error for moderate <em>n</em> is:</p>

<p>1.253 * sd / sqrt(n)</p>

<p>To determine the size of <em>n</em> that you would be happy with, I ran a quick Monte-Carlo simulation in R</p>

<pre><code>n = 10000
outside.ci.uni = 0
outside.ci.nor = 0
N=1000
for(i in 1:N){
  #Theoretical median is 0
  uni = runif(n, -10, 10)
  nor  = rnorm(n, 0, 10)

  if(abs(median(uni)) &gt; 1.96*1.253*sd(uni)/sqrt(n))
    outside.ci.uni = outside.ci.uni + 1

  if(abs(median(nor)) &gt; 1.96*1.253*sd(nor)/sqrt(n))
    outside.ci.nor = outside.ci.nor + 1
}

outside.ci.uni/N
outside.ci.nor/N
</code></pre>

<p>For n=10000, 15% of the uniform median estimates were outside the CI.</p>
",8,"2010-07-21 15:29:25",NULL,NULL,NULL,1,NULL,8,"2010-07-21 15:29:25",NULL,346,NULL,NULL,NULL
353,2,NULL,"2010-07-20 22:11:54",11,NULL,"<p>Fisher's scoring is just a version of Newton's method that happens to be identified with GLMs, there's nothing particularly special about it, other than the fact that the Fisher's information matrix happens to be rather easy to find for random variables in the exponential family.  It also ties in to a lot of other math-stat material that tends to come up about the same time, and gives a nice geometric intuition about what exactly Fisher information means.</p>

<p>There's absolutely no reason I can think of not to use some other optimizer if you prefer, other than that you might have to code it by hand rather than use a pre-existing package.  I suspect that any strong emphasis on Fisher scoring is a combination of (in order of decreasing weight) pedagogy, ease-of-derivation, historical bias, and "not-invented-here" syndrome.  </p>
",61,"2010-07-20 22:11:54",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,205,NULL,NULL,NULL
354,1,376,"2010-07-20 22:26:26",8,676,"<p>Why do we seek to minimize <code>x^2</code> instead of minimizing <code>|x|^1.95</code> or <code>|x|^2.05</code>.
Are there reasons why the number should be exactly two or is it simply a convention that has the advantage of simplifying the math?</p>
",3807,"2013-01-09 21:19:25","Bias towards natural numbers in the case of least squares",<standard-deviation><least-squares>,4,0,4,930,"2010-11-28 12:04:24",NULL,NULL,NULL,NULL,NULL
355,2,NULL,"2010-07-20 22:27:42",1,NULL,"<p>Check out <a href="http://wilmott.com/" rel="nofollow">Wilmott.com</a> as well.  It's oriented toward more advanced practitioners, but if I had to choose one person from whom to learn financial math, it would be Paul Wilmott.  Brilliant but grounded.</p>
",158,"2010-07-20 22:27:42",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,328,NULL,NULL,NULL
356,2,NULL,"2010-07-20 22:31:45",2,NULL,"<p>To fit the model you can use <a href="http://www-fis.iarc.fr/~martyn/software/jags/" rel="nofollow">JAGS</a> or <a href="http://www.mrc-bsu.cam.ac.uk/bugs/" rel="nofollow">Winbugs</a>. In fact if you look at the week 3 of the lecture notes at Paul Hewson's <a href="http://users.aims.ac.za/~paulhewson/" rel="nofollow">webpage</a>, the rats JAGS example is a beta binomial model. He puts gamma priors on alpha and beta.</p>
",8,"2010-07-20 22:31:45",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,288,NULL,NULL,NULL
357,2,NULL,"2010-07-20 23:07:48",5,NULL,"<p>You can't know whether there normality and that's why you have to make an assumption that's there.
You can only prove the absence of normality with statistic tests.</p>

<p>Even worse, when you work with real world data it's almost certain that there isn't true normality in your data.</p>

<p>That means that your statistical test is always a bit biased. The question is whether you can live with it's bias.
To do that you have to understand your data and the kind of normality that your statistical tool assumes.</p>

<p>It's the reason why Frequentist tools are as subjective as Bayesian tools. You can't determine based on the data that it's normally distributed. You have to assume normality.</p>
",3807,"2011-10-21 19:44:18",NULL,NULL,NULL,5,NULL,3807,"2011-10-21 19:44:18",NULL,2,NULL,NULL,NULL
358,2,NULL,"2010-07-20 23:21:05",6,NULL,"<p>We try to minimize the variance that is left within descriptors. Why variance? Read <a href="http://stats.stackexchange.com/questions/118/standard-deviation-why-square-the-difference-instead-of-taking-the-absolute-val" rel="nofollow">this question</a>; this also comes together with the (mostly silent) assumption that errors are normally distributed.</p>

<p><strong>Extension:</strong><br>
Two additional arguments:  </p>

<ol>
<li><p>For variances, we have this nice "law" that the sum of variances is equal to the variance of sum, for uncorrelated samples. If we assume that the error is not correlated with the case, minimizing residual of squares will work straightforward to maximizing explained variance, what is maybe a not-so-good but still popular quality measure.  </p></li>
<li><p>If we assume normality of an error, least squares error estimator is a maximal likelihood one.</p></li>
</ol>
",88,"2010-07-21 09:31:29",NULL,NULL,NULL,4,NULL,88,"2010-07-21 09:31:29",NULL,354,NULL,NULL,NULL
359,1,457,"2010-07-20 23:28:49",9,425,"<p>The Wald, Likelihood Ratio and Lagrange Multiplier tests in the context of maximum likelihood estimation are asymptotically equivalent. However, for small samples, they tend to diverge quite a bit, and in some cases they result in different conclusions.</p>

<p>How can they be ranked according to how likely they are to reject the null? What to do when the tests have conflicting answers? Can you just pick the one which gives the answer you want or is there a "rule" or "guideline" as to how to proceed?</p>
",90,"2010-09-20 02:40:27","The trinity of tests in maximum likelihood: what to do when faced with contradicting conclusions?",<hypothesis-testing><maximum-likelihood>,2,1,4,NULL,NULL,NULL,NULL,NULL,NULL,NULL
360,2,NULL,"2010-07-20 23:56:33",4,NULL,"<p>Does it really need some advanced model? Based on what I know about TB, in case there is no epidemy the infections are stochastic acts and so the count form month N shouldn't be correlated with count from month N-1. (You can check this assumption with autocorrelation). If so, analyzing just the distribution of monthly counts may be sufficient to decide if some count is significantly higher than normal.<br>
On the other hand you can look for correlations with some other variables, like season, travel traffic, or anything that you can imagine that may be correlated. If you would found something like this, it could be then used for some data normalization.</p>
",88,"2010-07-20 23:56:33",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,173,NULL,NULL,NULL
361,2,NULL,"2010-07-21 00:09:09",13,NULL,"<p><a href="http://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>:</p>

<ul>
<li>fast and perform well on most datasets</li>
<li>almost no parameters to tune</li>
<li>handles both discrete/continuous features</li>
<li>model is easily interpretable</li>
<li>(not really restricted to binary classifications)</li>
</ul>
",170,"2010-07-21 00:09:09",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 00:09:09",258,NULL,NULL,NULL
362,1,74954,"2010-07-21 00:24:35",14,25597,"<p>What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?  When will results from these two methods differ?</p>
",196,"2014-06-15 03:58:03","What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?",<distributions><statistical-significance><normality><kolmogorov-smirnov>,2,0,7,805,"2014-06-15 03:58:03",NULL,NULL,NULL,NULL,NULL
363,1,NULL,"2010-07-21 00:44:08",52,5188,"<p>If you could go back in time and tell yourself to read a specific book at the beginning of your career as a statistician, which book would it be?</p>
",74,"2012-02-14 19:22:31","What is the single most influential book every statistician should read?",<books>,19,2,65,NULL,NULL,"2010-07-21 00:54:29",NULL,NULL,NULL,NULL
365,2,NULL,"2010-07-21 01:04:08",4,NULL,"<p>Another option is <a href="http://www.gnuplot.info/" rel="nofollow">Gnuplot</a></p>
",226,"2010-07-21 01:04:08",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,257,NULL,NULL,NULL
366,2,NULL,"2010-07-21 01:23:00",1,NULL,"<p>For a linear regression you could use a repeated median straight line fit.</p>
",226,"2010-07-21 01:23:00",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,175,NULL,NULL,NULL
367,2,NULL,"2010-07-21 01:35:13",17,NULL,"<p>I am no statistician, and I haven't read that much on the topic, but perhaps </p>

<p><a href="http://rads.stackoverflow.com/amzn/click/0805071342" rel="nofollow">Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century</a></p>

<p>should be mentioned? It is no textbook, but still worth reading.</p>
",90,"2010-07-21 01:35:13",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 01:35:13",363,NULL,NULL,NULL
368,2,NULL,"2010-07-21 03:01:00",2,NULL,"<p>Suppose that the text has N words and that you require that an ASR should correctly predict at least 95% of words in the text. You currently have the observed error rate for the two methods. You can perform two type of tests.</p>

<p>Test 1: Do the ASR models meet your criteria of 95% prediction?</p>

<p>Test 2: Are the two ASR models equally good in speech recognition?</p>

<p>You could make different type of assumptions regarding the data generating mechanism for your ASR models. The simplest, although a bit naive, would assume that word detection of each word in the text is an iid bernoulli variable.</p>

<p>Under the above assumption you could do a test of proportions where you check if the error rate for each model is consistent with a true error rate of 5% (test 1) or a test of difference in proportions where you check if the error rates between the two models is the same (test 2). </p>
",NULL,"2010-07-21 03:01:00",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,212,NULL,user28,NULL
369,1,NULL,"2010-07-21 03:47:17",6,485,"<p>Say I've got a program that monitors a news feed and as I'm monitoring it I'd like to discover when a bunch of stories come out with a particular keyword in the title. Ideally I want to know when there are an unusual number of stories clustered around one another.</p>

<p>I'm entirely new to statistical analysis and I'm wondering how you would approach this problem. How do you select what variables to consider? What characteristics of the problem affect your choice of an algorithm? Then, what algorithm do you choose and why?</p>

<p>Thanks, and if the problem needs clarification please let me know.</p>
",191,"2012-05-26 04:00:14","Working through a clustering problem",<clustering>,4,0,6,88,"2010-09-17 20:53:12",NULL,NULL,NULL,NULL,NULL
370,2,NULL,"2010-07-21 03:48:55",27,NULL,"<p>Here are two to put on the list:</p>

<p><a href="http://rads.stackoverflow.com/amzn/click/096139210X">Tufte. The visual display of quantitative information</a><br>
<a href="http://rads.stackoverflow.com/amzn/click/0201076160">Tukey. Exploratory data analysis</a></p>
",159,"2010-07-21 03:48:55",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-21 03:48:55",363,NULL,NULL,NULL
371,2,NULL,"2010-07-21 04:19:00",15,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/0521592712">Probability Theory: The Logic of Science</a></p>
",34,"2010-07-21 04:19:00",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-21 04:19:00",363,NULL,NULL,NULL
372,1,381,"2010-07-21 04:26:07",2,754,"<p>What topics in statistics are most useful/relevant to data mining?</p>
",252,"2010-09-13 13:16:33","What are the key statistical concepts that relate to data mining?",<probability><data-mining><cart>,2,5,5,509,"2010-08-12 13:24:12","2010-07-21 08:05:16",NULL,NULL,NULL,NULL
373,1,378,"2010-07-21 04:30:50",24,1439,"<p>From Wikipedia :</p>

<blockquote>
  <p>Suppose you're on a game show, and
  you're given the choice of three
  doors: Behind one door is a car;
  behind the others, goats. You pick a
  door, say No. 1, and the host, who
  knows what's behind the doors, opens
  another door, say No. 3, which has a
  goat. He then says to you, "Do you
  want to pick door No. 2?" Is it to
  your advantage to switch your choice?</p>
</blockquote>

<p>The answer is, of course, yes - but it's incredibly un-inituitive. What misunderstanding do most people have about probability that leads to us scratching our heads -- or better put; what general rule can we take away from this puzzle to better train our intuition in the future?</p>
",252,"2013-06-12 23:06:43","The Monty Hall Problem - where does our intuition fail us?",<probability><puzzle>,12,2,12,22468,"2013-06-12 23:06:43",NULL,NULL,NULL,NULL,NULL
374,1,379,"2010-07-21 04:32:42",6,273,"<p>What are pivot tables, and how can they be helpful in analyzing data?</p>
",252,"2010-09-17 20:52:22","What are pivot tables, and how can they be helpful in analyzing data?",<multivariate-analysis>,1,0,1,88,"2010-09-17 20:52:22",NULL,NULL,NULL,NULL,NULL
375,1,NULL,"2010-07-21 04:34:45",7,437,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href="http://stats.stackexchange.com/questions/30/testing-random-variate-generation-algorithms">Testing random variate generation algorithms</a>  </p>
</blockquote>



<p>What's a good way to test a series of numbers to see if they're random (or at least psuedo-random)? Is there a good statistical measure of randomness that can be used to determine how random a set is?</p>

<p>More importantly, how can one <em>prove</em> a method of generating numbers is psuedo-random?</p>
",252,"2010-07-26 20:12:15","Testing (and proving) the randomness of numbers",<random-generation><proof><randomness>,0,1,0,190,"2010-07-26 11:26:49",NULL,NULL,"2013-01-08 20:43:45",NULL,NULL
376,2,NULL,"2010-07-21 04:48:50",9,NULL,"<p>There's no reason you couldn't try to minimize norms other than x^2, there have been entire books written on quantile regression, for instance, which is more or less minimizing |x| if you're working with the median.  It's just generally harder to do and, depending on the error model, may not give good estimators (depending on whether that means low-variance or unbiased or low MSE estimators in the context).  </p>

<p>As for why we prefer integer moments over real-number-valued moments, the main reason is likely that while integer powers of real numbers always result in real numbers, non-integer powers of negative real numbers create complex numbers, thus requiring the use of an absolute value.  In other words, while the 3rd moment of a real-valued random variable is real, the 3.2nd moment is not necessarily real, and so causes interpretation problems.</p>

<p>Other than that...</p>

<ol>
<li>Analytical expressions for the integer moments of random variables are typically much easier to find than real-valued moments, be it by generating functions or some other method.  Methods to minimize them are thus easier to write.</li>
<li>The use of integer moments leads to expressions that are more tractable than real-valued moments.</li>
<li>I can't think of a compelling reason that (for instance) the 1.95th moment of the absolute value of X would provide better fitting properties than (for instance) the 2nd moment of X, although that could be interesting to investigate</li>
<li>Specific to the L2 norm (or squared error), it can be written via dot products, which can lead to vast improvements in speed of computation.  It's also the only Lp space that's a Hilbert space, which is a nice feature to have.</li>
</ol>
",61,"2010-07-21 04:48:50",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,354,NULL,NULL,NULL
377,2,NULL,"2010-07-21 05:45:43",7,NULL,"<p>This doesn't give a general rule, but I think that one reason why it's a challenging puzzle is that our intuition doesn't handle conditional probability very well. There are plenty of <a href="http://www.stubbornmule.net/2010/06/probability-paradoxes/">other probability puzzles that play on the same phenomenon</a>. Since I'm linking to my blog, here's <a href="http://www.stubbornmule.net/2010/06/monty-hall/">a post specifically on Monty Hall</a>.</p>
",173,"2010-07-21 05:45:43",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,373,NULL,NULL,NULL
378,2,NULL,"2010-07-21 05:54:25",11,NULL,"<p>Consider two simple variations of the problem: </p>

<ol>
<li>No doors are opened for the contestant. The host offers no help in picking a door. In this case it is obvious that the odds of picking the correct door are 1/3.</li>
<li>Before the contestant is asked to venture a guess, the host opens a door and reveals a goat. After the host reveals a goat, the contestant has to pick the car from the two remaining doors. In this case it is obvious that the odds of picking the correct door is 1/2.</li>
</ol>

<p>For a contestant to know the probability of his door choice being correct, he has to know how many positive outcomes are available to him and divide that number by the amount of possible outcomes. Because of the two simple cases outlined above, it is very natural to think of all the possible outcomes available as the number of doors to choose from, and the amount of positive outcomes as the number of doors that conceal a car. Given this intuitive assumption, even if the host opens a door to reveal a goat <em>after</em> the contestant makes a guess, the probability of either door containing a car remains 1/2.</p>

<p>In reality, probability recognizes a set of possible outcomes larger than the three doors and it recognizes a set of positive outcomes that is larger than the singular door with the car. In the correct analysis of the problem, the host provides the contestant with new information making a new question to be addressed: what is the probability that my original guess is such that the new information provided by the host is sufficient to inform me of the correct door? In answering this question, the set of positive outcomes and the set of possible outcomes are not tangible doors and cars but rather abstract arrangements of the goats and car. The three possible outcomes are the three possible arrangements of two goats and one car behind three doors. The two positive outcomes are the two possible arrangements where the first guess of the contestant is false. In each of these two arrangements, the information given by the host (one of the two remaining doors is empty) is sufficient for the contestant to determine the door that conceals the car.</p>

<p><strong>In summation:</strong></p>

<p>We have a tendency to look for a simple mapping between physical manifestations of our choices (the doors and the cars) and the number of possible outcomes and desired outcomes in a question of probability. This works fine in cases where no new information is provided to the contestant. However, if the contestant is provided with more information (ie one of the doors you didn't choose is certainly not a car), this mapping breaks down and the correct question to be asked is found to be more abstract.</p>
",68,"2010-07-21 05:54:25",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,373,NULL,NULL,NULL
379,2,NULL,"2010-07-21 06:09:43",5,NULL,"<p>A pivot-table is a tool to dynamically show a slice and group multivariate data in tabular form.</p>

<p>For example, when we have the following data structure</p>

<pre><code>Region  Year  Product  Sales 
US      2008  Phones   125 
EU      2008  Phones   352 
US      2008  Mouses   52 
EU      2008  Mouses   65 
US      2009  Phones   140 
EU      2009  Phones   320 
US      2009  Mouses   60 
EU      2009  Mouses   100
</code></pre>

<p>A pivot table can for example display a table with the sum of all products with in the rows the years and in the columns the regions. </p>

<p>All dimensions of the table can be switched easily. Also the data fields shown can be changed. This is called pivoting.</p>

<p>The tool is useful in exploratory data analyses. Because it is a dynamic tool, it can be used to visually detect patterns and outliers etc.</p>

<p>Most spreadsheet applications have support for this kind of tables.</p>

<p>An image from wikipedia: <img src="http://upload.wikimedia.org/wikipedia/commons/6/63/Pivottable-Pivoted.PNG" alt="http://upload.wikimedia.org/wikipedia/commons/6/63/Pivottable-Pivoted.PNG"></p>
",190,"2010-07-21 08:00:10",NULL,NULL,NULL,0,NULL,190,"2010-07-21 08:00:10",NULL,374,NULL,NULL,NULL
380,2,NULL,"2010-07-21 06:15:36",3,NULL,"<p>You cannot prove, because it is impossible; you can only check if there are no any embarrassing autocorrelations or distribution disturbances, and indeed <a href="http://en.wikipedia.org/wiki/Diehard_tests" rel="nofollow">Diehard</a> is a standard for it. This is for statistics/physics, cryptographers will also mainly check (among other things) how hard is it to fit the generator to the data to obtain the future values.  </p>
",88,"2010-07-21 06:15:36",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,30,NULL,NULL,NULL
381,2,NULL,"2010-07-21 06:22:51",6,NULL,"<p>Understanding <strong>multivariate normal distribution</strong> <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution" rel="nofollow">http://en.wikipedia.org/wiki/Multivariate_normal_distribution</a> is important. </p>

<p>The concept of <strong>correlation</strong> and more generally (non linear) dependence is important. </p>

<p><strong>Concentration of measure, asymptotic normality, convergence of random variables</strong>.... how to make something from random to deterministic! <a href="http://en.wikipedia.org/wiki/Convergence_of_random_variables" rel="nofollow">http://en.wikipedia.org/wiki/Convergence_of_random_variables</a></p>

<p><strong>maximum likelihood estimation</strong> <a href="http://en.wikipedia.org/wiki/Maximum_likelihood" rel="nofollow">http://en.wikipedia.org/wiki/Maximum_likelihood</a> and before that, statistical modeling :) and more generally minimum contrast estimation. </p>

<p><strong>stationary process</strong> <a href="http://en.wikipedia.org/wiki/Stationary_process" rel="nofollow">http://en.wikipedia.org/wiki/Stationary_process</a> and more generally stationnarity assumption and ergodic property. </p>

<p>as Peter said, the question is so broad ... that the answer couldn't be given in a post ... </p>
",223,"2010-07-21 06:22:51",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-12 13:30:08",372,NULL,NULL,NULL
382,2,NULL,"2010-07-21 06:28:23",3,NULL,"<p>Firstly I can recommend you the book <a href="http://nlp.stanford.edu/fsnlp/" rel="nofollow">Foundations of statistical natural language processing</a> by Manning and Schütze.</p>

<p>The methods I would use are word-frequency distributions and ngram language models. The first works very well when you want to classify on topic and your topics are specific and expert (having keywords). Ngram modelling is the best way when you want to classify writing styles etc. </p>
",190,"2010-07-21 06:28:23",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,124,NULL,NULL,NULL
383,2,NULL,"2010-07-21 06:28:43",3,NULL,"<p>You might be interested in applying relative distribution methods.  Call one group the reference group, and the other the comparison group.  In a way similar to constructing a probability-probability plot, you can construct a relative CDF/PDF, which is a ratio of the densities.  This relative density can be used for inference.  If the distributions are identical, you expect a uniform relative distribution.  There are tools, graphical and statistical, to explore and examine departures from uniformity.  </p>

<p>A good starting point to get a better sense is <a href="http://www.csss.washington.edu/Papers/wp27.pdf" rel="nofollow">Applying Relative Distrbution Methods in R</a> and the <a href="http://cran.r-project.org/web/packages/reldist/index.html" rel="nofollow">reldist</a> package in R.  For details, you'll need to refer to the book, <a href="http://csde.washington.edu/~handcock/RelDist/" rel="nofollow">Relative Distribution Methods in the Social Sciences</a> by Handcock and Morris.  There's also a <a href="http://www.jstor.org/pss/270964" rel="nofollow">paper</a> by the authors covering the relevant techniques.</p>
",251,"2010-07-21 06:28:43",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,4,NULL,NULL,NULL
384,2,NULL,"2010-07-21 06:33:36",1,NULL,"<p>I would start with a frequency distribution. Collect for a big corpus the word-frequencies and select smartly the words that are keywords (not misspellings, with a very low frequency, and not stop words like "and", "or")</p>

<p>Then when you have a number of new feeds, compare their distribution with the distribution that you build from your training data. Look to the big differences in frequencies and select so the important keywords of that moment.</p>
",190,"2010-07-21 06:33:36",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,369,NULL,NULL,NULL
385,2,NULL,"2010-07-21 06:39:38",4,NULL,"<p>In principal unbalanced classes are not a problem at all for the k-nearest neighbor algorithm. </p>

<p>Because the algorithm is totally not influenced by the size of the class, it will not favor any on size. Try to run k-means with an obvious outlier and k+1 and you will see that most of the time the outlier will get it's own class.</p>

<p>Off course, with hard datasets it is always advisable to run the algorithm multiple times. This to avoid trouble because of a bad initialization.</p>
",190,"2010-07-21 06:39:38",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,341,NULL,NULL,NULL
386,2,NULL,"2010-07-21 06:46:11",10,NULL,"<p>I would do some sort of "leave one out testing algorithm" (n is the number of data):</p>

<p>for i=1 to n</p>

<ol>
<li><strong>compute a density estimation of the data set obtained by throwing $X_i$ away</strong>. (This density estimate should be done with some assumption if the dimension is high, for example, a gaussian assumption for which the density estimate is easy: mean and covariance)</li>
<li><strong>Calculate the likelihood of $X_i$ for the density estimated in step 1</strong>. call it $L_i$. </li>
</ol>

<p>end for </p>

<p>sort the $L_i$ (for i=1,..,n) and use a multiple hypothesis testing procedure to say which are not good ... </p>

<p>This will work if n is sufficiently large... you can also use "leave k out strategy" which can be more relevent when you have "groups" of outliers ...</p>
",223,"2011-06-14 18:26:43",NULL,NULL,NULL,5,NULL,223,"2011-06-14 18:26:43",NULL,213,NULL,NULL,NULL
387,2,NULL,"2010-07-21 07:04:26",36,NULL,"<p>The following list contains many data sets you may be interested:</p>

<ul>
<li><a href="http://www.usnews.com/usnews/edu/college/rankings/rankindex_brief.php">America's Best Colleges - U.S. News &amp; World Reports</a></li>
<li><a href="http://factfinder.census.gov/servlet/BasicFactsServlet">American FactFinder</a></li>
<li><a href="http://www.baseball1.com/">The Baseball Archive</a></li>
<li><a href="http://www.albany.edu/sourcebook/">The Bureau of Justice Statistics</a></li>
<li><a href="http://www.bls.gov/">The Bureau of Labor Statistics</a></li>
<li><a href="http://www.bts.gov/">The Bureau of Transportation Statistics</a></li>
<li><a href="http://www.census.gov/">The Census Bureau</a></li>
<li><a href="http://lib.stat.cmu.edu/DASL/">Data and Story Library (DASL)</a></li>
<li><a href="http://www.stat.ucla.edu/data/">Data Sets, UCLA Statistics Department</a></li>
<li><a href="http://www.cvgs.k12.va.us/DIGSTATS/">DIG Stats</a></li>
<li><a href="http://www.ers.usda.gov/Briefing/">Economic Research Service, US Department of Agriculture</a></li>
<li><a href="http://www.eia.doe.gov/index.html">Energy Information Administration</a></li>
<li><a href="http://epp.eurostat.ec.europa.eu/">Eurostat</a></li>
<li><a href="http://exploringdata.cqu.edu.au/">Exploring Data</a></li>
<li><a href="http://www.fedstats.gov/">FedStats</a></li>
<li><a href="http://www.gallup.com/">The Gallop Organization</a></li>
<li><a href="http://www.gtz.de/en/themen/umwelt-infrastruktur/transport/10285.htm">International Fuel Prices</a></li>
<li><a href="http://www.amstat.org/publications/jse/">Journal of Statistics Education Data Archive</a></li>
<li><a href="http://www.kentuckyderby.com/2003/derby_history/derby_statistics/">Kentucky Derby Race Statistics</a></li>
<li><a href="http://www.ed.gov/NCES/">National Center for Education Statistics</a></li>
<li><a href="http://www.cdc.gov/nchs/">National Center for Health Statistics</a></li>
<li><a href="http://www.ncdc.noaa.gov/oa/ncdc.html">National Climatic Data Center</a></li>
<li><a href="http://www.ngdc.noaa.gov/">National Geophysical Data Center</a></li>
<li><a href="http://www.noaa.gov/">National Oceanic and Atmospheric Administration</a></li>
<li><a href="http://www.amstat.org/sections/SIS/Sports%20Data%20Resources/">Sports Data Resources</a></li>
<li><a href="http://www.statcan.ca/start.html">Statistics Canada</a></li>
<li><a href="http://lib.stat.cmu.edu/datasets/">StatLib---Datasets Archive</a></li>
<li><a href="http://www.statistics.gov.uk/">UK Government Statistical Service</a></li>
<li><a href="http://www.un.org/cyberschoolbus/index.asp">United Nations: Cyber SchoolBus Resources</a></li>
</ul>
",69,"2010-07-21 07:04:26",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 07:04:26",7,NULL,NULL,NULL
388,2,NULL,"2010-07-21 07:15:54",4,NULL,"<p>On the math/foundations side: Harald Cramér's <a href="http://rads.stackoverflow.com/amzn/click/0691005478" rel="nofollow">Mathematical Methods of Statistics</a>. </p>
",251,"2010-07-21 07:15:54",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 07:15:54",363,NULL,NULL,NULL
389,2,NULL,"2010-07-21 07:51:34",12,NULL,"<p>I do think there is something to be said for just excluding the outliers. A regression line is supposed to summarise the data. Because of leverage you can have a situation where 1% of your data points affects the slope by 50%.</p>

<p>It's only dangerous from a moral and scientific point of view if you don't tell anybody that you excluded the outliers. As long as you point them out you can say:</p>

<p>"This regression line fits pretty well for most of the data. 1% of the time a value will come along that doesn't fit this trend, but hey, it's a crazy world, no system is perfect"</p>
",199,"2010-07-21 07:51:34",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,175,NULL,NULL,NULL
390,2,NULL,"2010-07-21 08:01:54",4,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/1587990717" rel="nofollow">Fooled By Randomness</a> by Taleb</p>

<p>Taleb is a professor at Columbia and an options trader. He made about $800 million dollars in 2008 betting against the market. He also wrote Black Swan. He discusses the absurdity of using the normal distribution to model markets, and philosophizes on our ability to use induction. </p>
",74,"2010-09-09 20:46:31",NULL,NULL,NULL,3,NULL,74,"2010-09-09 20:46:31","2010-07-21 08:01:54",363,NULL,NULL,NULL
392,2,NULL,"2010-07-21 08:19:36",5,NULL,"<p>Take a look at the sample galleries for three popular visualization libraries:</p>

<ul>
<li><a href="http://matplotlib.sourceforge.net/gallery.html">matplotlib gallery</a> (Python)</li>
<li><a href="http://addictedtor.free.fr/graphiques/">R graph gallery</a> (R) -- (also see <a href="http://had.co.nz/ggplot2/">ggplot2</a>, scroll down to reference)</li>
<li><a href="http://prefuse.org/gallery/">prefuse visualization gallery</a> (Java)</li>
</ul>

<p>For the first two, you can even view the associated source code -- the simple stuff is simple, not many lines of code.  The prefuse case will have the requisite Java boilerplate code.  All three support a number of backends/devices/renderers (pdf, ps, png, etc).  All three are clearly capable of high quality graphics.</p>

<p>I think it pretty much boils down to which language are you most comfortable working in.  Go with that.</p>
",251,"2010-07-21 08:19:36",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,257,NULL,NULL,NULL
393,2,NULL,"2010-07-21 09:02:11",26,NULL,"<p>The book from Hastie, Tibshirani and Friedman <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">http://www-stat.stanford.edu/~tibs/ElemStatLearn/</a> should be in any statistician's library ! </p>
",223,"2012-01-22 10:30:37",NULL,NULL,NULL,2,NULL,439,"2012-01-22 10:30:37","2010-07-21 09:02:11",363,NULL,NULL,NULL
394,2,NULL,"2010-07-21 09:53:40",0,NULL,"<p>My understanding is that because we are trying to minimise errors, we need to find a way of not getting ourselves in a situation where the sum of the negative difference in errors is equal to the sum of the positive difference in errors but we haven't found a good fit. We do this by squaring the sum of the difference in errors which means the negative and positive difference in errors both become positive ($-1\\times-1 = 1$). If we raised $x$ to the power of anything other than a positive integer we wouldn't address this problem because the errors would not have the same sign, or if we raised to the power of something that isn't an integer we'd enter the realms of complex numbers. </p>
",210,"2013-01-09 21:19:25",NULL,NULL,NULL,0,NULL,17230,"2013-01-09 21:19:25",NULL,354,NULL,NULL,NULL
395,1,NULL,"2010-07-21 10:13:25",4,174,"<p>I have a data set where a series of measurements are being taken each week. In general the data set shows a +/- 1mm change each week with a mean measurement staying at about 0mm. In plotting the data this week it appears that some noticeable movement has occurred at two points and looking back at the data set, it is also possible that movement occurred last week as well. 
What is the best way of looking at this data set to see how likely it is that the movements that have been seen are real movements rather than just some effect caused by the natural tolerance in the readings.</p>

<p><strong>Edit</strong></p>

<p>Some more information on the data set. Measurements have been taken at 39 locations which should behave in a similar way although only some of the points may show signs of movement. At each point the readings have now been taken 10 times on a bi-weekly basis and up until the most recent set of readings the measurements were between -1mm and 1mm. The measurements can only be taken with mm accuracy so we only receive results to the nearest mm. The results for one of the points showing a movement is 0mm, 1mm, 0mm, -1mm, -1mm, 0mm, -1mm, -1mm, 1mm, 3mm. We are not looking for statistically significant information, just an indicator of what might have occurred. The reason is that if a measurement reaches 5mm in a subsequent week we have a problem and we'd like to be forewarned that this might occur. </p>
",210,"2011-02-09 11:30:29","How to tell if something happened in a data set which monitors a value over time",<variance><monitoring>,2,1,1,556,"2011-02-09 11:30:29",NULL,NULL,NULL,NULL,NULL
396,1,399,"2010-07-21 11:00:44",31,2694,"<p>I usually make my own idiosyncratic choices when preparing plots. However, I wonder if there are any best practices for generating plots. </p>

<p>Note: <a href="http://stats.stackexchange.com/questions/257/what-is-the-easiest-way-to-create-publication-quality-plots-under-linux#comment152_261">Rob's comment</a> to an answer to this <a href="http://stats.stackexchange.com/questions/257/what-is-the-easiest-way-to-create-publication-quality-plots-under-linux">question</a> is very relevant here. </p>
",NULL,"2012-04-20 07:15:16","What best practices should I follow when preparing plots?",<data-visualization><best-practices>,15,0,20,9007,"2012-04-20 07:15:16","2011-06-01 03:42:00",NULL,NULL,user28,NULL
397,2,NULL,"2010-07-21 11:11:47",5,NULL,"<p>Just to add a bit to honk's answer, the <a href="http://en.wikipedia.org/wiki/Diehard_tests" rel="nofollow">Diehard Test Suite</a> (developed by George Marsaglia) are the standard tests for PRNG.</p>

<p>There's a nice <a href="http://www.phy.duke.edu/~rgb/General/dieharder.php" rel="nofollow">Diehard C library</a> that gives you access to these tests. As well as the standard Diehard tests it also provides functions for a few other PRNG tests involving (amongst other things) checking bit order. There is also a facilty for testing the speed of the RNG and writing your own tests.</p>

<p>There is a R interface to the Dieharder library, called <a href="http://dirk.eddelbuettel.com/code/rdieharder.html" rel="nofollow">RDieHarder</a>:</p>

<pre><code>library(RDieHarder)
dhtest = dieharder(rng="randu", test=10, psamples=100, seed=12345)
print(dhtest)

Diehard Count the 1s Test (byte)

       data:  Created by RNG `randu' with seed=12345, 
              sample of size 100 p-value &lt; 2.2e-16
</code></pre>

<p>This shows that the <a href="http://en.wikipedia.org/wiki/RANDU" rel="nofollow">RANDU</a> RNG generator fails the minimum-distance / 2dsphere test. </p>
",8,"2010-09-28 22:20:14",NULL,NULL,NULL,0,NULL,8,"2010-09-28 22:20:14",NULL,30,NULL,NULL,NULL
398,2,NULL,"2010-07-21 11:16:55",14,NULL,"<p>We could stay here all day denoting best practices, but you should start by reading Tufte.  My primary recommendation:</p>

<p><strong>Keep it simple.</strong> </p>

<p>Often people try to load up their charts with information. But you should really just have one main idea that you're trying to convey and if someone doesn't get your message almost immediately, then you should rethink how you have presented it.  So don't start working on your chart until the message itself is clear. Occam's razor applies here too. </p>
",5,"2010-07-21 11:16:55",NULL,NULL,NULL,1,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
399,2,NULL,"2010-07-21 11:18:56",22,NULL,"<p>The Tufte principles are very good practices when preparing plots. See also his book <a href="http://rads.stackoverflow.com/amzn/click/0961392177">Beautiful Evidence</a></p>

<p>The principles include:</p>

<ul>
<li>Keep a high data-ink ratio</li>
<li>Remove chart junk</li>
<li>Give graphical element multiple functions</li>
<li>Keep in mind the data density</li>
</ul>

<p>The term to search for is Information Visualization</p>
",190,"2010-07-21 11:18:56",NULL,NULL,NULL,3,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
400,2,NULL,"2010-07-21 11:20:45",10,NULL,"<p>One rule of thumb that I don't always follow but which is on occasion useful is to take into account that it is likely that your plot will at some point in it's future be</p>

<ul>
<li>sent by fax,</li>
<li>photocopied, and/or</li>
<li>reproduced in black-and-white.</li>
</ul>

<p>You need to try and make your plots clear enough that even if they are imprecisely reproduced in the future, the information the plot is trying to convey is still legible. </p>
",210,"2011-06-01 14:03:21",NULL,NULL,NULL,3,NULL,3874,"2011-06-01 14:03:21","2011-06-01 03:42:00",396,NULL,NULL,NULL
401,2,NULL,"2010-07-21 12:22:21",2,NULL,"<p>This problem you are asking about is known as text mining!</p>

<p>There are a few things you need to consider. For example in your question you mentioned using keywords in titles. One may ask "why not the text in the article rather than just the title?" which brings me to the first consideration: What data do you limit yourself to?</p>

<p>Secondly, as the previous answer suggests, using frequencies is a great start. To take the analysis further you may start looking at what words occur frequently together! For example, the word 'happy' may occur very frequently... however if always accompanied by a 'not' your conclusions would be very different!</p>

<p>There is a very nice Australian piece of software I have used in the past called <strong>Leximancer</strong>. I would advise anybody interested in text mining to have a look at their site and the examples they have... from memory one of which analysed speeches by 2 U.S.  presidential candidates. It makes for some very interesting reading!</p>
",256,"2012-05-26 04:00:14",NULL,NULL,NULL,0,NULL,5505,"2012-05-26 04:00:14",NULL,369,NULL,NULL,NULL
402,2,NULL,"2010-07-21 12:30:43",3,NULL,"<p>You might consider applying a <a href="http://gunston.gmu.edu/708/frTukey.asp" rel="nofollow">Tukey Control chart</a> to the data.</p>
",226,"2010-07-21 12:30:43",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,173,NULL,NULL,NULL
403,2,NULL,"2010-07-21 12:33:43",1,NULL,"<p>I must agree.. there is no single best analysis!
not just in cross tabulations or analysis of categorical data but in any data analysis... and thank god for that!
if there was just a single best way to address these analyses well many of us would not have a job to start with... not to mention the loss of the thrill of the hunt!</p>

<p>the joy of analysis is the unknown and the search for answers and evidence and how one question leads to another... that is what i love about statistics!</p>

<p>So back to the categorical data analysis... it really depends on what your doing. Are you looking to find how different variables affect each other as in drug tests for example we may look at treatment vs placebo crossed with disease and no disease... the question here is does treatment reduce disease.... chi square usually does well here (given a good sample size).
Another context ihad today was looking at missing value trends... i was looking to find if missing values in one categorical variable relate to another... in some cases i knew the result should be missing and yet there were observations that had values... a completely different context to the drug test!</p>
",256,"2010-07-21 12:33:43",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,192,NULL,NULL,NULL
404,2,NULL,"2010-07-21 12:37:31",6,NULL,"<p>You might want to have a look at <a href="http://cran.r-project.org/web/packages/strucchange/index.html" rel="nofollow">strucchange</a>: </p>

<blockquote>
  <p>Testing, monitoring and dating structural changes in (linear) regression models. strucchange features tests/methods from the generalized fluctuation test framework as well as from the F test (Chow test) framework. This includes methods to fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM, recursive/moving estimates) and F statistics, respectively. It is possible to monitor incoming data online using fluctuation processes. Finally, the breakpoints in regression models with structural changes can be estimated together with confidence intervals. Emphasis is always given to methods for visualizing the data."</p>
</blockquote>

<p>PS. Nice graphics ;)</p>
",46,"2010-07-21 12:37:31",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,173,NULL,NULL,NULL
405,2,NULL,"2010-07-21 12:38:51",1,NULL,"<p>What kind of movement are we talking about?</p>

<p>You could of course fit a distribution over your data and see whether the new weeks fit in this distribution or are in the tail of it (which means it is likely something significant, real that you are observing)</p>

<p>However, more information from your side would be helpful. Maybe you could provide a part of the dataset?</p>
",190,"2010-07-21 12:38:51",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,395,NULL,NULL,NULL
406,2,NULL,"2010-07-21 12:43:00",11,NULL,"<p>I disagree with this question as it suggests that machine learning and statistics are different or conflicting sciences.... when the opposite is true!</p>

<p>machine learning makes extensive use of statistics... a quick survey of any Machine learning or data mining software package will reveal Clustering techniques such as k-means also found in statistics.... will also show dimension reduction techniques such as Principal components analysis also a statistical technique... even logistic regression yet another.</p>

<p>In my view the main difference is that traditionally statistics was used to proove a pre conceived theory and usually the analysis was design around that principal theory. Where with data mining or machine learning the opposite approach is usually the norm in that we have the outcome we just want to find a way to predict it rather than ask the question or form the theory is this the outcome!</p>
",256,"2010-07-21 12:43:00",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,6,NULL,NULL,NULL
407,2,NULL,"2010-07-21 12:59:00",1,NULL,"<p>One of the above answers touched in mahalanobis distances.... perhaps anpther step further and calculating simultaneous confidence intervals would help detect outliers!</p>
",256,"2010-07-21 12:59:00",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,213,NULL,NULL,NULL
408,2,NULL,"2010-07-21 13:01:39",8,NULL,"<p>In addition to conveying a clear message I always try to remember the plotsmanship:</p>

<ul>
<li>font sizes for labels and legends should be big enough, preferably the same font size and font used in the final publication.</li>
<li>linewidths should be big enough (1 pt lines tend to disappear if plots are shrunk only slightly). I try to go to linewidths of 3 to 5 pt.</li>
<li>if plotting multiple datasets/curves with color make sure that they can be understood if printed in black-and-white, e.g. by using different symbols or linestyles in addition to color.</li>
<li>always use a lossless (or close to lossless) format, e.g. a vector format like pdf, ps or svg or high resolution png or gif (jpeg doesn't work at all and was never designed for line art).</li>
<li>prepare graphics in the final aspect ratio to be used in the publication. Changing the aspect ration later can give irritating font or symbol shapes.</li>
<li>always remove useless clutter from the plotting program like unused histogram information, trend lines (hardly useful) or default titles.</li>
</ul>

<p>I have configured my plotting software (matplotlib, ROOT or root2matplotlib) to do most of this right by default. Before I was using <code>gnuplot</code> which needed extra care here.</p>
",56,"2010-07-21 13:01:39",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
409,1,NULL,"2010-07-21 13:11:33",2,232,"<p>At the moment I use standard deviation of the mean to estimate uncertainty:</p>

<p><img src="http://upload.wikimedia.org/math/8/a/e/8ae657bf495238caf69d82d754e70ed8.png" alt="definition = stdev over sq.root of N"></p>

<p>where <em>N</em> is in hundreds and mean is a time series (monthly) mean. I
present it then like this: <img src="http://rogercortesi.com/eqn/tempimagedir/eqn1898.png" alt="mean plus-minus stdev about the mean"> for each element (month) in the (annual) time series.</p>

<p>Is this valid? Is this appropriate for time series?</p>
",219,"2010-08-30 13:55:03","How to approximate measurement uncertainty?",<time-series><standard-deviation><mean><uncertainty>,2,0,NULL,219,"2010-07-21 13:40:27",NULL,NULL,NULL,NULL,NULL
410,2,NULL,"2010-07-21 13:20:53",2,NULL,"<p>The answer to this question depends a lot on how your measurement uncertainty arises. If it is due to to uncorrelated normally distributed fluctuations in your measurement your measurement outcomes will also be normally distributed.</p>

<p>If this assumption is valid can be hard to prove, but plotting histograms of outcomes of independent measurement should give you a feeling for the shape (and size) of the uncertainty.</p>

<p>Since the normal distribution works on a unlimited range of values, typical cases where measurements are not normally distributed are counting experiments where the number to be measured is small (e.g. less than 20) and/or fluctuations are large, or when the measured quantity is defined to be in a range, e.g. a fraction. One might still use normal approximation in a certain range though.</p>
",56,"2010-07-21 13:20:53",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,409,NULL,NULL,NULL
411,1,NULL,"2010-07-21 13:39:06",31,3293,"<p>There are many ways to measure how similar two probability distributions are.  Among methods which are popular (in different circles) are:</p>

<ol>
<li><p>the Kolmogorov distance: the sup-distance between the distribution functions;</p></li>
<li><p>the Kantorovich-Rubinstein distance: the maximum difference between the expectations w.r.t. the two distributions of functions with Lipschitz constant $1$, which also turns out to be the $L^1$ distance between the distribution functions;</p></li>
<li><p>the bounded-Lipschitz distance: like the K-R distance but the functions are also required to have absolute value at most $1$.</p></li>
</ol>

<p>These have different advantages and disadvantages.  Only convergence in the sense of 3. actually corresponds precisely to convergence in distribution; convergence in the sense of 1. or 2. is slightly stronger in general.  (In particular, if $X_n=\\frac{1}{n}$ with probability $1$, then $X_n$ converges to $0$ in distribution, but not in the Kolmogorov distance. However, if the limit distribution is continuous then this pathology doesn't occur.) </p>

<p>From the perspective of elementary probability or measure theory, 1. is very natural because it compares the probabilities of being in some set. A more sophisticated probabilistic perspective, on the other hand, tends to focus more on expectations than probabilities. Also, from the perspective of functional analysis, distances like 2. or 3. based on duality with some function space are very appealing, because there is a large set of mathematical tools for working with such things.</p>

<p>However, my impression (correct me if I'm wrong!) is that in statistics, the Kolmogorov distance is the usually preferred way of measuring similarity of distributions.  I can guess one reason: if one of the distributions is discrete with finite support -- in particular, if it is the distribution of some real-world data -- then the Kolmogorov distance to a model distribution is easy to compute.  (The K-R distance would be slightly harder to compute, and the B-L distance would probably be impossible in practical terms.)</p>

<p>So my question (finally) is, are there other reasons, either practical or theoretical, to favor the Kolmogorov distance (or some other distance) for statistical purposes?</p>
",89,"2013-09-04 01:41:58","Motivation for Kolmogorov distance between distributions",<distributions><probability><hypothesis-testing><mathematical-statistics>,6,8,9,29617,"2013-09-04 01:41:58",NULL,NULL,NULL,NULL,NULL
412,2,NULL,"2010-07-21 13:40:02",4,NULL,"<p>I think you need look at statistical <a href="http://en.wikipedia.org/wiki/Control_chart" rel="nofollow">control charts</a>. The most common of which are cusum and Shewhart charts.</p>

<p>Basically, data arrives sequentially and is tested against a number of rules. For example,</p>

<ol>
<li>Is the data far away from the cumulative mean - say 3 standard deviations </li>
<li>Has the data been increasing for the last few points.</li>
<li>Does the data alternate between positive and negative values.</li>
</ol>

<p>In R you can use the <a href="http://cran.r-project.org/web/packages/qcc/index.html" rel="nofollow">qcc</a> package.</p>

<p>For example,</p>

<pre><code>#Taken from the documentation
library(qcc)
data(orangejuice)
attach(orangejuice)
plot(qcc(D[trial], sizes=size[trial], type="p"))
</code></pre>

<p>Gives the following plot, with possible problem points highlighted in red.</p>

<p><img src="http://img805.imageshack.us/img805/5858/tmp.jpg" alt="control chart"></p>
",8,"2010-07-21 13:40:02",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,395,NULL,NULL,NULL
413,2,NULL,"2010-07-21 13:48:15",5,NULL,"<p>I don't really know what the conceptual/historical difference between machine learning and statistic is but I am sure it is not that obvious... and I am not really interest in knowing if I am a machine learner or a statistician, I think 10 years after Breiman's paper, lots of people are both...</p>

<p>Anyway, I found  <strong>interesting the question about predictive accuracy of models</strong>. We have to remember that it is not always possible to measure the accuracy of a model and more precisely we are most often implicitly making some modeling when measuring errors.</p>

<p>For Example, mean absolute error in time series forecast is a mean over time and it measures the performance of a procedure to forecast the median with the assumption that performance is, in some sense, <strong>stationary</strong> and shows some <strong>ergodic</strong> property. If (for some reason) you need to forecast the mean temperature on earth for the next 50 years and if your modeling performs well for the last 50 years... it does not means that... </p>

<p>More generally, (if I remember, it is called no free lunch) you can't do anything without modeling... In addition, I think statistic is trying to find an answer to the question : "is something significant or not ", this is a very important question in science and can't be answered through a learning process. To state John Tukey (was he a statistician ?) :</p>

<blockquote>
  <p><em>The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data</em> </p>
</blockquote>

<p>Hope this helps ! </p>
",223,"2010-07-21 13:48:15",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,6,NULL,NULL,NULL
414,1,439,"2010-07-21 13:50:08",20,5291,"<p>What is a good introduction to statistics for a mathematician who is already well-versed in probability?  I have two distinct motivations for asking, which may well lead to different suggestions:</p>

<ol>
<li><p>I'd like to better understand the statistics motivation behind many problems considered by probabilists.</p></li>
<li><p>I'd like to know how to better interpret the results of Monte Carlo simulations which I sometimes do to form mathematical conjectures.</p></li>
</ol>

<p>I'm open to the possibility that the best way to go is not to look for something like "Statistics for Probabilists" and just go to a more introductory source.</p>
",89,"2010-08-13 21:22:56","Introduction to statistics for mathematicians",<books>,6,0,19,509,"2010-08-13 17:20:39","2010-07-21 13:50:08",NULL,NULL,NULL,NULL
415,2,NULL,"2010-07-21 13:53:13",4,NULL,"<p>I think you should take a look to the similar post from mathoverflow at <a href="http://mathoverflow.net/questions/31655/statistics-for-mathematicians/31665#31665" rel="nofollow">http://mathoverflow.net/questions/31655/statistics-for-mathematicians/31665#31665</a></p>

<p>My answer to this post was Asymptotic statistics from Van der Vaart <a href="http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521784504" rel="nofollow">http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521784504</a>. </p>
",223,"2010-07-21 13:53:13",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 13:53:13",414,NULL,NULL,NULL
416,2,NULL,"2010-07-21 14:00:03",14,NULL,"<p>The population is the set of entities under study. For example, the mean height of men. This is a hypothetical population because it includes all men that have lived, are alive and will live in the future. I like this example because it drives home the point that we, as analysts, choose the population that we wish to study. Typically it is impossible to survey/measure the entire population because not all members are observable (e.g. men who will exist in the future). If it is possible to enumerate the entire population it is often costly to do so and would take a great deal of time. In the example above we have a population "men" and a parameter of interest, their height.</p>

<p>Instead, we could take a subset of this population called a sample and use this sample to draw inferences about the population under study, given some conditions. Thus we could measure the mean height of men in a sample of the population which we call a statistic and use this to draw inferences about the parameter of interest in the population. It is an inference because there will be some uncertainty and inaccuracy involved in drawing conclusions about the population based upon a sample. This should be obvious - we have fewer members in our sample than our population therefore we have lost some information.</p>

<p>There are many ways to select a sample and the study of this is called sampling theory. A commonly used method is called Simple Random Sampling (SRS). In SRS each member of the population has an equal probability of being included in the sample, hence the term "random". There are many other sampling methods e.g. stratified sampling, cluster sampling, etc which all have their advantages and disadvantages.</p>

<p>It is important to remember that the sample we draw from the population is only one from a large number of potential samples. If ten researchers were all studying the same population, drawing their own samples then they may obtain different answers. Returning to our earlier example, each of the ten researchers may come up with a different mean height of men i.e. the statistic in question (mean height) varies of sample to sample -- it has a distribution called a sampling distribution. We can use this distribution to understand the uncertainty in our estimate of the population parameter.</p>

<p>The sampling distribution of the sample mean is known to be a normal distribution with a standard deviation equal to the sample standard deviation divided by the sample size. Because this could easily be confused with the standard deviation of the sample it more common to call the standard deviation of the sampling distribution the <strong>standard error</strong>.</p>
",215,"2010-07-21 14:00:03",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,269,NULL,NULL,NULL
417,2,NULL,"2010-07-21 14:01:46",6,NULL,"<p><a href="http://www.powells.com/biblio/61-9780691005478-1">Mathematical Methods of Statistics</a>, Harald Cramér is really great if you're coming to Statistics from the mathematical side.  It's a bit dated, but still relevant for all the basic mathematical statistics.</p>

<p>Two other noteworthy books come to mind for inference and estimation theory:</p>

<ul>
<li><a href="http://www.powells.com/biblio/9780387985022">Theory of Point Estimation</a>, E. L. Lehmann</li>
<li><a href="http://www.powells.com/biblio/0387945466">Theory of Statistics</a>, Schervish</li>
</ul>

<p>Not entirely sure if this is what you wanted, but you can check out the reviews and see if they meet your expectations.</p>
",251,"2010-07-21 14:01:46",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 14:01:46",414,NULL,NULL,NULL
418,1,NULL,"2010-07-21 14:30:42",13,1382,"<p>Coming from the field of computer vision, I've often used the <a href="http://en.wikipedia.org/wiki/RANSAC">RANSAC</a> (Random Sample Consensus) method for fitting models to data with lots of outliers. </p>

<p>However, I've never seen it used by statisticians, and I've always been under the impression that it wasn't considered a "statistically-sound" method. Why is that so? It is random in nature, which makes it harder to analyze, but so are bootstrapping methods. </p>

<p>Or is simply a case of academic silos not talking to one another?</p>
",77,"2014-08-27 01:43:09","Why isn't RANSAC most widely used in statistics?",<outliers><bootstrap><robust>,3,1,5,88,"2010-07-21 17:30:38",NULL,NULL,NULL,NULL,NULL
419,2,NULL,"2010-07-21 14:43:31",4,NULL,"<p>I agree that students find this problem very difficult. The typical response I get is that after you've been shown a goat there's a 50:50 chance of getting the car so why does it matter? Students seem to divorce their first choice from the decision they're now being asked to make i.e. they view these two actions as independent. I then remind them that they were twice as likely to have chosen the wrong door initially hence why they're better off switching. </p>

<p>In recent years I've started actually playing the game in glass and it helps students to understand the problem much better. I use three cardboard toilet roll "middles" and in two of them are paper clips and in the third is a £5 note. </p>
",215,"2010-07-21 14:43:31",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,373,NULL,NULL,NULL
420,2,NULL,"2010-07-21 14:55:06",8,NULL,"<p>I'd modify what Graham Cookson said slightly.  I think the really crucial thing that people overlook is not their first choice, but the <em>host's</em> choice, and the assumption that the host made sure <em>not</em> to reveal the car.  </p>

<p>In fact, when I discuss this problem in a class, I present it in part as a case study in being clear on your assumptions. It is to your advantage to switch <em>if the host is making sure only to reveal a goat</em>.  On the other hand, if the host picked randomly between doors 2 and 3, and happened to reveal a goat, then there is no advantage to switching.</p>

<p>(Of course, the practical upshot is that if you don't know the host's strategy, you should switch anyway.)</p>
",89,"2010-07-21 14:55:06",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,373,NULL,NULL,NULL
421,1,NULL,"2010-07-21 15:01:21",52,11458,"<p>What book would you recommend for scientists who are not statisticians?</p>

<p>Clear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.</p>
",219,"2014-06-06 14:03:35","What book would you recommend for non-statistician scientists?",<books><science>,28,4,56,2669,"2011-05-04 18:19:29","2010-07-21 15:01:21",NULL,NULL,NULL,NULL
422,2,NULL,"2010-07-21 15:09:11",0,NULL,"<p>That'll depend very much on their background, but I found <a href="http://www.amazon.co.uk/Statistics-Nutshell-Desktop-Reference-OReilly/dp/0596510497" rel="nofollow">"Statistics in a Nutshell"</a> to be pretty good.</p>
",247,"2012-08-03 10:07:04",NULL,NULL,NULL,1,NULL,NULL,"2012-08-03 10:07:04","2010-07-21 15:09:11",421,NULL,NULL,user10525
423,1,NULL,"2010-07-21 15:13:21",156,64481,"<p>This is one of my favorites:</p>

<p><img src="http://imgs.xkcd.com/comics/correlation.png" alt="alt text"></p>

<p>One entry per answer. This is in the vein of the Stack Overflow question <em><a href="http://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon">What’s your favorite “programmer” cartoon?</a></em>.</p>

<p>P.S. Do not hotlink the cartoon without the site's permission please.</p>
",5,"2014-08-16 17:48:56","What is your favorite "data analysis" cartoon?",<humor>,56,7,135,509,"2010-08-11 08:49:02","2010-07-21 15:13:21",NULL,NULL,NULL,NULL
424,2,NULL,"2010-07-21 15:21:33",109,NULL,"<p>Was XKCD, so time for Dilbert:</p>

<p><img src="http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/2000/300/2318/2318.strip.print.gif" alt="alt text"></p>
",88,"2010-07-21 15:21:33",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-07-21 15:21:33",423,NULL,NULL,NULL
425,2,NULL,"2010-07-21 15:23:53",88,NULL,"<p>One of my favorites from <a href="http://www.xkcd.com">xckd</a>:</p>

<h2>Random Number</h2>

<p><a href="http://xkcd.com/221/"><img src="http://imgs.xkcd.com/comics/random_number.png" alt="alt text"></a></p>

<blockquote>
  <p>RFC 1149.5 specifies 4 as the standard IEEE-vetted random number.</p>
</blockquote>
",13,"2010-07-21 18:19:37",NULL,NULL,NULL,0,NULL,13,"2010-07-21 18:19:37","2010-07-21 15:23:53",423,NULL,NULL,NULL
427,2,NULL,"2010-07-21 15:33:11",14,NULL,"<p><em>Before touching this topic, I always make sure that students are happy moving between percentages, decimals, odds and fractions. If they are not completely happy with this then they can get confused very quickly.</em></p>

<p>I like to explain hypothesis testing for the first time (and therefore p-values and test statistics) through Fisher's classic tea experiment. I have several reasons for this:</p>

<p>(i) I think working through an experiment and defining the terms as we go along makes more sense that just defining all of these terms to begin with.
(ii) You don't need to rely explicitly on probability distributions, areas under the curve, etc to get over the key points of hypothesis testing.
(iii) It explains this ridiculous notion of "as or more extreme than those observed" in a fairly sensible manner
(iv) I find students like to understand the history, origins and back story of what they are studying as it makes it more real than some abstract theories.
(v) It doesn't matter what discipline or subject the students come from, they can relate to the example of tea (N.B. Some international students have difficulty with this peculiarly British institution of tea with milk.)</p>

<p>[Note: I originally got this idea from Dennis Lindley's wonderful article "The Analysis of Experimental Data: The Appreciation of Tea &amp; Wine" in which he demonstrates why Bayesian methods are superior to classical methods.]</p>

<p>The back story is that Muriel Bristol visits Fisher one afternoon in the 1920's at Rothamsted Experimental Station for a cup of tea. When Fisher put the milk in last she complained saying that she could also tell whether the milk was poured first (or last) and that she preferred the former. To put this to the test he designed his classic tea experiment where Muriel is presented with a pair of tea cups and she must identify which one had the milk added first. This is repeated with six pairs of tea cups. Her choices are either Right (R) or Wrong (W) and her results are: RRRRRW.</p>

<p>Suppose that Muriel is actually just guessing and has no ability to discriminate whatsoever. This is called the <strong>Null Hypothesis</strong>. According to Fisher the purpose of the experiment is to discredit this null hypothesis. If Muriel is guessing she will identify the tea cup correctly with probability 0.5 on each turn and as they are independent the observed result has 0.5^6 = 0.016 (or 1/64). Fisher then argues that either:</p>

<p>(a) the null hypothesis (Muriel is guessing) is true and an event of small probability has occurred or,
(b) the null hypothesis is false and Muriel has discriminatory powers.</p>

<p>The p-value (or probability value) is the probability of observing this outcome (RRRRRW) given the null hypothesis is true - it's the small probability referred to in (a), above. In this instance it's 0.016. Since events with small probabilities only occur rarely (by definition) situation (b) might be a more preferable explanation of what occurred than situation (a). When we reject the null hypothesis we're in fact accepting the opposite hypothesis which is we call the alternative hypothesis. In this example, Muriel has discriminatory powers is the alternative hypothesis.</p>

<p>An important consideration is what do we class as a "small" probability? What's the cutoff point at which we're willing to say that an event is unlikely? The standard benchmark is 5% (0.05) and this is called the significance level. When the p-value is smaller than the significance level we reject the null hypothesis as being false and accept our alternative hypothesis. It is common parlance to claim a result is "significant" when the p-value is smaller than the significance level i.e. when the probability of what we observed occurring given the null hypothesis is true is smaller than our cutoff point. It is important to be clear that using 5% is completely subjective (as is using the other common significance levels of 1% and 10%). </p>

<p>Fisher realised that this doesn't work; every possible outcome with one wrong pair was equally suggestive of discriminatory powers. The relevant probability for situation (a), above, is therefore 6(0.5)^6 = 0.094 (or 6/64) which now is <strong>not significant</strong> at a significance level of 5%. To overcome this Fisher argued that if 1 error in 6 is considered evidence of discriminatory powers then so is no errors i.e. outcomes that more strongly indicate discriminatory powers than the one observed should be included when calculating the p-value. This resulted in the following amendment to the reasoning, either:</p>

<p>(a) the null hypothesis (Muriel is guessing) is true and the probability of events as, or more, extreme than that observed is small, or
(b) the null hypothesis is false and Muriel has discriminatory powers.</p>

<p>Back to our tea experiment and we find that the p-value under this set-up is 7(0.5)^6 = 0.109 which still is not significant at the 5% threshold. </p>

<p>I then get students to work with some other examples such as coin tossing to work out whether or not a coin is fair. This drills home the concepts of the null/alternative hypothesis, p-values and significance levels. We then move onto the case of a continuous variable and introduce the notion of a test-statistic. As we have already covered the normal distribution, standard normal distribution and the z-transformation in depth it's merely a matter of bolting together several concepts. </p>

<p>As well as calculating test-statistics, p-values and making a decision (significant/not significant) I get students to work through published papers in a fill in the missing blanks game. </p>
",215,"2010-07-21 15:33:11",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,31,NULL,NULL,NULL
428,2,NULL,"2010-07-21 15:35:45",3,NULL,"<p>Could you group the data set into much smaller data sets (say 100 or 1000 or 10,000 data points) If you then calculated the median of each of the groups. If you did this with enough data sets you could plot something like the average of the results of each of the smaller sets and this woul, by running enough smaller data sets converge to an 'average' solution. </p>
",210,"2010-07-21 15:35:45",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,346,NULL,NULL,NULL
429,2,NULL,"2010-07-21 15:36:34",14,NULL,"<p>Briefly stated, the Shapiro-Wilk test is a specific test for normality, whereas the method used by <a href="http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm">Kolmogorov-Smirnov test</a> is more general, but less powerful (meaning it correctly rejects the null hypothesis of normality less often). Both statistics take normality as the null and establishes a test statistic based on the sample, but how they do so is different from one another in ways that make them more or less sensitive to features of normal distributions.</p>

<p>How exactly W (the test statistic for Shapiro-Wilk) is calculated is <a href="http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/wilkshap.htm">a bit involved</a>, but conceptually, it involves arraying the sample values by size and measuring fit against expected means, variances and covariances.  These multiple comparisons against normality, as I understand it, give the test more power than the the Kolmogorov-Smirnov test, which is one way in which they may differ.</p>

<p>By contrast, the Kolmogorov-Smirnov test for normality is derived from a general approach for assessing goodness of fit by comparing the expected cumulative distribution against the empirical cumulative distribution, vis:</p>

<p><img src="http://www.itl.nist.gov/div898/handbook/eda/section3/gif/ecdf.gif" alt="alt text"> </p>

<p>As such, it is sensitive at the center of the distribution, and not the tails.  However, the K-S is test is convergent, in the sense that as n tends to infinity, the test converges to the true answer in probability (I believe that <a href="http://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko-Cantelli Theorem</a> applies here, but someone may correct me). These are two more ways in which these two tests might differ in their evaluation of normality.</p>
",39,"2010-07-21 15:36:34",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,362,NULL,NULL,NULL
430,2,NULL,"2010-07-21 15:37:17",14,NULL,"<p>I find that people find the solution more intuitive if you change it to 100 doors, close first, second, to 98 doors.  Similarly for 50 doors, etc.  </p>
",251,"2010-07-22 00:37:06",NULL,NULL,NULL,2,NULL,251,"2010-07-22 00:37:06",NULL,373,NULL,NULL,NULL
431,2,NULL,"2010-07-21 15:38:11",7,NULL,"<p>I think every statistician should read Stigler's <em><a href="http://rads.stackoverflow.com/amzn/click/067440341X">The History of Statistics: The Measurement of Uncertainty before 1900</a></em></p>

<p>It is beautifully written, thorough and it isn't a historian's perspective but a mathematician's, hence it doesn't avoid the technical details. </p>
",215,"2011-02-20 02:34:07",NULL,NULL,NULL,0,NULL,159,"2011-02-20 02:34:07","2010-07-21 15:38:11",363,NULL,NULL,NULL
432,2,NULL,"2010-07-21 15:38:46",96,NULL,"<p>My favourite Dilbert cartoon:</p>

<p><img src="http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/5000/600/5651/5651.strip.gif" alt="alt text"></p>
",8,"2010-07-21 15:38:46",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 15:38:46",423,NULL,NULL,NULL
433,2,NULL,"2010-07-21 15:43:44",40,NULL,"<p>Here's <a href="http://dilbert.com/strips/comic/2010-07-02/">another one from Dilbert</a>:</p>

<p><img src="http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/90000/3000/400/93464/93464.strip.gif" alt="alt text"></p>
",5,"2010-07-21 15:43:44",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 15:43:44",423,NULL,NULL,NULL
434,2,NULL,"2010-07-21 15:50:43",24,NULL,"<p><em>Very</em> crudely I would say that:</p>

<p><strong>Frequentist:</strong> Sampling is infinite and decision rules can be sharp. Data are a repeatable random sample - there is a frequency. Underlying parameters are fixed i.e. they remain constant during this repeatable sampling process.</p>

<p><strong>Bayesian:</strong>  Unknown quantities are treated probabilistically and the state of the world can always be updated. Data are observed from the realised sample. Parameters are unknown and described probabilistically. It is the data which are fixed.</p>

<p>There is a brilliant <a href="http://behind-the-enemy-lines.blogspot.com/2008/01/are-you-bayesian-or-frequentist-or.html">blog post</a> which gives an indepth example of how a Bayesian and Frequentist would tackle the same problem. Why not answer the problem for yourself and then check?</p>

<p>The problem (taken from Panos Ipeirotis' blog):</p>

<p>You have a coin that when flipped ends up head with probability p and ends up tail with probability 1-p. (The value of p is unknown.)</p>

<p>Trying to estimate p, you flip the coin 100 times. It ends up head 71 times.</p>

<p>Then you have to decide on the following event: "In the next two tosses we will get two heads in a row."</p>

<p>Would you bet that the event will happen or that it will not happen?</p>
",215,"2010-07-21 15:50:43",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,22,NULL,NULL,NULL
435,2,NULL,"2010-07-21 15:53:59",84,NULL,"<p>One more <a href="http://dilbert.com/fast/2008-05-08/">Dilbert</a>:
<img src="http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/5000/600/5652/5652.strip.print.gif" alt="alt text"></p>
",88,"2010-07-21 15:53:59",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 15:53:59",423,NULL,NULL,NULL
436,2,NULL,"2010-07-21 16:04:18",14,NULL,"<p>The answer would most definitely depend on their discipline, the methods/techniques that they would like to learn and their existing mathematical/statistical abilities.</p>

<p>For example, economists/social scientists who want to learn about cutting edge empirical econometrics could read Angrist and Pischke's <a href="http://rads.stackoverflow.com/amzn/click/0691120358">Mostly Harmless Econometrics</a>. This is a non-technical book covering the "natural experimental revolution" in economics. The book only presupposes that they know what regression is.</p>

<p>But I think the best book on applied regression is Gelman and Hill's <a href="http://rads.stackoverflow.com/amzn/click/052168689X">Data Analysis Using Regression and Multilevel/Hierarchical Models</a>. This covers basic regression, multilevel regression, and Bayesian methods in a clear and intuitive way. It would be good for any scientist with a basic background in statistics.</p>
",215,"2010-08-11 08:46:15",NULL,NULL,NULL,0,NULL,509,"2010-08-11 08:46:15","2010-07-21 16:04:18",421,NULL,NULL,NULL
437,2,NULL,"2010-07-21 16:07:20",2,NULL,"<p>For you I would suggest:</p>

<p>Introduction to the Mathematical and Statistical Foundations of Econometrics by Herman J. Bierens, CUP. The word "Introduction" in the title is a sick joke for most PhD econometrics students.</p>

<p>Markov Chain Monte Carlo by Dani Gamerman, Chapman &amp; Hall is also concise.</p>
",215,"2010-07-21 16:07:20",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-21 16:07:20",414,NULL,NULL,NULL
438,2,NULL,"2010-07-21 16:42:43",10,NULL,"<p>OK here's my best attempt at an informal and crude explanation.</p>

<p>A Markov Chain is a random process that has the property that the future depends only on the current state of the process and not the past i.e. it is memoryless. An example of a random process could be the stock exchange. An example of a Markov Chain would be a board game like Monopoly or Snakes and Ladders where your future position (after rolling the die) would depend only on where you started from before the roll, not any of your previous positions. A textbook example of a Markov Chain is the "drunkard's walk". Imagine somebody who is drunk and can move only left or right by one pace. The drunk moves left or right with equal probability. This is a Markov Chain where the drunk's future/next position depends only upon where he is at present.</p>

<p>Monte Carlo methods are computational algorithms (simply sets of instructions) which randomly sample from some process under study. They are a way of estimating something which is too difficult or time consuming to find deterministically. They're basically a form of computer simulation of some mathematical or physical process. The Monte Carlo moniker comes from the analogy between a casino and random number generation. Returning to our board game example earlier, perhaps we want to know if some properties on the Monopoly board are visited more often than others. A Monte Carlo experiment would involve rolling the dice repeatedly and counting the number of times you land on each property. It can also be used for calculating numerical integrals. (Very informally, we can think of an integral as the area under the graph of some function.)  Monte Carlo integration works great on a high-dimensional functions by taking a random sample of points of the function and calculating some type of average at these various points. By increasing the sample size, the law of large numbers tells us we can increase the accuracy of our approximation by covering more and more of the function.</p>

<p>These two concepts can be put together to solve some difficult problems in areas such as Bayesian inference, computational biology, etc where multi-dimensional integrals need to be calculated to solve common problems. The idea is to construct a Markov Chain which converges to the desired probability distribution after a number of steps. The state of the chain after a large number of steps is then used as a sample from the desired distribution and the process is repeated. There many different MCMC algorithms which use different techniques for generating the Markov Chain. Common ones include the Metropolis-Hastings and the Gibbs Sampler.</p>
",215,"2010-07-21 16:42:43",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-08-16 06:37:30",165,NULL,NULL,NULL
439,2,NULL,"2010-07-21 16:50:35",8,NULL,"<p>As you said, it's not necessarily the case that a mathematician may want a rigorous book. Maybe the goal is to get some intuition of the concepts quickly, and then fill in the details. I recommend two books from CMU professors, both published by Springer: "All of Statistics" by Larry Wasserman is quick and informal. "Theory of Statistics" by Mark Schervish is rigorous and relatively complete. It has decision theory, finite sample, some asymptotics and sequential analysis.</p>

<p>Added 7/28/10: There is one additional reference that is orthogonal to the other two: very rigorous, focused on learning theory, and short. It's by Smale (Steven Smale!) and Cucker, "<a href="http://www.ams.org/journals/bull/2002-39-01/S0273-0979-01-00923-5/home.html">On the Mathematical Foundations of Learning</a>". Not easy read, but the best crash course on the theory.</p>
",30,"2010-07-29 02:46:37",NULL,NULL,NULL,1,NULL,30,"2010-07-29 02:46:37","2010-07-21 16:50:35",414,NULL,NULL,NULL
440,2,NULL,"2010-07-21 16:53:35",13,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/0387790535">Peter Dalgaard's Introductory Statistics with R</a> is a great book for some introductory statistics with a focus on the R software for data analysis.</p>
",36,"2010-07-21 16:53:35",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 16:53:35",421,NULL,NULL,NULL
441,2,NULL,"2010-07-21 16:54:38",5,NULL,"<p>UCLA has the best free resources you'll find anywhere.</p>

<p><a href="http://www.ats.ucla.edu/stat/stata/" rel="nofollow">http://www.ats.ucla.edu/stat/stata/</a></p>
",36,"2010-07-21 16:54:38",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-11-16 20:17:12",290,NULL,NULL,NULL
442,2,NULL,"2010-07-21 16:56:45",7,NULL,"<p>I say the <a href="http://rads.stackoverflow.com/amzn/click/0961392142" rel="nofollow">visual display of quantitative information</a> by Tufte, and <a href="http://rads.stackoverflow.com/amzn/click/0060731338" rel="nofollow">Freakonomics</a> for something fun.</p>
",36,"2010-07-21 16:56:45",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 16:56:45",363,NULL,NULL,NULL
443,2,NULL,"2010-07-21 17:06:32",7,NULL,"<p>There's two aspects to this <em>post hoc ergo propter hoc</em> problem that I like to cover: (i) reverse causality and (ii) endogeneity </p>

<p>An example of "possible" reverse causality:
Social drinking and earnings - drinkers earn more money according to Bethany L. Peters &amp; Edward Stringham (2006. <em>"No Booze? You May Lose: Why Drinkers Earn More Money Than Nondrinkers,"</em> Journal of Labor Research, Transaction Publishers, vol. 27(3), pages 411-421, June). Or do people who earn more money drink more either because they have a greater disposable income or due to stress? This is a great paper to discuss for all sorts of reasons including measurement error, response bias, causality, etc.</p>

<p>An example of "possible" endogeneity:
The Mincer Equation explains log earnings by education, experience and experience squared. There is a long literature on this topic. Labour economists want to estimate the causal relationship of education on earnings but perhaps education is endogenous because "ability" could increase the amount of education an individual has (by lowering the cost of obtaining it) and could lead to an increase in earnings, irrespective of the level of education. A potential solution to this could be an instrumental variable. Angrist and Pischke's book, Mostly Harmless Econometrics covers this and relates topics in great detail and clarity.</p>

<p>Other silly examples that I have no support for include:
- Number of televisions per capita and the numbers of mortality rate. So let's send TVs to developing countries. Obviously both are endogenous to something like GDP.
- Number of shark attacks and ice cream sales. Both are endogenous to the temperature perhaps?</p>

<p>I also like to tell the terrible joke about the lunatic and the spider. A lunatic is wandering the corridors of an asylum with a spider he's carrying in the palm of his hand. He sees the doctor and says, "Look Doc, I can talk to spiders. Watch this. "Spider, go left!" The spider duly moves to the left. He continues, "Spider, go right." The spider shuffles to the right of his palm. The doctor replies, "Interesting, maybe we should talk about this in the next group session." The lunatic retorts, "That's nothing Doc. Watch this." He pulls off each of the spider's legs one by one and then shouts, "Spider, go left!" The spider lies motionless on his palm and the lunatic turns to the doctor and concludes, "If you pull off a spider's legs he'll go deaf." </p>
",215,"2012-03-31 10:45:48",NULL,NULL,NULL,0,NULL,9007,"2012-03-31 10:45:48","2010-08-16 13:01:42",36,NULL,NULL,NULL
445,2,NULL,"2010-07-21 17:13:33",2,NULL,"<p>The UCLA resource listed by Stephen Turner (above) are excellent if you just want to apply methods you're already familiar with using Stata.</p>

<p>If you're looking for textbooks which teach you statistics/econometrics while using Stata then these are solid recommendations (but it depends at what level you're looking at):</p>

<p><strong>Introductory Methods</strong>
An Introduction to Modern Econometrics Using Stata by Chris Baum
Introduction to Econometrics by Chris Dougherty</p>

<p><strong>Advanced/Specialised Methods</strong>
Multilevel and Longitudinal Modeling Using Stata by Rabe-Hesketh and Skrondal
Regression Models for Categorical Dependent Variables Using Stata by Long and Freese</p>
",215,"2010-07-21 17:13:33",NULL,NULL,NULL,2,NULL,NULL,NULL,"2011-11-16 20:17:12",290,NULL,NULL,NULL
446,2,NULL,"2010-07-21 17:17:45",7,NULL,"<p>The Gelman books are all excellent but not necessarily introductory in that they assume that you know some statistics already. Therefore they are an introduction to the Bayesian way of doing statistics rather than to statistics in general. I would still give them the thumbs up, however.</p>

<p>As an introductory statistics/econometrics book which takes a Bayesian perspective, I would recommend Gary Koop's <a href="http://en.wikipedia.org/wiki/Special%3aBookSources/0521855713" rel="nofollow">Bayesian Econometrics</a>.</p>
",215,"2012-07-24 05:47:17",NULL,NULL,NULL,0,NULL,9007,"2012-07-24 05:47:17","2010-07-21 17:17:45",125,NULL,NULL,NULL
447,2,NULL,"2010-07-21 17:26:14",5,NULL,"<p>I really like these two books by Daniel McFadden of Berkeley:</p>

<p><a href="http://elsa.berkeley.edu/users/mcfadden/e240a_sp98/e240a.html">http://elsa.berkeley.edu/users/mcfadden/e240a_sp98/e240a.html</a></p>

<p><a href="http://elsa.berkeley.edu/users/mcfadden/e240b_f01/e240b.html">http://elsa.berkeley.edu/users/mcfadden/e240b_f01/e240b.html</a></p>
",215,"2010-07-21 17:26:14",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,170,NULL,NULL,NULL
448,2,NULL,"2010-07-21 17:26:15",6,NULL,"<p>Here are my guidelines, based on the most common errors I see (in addition to all the other good points mentioned)</p>

<ul>
<li>Use scatter graphs, not line plots, if element order is not relevant.</li>
<li>When preparing plots that are meant to be compared, use the same scale factor for all of them. </li>
<li>Even better - find a way to combine the data in a single graph (eg: boxplots are a better than several histograms to compare a large number of distributions).</li>
<li>Do not forget to specify units</li>
<li>Use a legend only if you must - it's generally clearer to label curves directly. </li>
<li>If you must use a legend, move it inside the plot, in a blank area.</li>
<li>For line graphs, aim for an aspect ratio which yields <a href="http://processtrends.com/pg_data_vis_bank_to_45.htm" rel="nofollow">lines that are roughly at 45o with the page</a>.</li>
</ul>
",77,"2010-07-21 17:26:15",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
449,2,NULL,"2010-07-21 17:28:57",1,NULL,"<p>More from an economics perspectives I think these two sets of lecture notes are very good:</p>

<p><a href="http://home.datacomm.ch/paulsoderlind/Courses/OldCourses/FinEcmtAll.pdf" rel="nofollow">http://home.datacomm.ch/paulsoderlind/Courses/OldCourses/FinEcmtAll.pdf</a></p>

<p><a href="http://personal.lse.ac.uk/mele/files/fin_eco.pdf" rel="nofollow">http://personal.lse.ac.uk/mele/files/fin_eco.pdf</a></p>

<p>The first provides econometric methods for analysing financial data whereas the second provides the financial economics theory behind the models being applied. They're both MSc level texts.</p>
",215,"2010-07-21 17:28:57",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,328,NULL,NULL,NULL
450,2,NULL,"2010-07-21 17:30:02",6,NULL,"<p>For us, it is just one example of a robust regression -- I believe it is used by statisticians also, but maybe not so wide because it has some better known alternatives.</p>
",88,"2010-07-21 17:30:02",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,418,NULL,NULL,NULL
451,2,NULL,"2010-07-21 17:38:08",5,NULL,"<p>When describing a variable we typically summarise it using two measures: a measure of centre and a measure of spread. Common measures of centre include the mean, median and mode. Common measure of spread include the variance and interquartile range.</p>

<p>The variance (represented by the Greek lowercase sigma raised to the power two) is commonly used when the mean is reported. The variance is the average squared deviation of variable. The deviation is calculated by subtracting the mean from each observation. This is squared because the sum would otherwise be zero and squaring removes this problem while maintaining the relative size of the deviations. The problem with using the variation as a measure of spread is that it is in squared units. For example if our variable of interest was height measured in inches then the variance would be reported in squared-inches which makes little sense. The standard deviation (represented by the Greek lowercase sigma) is the square-root of the variance and returns the measure of spread to the original units. This is much more intuitive and is therefore more popular than the variance.</p>

<p>When using the standard deviation, one has to be careful of outliers as they will skew the standard deviation (and the mean) as they are not resistant measures of spread. A simple example will illustrate this property. The mean of my terrible cricket batting scores of 13, 14, 16, 23, 26, 28, 33, 39, and 61 is 28.11. If we consider 61 to be an outlier and deleted it, the mean would be 24. </p>
",215,"2013-04-20 01:19:50",NULL,NULL,NULL,1,NULL,24560,"2013-04-20 01:19:50",NULL,26,NULL,NULL,NULL
452,1,711,"2010-07-21 17:45:01",9,9951,"<p>It has been suggested by Angrist and Pischke that Robust (i.e. robust to heteroskedasticity or unequal variances) Standard Errors are reported as a matter of course rather than testing for it. Two questions:</p>

<ol>
<li>What is impact on the standard errors of doing so when there is homoskedasticity?</li>
<li>Does anybody actually do this in their work?</li>
</ol>
",215,"2011-12-02 02:12:29","Always Report Robust (White) Standard Errors?",<regression><error><standard-error>,5,4,2,8,"2010-08-13 13:35:57",NULL,NULL,NULL,NULL,NULL
453,2,NULL,"2010-07-21 17:54:05",8,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/041587291X">Statistics in Plain English</a> is pretty good.</p>

<p>4.5 on Amazon, 11 reviews.</p>

<p>Explains ANOVA pretty well too. </p>
",74,"2010-07-21 17:54:05",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 17:54:05",421,NULL,NULL,NULL
454,2,NULL,"2010-07-21 18:11:59",1,NULL,"<p>It depends on the way in which the plots will be discussed. </p>

<p>For instance, if I'm sending out plots for a group meeting that will be done with callers from different locations, I prefer putting them together in Powerpoint as opposed to Excel, so it's easier to flip around. </p>

<p>For one-on-one technical calls, I'll put something in excel so that the client be able to move a plot aside, and view the raw data. Or, I can enter p-values into cells along side regression coefficients, e.g. </p>

<p>Keep in mind: plots are cheap, especially for a slide show, or for emailing to a group. I'd rather make 10 clear plots that we can flip through than 5 plots where I try to put distinct cohorts (e.g. "males and females") on the same graph. </p>
",62,"2010-07-21 18:11:59",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
455,2,NULL,"2010-07-21 18:18:13",27,NULL,"<p>Allright, I think this one is hilarious- but let's see if it passes the Statistical Analysis Miller test.</p>

<h2>Fermirotica</h2>

<p><a href="http://xkcd.com/563/"><img src="http://imgs.xkcd.com/comics/fermirotica.png" alt="I love how Google handles dimensional analysis.  Stats are ballpark and vary wildly by time of day and whether your mom is in town."></a></p>

<blockquote>
  <p>I love how Google handles dimensional analysis.  Stats are ballpark and vary wildly by time of day and whether your mom is in town.</p>
</blockquote>
",13,"2010-07-23 15:27:06",NULL,NULL,NULL,1,NULL,3807,"2010-07-23 15:27:06","2010-07-21 18:18:13",423,NULL,NULL,NULL
456,2,NULL,"2010-07-21 18:26:02",6,NULL,"<p>As the <a href="http://books.google.com/books?id=6q2lOfLnwkAC&amp;dq=Encyclopedia+of+GIS" rel="nofollow">Encyclopedia of GIS</a> states, the conditional autoregressive model (CAR) is appropriate for situations with first order dependency or relatively local spatial autocorrelation, and simultaneous autoregressive model (SAR) is  more suitable where there are second order dependency or a more global spatial autocorrelation.</p>

<p>This is made clear by the fact that CAR obeys the spatial version of the <a href="http://en.wikipedia.org/wiki/Markov_property" rel="nofollow">Markov property</a>, namely it assumes that the state of a particular area is influenced its neighbors and not neighbors of neighbors, etc. (i.e. it is spatially “memoryless”, instead of temporally), whereas SAR does not assume such.  This is due to the different ways in which they specify their variance-covariance matrixes.  So, when the spatial Markov property obtains, CAR provides a simpler way to model autocorrelated geo-referenced areal data.</p>

<p>See <a href="http://www.geog.ucsb.edu/~good/papers/387.pdf" rel="nofollow">Gis And Spatial Data Analysis: Converging Perspectives</a> for more details.</p>
",39,"2010-07-27 19:11:57",NULL,NULL,NULL,0,NULL,39,"2010-07-27 19:11:57",NULL,277,NULL,NULL,NULL
457,2,NULL,"2010-07-21 18:28:27",4,NULL,"<p>I do not know the literature in the area well enough to offer a direct response. However, it seems to me that if the three tests differ then that is an indication that you need further research/data collection in order to definitively answer your question. </p>

<p>You may also want to look at <a href="http://scholar.google.com/scholar?q=small%20sample%20properties%20of%20wald%20likelihood%20ratio&amp;um=1&amp;ie=UTF-8&amp;sa=N&amp;hl=en&amp;tab=ws" rel="nofollow">this</a> Google Scholar search</p>

<p><strong>Update in response to your comment:</strong></p>

<p>If collecting additional data is not possible then there is one workaround. Do a simulation which mirrors your data structure, sample size and your proposed model. You can set the parameters to some pre-specified values. Estimate the model using the data generated and then check which one of the three tests points you to the right model. Such a simulation would offer some guidance as to which test to use for your real data. Does that make sense?</p>
",NULL,"2010-07-22 01:21:24",NULL,NULL,NULL,3,NULL,NULL,"2010-07-22 01:21:24",NULL,359,NULL,user28,user28
458,2,NULL,"2010-07-21 18:41:12",2,NULL,"<p>One way to test patterns in stock market data is discussed <a href="http://www.evidencebasedta.com/" rel="nofollow">here</a>. A similar approach would be to randomise the stock market data and identify your patterns of interest, which would obviously be devoid of any meaning due to the deliberate randomising process. These randomly generated patterns and their returns would form your null hypothesis. By statistically comparing the pattern returns in the actual data with the returns from the null hypothesis randomised data patterns you may be able to distinguish patterns which actually have some meaning or predictive value. </p>
",226,"2010-07-21 18:41:12",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,194,NULL,NULL,NULL
459,1,479,"2010-07-21 19:59:48",7,367,"<p>We're plotting time-series metrics in the context of network/server operations. The data has a 5-minute sample rate, and consists of things like CPU utilization, error rate, etc.</p>

<p>We're adding a horizontal "threshold" line to the graphs, to visually indicate a value threshold above which people should worry/take notice. For example, in the CPU utilization example, perhaps the "worry" threshold is 75%.</p>

<p>My team has some internal debate over what color this line should be:</p>

<ol>
<li>Something like a bright red that clearly stands out from the background grid and data lines, and indicates this is a warning condition</li>
<li>Something more subtle and definitely NOT red, since the "ink" for the line doesn't represent any actual data, and thus attention shouldn't be drawn to it unnecessarily.</li>
</ol>

<p>Would appreciate guidance / best practices...</p>
",259,"2013-05-15 04:14:18","Good line color for "threshold" line in a time-series graph?",<time-series><data-visualization>,4,0,NULL,805,"2013-05-15 04:14:18",NULL,NULL,NULL,NULL,NULL
460,2,NULL,"2010-07-21 20:03:40",6,NULL,"<p>For moderate dimensions, like 3, then some sort of kernel cross-validation technique as suggested elsewhere seems reasonable and is the best I can come up with.</p>

<p>For higher dimensions, I'm not sure that the problem is solvable; it lands pretty squarely into 'curse-of-dimensionality' territory.  The issue is that distance functions tend to converge to very large values very quickly as you increase dimensionality, including distances derived from distributions.  If you're defining an outlier as "a point with a comparatively large distance function relative to the others", and all your distance functions are beginning to converge because you're in a high-dimensional space, well, you're in trouble.</p>

<p>Without some sort of distributional assumption that will let you turn it into a probabilistic classification problem, or at least some rotation that lets you separate your space into "noise dimensions" and "informative dimensions", I think that the geometry of high-dimensional spaces is going to prohibit any easy -- or at least robust -- identification of outliers.</p>
",61,"2010-07-21 20:03:40",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,213,NULL,NULL,NULL
461,2,NULL,"2010-07-21 20:07:32",2,NULL,"<p>To me, whether or not the line represents actual data seems irrelevant.  What's the point of the plot?  If it's so that somebody will do something when utilization crosses a threshold, the line marking the threshold had better be very visible.  If the point of the plot is to give an overview of utilization over time, then why include the line at all?  Just put the major gridlines of your plot at intervals that will coincide with your threshold (25% in your example), and let the reader figure it out.</p>

<p>... y'all been reading too much Tufte.</p>
",71,"2010-07-21 20:07:32",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,459,NULL,NULL,NULL
462,2,NULL,"2010-07-21 20:11:02",8,NULL,"<p>In the physics field there is a rule that the whole paper/report should be understandable only from quick look at the plots. So I would mainly advice that they should be self-explanatory.<br>
This also implies that you must always check whether your audience is familiar with some kind of plot -- I had once made a big mistake assuming that every scientist knows what boxplots are, and then waisted an hour to explain it.</p>
",88,"2010-07-21 20:11:02",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
463,2,NULL,"2010-07-21 20:29:12",5,NULL,"<p>In addition to "The History of Statistics" suggested by Graham, another Stigler book worth reading is</p>

<p><a href="http://rads.stackoverflow.com/amzn/click/0674009797">Statistics on the Table: The History of Statistical Concepts and Methods</a></p>
",90,"2010-07-21 20:29:12",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 20:29:12",363,NULL,NULL,NULL
464,2,NULL,"2010-07-21 20:34:43",3,NULL,"<p>If this is about your "Qnotifier" I think that you should plot the threshold line in some darker gray so it is distinguishable but not disturbing. Then I would color the part of the  plot that reaches over the threshold in some alarmistic hue, like red. </p>
",88,"2010-07-21 20:34:43",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,459,NULL,NULL,NULL
465,2,NULL,"2010-07-21 20:45:08",2,NULL,"<p>I thought that the White Standard Error and the Standard Error computed in the "normal" way (eg, Hessian and/or OPG in the case of maximum likelihood) were asymptotically equivalent in the case of homoskedasticity? </p>

<p>Only if there is heteroskedasticity will the "normal" standard error be inappropriate, which means that the White Standard Error is appropriate with or without heteroskedasticity, that is, even when your model is homoskedastic.</p>

<p>I can't really talk about 2, but I don't see the why one wouldn't want to calculate the White SE and include in the results.</p>
",90,"2010-07-21 20:45:08",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,452,NULL,NULL,NULL
466,2,NULL,"2010-07-21 20:53:54",5,NULL,"<p>Not Statistics specific, but a good resource is:  <a href="http://www.reddit.com/r/mathbooks">http://www.reddit.com/r/mathbooks</a>
Also, George Cain at Georgia Tech maintains a list of freely available maths texts that includes some statistical texts.  <a href="http://people.math.gatech.edu/~cain/textbooks/onlinebooks.html">http://people.math.gatech.edu/~cain/textbooks/onlinebooks.html</a></p>
",115,"2010-07-21 20:53:54",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 20:53:54",170,NULL,NULL,NULL
467,2,NULL,"2010-07-21 21:04:03",5,NULL,"<p><a href="http://www.reddit.com/r/datasets">http://www.reddit.com/r/datasets</a>  and also, <a href="http://www.reddit.com/r/opendata">http://www.reddit.com/r/opendata</a> both contain a constantly growing list of pointers to various datasets.</p>
",115,"2010-07-21 21:04:03",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 21:04:03",7,NULL,NULL,NULL
468,2,NULL,"2010-07-21 22:05:04",4,NULL,"<p>If you're coming from the programming side, one option is to use the <a href="http://www.nltrk.org" rel="nofollow">Natural Language Toolkit</a> (NLTK) for Python.  There's an O'Reilly book, <a href="http://www.nltk.org/book" rel="nofollow">available freely</a>, which might be a less dense and more practical introduction to building classifiers for documents among other things.  </p>

<p>If you're interested in beefing up on the statistical side, Roger Levy's book in progress, <a href="http://idiom.ucsd.edu/~rlevy/textbook/text.html" rel="nofollow">Probabilistic Models in Study of Language</a>, might not be bad to peruse.  It's written for cogsci/compsci grad students starting out with statistical NLP techniques.</p>
",251,"2010-07-21 22:05:04",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,124,NULL,NULL,NULL
469,2,NULL,"2010-07-21 22:13:19",4,NULL,"<p>The <a href="http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/csa/" rel="nofollow">Handbook of Computation Statistics</a> (Gentle, Härdle, Mori) is available online and quite good.  You'll also find a number of other books by Härdle on statistics in finance, nonparametrics, among other topics at the <a href="http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/" rel="nofollow">same site</a>.  </p>
",251,"2010-07-21 22:13:19",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 22:13:19",170,NULL,NULL,NULL
470,2,NULL,"2010-07-21 22:35:38",32,NULL,"<p>The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is a standard text for statistics and data mining, and is now free:</p>

<p><a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">http://www-stat.stanford.edu/~tibs/ElemStatLearn/</a></p>

<p>Also Available <a href="http://rads.stackoverflow.com/amzn/click/0387848576">here</a>.</p>
",36,"2010-07-24 11:51:57",NULL,NULL,NULL,6,NULL,218,"2010-07-24 11:51:57","2010-07-21 22:35:38",170,NULL,NULL,NULL
471,2,NULL,"2010-07-21 22:57:58",14,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/0393310728">Darrell Huff -- How to Lie with Statistics</a></p>
",168,"2011-02-20 02:34:52",NULL,NULL,NULL,1,NULL,159,"2011-02-20 02:34:52","2010-07-21 22:57:58",363,NULL,NULL,NULL
472,2,NULL,"2010-07-21 23:00:34",6,NULL,"<p>For getting into stochastic processes and SDEs, Tom Kurtz's <a href="http://www.math.wisc.edu/~kurtz/m735.htm">lecture notes</a> are hard to beat.  It starts with a decent review of probability and some convergence results, and then dives right into continuous time stochastic processes in fairly clear, comprehensible language.  In general it's one of the best books on the topic -- free or otherwise -- I've found.</p>
",61,"2010-07-21 23:00:34",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-21 23:00:34",170,NULL,NULL,NULL
473,2,NULL,"2010-07-22 02:37:18",2,NULL,"<p>The traditional solution to this problem is to use the <a href="http://dx.doi.org/10.1137/S0036144598347035" rel="nofollow">vector representation</a> for the news stories and then cluster the vectors.  The vectors are arrays where each entry represents a word or word class.  The value associated to each word will be the <a href="http://en.wikipedia.org/wiki/TF_IDF" rel="nofollow">tf-idf</a> weight.  This value goes up the more frequent the word in the document and down the more frequent the word is in the whole collection of documents.</p>

<p>You may think of the titles as the documents, but sticking to just the title for news stories may be a bit risky for clustering similar stories.  The problem is that by using word counts you are discarding all information on the order of the words. Longer texts compensate for that loss information by distinguishing documents by the vocabulary used (articles mentioning <em>finance</em>, <em>money</em>, ... are closer to each other than those mentioning <em>ergodic</em>, <em>Poincare</em>).</p>

<p>If you want to stick to titles, one idea is to think of word pairs as the words you use in the vector representation.  So for the title <em>The eagle has landed</em>, you would think of <em>the eagle</em>, <em>eagle has</em>, <em>has landed</em>. as the &ldquo;words.&rdquo;</p>

<p>To discover when a cluster has become much bigger or different from the others you will need to develop a decision procedure.</p>
",260,"2010-07-22 02:37:18",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,369,NULL,NULL,NULL
474,2,NULL,"2010-07-22 03:21:18",5,NULL,"<p>For testing the numbers produced by random number generators the <a href="http://en.wikipedia.org/wiki/Diehard_tests">Diehard tests</a> are a practical approach. But those tests seem kind of arbitrary and one is may be left wondering if more should be included or if there is any way to really check the randomness.  </p>

<p>The best candidate for a definition of a random sequence seems to be the <a href="http://en.wikipedia.org/wiki/Algorithmically_random_sequence">Martin-Löf randomness</a>.   The main idea for this kind of randomness, is beautifully developed in <a href="http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">Knuth, section 3.5</a>, is to test for uniformity for all types of sub-sequences of the sequence of random numbers.  Getting that <em>all type of subsequences</em> definition right turned out to be be really hard even when one uses notions of computability.</p>

<p>The Diehard tests are just some of the possible subsequences one may consider and their failure would exclude Martin-Löf randomness.</p>
",260,"2010-07-22 03:21:18",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,30,NULL,NULL,NULL
475,2,NULL,"2010-07-22 03:55:48",3,NULL,"<p>There is also <a href="http://gephi.org/" rel="nofollow">Gephi</a> for plotting social networks.</p>

<p>(p.s: Here is how to <a href="http://www.r-bloggers.com/data-preparation-for-social-network-analysis-using-r-and-gephi/" rel="nofollow">connect it with R</a>)</p>
",253,"2010-07-22 03:55:48",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-10-12 08:48:39",224,NULL,NULL,NULL
476,2,NULL,"2010-07-22 04:34:32",7,NULL,"<p>I won't give a definitive answer in terms of ranking the three.  Build 95% CIs around your parameters based on each, and if they're radically different, then your first step should be to dig deeper.  Transform your data (though the LR will be invariant), regularize your likelihood, etc.  In a pinch though, I would probably opt for the LR test and associated CI.  A rough argument follows.</p>

<p>The LR is invariant under the choice of parametrization (e.g. T versus logit(T)).  The Wald statistic assumes normality of (T - T0)/SE(T).  If this fails, your CI is bad.  The nice thing about the LR is that you don't need to find a transform f(T) to satisfy normality.  The 95% CI based on T will be the same.  Also, if your likelihood isn't quadratic, the Wald 95% CI, which is symmetric, can be kooky since it may prefer values with lower likelihood to those with higher likelihood.</p>

<p>Another way to think about the LR is that it's using more information, loosely speaking, from the likelihood function.  The Wald is based on the MLE and the curvature of the likelihood at null.  The Score is based on the slope at null and curvature at null.  The LR evaluates the likelihood under the null, and the likelihood under the union of the null and alternative, and combines the two.  If you're forced to pick one, this may be intuitively satisfying for picking the LR.</p>

<p>Keep in mind that there are other reasons, such as convenience or computational, to opt for the Wald or Score.  The Wald is the simplest and, given a multivariate parameter, if you're testing for setting many individual ones to 0, there are convenient ways to approximate the likelihood.  Or if you want to add a variable at a time from some set, you may not want to maximize the likelihood for each new model, and the implementation of Score tests offers some convenience here.  The Wald and Score become attractive as your models and likelihood become unattractive.  (But I don't think this is what you were questioning, since you have all three available ...)</p>
",251,"2010-07-22 04:34:32",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,359,NULL,NULL,NULL
477,2,NULL,"2010-07-22 04:58:50",1,NULL,"<p>I assume your friend prefers something that's biostatistics oriented.  Glantz's <a href="http://www.powells.com/biblio/62-9780071435093-1" rel="nofollow">Primer of Biostatistics</a> is a small book, an easy and quick read, and tends to get rave reviews from a similar audience.  If an online reference works, I like Gerard Dallal's <a href="http://www.tufts.edu/~gdallal/LHSP.HTM" rel="nofollow">Handbook of Statitical Practice</a>, which may do the trick if he's just refreshing previous knowledge.</p>
",251,"2010-07-22 04:58:50",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,223,NULL,NULL,NULL
478,2,NULL,"2010-07-22 05:13:17",12,NULL,"<p>I'm going to assume some basic statistics knowledge and recommend:</p>

<ul>
<li><p><a href="http://www.powells.com/biblio/65-9780534389505-1" rel="nofollow">The Statistical Sleuth</a> (Ramsey, Schafer) which contain a good deal of mini case studies as they cover the basic statistical tools for data analysis.  </p></li>
<li><p><a href="http://rads.stackoverflow.com/amzn/click/038798206X" rel="nofollow">A First Course in Multivariate Statistics</a> (Flury) which covers the essential statistics required for data mining and the like.  </p></li>
</ul>
",251,"2012-08-16 01:32:09",NULL,NULL,NULL,4,NULL,13280,"2012-08-16 01:32:09","2010-07-22 05:13:17",421,NULL,NULL,NULL
479,2,NULL,"2010-07-22 07:28:42",9,NULL,"<p>If it does not break your styleguide I would rather color the background of the plots red/(yellow/)green than just plotting a line. In my imagination this should make it pretty clear to a user that values are fine on green and to be checked on red. Just my 5&#xa2;.</p>
",128,"2010-07-22 07:28:42",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,459,NULL,NULL,NULL
480,1,635,"2010-07-22 08:58:36",14,1711,"<p>I am not an expert of random forest but I clearly understand that the key issue with random forest is the (random) tree generation. Can you explain me how the trees are generated? (i.e. What is the used distribution for tree generation?)</p>

<p>Thanks in advance ! </p>
",223,"2013-02-14 05:32:34","How does random forest generate the random forest",<machine-learning><r><algorithms><cart><random-forest>,2,0,4,217,"2010-07-26 16:57:59",NULL,NULL,NULL,NULL,NULL
481,1,482,"2010-07-22 09:17:27",7,885,"<p>Another question about time series from me.</p>

<p>I have a dataset which gives daily records of violent incidents in a psychiatric hospital over three years. With the help from my previous question I have been fiddling with it and am a bit happier about it now.</p>

<p>The thing I have now is that the daily series is very noisy. It fluctuates wildly, up and down, from 0 at times up to 20. Using loess plots and the forecast package (which I can highly recommend for novices like me) I just get a totally flat line, with massive confidence intervals from the forecast.</p>

<p>However, aggregating weekly or monthly the data make a lot more sense. They sweep down from the start of the series, and then increase again in the middle. Loess plotting and the forecast package both produce something that looks a lot more meaningful.</p>

<p>It does feel a bit like cheating though. Am I just preferring the aggregated versions because they look nice with no real validity to it?</p>

<p>Or would it be better to compute a moving average and use that as the basis? I'm afraid I don't understand the theory behind all this well enough to be confident about what is acceptable</p>
",199,"2010-07-23 08:15:23","Is it valid to aggregate a time series to make it look more meaningful?",<time-series><forecasting>,3,0,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
482,2,NULL,"2010-07-22 09:24:52",5,NULL,"<p>This totally depends on your time series and what effect you want to discover/proof etc.</p>

<p>An important thing here is, what kind of periods do you have in your data. Make a spectrum of you data and see what frequencies are common in you data.</p>

<p>Anyway, you are not lying when you decide to display aggregated values. When you are looking to effects that are occurring over weeks (like, more violence in summer when it's hot weather) it is the right thing to do.</p>

<p>Maybe you can also take a look at the Hilbert Huang Transform. This will give you Intrinsic Mode Functions that are very handy for visual analyses.</p>
",190,"2010-07-22 09:24:52",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,481,NULL,NULL,NULL
483,2,NULL,"2010-07-22 09:53:52",14,NULL,"<p>The main idea is the bagging procedure, not making trees random. In detail, each tree is built on a sample of objects drawn with replacement from the original set; thus each tree has some objects that it hasn't seen, which is what makes the whole ensemble more heterogeneous and thus better in generalizing.</p>

<p>Furthermore, trees are being weakened in such a way that on the each split only M (or <code>mtry</code>) randomly selected attributes are considered; M is usually a square root of the number of attributes in the set. This ensures that the trees are overfitted less, since they are not pruned.  You can find more details <a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">here</a>. </p>

<p>On the other hand, there is a variant of RF called Extreme Random Forest, in which trees are made in a random way (there is no optimization of splits) -- consult, I think <a href="http://www.springerlink.com/index/10.1007/s10994-006-6226-1">this reference</a>.</p>
",88,"2013-02-14 05:32:34",NULL,NULL,NULL,2,NULL,2798,"2013-02-14 05:32:34",NULL,480,NULL,NULL,NULL
485,1,NULL,"2010-07-22 10:08:10",48,4470,"<p>A question previously sought recommendations for <a href="http://stats.stackexchange.com/questions/414/intro-to-statistics-for-mathematicians">textbooks on mathematical statistics</a></p>

<p>Does anyone know of any good online <strong>video lectures</strong> on <strong>mathematical statistics</strong>?
The closest that I've found are:</p>

<ul>
<li><a href="http://www.youtube.com/watch?v=UzxYlbK2c7E&amp;feature=PlayList&amp;p=A89DCFA6ADACE599&amp;index=0">Machine Learning</a> </li>
<li><a href="http://economistsview.typepad.com/economics421/">Econometrics</a> </li>
</ul>

<p><strong>UPDATE:</strong> A number of the suggestions mentioned below are good statistics-101 type videos.
However, I'm specifically wondering whether there are any videos that provide a rigorous mathematical presentation of statistics.
i.e., videos that might accompany a course that use a textbook mentioned in this <a href="http://mathoverflow.net/questions/31655/statistics-for-mathematicians">discussion on mathoverflow</a></p>
",183,"2013-02-18 07:41:01","Mathematical Statistics Videos",<mathematics><books><mathematical-statistics>,19,3,57,183,"2010-07-22 14:06:03","2010-07-22 14:10:29",NULL,NULL,NULL,NULL
486,1,720,"2010-07-22 10:11:15",17,23985,"<p>I have calculated AIC and AICc to compare two general linear mixed models; The AICs are positive with model 1 having a lower AIC than model 2.  However, the values for AICc are both negative (model 1 is still &lt; model 2).  Is it valid to use and compare negative AICc values? </p>
",266,"2013-11-17 20:31:57","Negative values for AICc (corrected Akaike Information Criterion)",<mixed-model><model-selection><aic>,4,4,4,449,"2011-01-30 18:09:40",NULL,NULL,NULL,NULL,NULL
487,2,NULL,"2010-07-22 10:15:28",9,NULL,"<p>Not a book, but I recently discovered an article by Jacob Cohen in American Psychologist entitled "Things I have learned (so far)."  It's available as a pdf <a href="http://www.uvm.edu/~bbeckage/Teaching/DataAnalysis/AssignedPapers/Cohen_1990.pdf">here</a>.  </p>
",266,"2010-07-22 10:15:28",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-22 10:15:28",363,NULL,NULL,NULL
488,2,NULL,"2010-07-22 10:16:18",10,NULL,"<p>A lot of Social Science / Psychology students with minimal mathematical background like Andy Field's book: <a href="http://rads.stackoverflow.com/amzn/click/0761944524" rel="nofollow">Discovering Statistics Using SPSS</a>. He also has a website that shares a <a href="http://www.statisticshell.com/html/woodofsuicides.html" rel="nofollow">lot of material</a>.</p>
",183,"2013-09-28 00:27:23",NULL,NULL,NULL,1,NULL,30807,"2013-09-28 00:27:23","2010-07-22 10:16:18",421,NULL,NULL,NULL
489,2,NULL,"2010-07-22 11:05:55",3,NULL,"<p>It also depends on where you wan't to publish your plots. You'll save yourself a lot of trouble by consulting the guide for authors before making any plots for a journal. </p>

<p>Also save the plots in a format that is easy to modify or save the code you have used to create them. Chances are that you need to make corrections. </p>
",214,"2010-07-22 11:05:55",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
490,1,606,"2010-07-22 11:10:29",16,2262,"<p>What are the <strong>variable/feature  selection that you prefer</strong> for binary classification when there are many more variables/feature than observations in the learning set? The aim here is to discuss what is the feature selection procedure that reduces the best the classification error. </p>

<p>We can <strong>fix notations</strong> for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. </p>

<p>Please give full references if you cannot give the details. </p>

<p>EDIT (updated continuously):  Procedures proposed in the answers below </p>

<ul>
<li><strong>Greedy forward selection</strong> <a href="http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/497#497">Variable selection procedure for binary classification</a></li>
<li><strong>Backward elimination</strong> <a href="http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/501#501">Variable selection procedure for binary classification</a></li>
<li><strong>Metropolis scanning / MCMC</strong> <a href="http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/505#505">Variable selection procedure for binary classification</a></li>
<li><strong>penalized logistic regression</strong> <a href="http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/606#606">Variable selection procedure for binary classification</a></li>
</ul>

<p>As this is community wiki there can be more discussion and update</p>

<p>I have one remark: in a certain sense, you all give a procedure that permit ordering of variables but not variable selection (you are quite evasive on how to select the number of features, I guess you all use cross validation?) Can you improve the answers in this direction? (as this is community wiki you don't need to be the answer writter to add an information about how to select the number of variables? I have openned a question in this direction here <a href="http://stats.stackexchange.com/questions/880/cross-validation-to-select-the-number-of-used-variables-in-very-high-dimensional">Cross validation in very high dimension (to select the number of used variables in very high dimensional classification)</a>)</p>
",223,"2012-05-04 14:52:38","Variable selection procedure for binary classification",<machine-learning><classification><multiple-comparisons><multivariate-analysis><feature-selection>,6,2,12,486,"2012-05-04 14:52:38","2010-07-22 11:58:13",NULL,NULL,NULL,NULL
491,2,NULL,"2010-07-22 11:19:10",2,NULL,"<p>I recently found <a href="http://rads.stackoverflow.com/amzn/click/0131467573" rel="nofollow">Even You Can Learn Statistics</a> to be pretty useful.</p>
",268,"2010-07-22 11:19:10",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-22 11:19:10",421,NULL,NULL,NULL
492,1,NULL,"2010-07-22 11:31:16",10,534,"<p>I am proposing to try and find a trend in some very noisy long term data. The data is basically weekly measurements of something which moved about 5mm over a period of about 8 months. The data is to 1mm accuracey and is very noisy regularly changing +/-1 or 2mm in a week. We only have the data to the nearest mm. </p>

<p>We plan to use some basic signal processing with a fast fourier transform to separate out the noise from the raw data. The basic assumption is if we mirror our data set and add it to the end of our existing data set we can create a full wavelength of the data and therefore our data will show up in a fast fourier transform and we can hopefully then separate it out. </p>

<p>Given that this sounds a little dubious to me, is this a method worth purusing or is the method of mirroring and appending our data set somehow fundamentally flawed? We are looking at other approaches such as using a low pass filter as well. </p>
",210,"2013-03-13 11:29:16","Dubious use of signal processing principles to identify a trend",<time-series><data-mining><signal-processing><trend>,6,3,2,919,"2012-06-07 20:31:39",NULL,NULL,NULL,NULL,NULL
493,2,NULL,"2010-07-22 11:33:47",3,NULL,"<p>Statsoft's <a href="http://www.statsoft.com/textbook/" rel="nofollow">Electronic Statistics Handbook</a> ('The only Internet Resource about Statistics Recommended by Encyclopedia Britannica') is worth checking out.</p>
",268,"2010-07-22 11:33:47",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-22 11:33:47",170,NULL,NULL,NULL
494,2,NULL,"2010-07-22 11:42:42",6,NULL,"<p>Generally, it is assumed that AIC (and so AICc) is defined up to adding a constant, so the fact if it is negative or positive is not meaningful at all. So the answer is yes, it is valid.</p>
",88,"2010-07-22 20:16:08",NULL,NULL,NULL,2,NULL,88,"2010-07-22 20:16:08",NULL,486,NULL,NULL,NULL
495,2,NULL,"2010-07-22 11:43:42",6,NULL,"<p>There is one called <a href="http://www.academicearth.org/courses/math-and-proability-for-life-sciences" rel="nofollow">Math and probability for life sciences</a>, but I haven't followed it so I can't tell you if its good or not.</p>
",214,"2010-07-22 11:43:42",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,485,NULL,NULL,NULL
496,2,NULL,"2010-07-22 11:44:13",6,NULL,"<p>I think you can get some distortion on the pasting point as not all the underlying waves will connect very well.</p>

<p>I would suggest using a Hilbert Huang transform for this. Just do the split into intrinsic mode functions and see what is left over as residue when calculating them.</p>
",190,"2010-07-22 11:44:13",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,492,NULL,NULL,NULL
497,2,NULL,"2010-07-22 11:53:51",3,NULL,"<p>Greedy forward selection.</p>

<p>The steps for this method are:</p>

<ul>
<li>Make sure you have a train and validation set</li>
<li>Repeat the following
<ul>
<li>Train a classifier with each single feature separately that is not selected yet and with all the previously selected features</li>
<li>If the result improves, add the best performing feature, else stop procedure</li>
</ul></li>
</ul>
",190,"2010-07-22 11:53:51",NULL,NULL,NULL,4,NULL,NULL,NULL,"2010-07-28 09:35:43",490,NULL,NULL,NULL
498,1,NULL,"2010-07-22 11:58:21",2,2137,"<p>Sometimes, I just want to do a copy &amp; paste from the output window in SAS. I can highlight text with a mouse-drag, but only SOMETIMES does that get copied to the clipboard. It doesn't matter if I use "CTRL-C" or right click -> copy, or edit -> copy</p>

<p>Any other SAS users experience this, and do you know a workaround/option/technique that can fix it? </p>

<p>Sometimes, I can fix it by clicking in another window, and coming back to the output window, but sometimes I just have to save the output window as a .lst and get the text from another editor. </p>
",62,"2013-05-23 14:07:51","In PC SAS, how do you copy & paste from the output window?",<sas><pc-sas>,5,11,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
499,1,3728,"2010-07-22 12:06:28",17,478,"<p>I've heard that when many regression model specifications (say, in OLS) are considered as possibilities for a dataset, this causes multiple comparison problems and the p-values and confidence intervals are no longer reliable. One extreme example of this is stepwise regression.</p>

<p>When can I use the data itself to help specify the model, and when is this not a valid approach? Do you always need to have a subject-matter-based theory to form  the model?</p>
",267,"2010-10-19 01:50:06","When can you use data-based criteria to specify a regression model?",<regression><frequentist><multiple-comparisons>,5,0,3,267,"2010-07-22 13:26:10",NULL,NULL,NULL,NULL,NULL
500,2,NULL,"2010-07-22 12:16:29",2,NULL,"<p>If I understand your question right, than the answer to your problem is to correct the p-values accordingly to the number of hypothesis.</p>

<p>For example Holm-Bonferoni corrections, where you sort the hypothesis (= your different models) by their p-value and reject those with a p samller than (desired p-value / index).</p>

<p>More about the topic can be found on <a href="http://en.wikipedia.org/wiki/Multiple_comparisons" rel="nofollow">Wikipedia</a></p>
",190,"2010-07-22 12:16:29",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,499,NULL,NULL,NULL
501,2,NULL,"2010-07-22 12:17:20",3,NULL,"<p>Backward elimination.</p>

<p>Start with the full set, then iteratively train the classifier on the remaining features and remove the feature with the smallest importance, stop when the classifier error rapidly increases/becomes unacceptable high.</p>

<p>Importance can be even obtained by removing iteratively each feature and check the error increase or adapted from the classifier if it produces it (like in case of Random Forest). </p>
",88,"2010-07-22 13:05:25",NULL,NULL,NULL,7,NULL,223,"2010-07-22 13:05:25","2010-07-22 12:17:20",490,NULL,NULL,NULL
502,2,NULL,"2010-07-22 12:25:42",5,NULL,"<p>I do not know at what level you want the videos to be but I have heard good things about Khan's Academy: <a href="http://www.khanacademy.org/#Statistics">http://www.khanacademy.org/#Statistics</a></p>
",NULL,"2010-07-22 12:25:42",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,485,NULL,user28,NULL
503,2,NULL,"2010-07-22 12:25:48",2,NULL,"<p>I have been using SAS a long time and have never had an issue with highlighting results from the output window.</p>

<p>However since you are having an issue... there are alarge number of solutions!</p>

<p>Perhaps the most i like... and probably a good habit to get into is to output your results into datasets... or into excel spread sheets directly (using the ODS) you can also output directly into pdf, rtf with 2 lines of the simplest code you can imagine!
if your code produces alot of output and you only have one table you want to copy you can specify the name of the table and it alone will be output using the ODS output.</p>

<p>usually you just need to wrap your Procedure (like Proc Means for example)
with </p>

<p>ods PDF;
Proc Means Data = blah N NMISS MEAN STD;
class upto you;
var you name it;
run;
ods PDF close;</p>

<p>of course there are many ways to get fancy with the way the output looks but that is a matter of trial and error and finding what you like or meets your needs.</p>
",256,"2010-07-22 12:25:48",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,498,NULL,NULL,NULL
504,2,NULL,"2010-07-22 12:33:49",4,NULL,"<p>Many of the <strong>Berkeley</strong> introductory statistics courses are available online (and on iTunes).  Here's an example: <a href="http://webcast.berkeley.edu/course_details.php?seriesid=1906978493" rel="nofollow"><strong>Stats 2</strong></a>.  You can <a href="http://www.google.com/search?hl=en&amp;rlz=1C1CHMP_en-USUS292US307&amp;q=statistics+video+site%3awebcast.berkeley.edu+&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=&amp;gs_rfai=" rel="nofollow">find more here</a>.</p>
",5,"2010-07-22 12:33:49",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,485,NULL,NULL,NULL
505,2,NULL,"2010-07-22 12:42:13",5,NULL,"<p>Metropolis scanning / MCMC</p>

<ul>
<li>Select few features randomly for a
start, train classifier only on them
and obtain the error. </li>
<li>Make some
random change to this working set --
either remove one feature, add
another at random or replace some
feature with one not being currently
used.</li>
<li>Train new classifier and get
its error; store in <code>dE</code> the difference
the error on the new set minus the error on the previous set. </li>
<li>With probability <code>min(1;exp(-beta*dE))</code> accept this change, otherwise reject it and try another random change.</li>
<li>Repeat it for a long time and finally return the working set that has globally achieved the smallest error.</li>
</ul>

<p>You may extend it with some wiser control of <code>beta</code> parameter. Simpler way is to use simulated annealing when you increase <code>beta</code> (lower the temperature in physical analogy) over the time to reduce fluctuations and drive the algorithm towards minimum. Harder is to use <a href="http://en.wikipedia.org/wiki/Replica_exchange">replica exchange</a>.</p>
",88,"2010-07-22 12:42:13",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-22 12:42:13",490,NULL,NULL,NULL
506,2,NULL,"2010-07-22 13:10:57",13,NULL,"<p>Both MOM and GMM are very general methods for estimating parameters of statistical models. GMM is - as the name suggests - a generalisation of MOM. It was developed by Lars Peter Hansen and first published in Econometrica [1]. As there are numerous textbooks on the subject (e.g. [2]) I presume you want a non-technical answer here.</p>

<p><strong>Traditional or Classical Method of Moments Estimator</strong></p>

<p>The MOM estimator is a consistent but inefficient estimator. Assume a vector of data y which were generated by a probability distribution indexed by a parameter vector theta with k elements. In the method of moments, theta is estimated by computing k sample moments of y, setting them equal to population moments derived from the assumed probability distribution, and solving for theta. For example, the population moment of mu is the expectation of y, whereas the sample moment of mu is the sample mean of y. You would repeat this for each of the k elements of theta. As sample moments are generally consistent estimators of population moments, theta-hat will be consistent for theta.</p>

<p><strong>Generalised Method of Moments</strong></p>

<p>In the example above, we had the same number of moment conditions as unknown parameters, so all we would have done is solved the k equations in k unknowns to obtain the parameter estimates. Hansen asked: What happens when we have more moment conditions than parameters as usually occurs in econometric models? How can we combine them optimally? That is the purpose of the GMM estimator. In GMM we estimate the parameter vector by minimising the sum of squares of the differences between the population moments and the sample moments, using the variance of the moments as a metric. This is the minimum variance estimator in the class of estimators that use these moment conditions. </p>

<p>[1] Hansen, L. P.  (1982): Large Sample Properties of Generalized Method of Moments Estimators, <em>Econometrica</em>, 50, 1029-1054</p>

<p>[2] Hall, A. R. (2005). <em>Generalized Method of Moments (Advanced Texts in Econometrics).</em> Oxford University Press</p>
",215,"2010-07-22 13:10:57",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,287,NULL,NULL,NULL
507,1,560,"2010-07-22 13:40:30",12,3303,"<p>What is your preferred method of checking for convergence when using Markov chain Monte Carlo for Bayesian inference, and why?</p>
",215,"2013-11-08 08:17:57","What is the best method for checking convergence in MCMC?",<bayesian><mcmc>,4,1,4,NULL,NULL,NULL,NULL,NULL,NULL,NULL
508,2,NULL,"2010-07-22 13:41:53",1,NULL,"<p>I would add that the choice of plot should reflect the type of statistical test used to analyse the data.  In other words, whatever characteristics of the data were used for analysis should be shown visually - so you would show means and standard errors if you used a t-test but boxplots if you used a Mann-Whitney test.</p>
",266,"2010-07-22 13:41:53",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-06-01 03:42:00",396,NULL,NULL,NULL
509,2,NULL,"2010-07-22 13:43:18",2,NULL,"<p>I like to do trace plots primarily and sometimes I use the Gelman-Rubin convergence diagnostic.</p>
",NULL,"2010-07-22 13:43:18",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,507,NULL,user28,NULL
510,2,NULL,"2010-07-22 13:49:37",35,NULL,"<p>More about design and power than analysis, but I like this one</p>

<p><img src="http://imgs.xkcd.com/comics/experimentation.png" alt="alt text"></p>
",266,"2010-07-22 13:49:37",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-22 13:49:37",423,NULL,NULL,NULL
511,2,NULL,"2010-07-22 14:32:20",26,NULL,"<p>From <a href="http://xkcd.com/701/">xkcd</a>:</p>

<p><img src="http://imgs.xkcd.com/comics/science_valentine.png" alt="You don't use science to show that you are right, you use science to become right."></p>

<p>If some people who really believe that everything should be scientifically tested would actually walk their talk than they this comic might even show an event that actually happens.</p>
",3807,"2010-07-22 14:32:20",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-22 14:32:20",423,NULL,NULL,NULL
512,2,NULL,"2010-07-22 14:52:54",8,NULL,"<p>If you want to filter the long term trend out using signal processing, why not just use a low-pass? </p>

<p>The simplest thing I can think of would be an exponential moving average.</p>
",33,"2010-07-22 14:52:54",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,492,NULL,NULL,NULL
513,2,NULL,"2010-07-22 15:22:20",2,NULL,"<p>I like to use resampling: I repeat whatever method I used with a subsample of the data (say 80% or even 50% of the total).  By doing this with many different subsamples, I get a feel for how robust the estimates are.  For many estimation procedures this can be made into a real (meaning publishable) estimate of your errors.</p>
",260,"2010-07-22 15:22:20",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,276,NULL,NULL,NULL
514,2,NULL,"2010-07-22 15:36:53",2,NULL,"<p>Here's how I would answer this question using a diagram.  </p>

<p>Let's say we weigh 30 cats and calculate the mean weight.  Then we produce a scatter plot, with weight on the y axis and cat identity on the x axis.  The mean weight can be drawn in as a horizontal line.  We can then draw in vertical lines which connect each data point to the mean line - these are the deviations of each data point from the mean, and we call them residuals.  Now, these residuals can be useful because they can tell us something about the spread of the data: if there are many big residuals, then cats vary a lot in mass.  Conversely, if the residuals are mainly small, then cats are fairly closely clustered around the average weight.  So if we could have some metric which tells us the <em>average</em> length of a residual in this data set, this would be a handy way of denoting how much spread there is in the data.  The standard deviation is, effectively, the length of the average residual. </p>

<p>I would follow on on from this by giving the calculation for  s.d., explaining why we square and then square root (I like Vaibhav's short and sweet explanation).  Then I would mention the problems of outliers, as Graham does in his last paragraph.</p>
",266,"2010-07-22 15:36:53",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,26,NULL,NULL,NULL
515,2,NULL,"2010-07-22 15:49:54",8,NULL,"<p>1) A good demonstration of how "random" needs to be defined in order to work out probability of certain events:</p>

<p>What is the chance that a random line drawn across a circle will be longer than the radius?</p>

<p>The question totally depends how you draw your line. Possibilities which you can describe in a real-world way for a circle drawn on the ground might include:</p>

<p>Draw two random points inside the circle and draw a line through those. (See where two flies / stones fall...)</p>

<p>Choose a fixed point on the circumference, then a random one elsewhere in the circle and join those. (In effect this is laying a stick across the circle at a variable angle through a given point and a random one e.g. where a stone falls.)</p>

<p>Draw a diameter. Randomly choose a point along it and draw a perpendicular through that. (Roll a stick along in a straight line so it rests across the circle.)</p>

<p>It is relatively easy to show someone who can do some geometry (but not necessarily stats) the answer to the question can vary quite widely (from about 2/3 to about 0.866 or so).</p>

<p>2) A reverse-engineered coin-toss: toss it (say) ten times and write down the result. Work out the probability of this exact sequence $\\left(\\frac{1}{2^{10}}\\right)$. A tiny chance, but you just saw it happen with your own eyes!... Every sequence <em>might</em> come up, including ten heads in a row, but it is hard for lay people to get their head round it. As an encore, try to convince them they have just as good a chance of winning the lottery with the numbers 1 through 6 as any other combination.</p>

<p>3) Explaining why medical diagnosis may seem really flawed. A test for disease foo which is 99.9% accurate at identifying those who have it but .1% false-positively diagnoses those who don't really have it may seem to be wrong really so often when the prevalence of the disease is really low (e.g. 1 in 1000) but many patients are tested for it. </p>

<p>This is one that is best explained with real numbers - imagine 1 million people are tested, so 1000 have the disease, 999 are correctly identified, but 0.1% of 999,000 is 999 who are told they have it but don't. So half those who are told they have it actually do not, despite the high level of accuracy (99.9%) and low level of false positives (0.1%). A second (ideally different) test will then separate these groups out. </p>

<p>[Incidentally, I chose the numbers because they are easy to work with, of course they do not have to add up to 100% as the accuracy / false positive rates are independent factors in the test.]</p>
",270,"2013-10-23 15:29:05",NULL,NULL,NULL,1,NULL,17230,"2013-10-23 15:29:05","2012-08-21 15:17:25",155,NULL,NULL,NULL
516,2,NULL,"2010-07-22 16:00:44",3,NULL,"<p>Yes it's valid to compare negative AICc values, in the same way as you would negative AIC values.  The correction factor in the AICc can become large with small sample size and relatively large number of parameters, and penalize heavier than the AIC.  So positive AIC values can correspond to negative AICc values.</p>
",251,"2010-07-22 16:00:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,486,NULL,NULL,NULL
517,1,522,"2010-07-22 16:21:06",13,1760,"<p>In the context of machine learning, what is the difference between unsupervised learning, supervised learning and semi-supervised learning?</p>

<p>And what are some of the main algorithmic approaches to look at?</p>
",68,"2011-07-19 08:50:57","Unsupervised, supervised and semi-supervised learning",<machine-learning>,3,2,4,68,"2010-07-22 16:44:09",NULL,NULL,NULL,NULL,NULL
518,2,NULL,"2010-07-22 16:24:10",3,NULL,"<p>Try <a href="http://www.r-project.org/" rel="nofollow">R</a>. <a href="http://cran.at.r-project.org/web/views/Cluster.html" rel="nofollow">Here</a> you have a list of clustering packages available.</p>
",88,"2010-07-22 16:24:10",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,183,NULL,NULL,NULL
519,2,NULL,"2010-07-22 16:32:57",4,NULL,"<p>I read (a long time ago) of an interesting example about a decline in birth rates (or fertility rates if you prefer that measure) especially in the US, starting in the early 1960's,  as nuclear weapons testing was at an all-time high (in 1961 the biggest nuclear bomb ever detonated was tested in the USSR). Rates continued to deline until towards the end of the twentieth century when most countried finally stopped doing this.</p>

<p>I can't find a reference which combines these figures now, but this Wikipedia article has figures on <a href="http://en.wikipedia.org/wiki/Nuclear_weapons_testing" rel="nofollow">nuclear weapons test</a> numbers by country.</p>

<p>Of course, it might make better sense to look at the correlation of birth rate with the introduction and legalisation of the contraceptive pill 'coincidentally' starting in the early 1960s.  (In only some states first, then all states for married women only, then some for unmarried, then across the board), But even that could only be part of the cause; lots of other aspects of equality, economic changes and other factors play a significant part.</p>
",270,"2010-07-22 16:32:57",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-08-16 13:01:42",36,NULL,NULL,NULL
520,2,NULL,"2010-07-22 16:33:43",2,NULL,"<p>Python will give you all the flexibility you need. With the NumPy and <a href="http://docs.scipy.org/doc/scipy/reference/cluster.html" rel="nofollow">SciPy cluster module</a> you have the tools you need, and the datatypes of NumPy give you a good insight in how much memory you will use.</p>
",190,"2010-07-22 16:33:43",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,183,NULL,NULL,NULL
521,2,NULL,"2010-07-22 16:39:05",7,NULL,"<p><strong>Unsupervised Learning</strong></p>

<p>Unsupervised learning is when you have no labeled data available for training. Examples of this are often clustering methods.</p>

<p><strong>Supervised Learning</strong></p>

<p>In this case your training data exists out of labeled data. The problem you solve here is often predicting the labels for data points without label.</p>

<p><strong>Semi-Supervised Learning</strong></p>

<p>In this case both labeled data and unlabeled data are used. This for example can be used in Deep belief networks, where some layers are learning the structure of the data (unsupervised) and one layer is used to make the classification (trained with supervised data)</p>
",190,"2010-07-22 16:39:05",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,517,NULL,NULL,NULL
522,2,NULL,"2010-07-22 18:03:43",12,NULL,"<p>Generally, the problems of machine learning may be considered variations on function estimation for classification, prediction or modeling.</p>

<p>In <strong>supervised learning</strong> one is furnished with input (x1, x2, . .,) and output (y1, y2, . .,) and are challenged with finding a function that approximates this behavior in a generalizable fashion.  The output could be a class label (in classification) or a real number (in regression)-- these are the "supervision" in supervised learning. </p>

<p>In the case of <strong>unsupervised learning</strong>, in the base case, you receives inputs x1, x2, . ., but neither target outputs, nor rewards from its environment are provided.  Based on the problem (classify, or predict) and your background knowledge of the space sampled, you may use various methods: density estimation (estimating some underlying PDF for prediction), k-means clustering (classifying unlabeled real valued data), k-modes clustering (classifying unlabeled categorical data), etc.</p>

<p><strong>Semi-supervised learning</strong> involves function estimation on labeled and unlabeled data.  This approach is motivated by the fact that labeled data is often costly to generate, whereas unlabeled data is generally not.  The challenge here mostly involves the technical question of how to treat data mixed in this fashion. See this <a href="http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf">Semi-Supervised Learning Literature Survey</a> for more details on semi-supervised learning methods.</p>

<p>In addition to these kinds of learning, there are others, such as <strong>reinforcement learning</strong> whereby the learning method interacts with its environment by producing actions a1, a2, . . .. that produce rewards or punishments r1, r2, ...</p>
",39,"2010-07-22 18:22:28",NULL,NULL,NULL,1,NULL,39,"2010-07-22 18:22:28",NULL,517,NULL,NULL,NULL
523,2,NULL,"2010-07-22 18:16:08",4,NULL,"<p>I don't think that supervised/unsupervised is the best way to think about it. For basic data mining, it's better to think about what you are trying to do. There are four main tasks:</p>

<ol>
<li><p>prediction. if you are predicting a real number, it is called regression. if you are predicting a whole number or class, it is called classification.</p></li>
<li><p>modeling. modeling is the same as prediction, but the model is comprehensible by humans. Neural networks and support vector machines work great, but do not produce comprehensible models [1]. decision trees and classic linear regression are examples of easy-to-understand models. </p></li>
<li><p>similarity. if you are trying to find natural groups of attributes, it is called factor analysis. if you are trying to find natural groups of observations, it is called clustering.</p></li>
<li><p>association. it's much like correlation, but for enormous binary datasets. </p></li>
</ol>

<p>[1] Apparently Goldman Sachs created tons of great neural networks for prediction, but then no one understood them, so they had to write other programs to try to explain the neural networks. </p>
",74,"2010-08-13 17:59:07",NULL,NULL,NULL,2,NULL,74,"2010-08-13 17:59:07",NULL,517,NULL,NULL,NULL
524,1,525,"2010-07-22 18:25:54",8,284,"<p>Debugging MCMC programs is notoriously difficult. The difficulty arises because of several issues some of which are:</p>

<p>(a) Cyclic nature of the algorithm</p>

<p>We iteratively draw parameters conditional on all other parameters. Thus, if a implementation is not working properly it is difficult to isolate the bug as the issue can be anywhere in the iterative sampler.</p>

<p>(b) The correct answer is not necessarily known. </p>

<p>We have no way to tell if we have achieved convergence. To some extent this can be mitigated by testing the code on simulated data.</p>

<p>In light of the above issues, I was wondering if there is a standard technique that can be used to debug MCMC programs. </p>

<p><strong>Edit</strong></p>

<p>I wanted to share the approach I use to debug my own programs. I, of course, do all of the things that PeterR mentioned. Apart from those, I perform the following tests using simulated data:</p>

<ol>
<li><p>Start all parameters from true values and see if the sampler diverges too far from the true values.</p></li>
<li><p>I have flags for each parameter in my iterative sampler that determines whether I am drawing that parameter in the iterative sampler. For example, if a flag 'gen_param1' is set to true then I draw 'param1' from its full conditional in the iterative sampler. If this is set to false then 'param1' is set to its true value.</p></li>
</ol>

<p>Once I finish writing up the sampler, I test the program using the following recipe:</p>

<ul>
<li>Set the generate flag for one parameter to true and everything else to false and assess convergence with respect to true value.</li>
<li>Set the generate flag for another parameter in conjunction with the first one and again assess convergence.</li>
</ul>

<p>The above steps have been incredibly helpful to me.</p>
",NULL,"2012-03-12 11:16:41","Is there a standard technique to debug MCMC programs?",<mcmc><code>,3,0,5,NULL,"2010-07-24 01:09:38",NULL,NULL,NULL,user28,user28
525,2,NULL,"2010-07-22 18:33:47",9,NULL,"<p>Standard programming practice:
-when debugging run the simulation with fixed sources of randomness (i.e. same seed) so that any changes are due to code changes and not different random numbers
-try your code on a model (or several models) where the answer IS known
-adopt good programming habits so that you introduce fewer bugs 
-think very hard &amp; long about the answers you do get, whether they make sense, etc.</p>

<p>I wish you good luck, and plenty of coffee!</p>
",247,"2010-07-22 18:33:47",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,524,NULL,NULL,NULL
526,1,585,"2010-07-22 19:17:41",7,466,"<p>As you know, there are two popular types of cross-validation, K-fold and random subsampling (as described in <a href="http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">Wikipedia</a>). Nevertheless, I know that some researchers are making and publishing papers where something that is described as a K-fold CV is indeed a random subsampling one, so in practice you never know what is really in the article you're reading.<br>
Usually of course the difference is unnoticeable, and so goes my question -- can you think of an example when the result of one type is significantly different from another? </p>
",88,"2010-07-24 22:54:13","Does the cross validation implementation influence its results?",<machine-learning><cross-validation>,2,0,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
527,1,2834,"2010-07-22 21:15:52",6,2504,"<p>I have two different analytical methods that can measure the concentration of a particular molecule in a matrix (for instance measure the amount of salt in water)</p>

<p>The two methods are different, and each has it's own error.  What ways exist to show the two methods are equivalent (or not).</p>

<p>I'm thinking that plotting the results from a number of samples measured by both methods on a scatter graph is a good first step, but are there any good statistical methods ?</p>
",114,"2012-03-28 14:15:49","What ways are there to show two analytical methods are equivalent?",<hypothesis-testing><measurement-error><method-comparison>,6,2,2,930,"2012-03-28 11:56:56",NULL,NULL,NULL,NULL,NULL
528,2,NULL,"2010-07-22 22:33:37",2,NULL,"<p>Teaching "Correlation does not mean causation" doesn't really help anyone because at the end of the day all deductive arguments are based in part on correlation.</p>

<p>Human are very bad at learning not to do something. </p>

<p>The goal should rather be constructive: Always think about alternatives to your starting assumptions that might produce the same data.</p>
",3807,"2010-07-22 22:33:37",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-08-16 13:01:42",36,NULL,NULL,NULL
529,2,NULL,"2010-07-22 22:38:05",1,NULL,"<p>Your use of the phrase 'analytical methods' is a bit confusing to me. I am going to assume that by 'analytical methods' you mean some specific model/estimation strategy. </p>

<p>Broadly, speaking there are two types of metrics you could use to choose between estimators.</p>

<p><strong>In-sample Metrics</strong></p>

<ul>
<li>Likelihood ratio / Wald test / Score test</li>
<li>R<sup>2</sup></li>
<li>In-sample Hit Rates (Percentage of correct predictions for sample data)</li>
<li>(Lots of other metrics depending on model / estimation context)</li>
</ul>

<p><strong>Out-of-sample Metrics</strong></p>

<ul>
<li>Out-of-sample Hit Rates (Percentage of correct predictions for out-of-sample data)</li>
</ul>

<p>If the estimates are equivalent they would perform equally well on these metrics. You could also see if the estimates are not statistically different from one another (like the two-sample test of equality of means) but the methodology for this would depend on model and method specifics.</p>
",NULL,"2010-07-22 22:38:05",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,527,NULL,user28,NULL
530,2,NULL,"2010-07-22 23:52:24",8,NULL,"<p>It's very common in forecasting to aggregate data in order to increase the signal/noise ratio.  There are several papers on the effect of temporal aggregation on forecast accuracy in economics, for example. What you're probably seeing in the daily data is a weak signal that is being swamped by noise, whereas the weekly and monthly data are showing a stronger signal that is more visible.</p>

<p>Whether you want to use temporal aggregation depends entirely on what your purpose is. If you need forecasts of daily incidents, then aggregation isn't going to be much use. If you are interested in exploring the effects of several covariates on the frequency of incidence, and all your data are available on a daily basis, then I would probably use the daily data as it will give a larger sample size and probably enable you to detect the effects more easily.</p>

<p>Since you are using the forecast package, presumably you are interested in time series forecasting. So do you need daily forecasts, weekly forecasts or monthly forecasts? The answer will determine whether aggregation is appropriate for you.</p>
",159,"2010-07-22 23:52:24",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,481,NULL,NULL,NULL
531,2,NULL,"2010-07-23 00:04:48",4,NULL,"<p>I don't think it is possible to do Bonferoni or similar corrections to adjust for variable selection in regression because all the tests and steps involved in model selection are not independent.</p>

<p>One approach is to formulate the model using one set of data, and do inference on a different set of data. This is done in forecasting all the time where we have a training set and a test set. It is not very common in other fields, probably because data are so precious that we want to use every single observation for model selection and for inference. However, as you note in your question, the downside is that the inference is actually misleading.</p>

<p>There are many situations where a theory-based approach is impossible as there is no well-developed theory. In fact, I think this is much more common than the cases where theory suggests a model.</p>
",159,"2010-07-23 00:04:48",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,499,NULL,NULL,NULL
532,2,NULL,"2010-07-23 01:44:22",12,NULL,"<p>I think Robin Girard's answer would work pretty well for 3 and possibly 4 dimensions, but the curse of dimensionality would prevent it working beyond that. However, his suggestion led me to a related approach which is to apply the cross-validated kernel density estimate to the first three principal component scores. Then a very high-dimensional data set can still be handled ok.</p>

<p>In summary, for i=1 to n</p>

<ol>
<li>Compute a density estimate of the first three principal component scores obtained from the data set without Xi. </li>
<li>Calculate the likelihood of Xi for the density estimated in step 1.
call it Li. </li>
</ol>

<p>end for</p>

<p>Sort the Li (for i=1,..,n) and the outliers are those with likelihood below some threshold. I'm not sure what would be a good threshold -- I'll leave that for whoever writes the paper on this! One possibility is to do a boxplot of the log(Li) values and see what outliers are detected at the negative end.</p>
",159,"2010-07-23 01:44:22",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,213,NULL,NULL,NULL
533,2,NULL,"2010-07-23 01:48:56",7,NULL,"<p>Along the lines of the mean as balance point, I like this view of the median as a balance point:</p>

<ul>
<li><a href="http://statpics.blogspot.com/2009/08/pearl-balanced-median-necklace.html">A Pearl: a Balanced Median Necklace</a></li>
</ul>
",251,"2010-07-23 01:48:56",NULL,NULL,NULL,1,NULL,NULL,NULL,"2012-08-21 15:17:25",155,NULL,NULL,NULL
534,1,538,"2010-07-23 01:56:02",44,11149,"<p>We all know the mantra "correlation does not imply causation" which is drummed into all first year statistics students. There are some nice examples <a href="http://stats.stackexchange.com/questions/36/correlation-does-not-mean-causation">here</a> to illustrate the idea.</p>

<p>But sometimes correlation <em>does</em> imply causation. The following example is taking from this <a href="http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation#Determining_causation">Wikipedia page</a></p>

<blockquote>
  <p>For example, one could run an experiment on identical twins who were known to consistently get the same grades on their tests. One twin is sent to study for six hours while the other is sent to the amusement park. If their test scores suddenly diverged by a large degree, this would be strong evidence that studying (or going to the amusement park) had a causal effect on test scores. In this case, correlation between studying and test scores would almost certainly imply causation.</p>
</blockquote>

<p>Are there other situations where correlation implies causation?</p>
",159,"2013-12-24 23:44:38","Under what conditions does correlation imply causation?",<correlation><causal-inference>,14,6,26,159,"2010-08-31 23:50:10",NULL,NULL,NULL,NULL,NULL
535,2,NULL,"2010-07-23 02:00:12",2,NULL,"<p>Almost surely in a well designed experiment.  (Designed, of course, to elicit such a <a href="http://18th.eserver.org/hume-enquiry.html#7" rel="nofollow">connexion</a>.)</p>
",251,"2010-07-23 02:06:23",NULL,NULL,NULL,0,NULL,251,"2010-07-23 02:06:23",NULL,534,NULL,NULL,NULL
537,2,NULL,"2010-07-23 02:42:39",11,NULL,"<p>Your example is that of a <a href="http://en.wikipedia.org/wiki/Controlled_experiment">controlled experiment</a>. The only other context that I know of where a correlation can imply causation is that of a <a href="http://en.wikipedia.org/wiki/Natural_experiment">natural experiment</a>. </p>

<p>Basically, a natural experiment takes advantage of an assignment of some respondents to a treatment that happens naturally in the real world. Since assignment of respondents to treatment and control groups is not controlled by the experimenter the extent to which correlation would imply causation is perhaps weaker to some extent.</p>

<p>See the wiki links for more information controlled / natural experiments.</p>
",NULL,"2010-07-23 02:42:39",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,534,NULL,user28,NULL
538,2,NULL,"2010-07-23 04:49:41",23,NULL,"<p>Correlation is not sufficient for causation. One can get around the Wikipedia example by imagining that those twins always cheated in their tests by having a device that gives them the answers. The twin that goes to the amusement park loses the device, hence the low grade.</p>

<p>A good way to get this stuff straight is to think of the structure of Bayesian network that may be generating the measured quantities, as done by Pearl in his book <a href="http://rads.stackoverflow.com/amzn/click/052189560X" rel="nofollow"><em>Causality</em></a>. His basic point is to look for hidden variables. If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation. Expose all hidden variables and you have causation.</p>
",260,"2013-12-24 23:44:38",NULL,NULL,NULL,5,NULL,556,"2013-12-24 23:44:38",NULL,534,NULL,NULL,NULL
539,1,4183,"2010-07-23 06:17:10",34,9369,"<p>In answering <a href="http://stats.stackexchange.com/questions/206/discrete-and-continuous" rel="nofollow">this question on discrete and continuous data</a> I glibly asserted that it rarely makes sense to treat categorical data as continuous.</p>

<p>On the face of it that seems self-evident, but intuition is often a poor guide for statistics, or at least mine is. So now I'm wondering: is it true? Or are there established analyses for which a transform from categorical data to some continuum is actually useful? Would it make a difference if the data were ordinal?</p>
",174,"2013-11-02 11:31:09","Does it ever make sense to treat categorical data as continuous?",<categorical-data><continuous-data><data-transformation>,8,2,15,NULL,NULL,NULL,NULL,NULL,NULL,NULL
540,2,NULL,"2010-07-23 06:17:56",9,NULL,"<p>In my opinion the APA Statistical Task force summarised it quite well</p>

<blockquote>
  <p>''Inferring causality from nonrandomized
  designs is a risky enterprise.
  Researchers using nonrandomized
  designs have an extra obligation to
  explain the logic behind covariates
  included in their designs and to alert
  the reader to plausible rival
  hypotheses that might explain their
  results. Even in randomized
  experiments, attributing causal
  effects to any one aspect of the
  treatment condition requires support
  from additional experimentation.''
  - <a href="http://www.loyola.edu/library/ref/articles/Wilkinson.pdf">APA Task Force</a></p>
</blockquote>
",183,"2010-07-23 06:17:56",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,534,NULL,NULL,NULL
541,2,NULL,"2010-07-23 06:22:04",14,NULL,"<p>If there are only two categories, then transforming them to (0,1) makes sense. In fact, this is commonly done where the resulting dummy variable is used in regression models.</p>

<p>If there are more than two categories, then I think it only makes sense if the data are ordinal, and then only in very specific circumstances. For example, if I am doing regression and fit a nonparametric nonlinear function to the ordinal-cum-numeric variable, I think that is ok. But if I use linear regression, then I am making very strong assumptions about the relative difference between consecutive values of the ordinal variable, and I'm usually reluctant to do that.</p>
",159,"2010-07-23 06:22:04",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,539,NULL,NULL,NULL
542,2,NULL,"2010-07-23 06:49:38",7,NULL,"<p>In the social science context where I come from, the issue is whether you are interested in (a) prediction or (b) testing a focused research question.
If the purpose is prediction then data driven approaches are appropriate.
If the purpose is to examine a focused research question then it is important to consider which regression model specifically tests your question.</p>

<p>For example, if your task was to select a set of selection tests to predict job performance, the aim can in some sense be seen as one of maximising prediction of job performance.
Thus, data driven approaches would be useful.</p>

<p>In contrast if you wanted to understand the relative role of personality variables and ability variables in influencing performance, then a specific model comparison approach might be more appropriate. </p>

<p>Typically when exploring focussed research questions the aim is to elucidate something about the underlying causal processes that are operating as opposed to developing a model with optimal prediction. </p>

<p>When I'm in the process of developing models about process based on cross-sectional data I'd be wary about:
(a) including predictors that could theoretically be thought of as consequences of the outcome variable. E.g., a person's belief that they are a good performer is a good predictor of job performance, but it is likely that this is at least partially caused by the fact that they have observed their own performance.
(b) including a large number of predictors that are all reflective of the same underlying phenomena. E.g., including 20 items all measuring satisfaction with life in different ways.</p>

<p>Thus, focused research questions rely a lot more on domain specific knowledge. 
This probably goes some way to explaining why data driven approaches are less often used in the social sciences.</p>
",183,"2010-07-23 06:49:38",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,499,NULL,NULL,NULL
543,2,NULL,"2010-07-23 07:13:03",5,NULL,"<p>In addition to what has already been said above about summated scales, I'd also mention that the issue can change when analysing data at the group-level. For example, if you were examining</p>

<ul>
<li>life satisfaction of states or countries,</li>
<li>job satisfaction of organisations or departments,</li>
<li>student satisfaction in subjects.</li>
</ul>

<p>In all these cases each aggregate measure (perhaps the mean) is based on many individual responses (e.g., n=50, 100, 1000, etc.). In these cases the original Likert item begins to take on properties that resemble an interval scale at the aggregate level.</p>
",183,"2010-07-23 07:13:03",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,10,NULL,NULL,NULL
544,2,NULL,"2010-07-23 07:27:31",13,NULL,"<p>It is common practice to treat ordered categorical variables with many categories as continuous. Examples of this:</p>

<ul>
<li>Number of items correct on a 100 item test</li>
<li>A summated psychological scale (e.g., that is the mean of 10 items each on a five point scale)</li>
</ul>

<p>And by "treating as continuous" I mean including the variable in a model that assumes a continuous random variable (e.g., as a dependent variable in a linear regression). I suppose the issue is how many scale points are required for this to be a reasonable simplifying assumption.</p>

<p>A few other thoughts:</p>

<ul>
<li><strong>Polychoric correlations</strong> attempt to model the relationship between two ordinal variables in terms of assumed latent continuous variables.</li>
<li><strong>Optimal scaling</strong> allows you to develop models where the scaling of a categorical variable is developed in a data driven way whilst respecting whatever scale constraints you impose (e.g., ordinality). For a good introduction see De Leeuw and Mair (2009)</li>
</ul>

<h3>References</h3>

<ul>
<li>De Leeuw, J., &amp; Mair, P. (2009). Gifi methods for optimal scaling in R: The package homals. Journal of Statistical Software, forthcoming, 1-30. <a href="http://www.jstatsoft.org/v31/i04/paper" rel="nofollow">PDF</a></li>
</ul>
",183,"2013-08-13 03:53:08",NULL,NULL,NULL,0,NULL,183,"2013-08-13 03:53:08",NULL,539,NULL,NULL,NULL
545,2,NULL,"2010-07-23 08:15:23",1,NULL,"<p>The problem (dilemma) you face appears to be the one of selecting an optimal (or otherwise good) sampling interval for revising your forecasts. To start with, see <a href="http://books.google.com.au/books?id=XXFNW_QaJYgC&amp;pg=PA42" rel="nofollow" title="chapter 3">link text</a> of Brown's famous book, which would also qualify as a good reference. It all boils down to "balancing the risk of not noticing a change quickly against the inherent variability of the data and the cost of revising plans frequently". If you are not prepared to revise your forecast (and the decisions that motivated it) daily, you don't really need to use the (noisiest) daily data. An important point, often lost in the contemporary forecasting literature, is that forecasts are only necessary to assist with making a decision (unless one also knows how to derive fun from them).</p>
",273,"2010-07-23 08:15:23",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,481,NULL,NULL,NULL
546,2,NULL,"2010-07-23 08:18:52",29,NULL,"<p>Another one from <a href="http://xkcd.com/388/">xkcd</a>:</p>

<p><img src="http://imgs.xkcd.com/comics/fuck_grapefruit.png" alt="Coconuts are so far down to the left they couldn't be fit on the chart.  Ever spent half an hour trying to open a coconut with a rock?  Down with coconuts."></p>
",183,"2010-08-11 09:20:21",NULL,NULL,NULL,5,NULL,509,"2010-08-11 09:20:21","2010-07-23 08:18:52",423,NULL,NULL,NULL
547,2,NULL,"2010-07-23 08:33:04",8,NULL,"<p>There is also a problem with the opposite case, when lack of correlation is used as a proof for the lack of causation. This problem is nonlinearity; when looking at correlation people usually check Pearson, which is only a tip of an iceberg. </p>
",88,"2010-07-23 08:33:04",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,534,NULL,NULL,NULL
548,1,559,"2010-07-23 10:51:52",5,571,"<p>In an answer to <a href="http://stats.stackexchange.com/questions/539/does-it-ever-make-sense-to-treat-categorical-data-as-continuous" rel="nofollow">this question about treating categorical data as continuous</a>, optimal scaling was mentioned. How does this method work and how is it applied?</p>
",266,"2010-07-23 16:30:41","How can I use optimal scaling to scale an ordinal categorical variable?",<categorical-data><data-transformation><optimal-scaling>,1,0,NULL,190,"2010-07-23 11:06:32",NULL,NULL,NULL,NULL,NULL
549,2,NULL,"2010-07-23 13:28:55",3,NULL,"<p>In an analysis of ranking by frequency, as with a Pareto chart and associated values (eg how many categories make up the top 80% of product faults)</p>
",270,"2010-07-23 13:28:55",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,539,NULL,NULL,NULL
550,2,NULL,"2010-07-23 13:58:07",11,NULL,"<p>I have used the drunkard's walk before for random walk, and the drunk and her dog for cointegration; they're very helpful (partially because they're amusing).</p>

<p>One of my favorite common examples is the <a href="http://mathworld.wolfram.com/BirthdayProblem.html">Birthday Paradox</a> (<a href="http://en.wikipedia.org/wiki/Birthday_problem">wikipedia entry</a>), which illustrates some important concepts of probability.  You can simulate this with a room full of people.</p>

<p>Incidentally, I strongly recommend Andrew Gelman's <a href="http://www.stat.columbia.edu/~gelman/bag-of-tricks/"><strong>"Teaching Statistics: A Bag of Tricks"</strong></a> for some examples of creative ways to teach statistical concepts (see the <a href="http://www.stat.columbia.edu/~gelman/bag-of-tricks/contents.pdf">table of contents</a>).  Also look at his paper about the course that he teaches on teaching statistics: <a href="http://www.stat.columbia.edu/~gelman/research/published/teachcourse3.pdf">"A Course on Teaching Statistics at the University Level"</a>.  And on <a href="http://www.stat.columbia.edu/~gelman/research/published/teachingbayes.pdf">"Teaching Bayes to Graduate Students in Political Science, Sociology,
Public Health, Education, Economics, ..."</a>.</p>

<p>For describing Bayesian methods, using an unfair coin and flipping it multiple times is a pretty common/effective approach.</p>
",5,"2010-07-23 16:25:03",NULL,NULL,NULL,0,NULL,5,"2010-07-23 16:25:03","2012-08-21 15:17:25",155,NULL,NULL,NULL
551,2,NULL,"2010-07-23 14:43:14",36,NULL,"<p>I always tell students there are three reasons to transform a variable by taking the natural logarithm. The reason for logging the variable will determine whether you want to log the independent variable(s), dependent or both. To be clear throughout I'm talking about taking the natural logarithm. </p>

<p>Firstly, to improve model fit as other posters have noted. For instance if your residuals aren't normally distributed then taking the logarithm of a skewed variable may improve the fit by altering the scale and making the variable more "normally" distributed. For instance, earnings is truncated at zero and often exhibits positive skew. If the variable has negative skew you could firstly invert the variable before taking the logarithm. I'm thinking here particularly of Likert scales that are inputted as continuous variables. While this usually applies to the dependent variable you occasionally have problems with the residuals (e.g. heteroscedasticity) caused by an independent variable which can be sometimes corrected by taking the logarithm of that variable. For example when running a model that explained lecturer evaluations on a set of lecturer and class covariates the variable "class size" (i.e. the number of students in the lecture) had outliers which induced heteroscedasticity because the variance in the lecturer evaluations was smaller in larger cohorts than smaller cohorts. Logging the student variable would help, although in this example either calculating Robust Standard Errors or using Weighted Least Squares may make interpretation easier.</p>

<p>The second reason for logging one or more variables in the model is for interpretation. I call this convenience reason. If you log both your dependent (Y) and independent (X) variable(s) your regression coefficients ($\\beta$) will be elasticities and interpretation would go as follows: a 1% increase in X would lead to a <em>ceteris paribus</em> $\\beta$% increase in Y (on average). Logging only one side of the regression "equation" would lead to alternative interpretations as outlined below:</p>

<p>Y and X -- a one unit increase in X would lead to a $\\beta$ increase/decrease in Y</p>

<p>Log Y and Log X -- a 1% increase in X would lead to a $\\beta$% increase/decrease in Y </p>

<p>Log Y and X -- a one unit increase in X would lead to a $\\beta*100$ % increase/decrease in Y</p>

<p>Y and Log X -- a 1% increase in X would lead to a $\\beta/100$ increase/decrease in Y</p>

<p>And finally there could be a theoretical reason for doing so. For example some models that we would like to estimate are multiplicative and therefore nonlinear. Taking logarithms allows these models to be estimated by linear regression. Good examples of this include the Cobb-Douglas production function in economics and the Mincer Equation in education. The Cobb-Douglas production function explains how inputs are converted into outputs:</p>

<p>$$Y = A L^\\alpha K^\\beta $$</p>

<p>where</p>

<p>$Y$ is the total production or output of some entity e.g. firm, farm, etc.</p>

<p>$A$ is the total factor productivity (the change in output not caused by the inputs e.g. by technology change or weather)</p>

<p>$L$ is the labour input</p>

<p>$K$ is the capital input</p>

<p>$\\alpha$ &amp; $\\beta$ are output elasticities.</p>

<p>Taking logarithms of this makes the function easy to estimate using OLS linear regression as such:</p>

<p>$$\\log(Y) = A + \\alpha\\log(L) + \\beta\\log(K)$$</p>
",215,"2011-10-17 14:06:30",NULL,NULL,NULL,4,NULL,919,"2011-10-17 14:06:30",NULL,298,NULL,NULL,NULL
552,2,NULL,"2010-07-23 14:53:50",10,NULL,"<p>AIC = -2Ln(L)+ 2k </p>

<p>where L is the maximised value of Likelihood function for that model and k is the number of parameters in the model.</p>

<p>In your example -2Ln(L)+ 2k &lt;0 means that the log-likelihood at the maximum was > 0
which means that the likelihood at the maximum was > 1.</p>

<p>There is no problem with a positive log-likelihood. It is a common misconception that the log-likelihood must be negative. If the likelihood is derived from a probability density it can quite reasonably exceed 1 which means that log-likelihood is positive, hence the deviance and the AIC are negative. This is what occurred in your model.</p>

<p>If you believe that comparing AICs is a good way to choose a model then it would still be the case that the (algebraically) lower AIC is preferred not the one with the lowest absolute AIC value. To reiterate you want the most negative number in your example.</p>
",215,"2010-07-23 14:53:50",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,486,NULL,NULL,NULL
553,2,NULL,"2010-07-23 15:02:40",6,NULL,"<p>The trivial answer is that more data are always preferred to less data. </p>

<p>The problem of small sample size is clear. In linear regression (OLS) technically you can fit a model such as OLS where n = k+1 but you will get rubbish out of it i.e. very large standard errors. There is a great paper by Arthur Goldberger called Micronumerocity on this topic which is summarized in chapter 23 of his book <em>A Course in Econometrics</em>.</p>

<p>A common heuristic is that you should have 20 observations for every parameter you want to estimate. It is always a trade off between the size of your standard errors (and therefore significance testing) and the size of your sample. This is one reason some of us hate significance testing as you can get an incredibly small (relative) standard error with an enormous sample and therefore find pointless statistical significance on naive tests such as whether a regression coefficient is zero. </p>

<p>While sample size is important the quality of your sample is more important e.g. whether the sample is generalisable to the population, is it a Simple Random Sample or some other appropriate sampling methodology (and have this been accounted for during analysis), is there measurement error, response bias, selection bias, etc.</p>
",215,"2010-07-23 15:20:15",NULL,NULL,NULL,0,NULL,215,"2010-07-23 15:20:15",NULL,276,NULL,NULL,NULL
554,2,NULL,"2010-07-23 15:09:02",10,NULL,"<p>I like to demonstrate sampling variation and essentially the Central Limit Theorem through an "in-class" exercise. Everybody in the class of say 100 students writes their age on a piece of paper. All pieces of paper are the same size and folded in the same fashion after I've calculated the average. This is the population and I calculate the average age. Then each student randomly selects 10 pieces of paper, writes down the ages and returns them to the bag. (S)he calculates the mean and passes the bag along to the next student. Eventually we have 100 samples of 10 students each estimating the population mean which we can describe through a histogram and some descriptive statistics. </p>

<p>We then repeat the demonstration this time using a set of 100 "opinions" that replicate some Yes/No question from recent polls e.g. If the (British General) election were called tomorrow would you consider voting for the British National Party. Students them sample 10 of these opinions.</p>

<p>At the end we've demonstrated sampling variation, the Central Limit Theorem, etc with both continuous and binary data.</p>
",215,"2010-07-23 15:09:02",NULL,NULL,NULL,0,NULL,NULL,NULL,"2012-08-21 15:17:25",155,NULL,NULL,NULL
555,1,557,"2010-07-23 15:17:56",46,6287,"<p>ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression. </p>

<p>In light of their equivalence, is there any reason why ANOVA is used instead of linear regression? </p>

<p>Note: I am particularly interested in hearing about <strong>technical</strong> reasons for the use of ANOVA instead of linear regression. </p>

<p><strong>Edit</strong></p>

<p>Here is one example using one-way ANOVA. Suppose, you want to know if the average height of male and females is the same. To test for your hypothesis you would collect data from a random   sample of male and females (say 30 each) and perform the ANOVA analysis (i.e., sum of squares for gender and error) to decide whether an effect exists.</p>

<p>You could also use linear regression to test for this as follows:</p>

<p>Define:  $\\text{Gender} = 1$ if respondent is a male and $0$ otherwise.
$$
\\text{Height} = \\text{Intercept} + \\beta * \\text{Gender} + \\text{error}
$$
where: $\\text{error}\\sim\\mathcal N(0,\\sigma^2)$</p>

<p>Then a test of whether $\\beta = 0$ is a an equivalent test for your hypothesis.</p>
",NULL,"2013-10-20 07:55:31","Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?",<regression><anova>,5,3,23,7290,"2012-08-18 16:21:38",NULL,NULL,NULL,user28,user28
556,2,NULL,"2010-07-23 15:18:34",-1,NULL,"<p>Discrete data can take on only integer values whereas continuous data can take on any value. For instance the number of cancer patients treated by a hospital each year is discrete but your weight is continuous. Some data are continuous but measured in a discrete way e.g. your age. It is common to report your age as say, 31.</p>
",215,"2010-07-23 15:18:34",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,206,NULL,NULL,NULL
557,2,NULL,"2010-07-23 15:35:55",33,NULL,"<p>As an economist, the analysis of variance (ANOVA) is taught and usually understood in relation to linear regression (e.g. in Arthur Goldberger's <em>A Course in Econometrics</em>). Economists/Econometricians typically view ANOVA as uninteresting and prefer to move straight to regression models. From the perspective of linear (or even generalised linear) models, ANOVA assigns coefficients into batches, with each batch corresponding to a "source of variation" in ANOVA terminology.</p>

<p>Generally you can replicate the inferences you would obtain from ANOVA using regression but not always OLS regression. Multilevel models are needed for analysing hierarchical data structures such as "split-plot designs," where between-group effects are compared to group-level errors, and within-group effects are compared to data-level errors. <a href="http://dx.doi.org/10.1214%2F009053604000001048">Gelman's paper</a> &#91;1&#93; goes into great detail about this problem and effectively argues that ANOVA is an important statistical tool that should still be taught for it's own sake.</p>

<p>In particular Gelman argues that ANOVA is a way of understanding and structuring multilevel models. Therefore ANOVA is not an alternative to regression but as a tool for summarizing complex high-dimensional inferences and for exploratory data analysis. </p>

<p>Gelman is a well-respected statistician and some credence should be given to his view. However, almost all of the empirical work that I do would be equally well served by linear regression and so I firmly fall into the camp of viewing it as a little bit pointless. Some disciplines with complex study designs (e.g. psychology) may find ANOVA useful.</p>

<p>&#91;1&#93; Gelman, A. (2005). Analysis of variance: why it is more important than ever (with discussion). <em>Annals of Statistics</em> 33, 1–53. <a href="http://dx.doi.org/10.1214%2F009053604000001048">doi:10.1214/009053604000001048</a></p>
",215,"2013-09-30 15:35:44",NULL,NULL,NULL,4,NULL,30872,"2013-09-30 15:35:44",NULL,555,NULL,NULL,NULL
558,1,NULL,"2010-07-23 16:15:33",11,339,"<p>I am trying to get a global perspective on some of the essential ideas in machine learning, and I was wondering if there is a comprehensive treatment of the different notions of loss (squared, log, hinge, proxy, etc.).  I was thinking something along the lines of a more comprehensive, formal presentation of John Langford’s excellent post on <a href="http://hunch.net/?p=269">Loss Function Semantics</a>.</p>
",39,"2012-06-03 07:06:48","Comprehensive overview of loss functions?",<loss-functions>,4,0,3,NULL,NULL,NULL,NULL,NULL,NULL,NULL
559,2,NULL,"2010-07-23 16:30:41",5,NULL,"<p>The general idea is that you should scale the categorical variable in such way that the resulting continuous variables will be just the most useful. So, it is always coupled with some regression or learning procedure and so the fitting of the model is accompanied by optimization (or trying various possibilities) of ordinal variables scaling.<br>
For some more practical issues, consult the docks of R <a href="http://cran.r-project.org/web/packages/aspect/index.html" rel="nofollow">aspect</a> and <a href="http://cran.r-project.org/web/packages/homals/index.html" rel="nofollow">homals</a> packages. </p>
",88,"2010-07-23 16:30:41",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,548,NULL,NULL,NULL
560,2,NULL,"2010-07-23 16:36:50",9,NULL,"<p>I use the Gelman-Rubin convergence diagnostic as well.  A potential problem with Gelman-Rubin is that it may mis-diagnose convergence if the shrink factor happens to be close to 1 by chance, in which case you can use a Gelman-Rubin-Brooks plot.  See the <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.2165">"General Methods for Monitoring Convergence of Iterative Simulations"</a> paper for details.  This is supported in <a href="http://cran.r-project.org/web/packages/coda/index.html">the <strong>coda</strong> package</a> in R (for "Output analysis and diagnostics for Markov Chain Monte Carlo simulations").  <code>coda</code> also includes other functions (such as the Geweke’s convergence diagnostic).  </p>

<p>You can also have a look at <a href="http://www.jstatsoft.org/v21/i11/paper">"boa: An R Package for MCMC Output Convergence
Assessment and Posterior Inference"</a>. </p>
",5,"2010-07-23 16:36:50",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,507,NULL,NULL,NULL
561,2,NULL,"2010-07-23 16:49:47",12,NULL,"<p>At the heart of your question is the question "when is a relationship causal?" It doesn't just need to be correlation implying (or not) causation.</p>

<p>A good book on this topic is called <em>Mostly Harmless Econometrics</em> by Johua Angrist and Jorn-Steffen Pischke. They start from the experimental ideal where we are able to randomise the "treatment" under study in some fashion and then they move onto alternative methods for generating this randomisation in order to draw causal influences. This begins with the study of so called natural experiments. </p>

<p>One of the first examples of a natural experiment being used to identify causal relationships is Angrist's 1989 paper on <a href="http://www.irs.princeton.edu/pubs/pdfs/251.pdf">"Lifetime Earnings and the Vietnam Era Draft Lottery."</a> This paper attempts to estimate the effect of military service on lifetime earnings. A key problem with estimating any causal effect is that certain types of people may be more likely to enlist, which may bias any measurement of the relationship. Angrist uses the natural experiment created by the Vietnam draft lottery to effectively "randomly assign" the treatment "military service" to a group of men. </p>

<p>So when do we have a causality? Under experimental conditions. When do we get close? Under natural experiments. There are also other techniques that get us close to "causality" i.e. they are much better than simply using statistical control. They include regression discontinuity, difference-in-differences, etc.</p>
",215,"2010-07-23 16:49:47",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,534,NULL,NULL,NULL
562,1,566,"2010-07-23 16:51:11",10,1005,"<p>This is a fairly general question:</p>

<p>I have typically found that using multiple different models outperforms one model when trying to predict a time series out of sample.  Are there any good papers that demonstrate that the combination of models will outperform a single model?  Are there any best-practices around combining multiple models?</p>

<p>Some references:</p>

<ul>
<li>Hui Zoua, Yuhong Yang <a href="http://www.stat.umn.edu/~hzou/Papers/after.pdf" rel="nofollow">"Combining time series models for forecasting"</a> International Journal of Forecasting 20 (2004) 69– 84</li>
</ul>
",5,"2012-03-24 02:28:14","When to use multiple models for prediction?",<time-series><modeling><model-comparison>,5,1,14,9007,"2012-03-23 03:44:56",NULL,NULL,NULL,NULL,NULL
563,1,NULL,"2010-07-23 16:53:20",8,1855,"<p>Instrumental variables are becoming increasingly common in applied economics and statistics. For the uninitiated, can we have some non-technical answers to the following questions:</p>

<ol>
<li>What is an instrumental variable?</li>
<li>When would one want to employ an instrumental variable?</li>
<li>How does one find or choose an instrumental variable?</li>
</ol>
",215,"2014-02-07 23:26:06","What is an instrumental variable?",<regression><econometrics><instrumental-variables>,3,5,4,8,"2010-11-23 17:06:45",NULL,NULL,NULL,NULL,NULL
564,1,NULL,"2010-07-23 16:57:50",5,1498,"<p>Difference in differences has long been popular as a non-experimental tool, especially in economics. Can somebody please provide a clear and non-technical answer to the following questions about difference-in-differences.</p>

<p>What is a difference-in-difference estimator?
Why is a difference-in-difference estimator any use?
Can we actually trust difference-in-difference estimates?</p>
",215,"2011-01-07 11:51:22","What is difference-in-differences?",<regression><econometrics>,1,2,NULL,8,"2010-11-23 17:06:13",NULL,NULL,NULL,NULL,NULL
565,2,NULL,"2010-07-23 17:02:28",2,NULL,"<p>The most spectacular example is the <a href="http://www.netflixprize.com/" rel="nofollow">Netflix challenge</a>, which made really boosted blending popularity.</p>
",88,"2010-07-23 17:02:28",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,562,NULL,NULL,NULL
566,2,NULL,"2010-07-23 17:05:52",7,NULL,"<p>Sometimes this kind of models are called an ensemble. For example <a href="http://wapedia.mobi/en/Machine_learning_ensemble">this page</a> gives a nice overview how it works. Also the references mentioned there are very useful.</p>
",190,"2010-07-23 17:05:52",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,562,NULL,NULL,NULL
567,2,NULL,"2010-07-23 17:11:35",11,NULL,"<p>I've often found the Engineering Statistics Handbook useful. It can be found <a href="http://www.itl.nist.gov/div898/handbook/">here</a>.  </p>

<p>Although I've never read it myself, I hear <a href="http://www.lulu.com/items/volume_68/8123000/8123594/3/print/IPSUR.pdf">Introduction to Probability and Statistics Using R</a> is very good. It's a full ~400 page ebook (also available as an actual book). As a bonus, it also teaches you R, which of course you want to learn anyways.</p>
",92,"2010-07-30 19:07:26",NULL,NULL,NULL,1,NULL,92,"2010-07-30 19:07:26","2010-07-23 17:11:35",170,NULL,NULL,NULL
568,2,NULL,"2010-07-23 17:29:50",2,NULL,"<p>In statistics you can never say something is absolutely certain, so statisticians use another approach to gauge whether a hypothesis is true or not. They try to reject all the other hypotheses that are not supported by the data. </p>

<p>To do this, statistical tests have a null hypothesis and an alternate hypothesis. The p-value reported from a statistical test is the likelihood of the result given that the null hypothesis was correct. That's why we want small p-values. The smaller they are, the less likely the result would be if the null hypothesis was correct. If the p-value is small enough (ie,it is very unlikely for the result to have occurred if the null hypothesis was correct), then the null hypothesis is rejected.   </p>

<p>In this fashion, null hypotheses can be formulated and subsequently rejected. If the null hypothesis is rejected, you accept the alternate hypothesis as the best explanation. Just remember though that the alternate hypothesis is never certain, since the null hypothesis could have, by chance, generated the results.</p>
",92,"2010-07-23 17:29:50",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,31,NULL,NULL,NULL
569,2,NULL,"2010-07-23 17:32:44",3,NULL,"<p>Following up on Peter's response on ensemble methods:</p>

<ul>
<li>This is covered in <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/" rel="nofollow">"The Elements of Statistical Learning"</a> (see page 288, for example).</li>
<li>Witten and Frank <a href="http://www.cs.waikato.ac.nz/~ml/weka/book.html" rel="nofollow">"Data Mining: Practical Machine Learning Tools and Techniques"</a> covers this in section 7.5, including a discussion of Bagging, Randomization, Boosting, Additive regression, Additive logistic regression, Option trees, Logistic model trees, and Stacking.</li>
<li>This is covered in Chapter 14 of Christopher M. Bishop <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" rel="nofollow">"Pattern Recognition and Machine Learning"</a>, including Bayesian Model Averaging, Boosting, Committees, Tree-based Models, and Conditional Mixture Models.</li>
</ul>
",5,"2010-07-23 17:32:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,562,NULL,NULL,NULL
570,1,3798,"2010-07-23 17:59:27",12,753,"<p>I'm curious if there are graphical techniques particular, or more applicable, to structural equation modeling.  I guess this could fall into categories for exploratory tools for covariance analysis or graphical diagnostics for SEM model evaluation.  (I'm not really thinking of path/graph diagrams here.) </p>
",251,"2013-02-25 06:23:08","What graphical techniques are used in Structural Equation Modeling?",<sem><data-visualization>,4,2,6,251,"2010-07-27 00:49:21",NULL,NULL,NULL,NULL,NULL
571,2,NULL,"2010-07-23 18:29:41",6,NULL,"<p>One useful sufficient condition for some definitions of causation:</p>

<p>Causation can be claimed when one of the correlated variables can be controlled (we can directly set its value) and correlation is still present. </p>
",217,"2010-07-23 18:29:41",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,534,NULL,NULL,NULL
572,2,NULL,"2010-07-23 18:42:05",14,NULL,"<p>I think Graham's second paragraph gets at the heart of the matter.  I suspect it's not so much technical than historical, probably due to the influence of "<a href="http://en.wikipedia.org/wiki/Ronald_Fisher">Statistical Methods for Research Workers</a>", and the ease of teaching/applying the tool for non-statisticans in experimental analysis involving discrete factors, rather than delving into model building and associated tools.  In statistics, ANOVA is usually taught as a special case of regression.  (I think this is similar to why biostatistics is filled with a myriad of eponymous "tests" rather than emphasizing model building.)</p>
",251,"2010-07-23 18:54:03",NULL,NULL,NULL,0,NULL,251,"2010-07-23 18:54:03",NULL,555,NULL,NULL,NULL
573,1,NULL,"2010-07-23 19:45:45",10,1043,"<p>In "<a href="http://dx.doi.org/10.1145/1553374.1553453">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</a>" by Lee et. al.(<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;rep=rep1&amp;type=pdf">PDF</a>) Convolutional DBN's are proposed. Also the method is evaluated for image classification. This sounds logical, as there are natural local image features, like small corners and edges etc.</p>

<p>In "<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.380&amp;rep=rep1&amp;type=pdf">Unsupervised feature learning for audio classification using convolutional deep belief networks</a>" by Lee et. al. this method is applied for audio in different types of classifications. Speaker identification, gender indentification, phone classification and also some music genre / artist classification.</p>

<p>How can the convolutional part of this network be interpreted for audio, like it can be explained for images as edges?</p>
",190,"2013-03-08 14:33:23","How to understand a convolutional deep belief network for audio classification?",<classification><intuition><unsupervised-learning>,2,3,2,930,"2012-08-11 11:31:32",NULL,NULL,NULL,NULL,NULL
574,2,NULL,"2010-07-23 20:07:51",0,NULL,"<p>Assuming there is some training involved, you may use some kind of cross-validation, or bootstrap of a train set.<br>
If not, stick to the Srikant solution. I would do it even simpler, just assuming that the number of error is Poisson distributed.</p>
",88,"2010-07-23 20:07:51",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,212,NULL,NULL,NULL
575,1,762,"2010-07-23 20:14:05",14,6570,"<p>What is the preferred method for for conducting post-hocs for within subjects tests?  I've seen published work where Tukey's HSD is employed but a review of Keppel and Maxwell &amp; Delaney suggests that the likely violation of sphericity in these designs makes the error term incorrect and this approach problematic.  Maxwell &amp; Delaney provide an approach to the problem in their book, but I've never seen it done that way in any stats package.  Is the approach they offer appropriate?  Would a Bonferroni or Sidak correction on multiple paired sample t-tests be reasonable?  An acceptable answer will provide general R code which can conduct post-hocs on simple, multiple-way, and mixed designs as produced by the <code>ezANOVA</code> function in the <code>ez</code> package, and appropriate citations that are likely to pass muster with reviewers.</p>
",196,"2013-07-16 15:46:47","Post-hocs for within subjects tests?",<r><repeated-measures><multiple-comparisons><post-hoc><sphericity>,5,3,17,17230,"2013-07-16 15:46:47",NULL,NULL,NULL,NULL,NULL
576,2,NULL,"2010-07-23 20:42:43",3,NULL,"<p><a href="http://en.wikipedia.org/wiki/Difference_in_differences" rel="nofollow">Wikipedia has a decent entry on this subject</a>, but why not just use linear regression allowing for interactions between your independent variables of interest? This seems more interpretable to me. Then you might read up on <a href="http://books.google.com/books?id=fuq94a8C0ioC&amp;lpg=PP1&amp;dq=applied%20multiple%20regression%20correlation%20analysis%20for%20the%20behavioral%20sciences&amp;pg=PA271#v=onepage&amp;q=simple%20slopes&amp;f=false" rel="nofollow">analysis of simple slopes (in the Cohen et al book free on Google Books)</a> if your variables of interest are quantitative.</p>
",36,"2010-07-23 20:42:43",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,564,NULL,NULL,NULL
577,1,767,"2010-07-23 20:49:12",79,22354,"<p>The AIC and BIC are both methods of assessing model fit penalized for the number of estimated parameters.  As I understand it, BIC penalizes models more for free parameters than does AIC.  Beyond a preference based on the stringency of the criteria, are there any other reasons to prefer AIC over BIC or vice versa?</p>
",196,"2014-04-11 11:31:10","Is there any reason to prefer the AIC or BIC over the other?",<modeling><aic><cross-validation><bic><model-selection>,10,2,68,196,"2010-07-25 05:09:18",NULL,NULL,NULL,NULL,NULL
578,2,NULL,"2010-07-23 21:18:52",2,NULL,"<p>If you have no way of knowing the true concentration, the simplest approach would be a correlation.  A step beyond that might be to conduct a simple regression predicting the outcome on method 2 using method 1 (or vice versa). If the methods are identical the intercept should be 0; if the intercept is greater or less than 0 it would indicate the bias of one method relative to another.  The unstandardized slope should be near 1 if the methods on average produce results that are identical (after controlling for an upward or downward bias in the intercept).  The error in the unstandardized slope might serve as an index of the extent to which the two methods agree.  </p>

<p>It seems to me that the difficulty with statistical methods here that you are seeking to affirm what is typically posed as a null hypothesis, that is, that there are no differences between the methods.  This isn't a death blow for using statistical methods so long as you don't need a p value and you can quantify what you mean by "equivalent" and can decide how much deviation the two methods can have from one another before you no longer consider them equivalent.  In the regression approach I detailed above, you might consider the methods equivalent if confidence interval around the slope estimate included 1 and the CI around the intercept included 0.</p>
",196,"2010-07-23 21:18:52",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,527,NULL,NULL,NULL
579,2,NULL,"2010-07-23 21:23:18",6,NULL,"<p>Indeed the only difference is that BIC is AIC extended to take number of objects (samples) into account. I would say that while both are quite weak (in comparison to for instance cross-validation) it is better to use AIC, than more people will be familiar with the abbreviation -- indeed I have never seen a paper or a program where BIC would be used (still I admit that I'm biased to problems where such criteria simply don't work).</p>

<p>Edit: AIC and BIC are equivalent to cross-validation provided two important assumptions -- when they are defined, so when the model is a maximum likelihood one and when you are only interested in model performance on a training data. In case of collapsing some data into some kind of consensus they are perfectly ok.<br>
In case of making a prediction machine for some real-world problem the first is false, since your training set represent only a scrap of information about the problem you are dealing with, so you just can't optimize your model; the second is false, because you expect that your model will handle the new data for which you can't even expect that the training set will be representative. 
And to this end CV was invented; to simulate the behavior of the model when confronted with an independent data. In case of model selection, CV gives you not only the quality approximate, but also quality approximation distribution, so it has this great advantage that it can say "I don't know, whatever the new data will come, either of them can be better."  </p>
",88,"2014-04-11 11:31:10",NULL,NULL,NULL,11,NULL,17230,"2014-04-11 11:31:10",NULL,577,NULL,NULL,NULL
581,1,588,"2010-07-23 22:12:36",4,3982,"<p>I am currently using Viterbi training for an image segmentation problem.   I wanted to know what the advantages/disadvantages are of using the Baum-Welch algorithm instead of Viterbi training.   </p>
",99,"2013-10-23 17:06:28","Differences between Baum-Welch and Viterbi training",<machine-learning><image-processing>,2,2,2,88,"2013-10-23 17:06:28",NULL,NULL,NULL,NULL,NULL
582,2,NULL,"2010-07-23 23:38:20",4,NULL,"<p>As you mentioned, AIC and BIC are methods to penalize models for having more regressor variables. A penalty function is used in these methods, which is a function of the number of parameters in the model. </p>

<ul>
<li><p>When applying AIC, the penalty function is <em>z(p)</em> = 2 <em>p</em>.</p></li>
<li><p>When applying BIC, the penalty function is <em>z(p)</em> = p ln(<em>n</em>), which is based on interpreting the penalty as deriving from prior information (hence the name Bayesian Information Criterion).</p></li>
</ul>

<p>When <em>n</em> is large the two models will produce quite different results. Then the BIC applies a much larger penalty for complex models, and hence will lead to simpler models than AIC. However, as stated in <a href="http://en.wikipedia.org/wiki/Bayesian_information_criterion" rel="nofollow">Wikipedia on BIC</a>: </p>

<blockquote>
  <p>it should be noted that in many
  applications..., BIC simply reduces to
  maximum likelihood selection because
  the number of parameters is equal for
  the models of interest.</p>
</blockquote>
",108,"2010-07-23 23:43:40",NULL,NULL,NULL,1,NULL,108,"2010-07-23 23:43:40",NULL,577,NULL,NULL,NULL
583,2,NULL,"2010-07-24 00:07:06",37,NULL,"<p>Though AIC and BIC are both <a href="http://en.wikipedia.org/wiki/Maximum_likelihood">maximum likelihood estimate</a> driven and penalize free parameters in an effort to combat overfitting, they do so in ways that result in significantly different behavior.  Lets look at one commonly presented version of the methods (which results form stipulating normally distributed errors and other well behaving assumptions):</p>

<ul>
<li><strong>AIC</strong> = -2*ln(likelihood) + 2*k,</li>
</ul>

<p>and </p>

<ul>
<li><strong>BIC</strong> = -2*ln(likelihood) + ln(N)*k,</li>
</ul>

<p>where:</p>

<ul>
<li>k = model degrees of freedom</li>
<li>N = number of observations</li>
</ul>

<p>The best model in the group compared is the one that minimizes these scores, in both cases.  Clearly, AIC does not depend directly on sample size.  Moreover, generally speaking, AIC presents the danger that it might overfit, whereas BIC presents the danger that it might underfit, simply in virtue of how they penalize free parameters (2*k in AIC; ln(N)*k in BIC). Diachronically, as data is introduced and the scores are recalculated, at relatively low N (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher N (as the natural log of N overcomes 2).</p>

<p>Additionally, AIC is aimed at finding the best approximating model to the unknown data generating process (via minimizing expected estimated <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">K-L divergence</a>).  As such, it fails to converge in probability to the true model (assuming one is present in the group evaluated), whereas BIC does converge as n tends to infinity.</p>

<p>So, as in many methodological questions, which is to be preferred depends upon what you are trying to do, what other methods are available, and whether or not any of the features outlined (convergence, relative tolerance for free parameters, minimizing expected K-L divergence) speak to your goals.</p>
",39,"2011-04-29 18:30:11",NULL,NULL,NULL,2,NULL,39,"2011-04-29 18:30:11",NULL,577,NULL,NULL,NULL
584,2,NULL,"2010-07-24 00:44:43",0,NULL,"<p>Forward-backward is used when you want to count 'invisible things'. For example, when using E-M to improve a model via unsupervised data. I think that Petrov's paper is an example. In the technique I'm thinking of, you first train a model with annotated data with fairly coarse annotations (e.g. a tag for 'Verb'). Then you arbitrarily split the probability mass for that state in two slightly unequal quantities, and retrain, running forward-backward to maximize likelihood by redistributing mass between the two states.</p>
",240,"2010-07-24 00:44:43",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,581,NULL,NULL,NULL
585,2,NULL,"2010-07-24 00:46:15",4,NULL,"<p>You can certainly get different results simply because you train on different examples. I very much doubt that there's an algorithm or problem domain where the results of the two would differ in some predictable way.</p>
",240,"2010-07-24 00:46:15",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,526,NULL,NULL,NULL
586,2,NULL,"2010-07-24 01:41:28",3,NULL,"<p>The <a href="http://yann.lecun.com/exdb/publis/#lecun-06" rel="nofollow">Tutorial on Energy-Based Learning</a> by LeCun et al. might get you a good part of the way there.  They describe a number of loss functions and discuss what makes them "good or bad" for energy based models.</p>
",251,"2010-07-24 01:41:28",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,558,NULL,NULL,NULL
587,2,NULL,"2010-07-24 03:58:58",31,NULL,"<p>My quick explanation is</p>

<ul>
<li>AIC is best for prediction as it is asymptotically equivalent to cross-validation.</li>
<li>BIC is best for explanation as it is allows consistent estimation of the underlying data generating process.</li>
</ul>
",159,"2010-07-24 03:58:58",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,577,NULL,NULL,NULL
588,2,NULL,"2010-07-24 04:22:01",6,NULL,"<p>The Baum-Welch algorithm and the Viterbi algorithm calculate different things.</p>

<p>If you know the transition probabilities for the hidden part of your model, and the emission probabilities for the visible outputs of your model, then the Viterbi algorithm gives you the most likely <em>complete</em> sequence of hidden states conditional on both your outputs and your model specification.</p>

<p>The Baum-Welch algorithm gives you both the mostl likely hidden transition probabilities as well as the most likely set of emission probabilities given only the observed states of the model (and, usually, an upper bound on the number of hidden states).  You also get the "pointwise" highest likelihood points in the hidden states, which is often slightly different from the single hidden sequence that is overall most likely.</p>

<p>If you know your model and just want the latent states, then there is no reason to use the Baum-Welch algorithm.  If you don't know your model, then you can't be using the Viterbi algorithm.</p>

<p>Edited to add: See Peter Smit's comment; there's some overlap/vagueness in nomenclature.  Some poking around led me to a chapter by Luis Javier Rodrıguez and Ines Torres in "Pattern Recognition and Image Analysis" (ISBN 978-3-540-40217-6, pp 845-857) which discusses the speed versus accuracy trade-offs of the two algorithms.</p>

<p>Briefly, the Baum-Welch algorithm is essentially the Expectation-Maximization algorithm applied to a HMM; as a strict EM-type algorithm you're guaranteed to converge to at least a local maximum, and so for unimodal problems find the MLE.  It requires two passes over your data for each step, though, and the complexity gets very big in the length of the data and number of training samples.  However you do end up with the full conditional likelihood for your hidden parameters.</p>

<p>The Viterbi training algorithm (as opposed to the "Viterbi algorithm") approximates the MLE to achieve a gain in speed at the cost of accuracy.  It segments the data and then applies the Viterbi algorithm (as I understood it) to get the most likely state sequence in the segment, then uses that most likely state sequence to re-estimate the hidden parameters.  This, unlike the Baum-Welch algorithm, doesn't give the full conditional likelihood of the hidden parameters, and so ends up reducing the accuracy while saving significant (the chapter reports 1 to 2 orders of magnitude) computational time.</p>
",61,"2010-07-24 19:31:34",NULL,NULL,NULL,4,NULL,61,"2010-07-24 19:31:34",NULL,581,NULL,NULL,NULL
589,2,NULL,"2010-07-24 04:54:53",6,NULL,"<p>Computational issues are the strongest argument I've heard one way or the other.  The single biggest advantage of the Kolmogorov distance is that it's very easy to compute analytically for pretty much any CDF.  Most other distance metrics don't have a closed-form expression except, sometimes, in the Gaussian case.</p>

<p>The Kolmogorov distance of a sample also has a known sampling distribution given the CDF (I don't think most other ones do), which ends up being related to the Wiener process.  This is the basis for the Kolmogorov-Smirnoff test for comparing a sample to a distribution or two samples to each other.</p>

<p>On a more functional-analysis note, the sup norm is nice in that (as you mention) it basically defines uniform convergence.  This leaves you with norm convergence implying pointwise convergence, and so you if you're clever about how you define your function sequences you can work within a RKHS and use all of the nice tools that that provides as well.</p>
",61,"2010-07-24 04:54:53",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,411,NULL,NULL,NULL
590,1,611,"2010-07-24 09:09:14",2,242,"<p>In some papers, for example in <a href="http://www.jstor.org/stable/2239262" rel="nofollow">"The Geometric Density with Unknown Location Parameter"</a> by Klotz, a Geometric Distribution is called a Geometric Density.</p>

<p>For me, this claim looks erroneous, however Klotz is a serious statistician and a professor in the field.</p>

<p>My question is, to what extend is it legitimate to call a Geometric Distribution a Geometric Density?</p>
",190,"2010-08-03 19:39:42","To what extent can we call a Geometric Distribution a Geometric Density",<distributions><discrete-data>,1,3,NULL,190,"2010-07-25 10:09:23",NULL,NULL,NULL,NULL,NULL
591,2,NULL,"2010-07-24 09:33:01",2,NULL,"<p>The loss function is given by the problem. It could be anything. For example, you could also penalize the used CPU time and space.</p>

<p>In reinforcement learning, the loss function is an unknown non-deterministic function.
You cannot redefine it without changing the problem.</p>
",200,"2010-07-24 09:33:01",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,558,NULL,NULL,NULL
592,2,NULL,"2010-07-24 09:52:52",3,NULL,"<p>It is a bit old, but I have found Chris Chatfield's book,</p>

<p><a href="http://rads.stackoverflow.com/amzn/click/0412253402" rel="nofollow">Statistics for Technology: A Course in Applied Technology</a></p>

<p>to be an excellent introduction.</p>

<p>It was how I first learned about statistics from a conceptual point of view.</p>
",57,"2010-08-11 08:38:27",NULL,NULL,NULL,2,NULL,509,"2010-08-11 08:38:27","2010-07-24 09:52:52",421,NULL,NULL,NULL
593,2,NULL,"2010-07-24 12:09:08",3,NULL,"<p>I recall some discussion on this in the past; I'm not aware of any implementation of Maxwell &amp; Delaney's approach, although it shouldn't be too difficult to do.  Have a look at "<a href="http://gribblelab.org/2009/03/09/repeated-measures-anova-using-r/" rel="nofollow">Repeated Measures ANOVA using R</a>" which also shows one method of addressing the sphericity issue in <a href="http://en.wikipedia.org/wiki/Tukey%27s_range_test" rel="nofollow">Tukey's HSD</a>.</p>

<p>You might also find <a href="http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/" rel="nofollow">this description of Friedman's test</a> of interest.</p>
",5,"2010-07-24 12:09:08",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,575,NULL,NULL,NULL
594,1,599,"2010-07-24 19:25:59",12,273,"<p>The E-M procedure appears, to the uninitiated, as more or less black magic. Estimate parameters of an HMM (for example) using supervised data. Then decode untagged data, using forward-backward to 'count' events as if the data were tagged, more or less. Why does this make the model better? I do know something about the math, but I keep wishing for some sort of mental picture of it.</p>
",240,"2013-05-15 04:27:01","E-M, is there an intuitive explanation?",<e-m><intuition>,1,1,6,NULL,NULL,NULL,NULL,NULL,NULL,NULL
595,2,NULL,"2010-07-24 19:43:24",7,NULL,"<p>You could use the (fast :) ) discrete <strong>wavelet transform</strong>. The package wavethresh under R will do all the work. 
Anyway, I like the solution of @James because it is simple and seems to go straigh to the point.  </p>
",223,"2010-08-03 19:40:40",NULL,NULL,NULL,5,NULL,223,"2010-08-03 19:40:40",NULL,492,NULL,NULL,NULL
596,1,NULL,"2010-07-24 19:50:48",0,247,"<p>E-M provides a way to improve the estimation of a generative model with unannotated data. Is there anything out there that works the same way for discriminative models (e.g. perceptrons)?</p>

<p>For example, consider averaged perceptron tagger. It would be handy to be able to throw the entire Gigaword through some process of unsupervised model improvement.</p>

<p><strong>EDIT</strong>:</p>

<p>So, I was pleasantly surprised to note that this site has the ambition of dealing with machine learning, but I'm learning by experiment what vocabulary is generic and what is very domain-specific. Apologies.</p>

<p>Consider a sequence classification problem, like part-of-speech tagging or named entity extraction. You can train a generative model (e.g. an HMM). Thats's a probability model, and you can apply E-M. However, the number of states grows prohibitive if you want to look at many features, and so the fashion tends toward things like CRFs (batch) or Perceptron (online).</p>

<p>For example, <a href="http://docs.google.com/viewer?a=v&amp;q=cache%3alFSlhiI9L5cJ%3awww.aclweb.org/anthology/E/E09/E09-1087.pdf+unsupervised+perceptron+training&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESjBE8kls2rWhD_U1fL9lz0Wq_elscJ6orH7KNx0ryr0hh37jnzp53sOA-jzJrSI0gea3AwN4lLkq7jsNFbAUcooyDgyYRO2BSqZdpBH4x84KlzLlsccaMcQNbk-1zLvFDAIZVy8&amp;sig=AHIEtbTx9N6TyrKV_7dza3xG9G_dhNxfVg" rel="nofollow">this</a> paper talks about unsupervised learning in for a perceptron POS tagger, but the details are that they add the output of several pre-existing taggers to the training set of their model.</p>
",240,"2013-12-10 00:53:27","Something like E-M for discriminative models?",<machine-learning><e-m>,2,7,NULL,240,"2013-12-10 00:53:27",NULL,NULL,NULL,NULL,NULL
597,2,NULL,"2010-07-24 19:55:30",2,NULL,"<p>Well, there's <a href="http://docs.google.com/viewer?a=v&amp;q=cache%3acfjRh4vE38kJ%3abooks.nips.cc/papers/files/nips16/NIPS2003_LT21.pdf+passive-aggressive+machine+learning&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEEShVh5WierOOrVz8pTpfOPVUCm8HIqkUCm1rxCsUPm5UUapNONytwRMjMq63zcOye0L3djRjQCZW6YN_r12SvIEaWCtbx3b5La7EJ8nQuoWm2Jw8n8DuLEjS19K56fh8DZLOfrHu&amp;sig=AHIEtbRBrb7Q7Lx6LSh_fKEFXyWaxW0RVw" rel="nofollow">this</a> and <a href="http://portal.acm.org/citation.cfm?id=1248547.1248566" rel="nofollow">that</a>. Two papers by Cramer and others discussing loss in the context of online learning algorithms.</p>
",240,"2010-07-24 19:55:30",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,558,NULL,NULL,NULL
598,2,NULL,"2010-07-24 20:15:20",5,NULL,"<p>Few more on top of already mentioned:</p>

<ul>
<li><a href="http://www.knime.org/" rel="nofollow">KNIME</a> together with R, Python and Weka integration extensions for data mining   </li>
<li><a href="http://rosuda.org/mondrian/" rel="nofollow">Mondrian</a> for quick EDA     </li>
</ul>

<p>And from spatial perspective:</p>

<ul>
<li><a href="http://geodacenter.asu.edu/ogeoda" rel="nofollow">GeoDa</a> for spatial EDA and clustering of areal data</li>
<li><a href="http://www.satscan.org/" rel="nofollow">SaTScan</a> for clustering of point data</li>
</ul>
",22,"2010-07-24 20:15:20",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-24 20:15:20",3,NULL,NULL,NULL
599,2,NULL,"2010-07-24 20:23:06",9,NULL,"<p>Just to save some typing, call the observed data $X$, the missing data $Z$ (e.g. the hidden states of the HMM), and the parameter vector we're trying to find $Q$ (e.g. transition/emission probabilities).</p>

<p>The intuitive explanation is that we basically cheat, pretend for a moment we know $Q$ so we can find a conditional distribution of Z that in turn lets us find the MLE for $Q$ (ignoring for the moment the fact that we're basically making a circular argument), then admit that we cheated, put in our new, better value for $Q$, and do it all over again until we don't have to cheat anymore.</p>

<p>Slightly more technically, by pretending that we know the real value $Q$, we can pretend we know something about the conditional distribution of $Z|\\{X,Q\\}$, which lets us improve our estimate for $Q$, which we now pretend is the real value for $Q$ so we can pretend we know something about the conditional distribution of $Z|\\{X,Q\\}$, which lets us improve our estimate for $Q$, which... and so on.</p>

<p>Even more technically, if we knew $Z$, we could maximize $\\log(f(Q|X,Z))$ and have the right answer.  The problem is that we don't know $Z$, and any estimate for $Q$ must depend on it.   But if we want to find the best estimate (or distribution) for $Z$, then we need to know $X$ and $Q$.  We're stuck in a chicken-and-egg situation if we want the unique maximizer analytically.</p>

<p>Our 'out' is that -- for any estimate of $Q$ (call it $Q_n$) -- we can find the distribution of $Z|\\{Q_n,X\\}$, and so we can maximize our <em>expected</em> joint log-likelihood of $Q|\\{X,Z\\}$, with respect to the conditional distribution of $Z|\\{Q_n,X\\}$.  This conditional distribution basically tells us how $Z$ depends on the current value of $Q$ given $X$, and lets us know how to change $Q$ to increase our likelihood for both $Q$ and $Z$ at the same time for a particular value of $Q$ (that we've called $Q_n$).  Once we've picked out a new $Q_{n+1}$, we have a different conditional distribution for $Z|\\{Q_{n+1}, X\\}$ and so have to re-calculate the expectation.</p>
",61,"2013-05-15 04:27:01",NULL,NULL,NULL,0,NULL,805,"2013-05-15 04:27:01",NULL,594,NULL,NULL,NULL
600,2,NULL,"2010-07-24 20:36:13",8,NULL,"<p>Mark,</p>

<p>the main reason of which I am aware for the use of K-S is because it arises naturally from Glivenko-Cantelli theorems in univariate empirical processes. The one reference I'd recommend is A.W.van der Vaart "Asymptotic Statistics", ch. 19. A more advanced monograph is "Weak Convergence and Empirical Processes" by Wellner and van der Vaart. </p>

<p>I'd add two quick notes:</p>

<ol>
<li>another measure of distance commonly used in univariate distributions is the Cramer-von Mises distance, which is an L^2 distance;</li>
<li>in general vector spaces different distances are employed; the space of interest in many papers is polish. A very good introduction is Billingsley's "Convergence of Probability Measures". </li>
</ol>

<p>I apologize if I can't be more specific. I hope this helps.</p>
",30,"2010-07-24 20:36:13",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,411,NULL,NULL,NULL
602,1,1733,"2010-07-24 22:29:40",3,247,"<p>Do you know any good heuristics for finding optimal value of ν in case of ν-SVM classification? In this particular problem I have a radial basis kernel, if it helps.</p>
",88,"2010-08-16 09:23:45","Heuristics for optimizing ν-SVM?",<machine-learning><svm>,2,2,2,NULL,NULL,NULL,NULL,NULL,NULL,NULL
603,2,NULL,"2010-07-24 22:54:13",3,NULL,"<blockquote>
  <p>Usually of course the difference is
  unnoticeable, and so goes my question
  -- can you think of an example when the result of one type is
  significantly different from another?</p>
</blockquote>

<p>I am not sure at all the difference is unnoticeable, and that only in ad hoc example it will be noticeable. Both cross-validation and bootstrapping (sub-sampling) methods depend critically on their design parameters, and this understanding is not complete yet. In general, results <em>within</em> k-fold cross-validation depend critically on the number of folds, so you can expect always different results from what you would observe in sub-sampling.</p>

<p>Case in point: say that you have a true linear model with a fixed number of parameters. If you use k-fold cross-validation (with a given, fixed k), and let the number of observations go to infinity, k-fold cross validation will be asymptotically inconsistent for model selection, i.e., it will identify an incorrect model with probability greater than 0. This surprising result is due to Jun Shao, "Linear Model Selection by Cross-Validation", <em>Journal of the American Statistical Association</em>, <strong>88</strong>, 486-494 (1993), but more papers can be found in this vein.</p>

<p>In general, respectable statistical papers specify the cross-validation protocol, exactly because results are not invariant. In the case where they choose a large number of folds for large datasets, they remark and try to correct for biases in model selection.</p>
",30,"2010-07-24 22:54:13",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,526,NULL,NULL,NULL
604,1,NULL,"2010-07-24 23:09:05",7,480,"<p>I am puzzled by something I found using Linear Discriminant Analysis. Here is the problem - I first ran the Discriminant analysis using 20 or so independent variables to predict 5 segments. Among the outputs, I asked for the Predicted Segments, which are the same as the original segments for around 80% of the cases. Then I ran again the Discriminant Analysis with the same independent variables, but now trying to predict the Predicted Segments. I was expecting I would get 100% of correct classification rate, but that did not happen and I am not sure why. It seems to me that if the Discriminant Analysis cannot predict with 100% accuracy it own predicted segments then somehow it is not a optimum procedure since a rule exist that will get 100% accuracy. I am missing something?</p>

<p>Note - This situation seems to be similar to that in Linear Regression Analysis. If you fit the model $y = a + bX + \\text{error}$ and use the estimated equation with the same data you will get $\\hat{y}$ [$= \\hat{a} + \\hat{b}X$]. Now if you estimate the model $\\hat{y} = \\hat{a} + \\hat{b}X + \\text{error}$, you will find the same $\\hat{a}$ and $\\hat{b}$ as before, no error, and R2 = 100% (perfect fit). I though this would also happen with Linear Discriminant Analysis, but it does not.</p>

<p>Note 2 - I run this test with Discriminant Analysis in SPSS.</p>
",165,"2011-05-03 09:00:53","Why prediction of a predicted variable from a discriminant analysis is imperfect",<regression><discriminant-analysis>,2,6,2,183,"2011-05-03 09:00:53",NULL,NULL,NULL,NULL,NULL
605,2,NULL,"2010-07-24 23:25:48",1,NULL,"<p>This is quite normal in case of machine learning -- it does not need to be optimal, it must be general. </p>
",88,"2010-07-24 23:25:48",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,604,NULL,NULL,NULL
606,2,NULL,"2010-07-25 02:37:06",11,NULL,"<p>A very popular approach is penalized logistic regression, in which one maximizes the sum of the log-likelihood and a penalization term consisting of the L1-norm ("lasso"), L2-norm ("ridge"), a combination of the two ("elastic"), or a penalty associated to groups of variables ("group lasso"). This approach has several advantages:</p>

<ol>
<li>It has strong theoretical properties, e.g., <a href="http://www-stat.stanford.edu/~candes/papers/LassoPredict.pdf">see this paper by Candes &amp; Plan</a> and close connections to compressed sensing;</li>
<li>It has accessible expositions, e.g., in <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html">Elements of Statistical Learning</a> by Friedman-Hastie-Tibshirani (available online);</li>
<li>It has readily available software to fit models. R has the <a href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a> package which is very fast and works well with pretty large datasets. Python has <a href="http://scikit-learn.sourceforge.net">scikit-learn</a>, which includes L1- and L2-penalized logistic regression;</li>
<li>It works very well in practice, as shown in many application papers in image recognition, signal processing, biometrics, and finance.</li>
</ol>
",30,"2010-07-25 02:37:06",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-25 02:37:06",490,NULL,NULL,NULL
607,2,NULL,"2010-07-25 03:29:41",70,NULL,"<p>I think the answer to your first question is simply in the affirmative. Take any issue of Statistical Science, JASA, Annals of Statistics of the past 10 years and you'll find papers on boosting, SVM, and neural networks, although this area is less active now. Statisticians have appropriated the work of Valiant and Vapnik, but on the other side, computer scientists have absorbed the work of Donoho and Talagrand. I don't think there is much difference in scope and methods any more. I have never bought Breiman's argument that CS people were only interested in minimizing loss using whatever works. That view was heavily influenced by his participation in Neural Networks conferences and his consulting work; but PAC, SVMs, Boosting have all solid foundations. And today, unlike in 2001, Statistics is more concerned with finite-sample properties, algorithms and massive datasets.</p>

<p>But I think that there are still three important differences that are not going away soon. </p>

<ol>
<li>Methodological Statistics papers are still overwhelmingly formal and deductive, whereas Machine Learning researchers are more tolerant of new approaches even if they don't come with a proof attached;</li>
<li>The ML community primarily shares new results and publications in conferences and related proceedings, whereas statisticians use journal papers. This slows down progress in Statistics and identification of star researchers. John Langford a <a href="http://hunch.net/?p=318">nice post</a> on the subject from a while back;</li>
<li>Statistics still covers areas that are (for now) of little concern to ML, such as survey design, sampling, industrial Statistics etc.</li>
</ol>
",30,"2013-06-07 06:37:55",NULL,NULL,NULL,1,NULL,22047,"2013-06-07 06:37:55",NULL,6,NULL,NULL,NULL
608,1,NULL,"2010-07-25 05:08:20",19,1244,"<p>In a <a href="http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other">question</a> elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...</p>

<pre><code>library(lme4) #for the BIC function

generate.data &lt;- function(seed)
{
    set.seed(seed) #Set a seed so the results are consistent (I hope)
    a &lt;- rnorm(60) #predictor
    b &lt;- rnorm(60) #predictor
    c &lt;- rnorm(60) #predictor
    y &lt;- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c
    data &lt;- data.frame(y,a,b,c) 
    return(data)    
}

data &lt;- generate.data(76)
good.model &lt;- lm(y ~ a+b,data=data)
bad.model &lt;- lm(y ~ a+b+c,data=data)
AIC(good.model)
BIC(logLik(good.model))
AIC(bad.model)
BIC(logLik(bad.model))
</code></pre>

<p>Per earlier comments, below I have provided a list of seeds from 1 to 10000 in which AIC and BIC disagree.  This was done by a simple search through the available seeds, but if someone could provide a way to generate data which would tend to produce divergent answers from these two information criteria it may be particularly informative.</p>

<pre><code>notable.seeds &lt;- read.csv("http://student.ucr.edu/~rpier001/res.csv")$seed
</code></pre>

<p>As an aside, I thought about ordering these seeds by the extent to which the AIC and BIC disagree which I've tried quantifying as the sum of the absolute differences of the AIC and BIC.  For example, </p>

<pre><code>AICDiff &lt;- AIC(bad.model) - AIC(good.model) 
BICDiff &lt;- BIC(logLik(bad.model)) - BIC(logLik(good.model))
disagreement &lt;- sum(abs(c(AICDiff,BICDiff)))
</code></pre>

<p>where my disagreement metric only reasonably applies when the observations are notable.  For example,</p>

<pre><code>are.diff &lt;- sum(sign(c(AICDiff,BICDiff)))
notable &lt;- ifelse(are.diff == 0 &amp; AICDiff != 0,TRUE,FALSE)
</code></pre>

<p>However in cases where AIC and BIC disagreed, the calculated disagreement value was always the same (and is a function of sample size).  Looking back at how AIC and BIC are calculated I can see why this might be the case computationally, but I'm not sure why it would be the case conceptually.  If someone could elucidate that issue as well, I'd appreciate it.</p>
",196,"2010-08-03 22:58:09","How can one empirically demonstrate in R which cross-validation methods the AIC and BIC are equivalent to?",<r><aic><cross-validation><bic>,1,9,9,196,"2010-08-03 22:18:43",NULL,NULL,NULL,NULL,NULL
609,2,NULL,"2010-07-25 07:18:44",6,NULL,"<p><strong>As a summary</strong>, my answer is : if you have an explicit expression or can figure out some how what your distance is measuring (what "differences" it gives weigth to), then you can say what it is better for. An other complementary way to analyse and compare such test is the minimax theory. </p>

<p>At the end some test will be good for some alternatives and some for others. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. </p>

<hr>

<p><strong>Some details</strong> </p>

<p>Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their "optimal detection boudary" <a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1085408492" rel="nofollow">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1085408492</a>.</p>

<p>Let me go distance by distance:</p>

<ol>
<li><p>KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.</p></li>
<li><p>Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  <a href="http://en.wikipedia.org/wiki/Wasserstein_metric" rel="nofollow">http://en.wikipedia.org/wiki/Wasserstein_metric</a> is ubiquitous and hence hard to compare. </p>

<ul>
<li>For the particular case of W2 it has been uses in <a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1017938923" rel="nofollow">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1017938923</a>  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. </li>
<li>For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a "balanced test" not giving importance to tails. </li>
</ul></li>
</ol>

<hr>

<p>To recall and extend the comment I made which complete the answer: </p>

<p>I know you did not meant to be exhaustive but you could add Anderson darling statistic (see <a href="http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test" rel="nofollow">http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test</a>). This made me remind of a paper fromo Jager and Wellner (see <a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1194461721" rel="nofollow">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1194461721</a>) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be minimax for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. </p>
",223,"2010-08-05 05:35:45",NULL,NULL,NULL,2,NULL,223,"2010-08-05 05:35:45",NULL,411,NULL,NULL,NULL
610,2,NULL,"2010-07-25 09:13:59",9,NULL,"<p>It sounds dodgy to me as the trend estimate will be biased near the point where you splice on the false data. An alternative approach is a nonparametric regression smoother such as loess or splines.</p>
",159,"2010-07-25 09:13:59",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,492,NULL,NULL,NULL
611,2,NULL,"2010-07-25 12:36:23",4,NULL,"<p><strong>Heuristic answer:</strong> Without much mathematic you could say that a continuous variable has a density with respect to the Lebesgue measure, and a discrete random variable has a density with respect to the counting measure.</p>

<p><strong>Developped answer:</strong> The concept of density is much wider than you may think. A density of a probability measure $P$ can be defined with respect to a measure $\\lambda$ that dominates $P$ by the Radon Nikodym Theorem (see <a href="http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem" rel="nofollow">http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem</a>). Here density should be understood as a density with respect to the counting measure defined on the mentionned countable set. I agree that it is not extremely rigorous not to mention the reference when talking about a density (but who mention density  wrt lesbesgue measure?), but it pose no problem while reading the paper in question so .... </p>

<hr>

<p>Additional Annex Notes
 I have seen a certain number of machine learning notes (I won't do delation) where  the reference measure is not the counting measure and we see things such as $P(X=x|Y=y)$ with X being a continuous variable (with a density wrt Lebesgues) (to apply Bayes principle and derive the Bayes rule). I guess people want to be pedagogic and do not want to bother students with technical details ;) My conclusion would be that even great mathematician can commit abuse of wording or of notation (it is not the case in the paper you mention because the authors of a paper in AoP may have mathematical background), it is not a problem as along as it is understood by everyone...  </p>
",223,"2010-08-03 19:39:42",NULL,NULL,NULL,2,NULL,223,"2010-08-03 19:39:42",NULL,590,NULL,NULL,NULL
612,1,849,"2010-07-25 14:31:31",12,3981,"<p>I have tried to reproduce some research (using PCA) from SPSS in R. In my experience, <a href="http://www.personality-project.org/R/html/principal.html"><code>principal()</code></a> function from package <code>psych</code> was the only function that came close (or if my memory serves me right, dead on) to match the output. To match the same results as in SPSS, I had to use parameter <code>principal(..., rotate = "varimax")</code>. I have seen papers talk about how they did PCA, but based on the output of SPSS and use of rotation, it sounds more like Factor analysis.</p>

<p>Question: Is PCA, even after rotation (using <code>varimax</code>), still PCA? I was under the impression that this might be in fact Factor analysis... In case it's not, what details am I missing?</p>
",144,"2013-03-28 11:47:29","is psych::principal function still PCA when using rotation?",<r><pca><spss><factor-analysis>,4,4,9,196,"2012-04-16 17:55:40",NULL,NULL,NULL,NULL,NULL
613,2,NULL,"2010-07-25 14:33:25",9,NULL,"<p>I have been a heavy R user for the past 6-7 years. As a language, it has several design limitations. Yet, for work in econometrics and in data analysis, I still wholeheartedly recommend it. It has a large number of packages that would be relevant to you for econometrics, time series, consumer choice modeling etc. and of course excellent visualization, good algebra and numerical libraries etc. I would not worry too much about data size limitations. Although R was not designed for "big data" (unlike, say, SAS) there are ways around it. The availability of packages is what makes the difference, really.</p>

<p>I've only read Clojure's language specs, and it's beautiful and clean. It addresses in a natural way issues of parallelization and scale.  And if you have some basic java or OOP knowledge, you can benefit from the large number of high-quality java libraries.</p>

<p>The issue I have with Clojure is that is a recent one-man (R.Hickey) operation, therefore 1) very risky 2) very immature 3) with niche adoption. Great for enthusiasts, early adopters, CS/ML people who want to try new things. For a user who sees a language as a means to an end and who needs very robust code that can be shared code with others, established languages seem a safer choice. Just know who you are.</p>
",30,"2010-07-25 14:33:25",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,130,NULL,NULL,NULL
614,1,NULL,"2010-07-25 14:53:50",22,2267,"<p>There have been a few questions about statistical <a href="http://stats.stackexchange.com/questions/tagged/textbook">textbooks</a>, such as the question <a href="http://stats.stackexchange.com/questions/170/free-statistical-textbooks">Free statistical textbooks</a>. However, I am looking for textbooks that are Open Source, for example, having an <a href="http://creativecommons.org/">Creative Commons</a> license. The reason is that in course material in other domains, you still want to include some text about basic statistics. In this case, it would be interesting to reuse existing material, instead of rewriting that material.</p>

<p>Therefore, what Open Source textbooks on statistics (and perhaps machine learning) are available?</p>
",107,"2014-06-20 07:54:48","Open Source statistical textbooks?",<books><open-source>,11,6,18,107,"2010-07-26 15:31:39","2010-07-26 15:31:39",NULL,NULL,NULL,NULL
615,2,NULL,"2010-07-25 14:56:32",2,NULL,"<p>Thanks to the chaos in definitions of both they are effectively a synonyms. Don't believe words and look deep into the docks to find the equations.</p>
",88,"2010-07-25 14:56:32",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,612,NULL,NULL,NULL
616,2,NULL,"2010-07-25 15:25:13",6,NULL,"<p>The <a href="http://en.wikibooks.org/wiki/Statistics">"Statistics"</a> book on wikibooks</p>
",5,"2010-07-25 15:32:14",NULL,NULL,NULL,0,NULL,5,"2010-07-25 15:32:14","2010-07-26 15:37:25",614,NULL,NULL,NULL
617,2,NULL,"2010-07-25 15:32:59",9,NULL,"<p><a href="http://www.opentextbook.org/2009/04/03/multivariate-statistics-with-r/">Multivariate statistics with R</a></p>
",5,"2010-07-25 15:32:59",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-26 15:37:10",614,NULL,NULL,NULL
618,2,NULL,"2010-07-25 18:02:06",15,NULL,"<p>I've always liked this one:</p>

<p><img src="http://i.imgur.com/dxfWK.jpg" alt="lemons vs deaths"></p>

<p>source: <a href="http://pubs.acs.org/doi/abs/10.1021/ci700332k">http://pubs.acs.org/doi/abs/10.1021/ci700332k</a></p>
",54,"2010-07-25 18:10:35",NULL,NULL,NULL,6,NULL,54,"2010-07-25 18:10:35","2010-08-16 13:01:42",36,NULL,NULL,NULL
619,2,NULL,"2010-07-25 20:08:35",4,NULL,"<p>I have a depressing and not-very-specific anecdote to share here. I spent some time as a co-worker of a statistical MT researcher. If you want to see a really big, complex, model, look no further.</p>

<p>He was putting me through NLP bootcamp for his own amusement. I am, in general, the sort of programmer who lives and dies by the unit test and the debugger. As a young person at Symbolics, I was struck by the aphorism, 'programming is debugging an empty editor buffer.' (Sort of like training a perceptron model.)</p>

<p>So, I asked him, 'how do you test and debug this stuff.' He answer was, "You get it right the first time. You think it through (in his case, often on paper) very carefully, and you code it very carefully. Because when you get it wrong, the chances of isolating the problem are very slim."</p>
",240,"2010-07-25 20:08:35",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,524,NULL,NULL,NULL
620,2,NULL,"2010-07-25 22:04:16",3,NULL,"<p><a href="http://en.wikibooks.org/wiki/R_Programming" rel="nofollow">R programming wiki book</a></p>
",253,"2010-07-25 22:04:16",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-26 15:52:05",614,NULL,NULL,NULL
621,2,NULL,"2010-07-25 23:56:43",3,NULL,"<p>Some googling found <a href="http://collegeopentextbooks.org/statisticsprobbooks.html" rel="nofollow">http://collegeopentextbooks.org/statisticsprobbooks.html</a> . Still, be aware that most of CC-ed material is share-aliked (so you must also publish your work on CC) or at least attributed (so you must add info that certain part was copied and from whom). The same works with GFDL (both SA &amp; A), it is even worse since in principle you should print it along with the document.</p>
",88,"2010-07-25 23:56:43",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-26 15:48:09",614,NULL,NULL,NULL
622,1,628,"2010-07-26 09:12:21",15,3833,"<p>I see these terms being used and I keep getting them mixed up. Is there a simple explanation of the differences between them?</p>
",159,"2010-07-31 00:34:24","What is the difference between a partial likelihood, profile likelihood and marginal likelihood?",<estimation><maximum-likelihood>,2,0,9,NULL,NULL,NULL,NULL,NULL,NULL,NULL
623,2,NULL,"2010-07-26 09:46:48",3,NULL,"<p>Garbage in, garbage out....</p>

<p>Implicit in getting the full benefit of linear regression is that the noise follows a normal distribution.  Ideally you have mostly data and a little noise.... not mostly noise and a little data.  You can test for normality of residuals after the linear fit by looking at the residuals.  You can also filter input data before the linear fit for obvious, glaring errors. </p>

<p>Here are some types of noise in garbage input data that do not typically fit a normal distribution:</p>

<ul>
<li>Digits missing or added with hand-entered data (off by a factor of 10 or more)</li>
<li>Wrong or incorrectly converted units (grams vs kilos vs pounds; meters, feet, miles, km), possibly from merging multiple data sets (Note: The Mars Orbiter was thought to be lost in this way, so even NASA rocket scientists can make this mistake)</li>
<li>Use of codes like 0, -1, -99999 or 99999 to mean something non-numeric like "not applicable" or "column unavailable" and just dumping this into a linear model along with valid data</li>
</ul>

<p>Writing a spec for what is "valid data" for each column can help you tag invalid data. For instance, a person's height in cm should be in a range, say, 100-300cm.  If you find 1.8 for height thats a typo, and while you can assume it was 1.8m and alter it to 180 -- I'd say it is usually safer to throw it out and best to document as much of the filtering as possible.  </p>
",87,"2010-07-26 09:46:48",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,175,NULL,NULL,NULL
624,1,625,"2010-07-26 11:53:11",3,239,"<p>In engineering, we usually have Handbooks that pretty much dictate the state of the practice. These books are usually devoid of theory and focus on the applied methodology. Is there a forecasting Handbook out there? that solely focuses on the technique and not the background?</p>
",59,"2010-09-30 21:25:14","Forecasting handbooks",<forecasting>,1,0,2,930,"2010-09-30 21:25:14","2010-07-26 15:00:23",NULL,NULL,NULL,NULL
625,2,NULL,"2010-07-26 12:01:58",4,NULL,"<p>There are two that I know of:</p>

<ol>
<li><a href="http://rads.stackoverflow.com/amzn/click/0444513957" rel="nofollow">Handbook of economic forecasting</a>. Relatively theoretical. Not for undergraduates. A narrow look at forecasting --- specifically about economic forecasting.</li>
<li><a href="http://rads.stackoverflow.com/amzn/click/0792379306" rel="nofollow">Principles of forecasting</a>. Simpler, broader. Widely used by forecasting practitioners. Often reflects the idiosyncratic opinions of the editor which are presented as established facts.</li>
</ol>

<p>Alternatively, you could use an intro textbook. <a href="http://robjhyndman.com/forecasting" rel="nofollow">My own textbook</a> is often used as a sort of handbook by forecasting practitioners working in a business environment.</p>
",159,"2010-07-26 12:20:12",NULL,NULL,NULL,2,NULL,159,"2010-07-26 12:20:12",NULL,624,NULL,NULL,NULL
626,2,NULL,"2010-07-26 12:38:40",5,NULL,"<p>"Bayesian Core: A Practical Approach to Computational Bayesian Statistics" by Marin and Robert, <em>Springer-Verlag</em> (2007)</p>
",30,"2010-07-26 12:38:40",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-07-26 12:38:40",125,NULL,NULL,NULL
627,2,NULL,"2010-07-26 12:53:22",10,NULL,"<p>I really like  <a href="http://www.tufts.edu/~gdallal/LHSP.HTM">The Little Handbook of Statistical Practice</a> by Gerard E. Dallal</p>
",236,"2010-07-26 12:53:22",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-26 12:53:22",170,NULL,NULL,NULL
628,2,NULL,"2010-07-26 14:40:27",16,NULL,"<p>The likelihood function usually depends on many parameters. Depending on the application, we are usually interested in only a subset of these parameters. For example, in linear regression, interest typically lies in the slope coefficients and not on the error variance. </p>

<p>Denote the parameters we are interested in as &beta; and the parameters that are not of primary interest as &theta;. The standard way to approach the estimation problem is to maximize the likelihood function so that we obtain estimates of &beta; and &theta;. However, since the primary interest lies in &beta; partial, profile and marginal likelihood offer alternative ways to estimate &beta; without estimating &theta;</p>

<p>In order to see the difference denote the standard likelihood by L(&beta;, &theta;|data). </p>

<p><strong>Maximum Likelihood</strong></p>

<p>Find &beta; and &theta; that maximizes L(&beta;, &theta;|data).</p>

<p><strong>Partial Likelihood</strong></p>

<p>If we can write the likelihood function as:</p>

<p>L(&beta;, &theta;|data) = L1(&beta;|data) L2(&theta;|data)</p>

<p>Then we simply maximize L1(&beta;|data).</p>

<p><strong>Profile Likelihood</strong></p>

<p>If we can express &theta; as a function of &beta; then we replace &theta; with the corresponding function. </p>

<p>Say, &theta; = g(&beta;). Then, we maximize:</p>

<p>L(&beta;, g(&beta;)|data)</p>

<p><strong>Marginal Likelihood</strong></p>

<p>We integrate out &theta; from the likelihood equation by exploiting the fact that we can identify the probability distribution of &theta; conditional on &beta;.</p>
",NULL,"2010-07-31 00:34:24",NULL,NULL,NULL,3,NULL,NULL,"2010-07-31 00:34:24",NULL,622,NULL,user28,user28
631,1,28567,"2010-07-26 16:10:45",22,4341,"<p>What is an estimator of standard deviation of standard deviation if normality of data can be assumed?</p>
",88,"2012-06-08 12:25:22","Standard deviation of standard deviation",<standard-deviation>,4,0,9,NULL,NULL,NULL,NULL,NULL,NULL,NULL
632,2,NULL,"2010-07-26 16:23:25",4,NULL,"<p>Assume you observe $X_1,\\dots,X_n$ iid from a normal with mean zero and variance $\\sigma^2$. The (empirical) standard deviation is the square root of the estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ (unbiased or not that is not the question). As an estimator (obtained with $X_1,\\dots,X_n$), $\\hat{\\sigma}$ has a variance that can be calculated theoretically. Maybe what you call the standard deviation of standard deviation is actually the square root of the variance of the  standard deviation, i.e. $\\sqrt{E[(\\sigma-\\hat{\\sigma})^2]}$?  It is not an estimator, it is a theoretical quantity (something like $\\sigma/\\sqrt{n}$ to be confirmed) that can be calculated explicitely !</p>
",223,"2012-06-08 12:25:22",NULL,NULL,NULL,4,NULL,4856,"2012-06-08 12:25:22",NULL,631,NULL,NULL,NULL
633,1,634,"2010-07-26 17:29:19",4,411,"<p>I have a data set of about 3,000 field observations. </p>

<p>The data collected is divided into 20 variables (real numbers), 30 boolean variables, and 10 or so look up variables and one "answer" variable</p>

<p>We have about 20,000 objects in the field, and i'm trying to produce an "answer" for the 20,000 objects based on the 3,000 observations.</p>

<p>What are some of the available methods that incorporate booleans and look up tables?</p>

<p>any suggestions on how i should proceed?</p>

<p><strong>EDIT</strong></p>

<p>the answer variable is a boolean as well</p>

<p><strong>EDIT 2</strong></p>

<p>a sample of the variable data:</p>

<ul>
<li>Age of specimen</li>
<li>length, area, volume</li>
<li>time since last inspection</li>
<li>height</li>
<li>design life</li>
</ul>

<p>Lookup table</p>

<ul>
<li>material type </li>
<li>coating type</li>
<li>design standard</li>
<li>design effectiveness</li>
</ul>

<p>a sample of the boolean</p>

<ul>
<li>is it inspected?</li>
<li>is it in bad shape</li>
<li>does it need repairs soon</li>
</ul>

<p>the answer variable which is my f(x) is:</p>

<ul>
<li>is it useable</li>
</ul>
",59,"2010-11-02 18:47:07","Incorporating boolean data into analysis",<modeling><categorical-data><model-selection><binary-data>,4,0,1,930,"2010-10-17 19:07:16",NULL,NULL,NULL,NULL,NULL
634,2,NULL,"2010-07-26 17:46:45",7,NULL,"<p>You are decribing "<a href="http://www.oswego.edu/~srp/stats/variable_types.htm" rel="nofollow"><strong>categorical</strong> variables</a>" (represented in R a factors). These can be incorporated into almost any statistical model by being assigned levels.  You would need to give more detail about your particular problem in order to be advised on a particular method.  </p>

<p><em>Edit</em></p>

<p>If the response variable has two possible outcomes, you might consider <a href="http://en.m.wikipedia.org/wiki/Binomial_regression" rel="nofollow">binomial</a> or <a href="http://en.wikipedia.org/wiki/Logistic_regression" rel="nofollow">logistic</a> regression. </p>

<p>Note: If you're not familiar with the different kinds of variables in statistics, I suggest reading the first few chapters of Andrew Gelman's "<a href="http://www.stat.columbia.edu/~gelman/arm/" rel="nofollow">Data Analysis Using Regression and Multilevel/Hierarchical Models</a>" which covers this in a very understandable manner.</p>
",5,"2010-07-26 18:25:45",NULL,NULL,NULL,0,NULL,5,"2010-07-26 18:25:45",NULL,633,NULL,NULL,NULL
635,2,NULL,"2010-07-26 17:50:21",13,NULL,"<p>Implementations of RF differ slightly. I know that Salford Systems' <a href="http://salford-systems.com/products/randomforests/overview.html">proprietary implementation</a> is supposed to be better than the <a href="http://cran.r-project.org/web/packages/randomForest/">vanilla one</a> in R. A description of the algorithm is in <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html">ESL by Friedman-Hastie-Tibshirani, 2nd ed, 3rd printing</a>. An entire chapter (15th) is devoted to RF, and I find it actually clearer than the original paper. The tree construction algorithm is detailed on p.588; no need for me to reproduce it here, since the book is available online.</p>
",30,"2011-08-21 20:53:14",NULL,NULL,NULL,1,NULL,740,"2011-08-21 20:53:14",NULL,480,NULL,NULL,NULL
636,2,NULL,"2010-07-26 17:50:37",1,NULL,"<p>Try Random Forest; from my experience it may perform well on such kind of data, and gives you a some additional interesting information, like variable importance and object similarity measure. </p>
",88,"2010-07-26 18:21:26",NULL,NULL,NULL,4,NULL,88,"2010-07-26 18:21:26",NULL,633,NULL,NULL,NULL
637,2,NULL,"2010-07-26 18:22:33",4,NULL,"<p>It sounds like you are trying to predict your boolean response, yes?</p>

<p>This is called classification.</p>

<p>Logistic Regression is the obvious choice here, but there are other methods too. You can't do traditional regression, because the response is not a real number.</p>

<p>The lookup variables are called nominals, and can be dealt with in regression by using "dummy" variables.</p>

<p>For example, if your lookup variable is type=[steel, aluminum, plastic] (N=3), then your dummy variables would look like this:</p>

<p>IsSteel = [1,0]
IsAlum = [1,0]</p>

<p>There would only be two (N-1) dummy variables, as IsSteel=0 AND IsAlum=0 represents "IsPlastic"=1</p>

<p>But any good stats program should handle this.</p>

<p>If you need a book, I recommend Multivariate Data Analysis by Hair.</p>
",74,"2010-07-26 18:22:33",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,633,NULL,NULL,NULL
638,1,644,"2010-07-26 18:24:35",8,771,"<p>Provided a sample size S that I plan on using to forecast data. What are some of the ways to subdivide the data so that I use some of it to establish a model, and the remainder data to validate the model?</p>

<p>I know there is no black and white answer to this but it would be interesting to know  some "rules of thumb" or usually used ratios. I know back at university, one of our professors used to say model on 60% and validate on 40%.</p>
",59,"2010-09-16 06:56:07","How to calculate the ratio of "data used in analysis" and "saved data for validation" from a sample set?",<machine-learning><best-practices>,4,0,4,88,"2010-09-16 06:56:07",NULL,NULL,NULL,NULL,NULL
640,2,NULL,"2010-07-26 18:40:54",12,NULL,"<p>I suppose that you are looking for the <a href="http://en.wikipedia.org/wiki/Variance#Distribution_of_the_sample_variance" rel="nofollow">distribution of the sample variance</a>.</p>
",NULL,"2010-07-26 18:40:54",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,631,NULL,user28,NULL
641,1,646,"2010-07-26 19:20:53",4,1286,"<p>Sites like eMarketer offer general survey results about internet usage.</p>

<p>Who else has a big set of survey results, or regularly releases them?</p>

<ul>
<li>Preferably marketing research focused.</li>
</ul>

<p>Thanks!</p>
",74,"2010-09-16 12:42:31","Where is a good place to find survey results?",<dataset><survey>,2,2,3,88,"2010-09-16 12:42:31","2010-07-26 19:41:11",NULL,NULL,NULL,NULL
642,2,NULL,"2010-07-26 19:23:19",1,NULL,"<p>government websites usually .... I use the <a href="http://www.rita.dot.gov/" rel="nofollow">RITA</a> a lot</p>
",59,"2010-07-26 19:23:19",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,641,NULL,NULL,NULL
643,1,NULL,"2010-07-26 19:26:37",21,1582,"<p>My father is a math enthusiast, but not interested in statistics much. It would be neat to <em>try</em> to illustrate some of the wonderful bits of statistics, and the CLT is a prime candidate. How would you convey the mathematical beauty and impact of the central limit theorem to a non-statistician? </p>
",7,"2011-06-09 21:40:25","How do you convey the beauty of the Central Limit Theorem to a non-statistician?",<theory><central-limit-theorem>,7,2,7,88,"2010-10-19 06:42:19","2011-06-02 11:09:11",NULL,NULL,NULL,NULL
644,2,NULL,"2010-07-26 19:27:49",5,NULL,"<p>Well as you said there is no black and white answer. I generally don't divide the data in 2 parts but use methods like k-fold cross validation instead. </p>

<p>In k-fold cross validation you divide your data randomly into k parts and fit your model on k-1 parts and test the errors on the left out part. You repeat the process k times leaving each part out of fitting one by one. You can take the mean error from each of the k iterations as an indication of the model error. This works really well if you want to compare the predictive power of different models. </p>

<p>One extreme form of k-fold cross validation is the generalized cross validation where you just leave out one data point for testing and fit the model to all the remaining points. Then repeat the process n times leaving out each data point one by one. I generally prefer k-fold cross validation over the generalized cross validation ... just a personal choice</p>
",288,"2010-07-26 19:27:49",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,638,NULL,NULL,NULL
645,1,680,"2010-07-26 19:28:53",11,2750,"<p>Having just recently started teaching myself Machine Learning and Data Analysis I'm finding myself hitting a brick wall on the need for creating and querying large sets of data. I would like to take data I've been aggregating in my professional and personal life and analyze it but I'm uncertain of the best way to do the following:</p>

<p>1) How should I be storing this data? Excel? SQL? ??</p>

<p>2) What is a good way for a beginner to begin trying to analyze this data? I am a professional computer programmer so the complexity is not in writing programs but more or less specific to the domain of data analysis. </p>

<p>EDIT: Apologies for my vagueness, when you first start learning about something it's hard to know what you don't know, ya know? ;)</p>

<p>Having said that, my aim is to apply this to two main topics:</p>

<p>1) Software team metrics (think Agile velocity, quantifying risk, likelihood of a successfully completed iteration given x number of story points)</p>

<p>2) Machine learning (ex. system exceptions have occurred in a given set of modules what is the likelihood that a module will throw an exception in the field, how much will that cost, what can the data tell me about key modules to improve that will get me the best bang for my buck, predict what portion of the system the user will want to use next in order to start loading data, etc)</p>
",9426,"2012-07-22 11:15:41","Best Way to Aggregate and Analyze Data",<best-practices>,4,1,9,88,"2010-09-16 06:50:58",NULL,NULL,NULL,NULL,NULL
646,2,NULL,"2010-07-26 19:31:36",9,NULL,"<p>The best place to find survey data related to the social sciences is the ICPSR data clearinghouse: <a href="http://www.icpsr.umich.edu/icpsrweb/ICPSR/access/index.jsp" rel="nofollow">http://www.icpsr.umich.edu/icpsrweb/ICPSR/access/index.jsp</a></p>

<p>Also, the 'survey' tag on Infochimps has many interesting and free data sets: <a href="http://infochimps.org/tags/survey" rel="nofollow">http://infochimps.org/tags/survey</a></p>
",302,"2010-07-26 19:31:36",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,641,NULL,NULL,NULL
647,2,NULL,"2010-07-26 19:34:07",-2,NULL,"<p>I think both the concept and the terminology of "SD of SD" is too slippery to tackle. But it is easier to think about the confidence interval of a SD. You compute the SD from a sample of data and want to compute a confidence interval that is 95% (or some other confidence level) likely to contain the true SD of the population from which the data were sampled.</p>

<p><a href="http://www.graphpad.com/faq/viewfaq.cfm?faq=1381" rel="nofollow" title="Here">Detailed explanation</a></p>

<p><a href="http://www.graphpad.com/quickcalcs/CISD1.cfm" rel="nofollow">Free web calculator</a></p>
",25,"2010-07-26 19:34:07",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,631,NULL,NULL,NULL
648,2,NULL,"2010-07-26 19:34:09",13,NULL,"<p>To fully appreciate the CLT, it should be seen.</p>

<p>Hence the notion of the <a href="http://en.wikipedia.org/wiki/Galton_board">bean machine</a> and plenty of youtube <a href="http://www.youtube.com/results?search_query=galton+board&amp;aq=0">videos</a> for illustration.</p>
",68,"2010-07-26 19:34:09",NULL,NULL,NULL,7,NULL,NULL,NULL,"2011-06-02 11:09:11",643,NULL,NULL,NULL
650,2,NULL,"2010-07-26 19:34:52",3,NULL,"<p>Your question is so broad that the answer is: it depends. Still, to give some more useful answer I'll indicate what I think are common in Research.</p>

<p>Storing of data is very often done in text files. When doing statistical analyses you mostly work with a collection of one type of vectors. This can be seen as a table and written in csv format. The reason thins are often stored in plain-text, is because simply every tool can read them and it is easy to transform them.</p>

<p>About analyzing, this is a bit harder to be specific. If it is 2 dimensional, make a scatterplot. If it is high-dimensional, do PCA and see where the first principal components exist of to discover important variables. If you have time data, plot it. This is all so general that to be useful you have to really indicate better what your data is.</p>
",190,"2010-07-26 19:34:52",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,645,NULL,NULL,NULL
651,2,NULL,"2010-07-26 19:39:05",2,NULL,"<p>My book, <a href="http://www.intuitivebiostatistics.com" rel="nofollow"><em>Intuitive Biostatistics</em></a>, is written partly from a medical point of view. It focusses on the practical parts of interpreting statistical results, with almost no math.</p>
",25,"2010-07-26 19:39:05",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,223,NULL,NULL,NULL
652,1,682,"2010-07-26 19:39:49",16,16577,"<p>I bought this book:</p>

<p><a href="http://rcm.amazon.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=justibozon-20&amp;o=1&amp;p=8&amp;l=as1&amp;m=amazon&amp;f=ifr&amp;md=10FE9736YVPPT7A0FBG2&amp;asins=0470539399">How to Measure Anything: Finding the Value of Intangibles in Business</a> </p>

<p>and </p>

<p><a href="http://rcm.amazon.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=justibozon-20&amp;o=1&amp;p=8&amp;l=as1&amp;m=amazon&amp;f=ifr&amp;md=10FE9736YVPPT7A0FBG2&amp;asins=0596153937">Head First Data Analysis: A Learner's Guide to Big Numbers, Statistics, and Good Decisions</a></p>

<p>What other books would you recommend?</p>
",9426,"2013-05-27 02:35:13","Best books for an introduction to statistical data analysis?",<machine-learning><bayesian><books>,12,12,23,88,"2010-09-10 18:26:46","2010-07-26 19:53:10",NULL,NULL,NULL,NULL
653,2,NULL,"2010-07-26 19:42:50",4,NULL,"<p>It really depends on the amount of data you have, the specific cost of methods and how exactly you want your result to be.</p>

<p>Some examples:</p>

<p>If you have little data, you probably want to use cross-validation (k-fold, leave-one-out, etc.) Your model will probably not take much resources to train and test anyway. It are good ways to get the most out of your data</p>

<p>You have a lot of data: you probably want to take a reasonably large test-set, ensuring that there will be little possibility that some strange samples will give to much variance to your results. How much data you should take? It depends completely on your data and model. In speech recognition for example, if you would take too much data (let's say 3000 sentences), your experiments would take days, as a realtime factor of 7-10 is common. If you would take too little, it is too much dependent on the speakers that you are choosing (which are not allowed in the training set).</p>

<p>Remember also, in a lot of cases it is good to have a validation/development set too!</p>
",190,"2010-07-26 19:42:50",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,638,NULL,NULL,NULL
654,2,NULL,"2010-07-26 19:44:44",5,NULL,"<p>1:10 test:train ratio is popular because it looks round, 1:9 is popular because of 10-fold CV, 1:2 is popular because it is also round and reassembles bootstrap. Sometimes one gets a test from some data-specific criteria, for instance last year for testing, years before for training. </p>

<p>The general rule is such: the train must be large enough to so the accuracy won't drop significantly, and the test must be large enough to silence random fluctuations.</p>

<p>Still I prefer CV, since it gives you also a distribution of error.</p>
",88,"2010-07-26 19:44:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,638,NULL,NULL,NULL
655,2,NULL,"2010-07-26 19:49:53",2,NULL,"<p>You might find useful this one: <a href="http://rads.stackoverflow.com/amzn/click/0387848576" rel="nofollow">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</a></p>

<p>UPDATE #1:</p>

<p>This book might be useful as well: <a href="http://rads.stackoverflow.com/amzn/click/0596510497" rel="nofollow">O'Reilly: Statistics in a Nutshel</a>l</p>
",315,"2010-08-03 09:09:40",NULL,NULL,NULL,2,NULL,315,"2010-08-03 09:09:40",NULL,652,NULL,NULL,NULL
656,2,NULL,"2010-07-26 19:51:45",61,NULL,"<p><img src="http://bp1.blogger.com/_x7QjiZypFj0/Rp9dcGTsBKI/AAAAAAAAAA0/VWwfWDv6nzM/s400/Outlier.jpg" alt="Image at bp1.blogger.com."></p>
",25,"2010-08-11 08:50:54",NULL,NULL,NULL,1,NULL,509,"2010-08-11 08:50:54","2010-07-26 19:51:45",423,NULL,NULL,NULL
657,2,NULL,"2010-07-26 19:53:45",3,NULL,"<p>I liked these lectures: <a href="http://www.youtube.com/results?search_query=statistical+aspects+of+data+mining&amp;aq=0" rel="nofollow">Statistical Aspects of Data Mining</a>. The lecturer is solving example problems using R.</p>
",315,"2010-07-26 19:53:45",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
658,2,NULL,"2010-07-26 19:56:23",14,NULL,"<p>What I loved most with CLT is the cases when it is not applicable -- this gives me a hope that the life is a bit more interesting that Gauss curve suggests. So show him the Cauchy distribution.</p>
",88,"2010-07-27 18:06:52",NULL,NULL,NULL,13,NULL,88,"2010-07-27 18:06:52","2011-06-02 11:09:11",643,NULL,NULL,NULL
659,2,NULL,"2010-07-26 19:58:28",2,NULL,"<p>The principal components of a data matrix are the eigenvector-eigenvalue pairs of its variance-covariance matrix.  In essence, they are the decorrelated pieces of the variance.  Each one is a linear combination of the variables for an observation -- suppose you measure w, x, y,z on each of a bunch of subjects.  Your first PC might work out to be something like</p>

<p>0.5w + 4x + 5y - 1.5z</p>

<p>The loadings (eigenvectors) here are (0.5, 4, 5, -1.5).  The score (eigenvalue) for each observation is the resulting value when you substitute in the observed (w, x, y, z) and compute the total.</p>

<p>This comes in handy when you project things onto their principal components (for, say, outlier detection) because you just plot the scores on each like you would any other data.  This can reveal a lot about your data if much of the variance is correlated (== in the first few PCs).</p>
",317,"2010-07-26 19:58:28",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,222,NULL,NULL,NULL
660,1,677,"2010-07-26 20:03:02",7,510,"<p>I am collecting textual data surrounding press releases, blog posts, reviews, etc of certain companies' products and performance.</p>

<p>Specifically, I am looking to see if there are correlations between certain <em>types</em> and/or <em>sources</em> of such "textual" content with market valuations of the companies' stock symbols.</p>

<p>Such <em>apparent</em> correlations can be found by the human mind fairly quickly - but that is not scalable. How can I go about automating such analysis of disparate sources?</p>
",292,"2013-10-04 06:19:18","Automating statistical correlation between "texts" and "data"",<finance><correlation><text-mining>,2,3,5,NULL,NULL,NULL,NULL,NULL,NULL,NULL
661,2,NULL,"2010-07-26 20:06:09",7,NULL,"<p>Sam Savage's book <a href="http://rads.stackoverflow.com/amzn/click/0471381977">Flaw of Averages</a> is filled with good layman explanations of statistical concepts.  In particular, he has a good explanation of Jensen's inequality.  If the graph of your return on an investment is convex, i.e. it "smiles at you", then randomness is in your favor: your average return is greater than your return at the average.</p>
",319,"2010-07-26 20:06:09",NULL,NULL,NULL,0,NULL,NULL,NULL,"2012-08-21 15:17:25",155,NULL,NULL,NULL
662,2,NULL,"2010-07-26 20:08:09",8,NULL,"<p><a href="http://www.math.umass.edu/~lavine/Book/book.html">Introduction to Statistical Thought</a></p>
",319,"2010-07-26 20:08:09",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-26 20:08:09",614,NULL,NULL,NULL
663,2,NULL,"2010-07-26 20:09:20",17,NULL,"<p><a href="http://www.math.umass.edu/~lavine/Book/book.html">Introduction to Statistical Thought</a></p>
",319,"2010-07-26 20:09:20",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-26 20:09:20",170,NULL,NULL,NULL
664,2,NULL,"2010-07-26 20:12:15",3,NULL,"<p>It's seldom useful to conclude that something is "random" in the abstract.  More often you want to test whether it has a certain kind of random structure.  For example, you might want to test whether something has a uniform distribution, with all values in a certain range equally likely.  Or you might want to test whether something has a normal distribution, etc.  To test whether data has a particular distribution, you can use a goodness of fit test such as the chi square test or the Kolmogorov-Smirnov test.</p>
",319,"2010-07-26 20:12:15",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,30,NULL,NULL,NULL
665,1,NULL,"2010-07-26 20:17:17",39,11200,"<p>What's the difference between probability and statistics, and why are they studied together?</p>
",327,"2013-02-12 17:26:24","What's the difference between probability and statistics?",<probability><teaching><mathematical-statistics>,16,0,18,2645,"2011-03-20 16:07:28",NULL,NULL,NULL,NULL,NULL
667,2,NULL,"2010-07-26 20:18:46",6,NULL,"<p>Probability is a pure science (math), statistics is about data. They are connected since probability forms some kind of fundament for statistics, providing basic ideas.</p>
",88,"2010-07-26 20:18:46",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,665,NULL,NULL,NULL
668,2,NULL,"2010-07-26 20:19:12",4,NULL,"<p>If you already know another programming language, <a href="http://www.johndcook.com/R_language_for_programmers.html" rel="nofollow">these notes</a> may help point out some of the ways R might surprise you.</p>
",319,"2010-07-26 20:19:12",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
669,2,NULL,"2010-07-26 20:22:30",4,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/0471381977">The Flaw of Averages</a> by Sam Savage.</p>
",319,"2010-07-26 20:22:30",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-26 20:22:30",421,NULL,NULL,NULL
671,2,NULL,"2010-07-26 20:25:16",7,NULL,"<p>If you have experience in other languages, these "R Rosetta Stone" videos may be useful:</p>

<ol>
<li><a href="http://www.vcasmo.com/video/drewconway/7183">Python</a></li>
<li><a href="http://www.vcasmo.com/video/drewconway/7211">MATLAB</a></li>
<li><a href="http://www.vcasmo.com/video/drewconway/7210">SQL</a></li>
</ol>

<p>These are all included in the <a href="http://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html">video list added by Jeromy</a>, so big +1 for his list.</p>
",302,"2010-07-26 20:25:16",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
672,1,824,"2010-07-26 20:30:36",21,5405,"<p>What are the main ideas, that is, concepts related to <a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes' theorem</a>?
I am not asking for any derivations of complex mathematical notation.</p>
",333,"2011-11-11 10:10:11","What is Bayes' theorem all about?",<probability><bayesian><theory>,8,2,6,509,"2011-02-02 19:00:52",NULL,NULL,NULL,NULL,NULL
673,2,NULL,"2010-07-26 20:34:45",8,NULL,"<p>Table 3.1 of <a href="http://www.intuitivebiostatistics.com"><em>Intuitive Biostatistics</em></a> answers this question with the diagram shown below. Note that all the arrows point to the right for probability, and point to the left for statistics.</p>

<p>PROBABILITY</p>

<blockquote>
  <p>General ---> Specific</p>
  
  <p>Population ---> Sample</p>
  
  <p>Model ---> Data</p>
</blockquote>

<p>STATISTICS</p>

<blockquote>
  <p>General &lt;--- Specific</p>
  
  <p>Population &lt;--- Sample</p>
  
  <p>Model &lt;--- Data</p>
</blockquote>
",25,"2010-07-26 20:34:45",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,665,NULL,NULL,NULL
674,2,NULL,"2010-07-26 20:45:36",4,NULL,"<p>Probability is about quantifying uncertainty whereas statistics is explaining the variation in some measure of interest (e.g., why do income levels vary?) that we observe in the real world. </p>

<p>We explain the variation by using some observable factors (e.g., gender, education level, age etc for the income example). However, since we cannot possibly take into account all possible factors that affect income, we leave any unexplained variation to random errors (which is where quantifying uncertainty comes in).</p>

<p>Since, we attribute "Variation = Effect of Observable Factors + Effect of Random Errors" we need the tools provided by probability to account for the effect of random errors on the variation that we observe.</p>

<p>Some examples follow:</p>

<p><strong>Quantifying Uncertainty</strong></p>

<p>Example 1: You roll a 6-sided dice. What is the probability of obtaining a 1?</p>

<p>Example 2: What is the probability that the annual income of an adult person selected at random from the United States is less than $40,000?</p>

<p><strong>Explaining Variation</strong></p>

<p>Example 1: We observe that the annual income of a person varies. What factors explain the variation in a person's income? </p>

<p>Clearly, we cannot account for all factors. Thus, we attribute a person's income to some observable factors (e.g, education level, gender, age etc) and leave any remaining variation to uncertainty (or in the language of statistics: to random errors). </p>

<p>Example 2: We observe that some consumers choose Tide most of the time they buy a detergent whereas some other consumers choose detergent brand xyz. What explains the variation in choice? We attribute the variation in choices to some observable factors such as price, brand name etc and leave any unexplained variation to random errors (or uncertainty).</p>
",NULL,"2010-07-26 21:01:25",NULL,NULL,NULL,2,NULL,NULL,"2010-07-26 21:01:25",NULL,665,NULL,user28,user28
675,2,NULL,"2010-07-26 20:47:19",49,NULL,"<p>The short answer to this I've heard from Persi Diaconis is the following: the problems considered by probability and statistics are inverse to each other.  In probability theory we consider some underlying process which has some randomness or uncertainty modeled by random variables, and we figure out what happens.  In statistics we observe something that has happened, and try to figure out what underlying process would explain those observations.</p>
",89,"2010-07-26 20:47:19",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,665,NULL,NULL,NULL
676,2,NULL,"2010-07-26 20:49:33",2,NULL,"<p>Well my Prof. used these in Introductory probability class:</p>

<p>1) Shoe size are correlated with reading ability</p>

<p>2) Shark attack is correlated with sale of ice cream. </p>
",288,"2010-07-26 20:49:33",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-16 13:01:42",36,NULL,NULL,NULL
677,2,NULL,"2010-07-26 20:56:22",5,NULL,"<p>My students do this as their class project. A few teams hit the 70%s for accuracy, with pretty small samples, which ain't bad. </p>

<p>Let's say you have some data like this:</p>

<pre><code>Return Symbol News Text
-4%  DELL   Centegra and Dell Services recognized with Outsourcing Center's...
7%   MSFT   Rising Service Revenues Benefit VMWare
1%   CSCO   Cisco Systems (CSCO) Receives 5 Star Strong Buy Rating From S&amp;P
4%   GOOG   Summary Box: Google eyes more government deals
7%   AAPL   Sohu says 2nd-quarter net income rises 10 percent on higher...
</code></pre>

<p>You want to predict the return based on the text.</p>

<p>This is called Text Mining. </p>

<p>What you do ultimately is create an enormous matrix like this:</p>

<pre><code>Return Centegra Rising Services Recognized...
-4%    0.23     0      0.11     0.34
7%     0        0.1    0.23     0
...
</code></pre>

<p>That has one column for every unique word, and one row for each return, and a weighted score for each word. The score is often the TFIDF score, or relative frequency of the word in the doc. </p>

<p>Then you run a regression and see if you can predict which words predict the return. You'll probably need to use PCA first. </p>

<p>Book: Fundamentals of Predictive Text Mining, Weiss</p>

<p>Software: RapidMiner with Text Plugin or R</p>

<p>You should also do a search on Google Scholar and read up on the ins and outs. </p>

<p>You can see my series of text mining videos <a href="http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html" rel="nofollow">here</a></p>
",74,"2013-10-04 06:19:18",NULL,NULL,NULL,1,NULL,74,"2013-10-04 06:19:18",NULL,660,NULL,NULL,NULL
678,2,NULL,"2010-07-26 21:00:00",3,NULL,"<p>The <strong>probability</strong> of an event is its long-run relative frequency. So it's basically telling you the <strong><em>chance</em></strong> of, for example, getting a 'head' on the next flip of a coin, or getting a '3' on the next roll of a die.</p>

<p>A <strong>statistic</strong> is any numerical measure computed from a sample of the population. For example, the sample mean. We use this as a statistic which estimates the population mean, which is a parameter. So basically it's giving you some kind of <strong><em>summary</em></strong> of a sample.</p>

<ul>
<li><em>You can only get a statistic from a
sample, otherwise if you compute a
numerical measure on a population, it
is called a population parameter.</em></li>
</ul>
",81,"2010-07-26 21:05:06",NULL,NULL,NULL,0,NULL,81,"2010-07-26 21:05:06",NULL,665,NULL,NULL,NULL
680,2,NULL,"2010-07-26 21:11:25",17,NULL,"<p>If you have large data sets - ones that make Excel or Notepad load slowly, then a database is a good way to go. Postgres is open-source and very well-made, and it's easy to connect with JMP, SPSS and other programs. You may want to sample in this case. You don't have to normalize the data in the database. Otherwise, CSV is sharing-friendly. </p>

<p>Consider Apache Hive if you have 100M+ rows. </p>

<p>In terms of analysis, here are some starting points:</p>

<p><strong>Describe one variable:</strong></p>

<ul>
<li>Histogram  </li>
<li>Summary statistics (mean, range, standard deviation, min, max, etc)</li>
<li>Are there outliers? (greater than 1.5x inter-quartile range)</li>
<li>What sort of distribution does it follow? (normal, etc)</li>
</ul>

<p><strong>Describe relationship between variables:</strong></p>

<ul>
<li>Scatter Plot  </li>
<li>Correlation  </li>
<li><p>Outliers? check out Mahalanobis distance</p></li>
<li><p>Mosaic plot for categorical  </p></li>
<li>Contingency table for categorical  </li>
</ul>

<p><strong>Predict a real number (like price): regression</strong></p>

<ul>
<li><p>OLS regression or machine learning regression techniques</p></li>
<li><p>when the technique used to predict is understandable by humans, this is called modeling. For example, a neural network can make predictions, but is generally not understandable. You can use regression to find Key Performance Indicators too. </p></li>
</ul>

<p><strong>Predict class membership or probability of class membership (like passed/failed): classification</strong></p>

<ul>
<li>logistic regression or machine learning techniques, such as SVM</li>
</ul>

<p><strong>Put observations into "natural" groups: clustering</strong></p>

<ul>
<li>Generally one finds "similar" observations by calculating the distance between them. </li>
</ul>

<p><strong>Put attributes into "natural" groups: factoring</strong></p>

<ul>
<li>And other matrix operations such as PCA, NMF</li>
</ul>

<p><strong>Quantifying Risk</strong> = Standard Deviation, or proportion of times that "bad things" happen x how bad they are</p>

<p><strong>Likelihood of a successfully completed iteration given x number of story points</strong> = Logistic Regression</p>

<p>Good luck!</p>
",74,"2012-07-22 11:15:41",NULL,NULL,NULL,2,NULL,74,"2012-07-22 11:15:41",NULL,645,NULL,NULL,NULL
681,2,NULL,"2010-07-26 21:13:43",2,NULL,"<p>You don't necessarily have to go Bayesian on your model, plain maximum likelihood estimation works just fine (though has no explicit solution). Multiple R packages (eg. aod or VGAM) will fit the distribution for you.</p>

<p>Alternatively, you can use the quasi-likelihood based overdispersed binomial model that does not assume a beta-binomial distribution, just adjusts for the overdispersion. The <code>glm</code> function with the <code>quasibinomial</code> family will fit this model in R.</p>
",279,"2010-07-26 21:13:43",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,288,NULL,NULL,NULL
682,2,NULL,"2010-07-26 21:17:29",6,NULL,"<p>I didn't find <em>How To Measure Anything</em>, nor <em>Head First</em>, particularly good.</p>

<p><em>Statistics In Plain English</em> (Urdan) is a good starter book.</p>

<p>Once you finish that, <em>Multivariate Data Analysis</em> (Joseph Hair <em>et al.</em>) is fantastic.</p>

<p>Good luck!</p>
",74,"2013-05-27 02:35:13",NULL,NULL,NULL,3,NULL,74,"2013-05-27 02:35:13","2010-07-26 21:17:29",652,NULL,NULL,NULL
683,2,NULL,"2010-07-26 21:34:14",2,NULL,"<p>If your interested in the mathematical statistic around entropy, you may consult this book </p>

<p><a href="http://www.renyi.hu/~csiszar/Publications/Information_Theory_and_Statistics:_A_Tutorial.pdf" rel="nofollow">http://www.renyi.hu/~csiszar/Publications/Information_Theory_and_Statistics:_A_Tutorial.pdf</a></p>

<p>it is freely available ! </p>
",223,"2010-09-02 09:21:53",NULL,NULL,NULL,0,NULL,223,"2010-09-02 09:21:53",NULL,322,NULL,NULL,NULL
684,2,NULL,"2010-07-26 21:35:29",5,NULL,"<p>There are two main schools of thought is Statistics: <a href="http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english">frequentist and Bayesian</a>. </p>

<p>Bayes theorem is to do with the latter and can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence. This is known as conditional probability. You might want to look at <a href="http://stattrek.com/Lesson1/Bayes.aspx" rel="nofollow">this</a> to get a handle on the math.</p>
",81,"2011-11-11 10:10:11",NULL,NULL,NULL,6,NULL,81,"2011-11-11 10:10:11",NULL,672,NULL,NULL,NULL
685,1,NULL,"2010-07-26 21:40:20",-3,288,"<p>Is there something about statistics that lends itself to this sort of saying, or is it just that people will say anything to support their case, and this includes citing irrelevant or incomplete statistics?</p>
",327,"2010-07-26 22:06:32","Lies, Damn Lies and Statistics",<theory>,1,5,NULL,NULL,NULL,NULL,NULL,"2010-07-29 23:45:11",NULL,NULL
686,2,NULL,"2010-07-26 21:58:37",8,NULL,"<p>Check out the following links. I'm not sure what exactly are you looking for.</p>

<p><a href="http://videolectures.net/mlss08au_freitas_asm/">Monte Carlo Simulation for Statistical Inference</a></p>

<p><a href="http://videolectures.net/mlss08au_smola_ksvm/">Kernel methods and Support Vector Machines</a></p>

<p><a href="http://videolectures.net/epsrcws08_campbell_isvm/">Introduction to Support Vector Machines</a></p>

<p><a href="http://academicearth.org/lectures/monte-carlo-simulations-application-to-lattice-models">Monte Carlo Simulations</a></p>

<p><a href="http://freescienceonline.blogspot.com/2009_09_01_archive.html">Free Science and Video Lectures Online!</a></p>

<p><a href="http://videolectures.net/Top/Computer_Science/Machine_Learning/">Video lectures on Machine Learning</a></p>
",339,"2010-07-26 21:58:37",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-26 21:58:37",485,NULL,NULL,NULL
687,2,NULL,"2010-07-26 22:00:01",2,NULL,"<p>Statistics is about inferring something about a population, and that  requires some level of interpretation.</p>

<p>More intuitively, "is the glass half full or half empty?". They both mean the same thing, but  may have a different effect on the person who hears it.</p>

<p>So I would say it's the interpretation aspect which is the problem</p>

<p>P.S. There's an interesting article on the <a href="http://www.bbc.co.uk/dna/h2g2/A1091350" rel="nofollow">BBC website</a> which may be worth a read.</p>

<p>P.P.S. If you meant this more generally, then there could be a case for saying that the frequentest approach to statistics can give a different result to the Bayesian approach.</p>
",81,"2010-07-26 22:00:01",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,685,NULL,NULL,NULL
689,2,NULL,"2010-07-26 22:12:58",7,NULL,"<p>See <a href="http://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3a+jeromyanglim+%28Jeromy+Anglim%27s+Blog%3a+Psychology+and+Statistics%29" rel="nofollow">Videos on data analysis using R</a> on Jeromy Anglim's blog.  There are many links at that page and he updates it.  He has another post with many links to videos on probability and statistics as well as linear algebra and calculus.</p>
",NULL,"2010-07-26 22:12:58",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-26 22:12:58",485,NULL,"Michael Bishop",NULL
691,2,NULL,"2010-07-26 22:22:21",8,NULL,"<p>Just so people know, there is a Math Overflow question on the same topic.</p>

<p><a href="http://mathoverflow.net/questions/1048/why-is-it-so-cool-to-square-numbers-in-terms-of-finding-the-standard-deviation">Why-is-it-so-cool-to-square-numbers-in-terms-of-finding-the-standard-deviation</a></p>

<p>The take away message is that using the square root of the variance leads to easier maths. A similar response is given by Rich and Reed above. </p>
",352,"2010-07-26 22:22:21",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,118,NULL,NULL,NULL
692,1,741,"2010-07-26 22:29:48",9,332,"<p>Oversimplifying a bit, I have about a million records that record the entry time and exit time of people in a system spanning about ten years.  Every record has an entry time, but not every record has an exit time.  The mean time in the system is ~1 year. </p>

<p>The missing exit times happen for two reasons:</p>

<ol>
<li>The person has not left the system at the time the data was captured.</li>
<li>The person's exit time was not recorded. This happens to say 50% of the records</li>
</ol>

<p>The questions of interest are:</p>

<ol>
<li>Are people spending less time in the system, and how much less time.</li>
<li>Are more exit times being recorded, and how many.</li>
</ol>

<p>We can model this by saying that the probability that an exit gets recorded varies linearly with time, and that the time in the system has a Weibull whose parameters vary linearly with time.  We can then make a maximum likelihood estimate of the various parameters and eyeball the results and deem them plausible.  We chose the Weibull distribution because it seems to be used in measuring lifetimes and is fun to say as opposed to fitting the data better than say a gamma distribution.</p>

<p>Where should I look to get a clue as to how to do this correctly?  We are somewhat mathematically savvy, but not extremely statistically savvy.   </p>
",72,"2010-09-16 12:35:10","How do I determine if a survival model with missing data is appropriate?",<survival><missing-data>,2,0,2,88,"2010-09-16 12:35:10",NULL,NULL,NULL,NULL,NULL
693,2,NULL,"2010-07-26 22:48:51",32,NULL,"<p>I like the example of a jar of red and green jelly beans.  </p>

<p>A probabilist starts by knowing the proportion of each and asks the probability of drawing a red jelly bean.  A statistician infers the proportion of red jelly beans by sampling from the jar.</p>
",319,"2010-07-26 22:48:51",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,665,NULL,NULL,NULL
694,2,NULL,"2010-07-26 23:04:39",66,NULL,"<p>This isn't technically a cartoon, but close enough:</p>

<p><img src="http://pics.livejournal.com/tongodeon/pic/000598dc" alt="alt text"></p>
",74,"2012-08-22 17:55:28",NULL,NULL,NULL,1,NULL,74,"2012-08-22 17:55:28","2010-07-26 23:04:39",423,NULL,NULL,NULL
695,2,NULL,"2010-07-26 23:10:16",3,NULL,"<p>My favourite book on Statistics is is David William's <a href="http://amzn.to/aRoxQq" rel="nofollow">Weighing the Odds</a>. Davison's <a href="http://amzn.to/90gOnD" rel="nofollow">Statistical Models</a> is good too.</p>
",173,"2010-07-26 23:10:16",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-26 23:10:16",652,NULL,NULL,NULL
696,2,NULL,"2010-07-26 23:22:39",1,NULL,"<p>I think that my question is subsumed by this more general discussion: <a href="http://stats.stackexchange.com/questions/411/motivation-for-kolmogorov-distance-between-distributions" rel="nofollow">http://stats.stackexchange.com/questions/411/motivation-for-kolmogorov-distance-between-distributions</a></p>
",173,"2010-07-26 23:22:39",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,195,NULL,NULL,NULL
697,2,NULL,"2010-07-26 23:48:24",1,NULL,"<p>As per above, you need a set of articles and responses, and then you train eg. a Neural Net to them. RapidMiner will let you do this but there are many other tools out there that will let you do regressions of this size. Ideally your response variable will be consistent (ie % change after 1 hour exactly, or % change after 1 day exactly etc). </p>

<p>You may also want to apply some sort of filtering or classification to your training variables ie the words in the article. This could be as simple as filtering some words (eg prepositions, pronouns) or more complex like using syntax to choose which words should go into the regression. Note that any filtering you do risks biasing the result.</p>

<p>Some folks at University of Arizona already made a system that does this - their paper is on acm here and you may find it interesting. <a href="http://www.computer.org/portal/web/csdl/doi/10.1109/MC.2010.2" rel="nofollow">http://www.computer.org/portal/web/csdl/doi/10.1109/MC.2010.2</a> (you'll need a subscription to access if you're not eg at university). The references may also help point you in the right direction.</p>
",367,"2010-07-26 23:48:24",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,660,NULL,NULL,NULL
699,2,NULL,"2010-07-27 00:01:58",7,NULL,"<p>The <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Linear_general_selection_algorithm_-_Median_of_Medians_algorithm">Rivest-Tarjan-Selection algorithm</a> (sometimes also called the median-of-medians algorithm) will let you compute the median element in linear-time without any sorting. For large data sets this is can be quite a bit faster than log-linear sorting. However, it won't solve your memory storage problem.</p>
",352,"2010-07-27 00:01:58",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,346,NULL,NULL,NULL
700,2,NULL,"2010-07-27 00:21:32",4,NULL,"<p>These <a href="http://www.statslab.cam.ac.uk/~yms/ICL0706.ps" rel="nofollow">lecture notes</a> on information theory by O. Johnson contain a good introduction to different kinds of entropy.</p>
",368,"2010-07-27 00:21:32",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,322,NULL,NULL,NULL
701,2,NULL,"2010-07-27 00:24:09",2,NULL,"<p>Because squares can allow use of many other mathematical operations or functions more easily than absolute values.</p>

<p>Example: squares can be integrated, differentiated, can be used in trigonometric, logarithmic and other functions, with ease.</p>
",369,"2010-07-27 00:24:09",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,118,NULL,NULL,NULL
702,2,NULL,"2010-07-27 00:37:33",2,NULL,"<p>You could use the estimated model to predict the exit times for all the people in your system. You could then compare the estimated exit times with the actual exit times (where you have this data) and compute a metric such as <a href="http://en.wikipedia.org/wiki/Root_mean_square_deviation" rel="nofollow">RMSE</a> to assess how good your predictions are which will in turn give you a sense of model fit. See also this <a href="http://www.statsoft.com/textbook/survival-failure-time-analysis/#dgoodness" rel="nofollow">link</a>.</p>
",NULL,"2010-07-27 00:37:33",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,692,NULL,user28,NULL
703,2,NULL,"2010-07-27 00:47:25",3,NULL,"<p>All three are used when dealing with nuisance parameters in the completely specified likelihood function.  </p>

<p>The marginal likelihood  is the primary method to eliminate nuisance parameters in theory.  It's a true likelihood function (i.e. it's proportional to the (marginal) probability of the observed data).</p>

<p>The partial likelihood is not a true likelihood in general.  However, in some cases it can be treated as a likelihood for asymptotic inference.  For example in Cox proportional hazards models, where it originated, we're interested in the observed rankings in the data (T1 > T2 > ..) without specifying the baseline hazard.  Efron showed that the partial likelihood loses little to no information for a variety of hazard functions.</p>

<p>The profile likelihood is convenient when we have a multidimensional likelihood function and a single parameter of interest.  It's specified by replacing the nuisance S by its MLE at each fixed T (the parameter of interest), i.e. L(T) = L(T, S(T)).  This can work well in practice, though there is a potential bias in the MLE obtained in this way; the marginal likelihood corrects for this bias.</p>
",251,"2010-07-27 00:47:25",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,622,NULL,NULL,NULL
704,2,NULL,"2010-07-27 00:54:58",0,NULL,"<p>Here's an answer to the question asked on stackoverflow: <a href="http://stackoverflow.com/questions/1058813/on-line-iterator-algorithms-for-estimating-statistical-median-mode-skewness/2144754#2144754">http://stackoverflow.com/questions/1058813/on-line-iterator-algorithms-for-estimating-statistical-median-mode-skewness/2144754#2144754</a></p>

<p>The iterative update median += eta * sgn(sample - median) sounds like it could be a way to go.</p>
",NULL,"2010-07-27 00:54:58",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,346,NULL,"Guy Srinivasan",NULL
705,2,NULL,"2010-07-27 01:07:48",4,NULL,"<p>This is a very interesting question. Suppose that we have a 2 dimensional covariance matrix (very unrealistic example for SEM but please bear with me). Then you can plot the iso-contours for the observed covariance matrix vis-a-vis the estimated covariance matrix to get a sense of model fit.</p>

<p>However, in reality you will a high-dimensional covariance matrix. In such a situation, you could probably do several 2 dimensional plots taking 2 variables at a time. Not the ideal solution but perhaps may help to some extent.  </p>

<p><strong>Edit</strong></p>

<p>A slightly better method is to perform <a href="http://en.wikipedia.org/wiki/Principal_component_analysis" rel="nofollow">Principal Component Analysis (PCA)</a> on the observed covariance matrix. Save the projection matrix from the PCA analysis on the observed covariance matrix. Use this projection matrix to transform the estimated covariance matrix. </p>

<p>We then plot iso-contours for the two highest variances of the rotated observed covariance matrix vis-a-vis the estimated covariance matrix. Depending on how many plots we want to do we can take the second and the third highest variances etc. We start from the highest variances as we want to explain as much variation in our data as possible. </p>
",NULL,"2010-07-27 02:14:37",NULL,NULL,NULL,3,NULL,NULL,"2010-07-27 02:14:37",NULL,570,NULL,user28,user28
706,2,NULL,"2010-07-27 01:51:15",8,NULL,"<p>Yet another reason (in addition to the excellent ones above) comes from Fisher himself, who showed that the standard deviation is more "efficient" than the absolute deviation. Here, efficient has to do with how much a statistic will fluctuate in value on different samplings from a population. If your population is normally distributed, the standard deviation of various samples from that population will, on average, tend to give you values that are pretty similar to each other, whereas the absolute deviation will give you numbers that spread out a bit more. Now, obviously this is in ideal circumstances, but this reason convinced a lot of people (along with the math being cleaner), so most people worked with standard deviations.</p>
",378,"2010-07-27 01:51:15",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,118,NULL,NULL,NULL
707,2,NULL,"2010-07-27 02:19:56",4,NULL,"<p>As an extension on the k-fold answer, the "usual" choice of k is either 5 or 10. The leave-one-out method has a tendency to produce models that are too conservative. FYI, here is a reference on that fact: </p>

<p>Shao, J. (1993), Linear Model Selection by Cross-Validation, Journal of the American
Statistical Association, Vol. 88, No. 422, pp. 486-494</p>
",188,"2010-07-27 02:19:56",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,638,NULL,NULL,NULL
708,2,NULL,"2010-07-27 02:31:38",3,NULL,"<p>Let me give you a very very intuitional insight.  Suppose you are tossing a coin 10 times and you get 8 heads and 2 tails. The question that would come to your mind is whether this coin is biased towards heads or not. </p>

<p>Now if you go by conventional definitions or the frequentist approach of probability you might say that the coin is unbiased and this is an exceptional occurrence. Hence you would conclude that the possibility of getting a head next toss is also 50%. </p>

<p>But suppose you are a Bayesian.  You would actually think that since you have got exceptionally high number of heads, the coin has a bias towards the head side. There are methods to calculate this possible bias. You would calculate them and then when you toss the coin next time, you would definitely call a heads.</p>

<p>So, Bayesian probability is about the belief that you develop based on the data you observe. I hope that was simple enough.</p>
",25692,"2011-02-02 19:11:03",NULL,NULL,NULL,1,NULL,509,"2011-02-02 19:11:03",NULL,672,NULL,NULL,NULL
709,2,NULL,"2010-07-27 03:46:45",8,NULL,"<p><a href="http://cgibbons.us/courses/econ140/IVSlides.pdf" rel="nofollow">Here are some slides that I prepared for an econometrics course at UC Berkeley.</a> I hope that you find them useful---I believe that they answer your questions and provide some examples.</p>

<p>Charlie</p>
",401,"2014-02-07 23:26:06",NULL,NULL,NULL,3,NULL,401,"2014-02-07 23:26:06",NULL,563,NULL,NULL,NULL
710,2,NULL,"2010-07-27 03:54:35",3,NULL,"<p>My understanding is that the distinction between PCA and Factor analysis primarily is in whether there is an error term. Thus PCA can, and will, faithfully represent the data whereas factor analysis is less faithful to the data it is trained on but attempts to represent underlying trends or communality in the data.  Under a standard approach PCA is not rotated, but it is mathematically possible to do so, so people do it from time to time.  I agree with the commenters in that the "meaning" of these methods is somewhat up for grabs and that it probably is wise to be sure the function you are using does what you intend - for example, as you note R has some functions that perform a different sort of PCA than users of SPSS are familiar with.</p>
",196,"2010-07-27 03:54:35",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,612,NULL,NULL,NULL
711,2,NULL,"2010-07-27 03:54:51",7,NULL,"<p>Using robust standard errors has become common practice in economics. Robust standard errors are typically larger than non-robust (standard?) standard errors, so the practice can be viewed as an effort to be conservative. </p>

<p>In large samples (<i>e.g.,</i> if you are working with Census data with millions of observations or data sets with "just" thousands of observations), heteroskedasticity tests will almost surely turn up positive, so this approach is appropriate. </p>

<p>Another means for combating heteroskedasticity is weighted least squares, but this approach has become looked down upon because it changes the estimates for parameters, unlike the use of robust standard errors. If your weights are incorrect, your estimates are biased. If your weights are right, however, you get smaller ("more efficient") standard errors than OLS with robust standard errors.</p>

<p>Charlie</p>
",401,"2010-07-27 03:54:51",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,452,NULL,NULL,NULL
712,1,NULL,"2010-07-27 03:58:22",12,1343,"<p>Is anyone aware of good data anonymization software? Or perhaps a package for R that does data anonymization? Obviously not expecting uncrackable anonymization - just want to make it difficult. </p>
",402,"2012-02-11 17:26:35","Data anonymization software",<software>,4,1,2,930,"2011-05-01 17:23:11",NULL,NULL,NULL,NULL,NULL
713,2,NULL,"2010-07-27 04:04:15",3,NULL,"<p>Naturally you can describe dispersion of a distribution in any way meaningful (absolute deviation, quantiles, etc.). </p>

<p>One nice fact is that the variance is the second central moment, and every distribution is uniquely described by its moments if they exist.
Another nice fact is that the variance is much more tractable mathematically than any comparable metric.
Another fact is that the variance is one of two parameters of the normal distribution for the usual parametrization, and the normal distribution only has 2 non-zero central moments which are those two very parameters. Even for non-normal distributions it can be helpful to think in a normal framework.</p>

<p>As I see it, the reason the standard deviation exists as such is that in applications the square-root of the variance regularly appears (such as to standardize a random varianble), which necessitated a name for it.</p>
",NULL,"2010-07-27 04:04:15",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,118,NULL,arik,NULL
714,2,NULL,"2010-07-27 04:27:33",6,NULL,"<p>data.table is my favorite now! Very look forward to the new version with the more wishlist implemented. </p>
",NULL,"2010-07-27 04:27:33",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 04:27:33",73,NULL,Branson,NULL
715,1,NULL,"2010-07-27 04:42:25",2,594,"<p>I am developing a multi-class perceptron algorithm and was wondering if there are any datasets that could be used to test a multi-class perceptron? - A dataset where the classes are linearly separable and have at least 100 or more instances for training?</p>
",130,"2010-08-30 15:13:35","Dataset for multi class perceptron",<classification><dataset><multivariable>,1,0,NULL,442,"2010-08-30 15:13:35",NULL,NULL,NULL,NULL,NULL
717,2,NULL,"2010-07-27 05:04:45",1,NULL,"<p>I asked about why there was a difference between the average of the maximum of 100 draws from a random normal distribution and the 98th percentile of the normal distribution.  The answer I received from Rob Hyndman was mostly acceptable, but too technically dense to accept without revision.  I was left wondering whether it was possible to provide an answer that explains in intuitively understandable plain language why these two values are not equal.  </p>

<p>Ultimately, my answer may be unsatisfyingly circular; but conceptually, the reason max(rnorm(100)) tends to be higher than qnorm(.98) is, in short, because on average the highest of 100 random normally distributed scores will on occasion exceed its expected value.  However this distortion is non-symmetrical, since when low scores are drawn they are unlikely to end up being the highest out of the 100 scores.  Each independent draw is a new chance to exceed the expected value, or to be ignored because the obtained value isn't the maximum of the 100 drawn values.  For a visual demonstration compare the histogram of the maximum of 20 values to the histogram of the maximum of 100 values, the difference in skew, especially in the tails, is stark.</p>

<p>I arrived at this answer indirectly while working through a related problem/question I had asked in the comments.  Specifically, if I found that someone's test scores were ranked in the 95th percentile, I'd expect that on average if I put them in a room with 99 other test takers that their rank would average out to be 95.  This turns out to be more or less the case (R code)...</p>

<pre><code>for (i in 1:NSIM)
{
    rank[i] &lt;- seq(1,100)[order(c(qnorm(.95),rnorm(99)))==1]
}
summary(rank)
</code></pre>

<p>As an extension of that logic, I had likewise been expecting that if I took 100 people in a room and selected the person with 95th highest score, then took another 99 people and had them take the same test, that <em>on average</em> the selected person would be ranked 95th in the new group. But this is not the case (R code)...</p>

<pre><code>for (i in 1:NSIM)
{
    testtakers &lt;- rnorm(100)
    testtakers &lt;- testtakers[order(testtakers)]
    testtakers &lt;- testtakers[order(testtakers)]
    ranked95 &lt;- testtakers[95]
    rank[i] &lt;- seq(1,100)[order(c(ranked95,rnorm(99)))==1]
}
summary(rank)
</code></pre>

<p>What makes the first case different from the second case is that in the first case the individual's score places them at exactly the 95th percentile.  In the second case their score may turn out to be somewhat higher or lower than the true 95th percentile.  Since they can not possibly rank higher than 100, groups that produce a rank 95 score that is actually at the 99th percentile or higher can not offset (in terms of average rank) those cases where the rank 95 score is much lower than the true 90th percentile.  If you look at the histograms for the two rank vectors provided in this answer it is easy to see that there is a restriction of range in the upper ends that is a consequence of this process I have been describing.</p>
",196,"2010-07-27 05:04:45",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,225,NULL,NULL,NULL
718,2,NULL,"2010-07-27 05:23:38",5,NULL,"<p><a href="http://www.uvm.edu/~dhowell/StatPages/More_Stuff/RepMeasMultComp/RepMeasMultComp.html">This article</a> by David Howell explains the problems and several solutions.</p>
",25,"2010-07-27 05:23:38",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,575,NULL,NULL,NULL
720,2,NULL,"2010-07-27 05:36:14",18,NULL,"<p>All that matters is the difference between two AIC (or, better, AICc) values, representing the fit to two models.  The actual value of the AIC (or AICc), and whether it is positive or negative, means nothing. If you simply changed the units the data are expressed in, the AIC (and AICc) would change dramatically. But the difference between the AIC of the two alternative models would not change at all.</p>

<p>Bottom line: Ignore the actual value of AIC (or AICc) and whether it is positive or negative. Ignore also the ratio of two AIC (or AICc) values. Pay attention only to the difference.</p>
",25,"2010-07-27 05:36:14",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,486,NULL,NULL,NULL
721,2,NULL,"2010-07-27 05:40:44",4,NULL,"<p>The <a href="http://www.ats.ucla.edu/stat/" rel="nofollow">UCLA Statistical Computing</a> site has a number of examples in various languages (SAS, R, etc).  In particular, see the following pages (look among the links titled logistic regression, categorical data analysis and generalized linear models):</p>

<ul>
<li><a href="http://www.ats.ucla.edu/stat/dae/" rel="nofollow">Data Analysis Examples</a></li>
<li><a href="http://www.ats.ucla.edu/stat/examples/default.htm" rel="nofollow">Textbook Examples</a></li>
</ul>
",251,"2010-07-27 05:40:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,39,NULL,NULL,NULL
722,2,NULL,"2010-07-27 05:41:11",5,NULL,"<p>I've published a method for identifying outliers in nonlinear regression, and it can be also used when fitting a linear model.</p>

<p>HJ Motulsky and RE Brown. <a href="http://www.biomedcentral.com/1471-2105/7/123/abstract/">Detecting outliers when fitting data with nonlinear regression – a new method based on robust nonlinear regression and the false discovery rate</a>. BMC Bioinformatics 2006, 7:123</p>
",25,"2010-07-27 05:41:11",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,175,NULL,NULL,NULL
723,1,758,"2010-07-27 06:00:07",7,1836,"<p>I'm doing shopping cart analyses my dataset is set of transaction vectors, with the items the products being bought.</p>

<p>When applying k-means on the transactions, I will always get <em>some</em> result. A random matrix would probably also show some clusters.</p>

<p>Is there a way to test whether the clustering I find is a significant one, or that is can be very well be a coincidence. If yes, how can I do it.</p>
",190,"2010-07-27 14:44:54","How can I test whether my clustering of binary data is significant",<clustering><statistical-significance><binary-data>,3,0,10,NULL,NULL,NULL,NULL,NULL,NULL,NULL
724,2,NULL,"2010-07-27 06:01:10",9,NULL,"<p>The <a href="http://sourceforge.net/projects/anony-toolkit/">Cornell Anonymization Tookit</a> is open source.  Their <a href="http://www.cs.cornell.edu/bigreddata/privacy/">research page</a> has links to associated publications.</p>
",251,"2011-04-13 10:35:33",NULL,NULL,NULL,0,NULL,930,"2011-04-13 10:35:33",NULL,712,NULL,NULL,NULL
725,1,759,"2010-07-27 06:10:43",5,644,"<p>An <strong>hyperspectral image is</strong> a multidimensional image with more than 200 spectral bands i.e. an image for which each pixel is a vector of dimension 200 (most often it is a sampled spectral curve that is encoutered in satellite imagery or medical imagery). </p>

<p>What are the <strong>implemented package</strong> (I am especially interested in R packages but if other free algorithms exist, I will try them) for <strong>frontier detection</strong> and (unsupervised) <strong>segmentation</strong> of this type of images?  </p>
",223,"2011-01-25 20:40:17","Suggested R packages for frontier estimation or segmentation of hyperspectral images",<machine-learning><multivariate-analysis><image-processing>,4,0,NULL,88,"2010-12-17 10:22:25",NULL,NULL,NULL,NULL,NULL
726,1,NULL,"2010-07-27 06:20:38",124,34780,"<p>What is your favorite statistician quote? 
This is community wiki, so please one quote per answer.  </p>
",223,"2014-09-11 13:38:34","Famous statistician quotes",<big-list><history>,136,1,127,223,"2010-12-17 08:01:29","2010-07-27 06:20:38",NULL,NULL,NULL,NULL
727,2,NULL,"2010-07-27 06:23:57",61,NULL,"<blockquote>
  <p>The combination of some data and an
  aching desire for an answer does not
  ensure that a reasonable answer can be
  extracted from a given body of data</p>
</blockquote>

<p>Tukey</p>
",223,"2010-07-27 06:23:57",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-27 06:23:57",726,NULL,NULL,NULL
728,2,NULL,"2010-07-27 06:26:04",32,NULL,"<blockquote>
  <p>All we know about the world teaches us that the effects of A and B are always different---in some decimal place---for any A and B. Thus asking "are the effects different?" is foolish.</p>
</blockquote>

<p>Tukey (again but this one is my favorite)</p>
",223,"2010-07-27 06:42:07",NULL,NULL,NULL,2,NULL,159,"2010-07-27 06:42:07","2010-07-27 06:26:04",726,NULL,NULL,NULL
729,2,NULL,"2010-07-27 06:36:26",74,NULL,"<blockquote>
  <p>In God we trust. All others must bring
  data. </p>
</blockquote>

<p>(W. Edwards Deming)</p>
",159,"2010-07-31 00:19:53",NULL,NULL,NULL,6,NULL,461,"2010-07-31 00:19:53","2010-07-27 06:36:26",726,NULL,NULL,NULL
730,2,NULL,"2010-07-27 06:37:30",146,NULL,"<blockquote>
  <p>All models are wrong, but some are useful. (George E. P. Box)</p>
</blockquote>

<p>Reference: Box &amp; Draper (1987), <em>Empirical model-building and response surfaces</em>, Wiley, p. 424.</p>
",159,"2013-08-21 01:43:17",NULL,NULL,NULL,5,NULL,9007,"2013-08-21 01:43:17","2010-07-27 06:37:30",726,NULL,NULL,NULL
731,2,NULL,"2010-07-27 06:45:31",4,NULL,"<p>If you're looking at system faults, you might be interested in the following paper employing machine learning techniques for fault diagnosis at eBay.  It may give you a sense of what kind of data to collect or how one team approached a specific problem in a similar domain.</p>

<ul>
<li><a href="http://www.cs.berkeley.edu/~brewer/papers/icac2004_chen_diagnosis.pdf" rel="nofollow">Fault Diagnosis Using Decision Trees</a></li>
</ul>

<p>If you're just getting started, something like <a href="http://rapid-i.com/content/view/181/190/" rel="nofollow">RapidMiner</a> or <a href="http://www.ailab.si/orange/" rel="nofollow">Orange</a> might be a good software system to start playing with your data pretty quickly.  Both of them can access the data in a variety of formats (file csv, database, among others).  </p>
",251,"2010-07-27 06:45:31",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,645,NULL,NULL,NULL
732,2,NULL,"2010-07-27 06:51:24",57,NULL,"<blockquote>
  <p>Strange events permit themselves the
  luxury of occurring.</p>
</blockquote>

<p>-- <a href="http://gutenberg.net.au/ebooks02/0200691.txt">Charlie Chan</a></p>
",251,"2010-08-07 10:08:04",NULL,NULL,NULL,4,NULL,380,"2010-08-07 10:08:04","2010-07-27 06:51:24",726,NULL,NULL,NULL
733,2,NULL,"2010-07-27 06:52:09",3,NULL,"<p>There is something like <a href="http://en.wikipedia.org/wiki/Silhouette_%28clustering%29" rel="nofollow">silhouette</a>, which to some extent defines statistic that determines the cluster quality (for instance it is used in optimizing k). Now a possible Monte Carlo would go as follows: you generate a lot of random dataset similar to your original (for instance by shuffling values between rows in each column), cluster and obtain a distribution of mean silhouette that then may be used to test significance of silhouette in real data. Still I admin that I have never tried this idea.</p>
",88,"2010-07-27 06:52:09",NULL,NULL,NULL,4,NULL,NULL,NULL,NULL,723,NULL,NULL,NULL
734,2,NULL,"2010-07-27 06:55:06",2,NULL,"<p>Maybe the good old <a href="http://archive.ics.uci.edu/ml/datasets/Iris" rel="nofollow">iris</a>? It suits your needs and is good for start. </p>
",88,"2010-07-27 06:55:06",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,715,NULL,NULL,NULL
735,2,NULL,"2010-07-27 07:19:13",0,NULL,"<p>The one thing <a href="http://root.cern.ch" rel="nofollow">ROOT</a> is really good at is storing enourmous amounts of data. ROOT is a C++ library used in particle physics; it also comes with Ruby and Python bindings, so you could use packages in these languages (e.g. NumPy or Scipy) to analyze the data when you find that ROOT offers to few possibilities out-of-the-box.</p>

<p>The ROOT fileformat can store trees or tuples, and entries can be read sequentially, so you do not need to keep all data in memory at the same time. This allows to analyze petabytes of data, something you wouldn't want to try with Excel or R.</p>

<p>The ROOT I/O documentation can be reached from <a href="http://root.cern.ch/drupal/content/root-files-1" rel="nofollow">here</a>.</p>
",56,"2010-07-27 07:19:13",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,645,NULL,NULL,NULL
736,2,NULL,"2010-07-27 07:24:19",3,NULL,"<p>zoo and xts are a must in my work!</p>
",300,"2010-07-27 07:24:19",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 07:24:19",73,NULL,NULL,NULL
737,2,NULL,"2010-07-27 07:26:34",45,NULL,"<blockquote>
  <p>There are no routine statistical
  questions, only questionable
  statistical routines.</p>
</blockquote>

<p>D.R. Cox</p>
",NULL,"2010-08-07 10:09:08",NULL,NULL,NULL,2,NULL,380,"2010-08-07 10:09:08","2010-07-27 07:26:34",726,NULL,Tzippy,NULL
738,2,NULL,"2010-07-27 07:38:19",46,NULL,"<blockquote>
  <p>Say you were standing with one foot in the oven and one foot in an ice bucket.  According to the percentage people, you should be perfectly comfortable.  </p>
</blockquote>

<p>-Bobby Bragan, 1963</p>
",188,"2010-12-03 04:00:23",NULL,NULL,NULL,1,NULL,795,"2010-12-03 04:00:23","2010-07-27 07:38:19",726,NULL,NULL,NULL
739,2,NULL,"2010-07-27 07:41:56",94,NULL,"<blockquote>
  <p>"To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of."<br></p>
</blockquote>

<p>-- Ronald Fisher (1938)</p>

<p>The published quote can be read on page 17 of the article and is:</p>

<p>To consult the statistician after an experiment is finished is often merely to ask him to conduct a <em>post mortem</em> examination. He can perhaps say what the experiment died of. </p>

<p>R. A. Fisher. Presidential Address by Professor R. A. Fisher, Sc.D., F.R.S. <em>Sankhyā: The Indian Journal of Statistics</em> (1933-1960), Vol. 4, No. 1 (1938), pp. 14-17. 
<a href="http://www.jstor.org/stable/40383882">http://www.jstor.org/stable/40383882</a></p>
",NULL,"2012-10-12 18:46:16",NULL,NULL,NULL,3,NULL,-1,"2012-10-12 18:46:16","2010-07-27 07:41:56",726,NULL,sjcockell,sjcockell
741,2,NULL,"2010-07-27 07:56:42",4,NULL,"<p>The basic way to see if your data is Weibull is to <a href="http://www.itl.nist.gov/div898/handbook/eda/section3/weibplot.htm" rel="nofollow">plot</a> the log of cumulative hazards versus log of times and see if a straight line might be a good fit.  The cumulative hazard can be found using the non-parametric Nelson-Aalen estimator.  There are similar graphical <a href="http://www.weibull.com/hotwire/issue71/relbasics71.htm" rel="nofollow">diagnostics</a> for Weibull regression if you fit your data with covariates and some references follow.  </p>

<p>The <a href="http://www.powells.com/biblio/72-9780387239187-0" rel="nofollow">Klein &amp; Moeschberger</a> text is pretty good and covers a lot of ground with model building/diagnostics for parametric and semi-parametric models (though mostly the latter).  If you're working in R, <a href="http://www.powells.com/biblio/61-9780387987842-1" rel="nofollow">Theneau's book</a> is pretty good (I believe he wrote the <a href="http://cran.r-project.org/web/views/Survival.html" rel="nofollow">survival</a> package).  It covers a lot of Cox PH and associated models, but I don't recall if it has much coverage of parametric models, like the one you're building.</p>

<p>BTW, is this a million subjects each with one entry/exit or recurrent entry/exit events for some smaller pool of people? Are you conditioning your likelihood to account for the censoring mechanism?  </p>
",251,"2010-07-27 09:45:01",NULL,NULL,NULL,1,NULL,251,"2010-07-27 09:45:01",NULL,692,NULL,NULL,NULL
743,1,839,"2010-07-27 08:13:26",4,441,"<p>I was having a look round a few things yesturday and came across <a href="http://en.wikipedia.org/wiki/Bayesian_search_theory" rel="nofollow">Bayesian Search Theory</a>. Thinking about this theory led me to think about a problem I was working on a few years ago regarding geological interpretation. </p>

<p>We were looking at the geology at one specific site and it was essentially made up from two different types of rocks. Boreholes had been drilled at different locations and showed differing amounts of the two different types of rocks at different levels in the ground along with different amounts of weathering of the rock. A number of geologists looked at the available data and all came up with different interpretations. It seems to me that Bayesian Search Theory could have been used in this case, particualrly where extra data was gathered with time, to give some indication of how likely the different interpretations were. </p>

<p>Has anyone encountered a case where Bayesian Search Theory has been used in this case. Is there a standard frameowrk for doing this? I would have thought this may be something that the oil industry may have a lot of research on because it would be applicable to the search for oil. </p>
",210,"2010-07-27 22:33:05","Use of Bayesian Search Theory in geological interpretation",<bayesian><search-theory>,2,0,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
744,2,NULL,"2010-07-27 08:42:23",115,NULL,"<blockquote>
  <p>"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." -- John Tukey</p>
</blockquote>
",319,"2010-07-28 06:55:37",NULL,NULL,NULL,6,NULL,159,"2010-07-28 06:55:37","2010-07-27 08:42:23",726,NULL,NULL,NULL
745,2,NULL,"2010-07-27 08:42:25",2,NULL,"<p>I use </p>

<p>car, doBy, Epi, ggplot2, gregmisc (gdata, gmodels, gplots, gtools), Hmisc, plyr, RCurl, RDCOMClient, reshape, RODBC, TeachingDemos, XML.</p>

<p>a lot.</p>
",NULL,"2010-07-27 08:42:25",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 08:42:25",73,NULL,esco,NULL
746,2,NULL,"2010-07-27 08:43:15",6,NULL,"<blockquote>
  <p>efficiency = statistical efficiency x usage.</p>
</blockquote>

<p>-- John Tukey</p>
",319,"2010-12-03 04:03:20",NULL,NULL,NULL,0,NULL,795,"2010-12-03 04:03:20","2010-07-27 08:43:15",726,NULL,NULL,NULL
747,2,NULL,"2010-07-27 09:00:42",5,NULL,"<p>Packages I often use are <a href="http://cran.r-project.org/web/packages/raster/index.html" rel="nofollow">raster</a>, <a href="http://cran.r-project.org/web/packages/sp/index.html" rel="nofollow">sp</a>, <a href="http://cran.r-project.org/web/packages/spatstat/index.html" rel="nofollow">spatstat</a>, <a href="http://cran.r-project.org/package=vegan" rel="nofollow">vegan</a> and <a href="http://cran.r-project.org/web/packages/splancs/index.html" rel="nofollow">splancs</a>. I sometimes use ggplot2, tcltk and lattice.</p>
",144,"2010-07-27 09:00:42",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 09:00:42",73,NULL,NULL,NULL
748,2,NULL,"2010-07-27 09:09:25",58,NULL,"<blockquote>
  <p>A big computer, a complex algorithm and a long time does not equal science.</p>
</blockquote>

<p>-- Robert Gentleman</p>
",434,"2010-10-02 17:09:45",NULL,NULL,NULL,1,NULL,795,"2010-10-02 17:09:45","2010-07-27 09:09:25",726,NULL,NULL,NULL
749,2,NULL,"2010-07-27 09:10:20",11,NULL,"<p>Regarding shopping cart analysis, I think that the main objective is to individuate the most frequent combinations of products bought by the customers. The <code>association rules</code> represent the most natural methodology here (indeed they were actually developed for this purpose). Analysing the combinations of products bought by the customers, and the number of times these combinations are repeated, leads to a rule of the type ‘if condition, then result’ with a corresponding interestingness measurement. You may also consider <code>Log-linear models</code> in order to investigate the associations between the considered variables.</p>

<p>Now as for clustering, here are some information that may come in handy:</p>

<p>At first consider <code>Variable clustering</code>. Variable clustering is used for assessing collinearity, redundancy, and for separating variables into clusters that can be scored as a single variable, thus resulting in data reduction. Look for the <code>varclus</code> function (package Hmisc in R)</p>

<p>Assessment of the clusterwise stability: function <code>clusterboot</code> {R package  fpc}</p>

<p>Distance based statistics for cluster validation: function <code>cluster.stats</code> {R package  fpc}</p>

<p>As mbq have mentioned, use the silhouette widths for assessing the best number of clusters. Watch <a href="http://www.google.com/codesearch/p?hl=en#sTQFIWS4uR8/afs/sipb/project/r-project/arch/sun4x_510/lib/R/library/cluster/R-ex/pam.object.R&amp;q=lang%3ar%20%22optimal%20number%20of%20clusters%22&amp;sa=N&amp;cd=6&amp;ct=rc">this</a>. Regarding silhouette widths, see also the <a href="http://finzi.psych.upenn.edu/R/library/optpart/html/optsil.html">optsil</a> function. </p>

<p>Estimate the number of clusters in a data set via the <a href="http://finzi.psych.upenn.edu/R/library/clusterSim/html/index.GAP.html">gap statistic</a></p>

<p>For calculating Dissimilarity Indices and Distance Measures see <a href="http://finzi.psych.upenn.edu/R/library/labdsv/html/dsvdis.html">dsvdis</a> and <a href="http://finzi.psych.upenn.edu/R/library/vegan/html/vegdist.html">vegdist</a></p>

<p>EM clustering algorithm can decide how many clusters to create by cross validation, (if you can't specify apriori how many clusters to generate). <em>Although the EM algorithm is guaranteed to converge to a maximum, this is a local maximum and may not necessarily be the same as the global maximum. For a better chance of obtaining the global maximum, the whole procedure should be repeated several times, with different initial guesses for the parameter values. The overall log-likelihood figure can be used to compare the different final configurations obtained: just choose the largest of the local maxima</em>.
You can find an implementation of the EM clusterer in the open-source project <a href="http://www.cs.waikato.ac.nz/~ml/weka/">WEKA</a></p>

<p><a href="http://zoonek2.free.fr/UNIX/48_R/06.html">This</a> is also an interesting link.</p>

<p>Also search <a href="http://www.statsoft.com/textbook/cluster-analysis/#vfold">here</a> for <code>Finding the Right Number of Clusters in k-Means and EM Clustering: v-Fold Cross-Validation</code></p>

<p>Finally, you may explore clustering results using <a href="http://had.co.nz/model-vis/">clusterfly</a> </p>
",339,"2010-07-27 14:44:54",NULL,NULL,NULL,2,NULL,339,"2010-07-27 14:44:54",NULL,723,NULL,NULL,NULL
750,2,NULL,"2010-07-27 09:16:48",17,NULL,"<blockquote>
  <p>There are three kinds of lies: lies,
  damned lies, and statistics.</p>
</blockquote>

<p>-- <a href="https://secure.wikimedia.org/wikipedia/en/wiki/Lies,_damned_lies,_and_statistics">probably: Charles Wentworth Dilke (1843–1911).</a></p>
",127,"2010-07-27 09:16:48",NULL,NULL,NULL,6,NULL,NULL,NULL,"2010-07-27 09:16:48",726,NULL,NULL,NULL
751,2,NULL,"2010-07-27 09:19:53",4,NULL,"<blockquote>
  <p>The Median Isn't the Message</p>
</blockquote>

<p>--<a href="http://cancerguide.org/median_not_msg.html" rel="nofollow">Stephen Jay Gould</a></p>
",127,"2010-07-27 09:19:53",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-27 09:19:53",726,NULL,NULL,NULL
752,2,NULL,"2010-07-27 09:20:27",28,NULL,"<blockquote>
  <p>Figures don't lie, but liars do figure</p>
</blockquote>

<p>--Mark Twain</p>
",127,"2010-07-27 09:20:27",NULL,NULL,NULL,6,NULL,NULL,NULL,"2010-07-27 09:20:27",726,NULL,NULL,NULL
753,2,NULL,"2010-07-27 09:22:04",78,NULL,"<blockquote>
  <p>Statistics are like bikinis.  What
  they reveal is suggestive, but what
  they conceal is vital.</p>
</blockquote>

<p>-Aaron Levenstein</p>
",127,"2013-03-26 15:44:51",NULL,NULL,NULL,1,NULL,603,"2013-03-26 15:44:51","2010-07-27 09:22:04",726,NULL,NULL,NULL
754,2,NULL,"2010-07-27 09:25:00",4,NULL,"<blockquote>
  <p>The mathematician, carried along on his flood of symbols, dealing apparently with purely formal thruths, may still reach results of endless importance for our description of physical universe</p>
</blockquote>

<p>-- Karl Pearson</p>
",223,"2010-10-02 17:11:34",NULL,NULL,NULL,1,NULL,795,"2010-10-02 17:11:34","2010-07-27 09:25:00",726,NULL,NULL,NULL
755,2,NULL,"2010-07-27 09:44:12",36,NULL,"<p>I don't know about famous, but the following is one of my favourites:</p>

<blockquote>
  <p>Conducting data analysis is like
  drinking a fine wine. It is important
  to swirl and sniff the wine, to unpack
  the complex bouquet and to appreciate
  the experience. Gulping the wine
  doesn’t work.</p>
</blockquote>

<p>-Daniel B. Wright (2003), see <a href="http://www.baomee.info/pdf/MakeFriends/1.pdf">PDF of Article</a>.</p>

<p><strong>Reference</strong>:
Wright, D. B. (2003). Making friends with your data: Improving how statistics are conducted and reported1. British Journal of Educational Psychology, 73(1), 123-136.</p>
",183,"2013-02-25 00:40:34",NULL,NULL,NULL,1,NULL,183,"2013-02-25 00:40:34","2010-07-27 09:44:12",726,NULL,NULL,NULL
757,2,NULL,"2010-07-27 09:51:19",3,NULL,"<p>I find <a href="http://cran.r-project.org/web/packages/lattice/index.html" rel="nofollow">lattice</a> along with the companion book "Lattice: Multivariate Data Visualization with R" by Deepayan Sarkar invaluable.</p>
",439,"2010-07-27 09:51:19",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 09:51:19",73,NULL,NULL,NULL
758,2,NULL,"2010-07-27 10:00:23",6,NULL,"<p>This is a method using Monte Carlo to show whether a result is correct.</p>

<p>Our Null Hypothesis H_0 is that our dataset does not have an interesting clustering. Our alternative hypothesis H_1 is that our dataset contains an interesting clustering.</p>

<p>Hereby we think of interesting as, more interesting than the clustering structure of a random dataset with the same row and column margins. Of course other constraints could be chosen, but to loose constraints will make our result too general, and to narrow constraints will fix the clustering to much, therefore making our result insignificant automatically. The margins, as we will see, are a good choice because of the methods existing for randomizing with it.</p>

<p>Let's define as our test statistic the clustering error (squared in-cluster distance), T from Π_0. The value for our original dataset is <em>t</em>.</p>

<p>We don't know anything about this distribution, except that we can draw samples from it. Which makes it a good candidate for Monte Carlo.</p>

<p>Now we draw <em>n</em> (i.i.d) random samples from Π_0 and calculate the empirical <em>p</em>-value with the formula p_emp = 1 / (n+1) * (Σ_i=1-n I(t_i >= t)  + 1)</p>

<p>The random sampling can be done by swap randomization. In simple words, a square is searched with on two opposite corners an 1 and on the other two corners a 0. Then the corners are flipped. This is keeping the column and row margins. The procedure is repeated enough times until the dataset is randomized enough (this will take some experiments). More info about this can be found in <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.3286&amp;rep=rep1&amp;type=pdf" rel="nofollow">Assessing Data Mining Results via Swap Randomization by Gionis et. al.</a></p>

<p>One method to do this is defining the distribution of your data and taking the clustering error as test-statistic t.</p>

<p>For example, if we consider all data sets with the same row and column margins as being our data distribution, than we can take n random matrices Xi from this distribution and calculate the clustering error for them. Then we can calculate the emperical p-value by the formula </p>
",190,"2010-07-27 10:00:23",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,723,NULL,NULL,NULL
759,2,NULL,"2010-07-27 10:07:42",3,NULL,"<p>I am afraid there is no; during my little adventure with such data we have just converted it to a data frame form, added some extra attributes made from neighborhoods of pixels and used standard methods. Still, packages <a href="http://cran.r-project.org/web/packages/ripa/index.html" rel="nofollow">ripa</a> and <a href="http://cran.r-project.org/web/packages/hyperSpec/index.html" rel="nofollow">hyperSpec</a> might be useful.<br>
For other software, I've got an impression that most of sensible applications are commercial.</p>
",88,"2010-07-27 10:07:42",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,725,NULL,NULL,NULL
760,2,NULL,"2010-07-27 10:24:44",14,NULL,"<p>I am currently writing a paper in which I have the pleasure to conduct both between and within subjects comparisons. After discussion with my supervisor we decided to run <em>t</em>-tests and use the pretty simple <code>Holm-Bonferroni method</code> (<a href="http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method">wikipedia</a>) for correcting for alpha error cumulation. It controls for familwise error rate but has a greater power than the ordinary Bonferroni procedure.
Procedure:</p>

<ol>
<li>You run the <em>t</em>-tests for all comparisons you want to do.</li>
<li>You order the <em>p</em>-values according to their value.</li>
<li>You test the smallest <em>p</em>-value against <em>alpha</em> / <em>k</em>, the second smallest against <em>alpha</em> /( <em>k</em> - 1), and so forth until the first test turns out non-significant in this sequence of tests.</li>
</ol>

<p>Cite Holm (1979) which can be downloaded via the link at <a href="http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method">wikipedia</a>.</p>
",442,"2010-07-27 13:08:54",NULL,NULL,NULL,4,NULL,442,"2010-07-27 13:08:54",NULL,575,NULL,NULL,NULL
761,2,NULL,"2010-07-27 10:26:36",7,NULL,"<p>I like this from Steve Skienna's <a href="http://books.google.com/books?id=UvWGgaE4ZA8C&amp;lpg=PA86&amp;ots=RXQyDddYcu&amp;dq=In%20summary%2C%20probability%20theory%20enables%20us%20to%20find%20the%20consequences%20of%20a%20given%20ideal%20world%2C%20while%20statistical%20theory%20enables%20us%20to%20to%20measure%20the%20extent%20to%20which%20our%20world%20is%20ideal.&amp;pg=PA86#v=onepage&amp;q&amp;f=false">Calculated Bets</a> (see the link for complete discussion):</p>

<blockquote>
  <p>In summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to to measure the extent to which our world is ideal.</p>
</blockquote>
",251,"2010-07-27 10:26:36",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,665,NULL,NULL,NULL
762,2,NULL,"2010-07-27 11:04:03",5,NULL,"<p>Have a look at the <a href="http://cran.r-project.org/web/packages/multcomp/index.html">multcomp</a>-package and its vignette <a href="http://cran.r-project.org/web/packages/multcomp/vignettes/generalsiminf.pdf">Simultaneous Inference in General Parametric Models</a>. I think it should do what wan't and the vignette has very good examples and extensive references.</p>
",214,"2010-07-27 11:04:03",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,575,NULL,NULL,NULL
763,2,NULL,"2010-07-27 11:04:08",10,NULL,"<p>The final year of the NetFlix Prize competition (2009) seemed to me to have sharply changed the general community-wide presumption against combining multiple learning algorithms. </p>

<p>For instance, my formal training (university courses) and later on-the-job oversight/mentoring taught us to avoid algorithm combination unless we had an explicit reason to do so--and "to improve resolution of my current algorithm", wasn't really deemed a good reason. (Others might have a different experience--of course i'm inferring a community-wide view based solely on my own experience, though my experience in coding poorly-performing ML algorithms is substantial.) </p>

<p>Still, there were a few "patterns" in which combining algorithms in one way or another was accepted, and actually improved performance. For me, the most frequent example involved some ML algorithm configured in machine mode (assigning a class label to each data point) and in which there were more than two classes (usually many more). When for instance, using a supervised-learning algorithm  to resolve four classes, and we would see excellent separation <em>except for</em> let's say Class III versus Class IV. So out of those six decision boundaries, only one resolved below the required threshold. Particularly when classes III and IV together accounted for a small percent of the data, adding an additional algorithm <em>optimized just on the resolution of those two classes</em>, was a fairly common solution to this analytical problem type. (Usually that 'blind spot' was an inherent limitation of the primary algorithm--e.g., it was a linear classifier and the III/IV decision boundary was non-linear. </p>

<p>In other words, when we had a reliable algorithm suited to the processing environment (which was usually streaming data) and that performed within the spec except for a single blind spot that caused it to fail to resolve two (or more) classes that accounted for a small fraction of the data, then it was always better to 'bolt-on' another specialized algorithm to catch what the main algorithm was systematically missing.</p>

<p>Lastly, on this topic, i would like to recommend highly Chapter 17, <em>Combining Multiple Learners</em>, in <strong><em><a href="http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/">Introduction to Machine Learning</a></em></strong>, 2d, by Ethem Alpaydin, MIT Press, 2010. Note that this is the <em>second edition</em> published a few months ago; the first edition was published in 2004 and i doubt it has the same coverage of this topic. (Actually i recommend the entire text, but that chapter in particular since it relates to Shane's Question.)</p>

<p>In 25 pages, the author summarizes probably every ML algorithm-combination scheme whose utility has been demonstrated in the academic literature or practice--e.g., bagging, boosting, mixture of experts, stacked generalization, cascading, voting, error-correcdting,....</p>
",438,"2010-08-04 07:39:52",NULL,NULL,NULL,2,NULL,438,"2010-08-04 07:39:52",NULL,562,NULL,NULL,NULL
764,1,772,"2010-07-27 11:29:20",12,1725,"<p>I have commonly heard that LME models are more sound in the analysis of accuracy data (i.e., in psychology experiments), in that they can work with binomial and other non-normal distributions that traditional approaches (e.g., ANOVA) can't.</p>

<p>What is the mathematical basis of LME models that allow them to incorporate these other distributions, and what are some not-overly-technical papers describing this?</p>
",445,"2010-08-07 17:52:12","Linear Mixed Effects Models",<mixed-model>,3,0,11,88,"2010-08-07 17:52:12",NULL,NULL,NULL,NULL,NULL
765,2,NULL,"2010-07-27 11:56:42",8,NULL,"<p>A model is saturated if and only if it has as many parameters as it has data points (observations). Or put otherwise, in non-saturated models the degrees of freedom are bigger than zero.</p>

<p>This basically means that this model is useless, because it does not describe the data more parsimoniously than the raw data does (and describing data parsimoniously is generally the idea behind using a model). Furthermore, saturated models can (but don't necessarily) provide a (useless) perfect fit because they just interpolate or iterate the data.</p>

<p>Take for example the mean as a model for some data. If you have only one data point (e.g., 5) using the mean (i.e., 5; note that the mean is a saturated model for only one data point) does not help at all. However if you already have two data points (e.g., 5 and 7) using the mean (i.e., 6) as a model provides you with a more  parsimonious description than the original data.</p>
",442,"2010-07-27 12:32:15",NULL,NULL,NULL,1,NULL,442,"2010-07-27 12:32:15",NULL,283,NULL,NULL,NULL
766,2,NULL,"2010-07-27 12:10:34",3,NULL,"<p>Most statistical packages have a function to calculate the natural logarithm of the factorial directly (e.g. the lfactorial() function in R, the lnfactorial() function in Stata). This allows you to include the constant term in the log-likelihood if you want.</p>
",449,"2010-07-27 12:10:34",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,270,NULL,NULL,NULL
767,2,NULL,"2010-07-27 12:31:57",83,NULL,"<p>Your question implies that AIC and BIC try to answer the same question, which is not true.
AIC tries to select the model that most adequately describes an unknown, high dimensional reality. This means that reality is never in the set of candidate models that are being considered.  On the contrary, BIC tries to find the TRUE model among the set of candidates. I find it quite odd the assumption that reality is instantiated in one of the model that the researchers built along the way. This is a real issue for BIC.</p>

<p>Nevertheless, there are a lot of researchers who say BIC is better than AIC, using model recovery simulations as an argument. These simulations consist of generating data from models A and B, and then fitting both datasets with the two models. Overfitting occurs when the wrong model fits the data better than the generating. The point of these simulations is to see how well AIC and BIC correct these overfits. Usually, the results point to the fact that AIC is too liberal and still frequently prefers a more complex, wrong model over a simpler, true model. At first glance these simulations seem to be really good arguments, but the problem with them is that they are meaningless for AIC. As I said before, AIC does not consider that any of the candidate models being tested is actually true. According to AIC, all models are approximations to reality, and reality should never have a low dimensionality. At least lower than some of the candidate models. </p>

<p>my recommendation: use both AIC and BIC. Most of the times they will agree on the preferred model, when they dont, just report it.</p>

<p>If you are unhappy with both AIC and BIC, and you have free time to invest, look up for Minimum Description Length (MDL), a totally different approach that overcomes the limitations of AIC and BIC. There are several measures stemming from MDL, like normalized maximum likelihood or the Fisher Information approximation. The problem with MDL is that its mathematically demanding and/or computationally intensive. </p>

<p>Still, if you wanna stick to simple solutions, a nice way for assessing model flexibility (especially when the number of parameters are equal, rendering AIC and BIC useless) is doing Parametric Bootstrap, which is quite easy to implement. here is a link to a paper on it:
<a href="http://www.ejwagenmakers.com/2004/PBCM.pdf">link text</a></p>

<p>some people here advocate the use of cross-validation. I personally have used it, and dont have anything against it, but the issue with it is that the choice among the sample-cutting rule (leave-one-out, K-fold, etc) is an unprincipled one. </p>

<p>cheers</p>
",447,"2010-07-27 12:39:36",NULL,NULL,NULL,7,NULL,447,"2010-07-27 12:39:36",NULL,577,NULL,NULL,NULL
768,2,NULL,"2010-07-27 12:36:47",2,NULL,"<p>In probability theory, we are given random variables X1, X2, ... in some way, and then we study their properties, i.e. calculate probability P{ X1 \\in B1 }, study the convergence of X1, X2, ... etc.</p>

<p>In mathematical statistics, we are given n realizations of some random variable X, and set of distributions D; the problem is to find amongst distributions from D one which is most likely to generate the data we observed. </p>
",NULL,"2010-07-27 12:36:47",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,665,NULL,zoran,NULL
769,1,NULL,"2010-07-27 12:46:48",2,247,"<p>When does data analysis cease to be statistics ?</p>

<p>Are the following examples all applications of statistics ?: computer vision, face recognition, compressed sensing, lossy data compression, signal processing.</p>
",327,"2010-07-27 16:00:22","What types of data analysis do not count as statistics?",<theory>,2,4,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
770,1,NULL,"2010-07-27 12:53:30",6,282,"<p>During every machine learning tutorial you'll find, there is the common "You will need to know x amount of stats before starting this tutorial".  As such, using your knowledge of stats, you will learn about machine learning.</p>

<p>My question is whether this can be reversed.  Can a computer science student learn statistics through studying machine learning algorithms?  Has this been tested, at all?  Are there examples where this is the case already?</p>
",453,"2011-02-14 23:04:02","Is it possible to use machine learning as a method for learning stats, rather than vice-versa?",<machine-learning><teaching>,5,0,NULL,88,"2010-09-17 20:31:31",NULL,NULL,NULL,NULL,NULL
771,2,NULL,"2010-07-27 13:02:58",0,NULL,"<p>I think that learning machine learning requires only an elementary subset of statistics; too much may be dangerous, since some intuitions are in conflict. Still, the answer to the question can it be reversed is no.</p>
",88,"2010-07-27 13:02:58",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,770,NULL,NULL,NULL
772,2,NULL,"2010-07-27 13:07:38",11,NULL,"<p>One major benefit of mixed-effects models is that they don't assume independence amongst observations, and there can be a correlated observations within a unit or cluster.</p>

<p>This is covered concisely in "Modern Applied Statistics with S" (MASS) in the first section of chapter 10 on "Random and Mixed Effects".  V&amp;R walk through an example with gasoline data comparing ANOVA and lme in that section, so it's a good overview.  The R function to be used in <code>lme</code> in the <code>nlme</code> package.  </p>

<p>The model formulation is based on Laird and Ware (1982), so you can refer to that as a primary source although it's certainly not good for an introduction.</p>

<ul>
<li>Laird, N.M. and Ware, J.H. (1982) "Random-Effects Models for Longitudinal Data", Biometrics, 38, 963–974.</li>
<li>Venables, W.N. and Ripley, B.D. (2002) "<a href="http://www.stats.ox.ac.uk/pub/MASS4/">Modern Applied Statistics with S</a>", 4th Edition, Springer-Verlag.</li>
</ul>

<p>You can also have a look at the <a href="http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf">"Linear Mixed Models"</a> (PDF) appendix to John Fox's "An R and S-PLUS Companion to Applied Regression".  And <a href="http://idiom.ucsd.edu/~rlevy/lign251/fall2007/lecture_14.pdf">this lecture by Roger Levy</a> (PDF) discusses mixed effects models w.r.t. a multivariate normal distribution.</p>
",5,"2010-07-27 13:07:38",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,764,NULL,NULL,NULL
773,2,NULL,"2010-07-27 13:09:54",9,NULL,"<p>As everybody else said before, it means that you have as much parameters have you have data points. So, no goodness of fit testing. But this does not mean that "by definition", the model can perfectly fit any data point. I can tell you by personal experience of working with some saturated models that could not predict specific data points. It is quite rare, but possible.</p>

<p>Another important issue is that saturated does not mean useless. For instance, in mathematical models of human cognition, model parameters are associated with specific cognitive processes that have a theoretical background. If a model is saturated, you can test its adequacy by doing focused experiments with manipulations that should affect only specific parameters. If the theoretical predictions match the observed differences (or lack of) in parameter estimates, then one can say that the model is valid. </p>

<p>An example: Imagine for instance a model that has two sets of parameters, one for cognitive processing, and another for motor responses. Imagine now that you have an experiment with two conditions, one in which the participants ability to respond is impaired (they can only use one hand instead of two), and in the other condition there is no impairment. If the model is valid, differences in parameter estimates for both conditions should only occur for the motor response parameters.</p>

<p>Also, be aware that even if one model is non-saturated, it might still be non-identifiable, which means that different combinations of parameter values produce the same result, which compromises any model fit.</p>

<p>If you wanna find more information on these issues in general, you might wanna take look at these papers:</p>

<p>Bamber, D., &amp; van Santen, J. P. H. (1985). How many parameters can a model have and still be testable? Journal of Mathematical Psychology, 29, 443-473. </p>

<p>Bamber, D., &amp; van Santen, J. P. H. (2000). How to Assess a Model's Testability and Identifiability. Journal of Mathematical Psychology, 44, 20-40. </p>

<p>cheers</p>
",447,"2010-07-27 13:09:54",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,283,NULL,NULL,NULL
774,2,NULL,"2010-07-27 13:14:19",7,NULL,"<p>I really wouldn't suggest using machine learning in order to learn statistics.  The mathematics employed in machine learning is often different because there's a real emphasis on the computational algorithm.  Even treatment of the same concept will be different.  </p>

<p>A simple example of this would be to compare the treatment of linear regression between a basic statistics textbook and a machine learning textbook.  Most machine learning texts give a heavy treatment to concepts like "<a href="http://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>" and other optimzation techniques, while a statistics textbook will typically just cover <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a> (if even that).</p>

<p>Lastly, machine learning generally doesn't cover the same material when it comes to things like model comparison, sampling, etc.  So while some of the basic models are the same, the conceptual frameworks can be very different.</p>
",5,"2010-07-27 13:14:19",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,770,NULL,NULL,NULL
775,1,777,"2010-07-27 13:14:54",9,1423,"<p>What is the difference between operations research and statistical analysis?</p>
",460,"2014-02-04 05:06:46","Operations research versus statistical analysis?",<operations-research>,4,0,1,88,"2012-02-23 10:41:34",NULL,NULL,NULL,NULL,NULL
776,2,NULL,"2010-07-27 13:25:13",2,NULL,"<p>This may be slightly controversial, but certainly most of the examples that you give are not statistics (some of these would more properly fall under machine learning):</p>

<p>Statistics usually covers the scenario where you are making inferences something when you only have a subset of the data (hence the use of things like <a href="http://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests" rel="nofollow">the p-value</a>).  The goal is to come to a deeper understanding of the underlying process that generates the data.  </p>

<p>In the examples that you provide, the goal is to perform some kind of action; whether the model itself is an accurate depiction of the underlying process is completely irrelevant so long as the predictions are accurate.  See <a href="http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning" rel="nofollow">my related question</a> about the difference between machine learning and statistics.</p>
",5,"2010-07-27 13:25:13",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,769,NULL,NULL,NULL
777,2,NULL,"2010-07-27 13:35:21",11,NULL,"<p>Those are entire academic discplines so I do not think you can expect much more here than pointers to further, and more extensive, documentation as e.g. Wikipedia on <a href="http://en.wikipedia.org/wiki/Operations_research">Operations Research</a> and <a href="http://en.wikipedia.org/wiki/Statistics">Statistics</a>.</p>

<p>Let me try a personal definition which may be grossly simplifying:</p>

<ul>
<li>Operations Research is concerned with process modeling and optimisation</li>
<li>Statistical Modeling is concerning with describing the so-called 'data generating process': find a model that describes something observed, and then do estimation, inference and possibly prediction.</li>
</ul>
",334,"2010-07-27 13:35:21",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,775,NULL,NULL,NULL
778,2,NULL,"2010-07-27 13:39:35",8,NULL,"<p>I'm sorry, but there seems to be some confusion here: 
Bayes' theorem is not up for discussion of the neverending Bayesian-<a href="http://en.wikipedia.org/wiki/Probability_interpretations#Frequentism" rel="nofollow">Frequentist</a> debate. It is a theorem that is consistent with both schools of thought (given that it is consistent with Kolmogorov's probability axioms).</p>

<p>Of course, Bayes' theorem is the core of Bayesian statistics, but the theorem itself is universal. The clash between frequentists and Bayesians mostly pertains to how prior distributions can be defined or not.</p>

<p>So, if the question is about Bayes' theorem (and not Bayesian statistics):</p>

<p>Bayes' theorem defines how one can calculate specific conditional probabilities. Imagine for instance that you know: the probability of somebody having symptom A, given that they have disease X p(A|X); the probability of somebody in general having disease X p(X); the probability of somebody in general having symptom A p(A). with these 3 pieces of information you can calculate the probability of somebody having disease X, given that they have sympotm A p(X|A).  </p>
",447,"2011-02-02 19:09:56",NULL,NULL,NULL,1,NULL,509,"2011-02-02 19:09:56",NULL,672,NULL,NULL,NULL
779,1,NULL,"2010-07-27 13:49:41",8,857,"<p>I've heard that a lot of quantities that occur in nature are normally distributed. This is typically justified using the central limit theorem, which says that when you average a large number of iid random variables, you get a normal distribution. So, for instance, a trait that is determined by the additive effect of a large number of genes may be approximately normally distributed since the gene values may behave roughly like iid random variables.</p>

<p>Now, what confuses me is that the property of being normally distributed is clearly not invariant under monotonic transformations. So, if there are two ways of measuring something that are related by a monotonic transformation, they are unlikely to both be normally distributed (unless that monotonic transformation is linear). For instance, we can measure the sizes of raindrops by diameter, by surface area, or by volume. Assuming similar shapes for all raindrops, the surface area is proportional to the square of the diameter, and the volume is proportional to the cube of the diameter. So all of these ways of measuring cannot be normally distributed.</p>

<p>So my question is whether the particular way of scaling (i.e., the particular choice of monotonic transformation) under which the distribution does become normal, must carry a physical significance. For instance, should heights be normally distributed or the square of height, or the logarithm of height, or the square root of height? Is there a way of answering that question by understanding the processes that affect height?</p>
",463,"2013-03-16 00:54:30","Normal distribution and monotonic transformations",<data-transformation><normality>,7,5,2,805,"2013-03-16 00:54:30",NULL,NULL,NULL,NULL,NULL
780,2,NULL,"2010-07-27 13:55:18",9,NULL,"<p>Operations Research (OR), sometimes called "Management Science", consists of three main topics, Optimization, Stochastic Processes, Process and Production Methodologies. </p>

<p>OR uses statistical analysis in many contexts (for example discrete event simulations) but they should not be considered the same, additionally one of the main topics in OR is optimization (linear, and nonlinear) which can make it more clear why these two fields should be considered different </p>

<p>There is <a href="http://www.or-exchange.com/questions">another exchange website for OR</a> if you are interested </p>
",172,"2010-07-27 19:10:25",NULL,NULL,NULL,1,NULL,172,"2010-07-27 19:10:25",NULL,775,NULL,NULL,NULL
781,2,NULL,"2010-07-27 13:59:02",3,NULL,"<p>One approach would be to use Bloom filters. Check <a href="http://www.uni-due.de/soziologie/schnell_forschung_safelink_software.php" rel="nofollow">SAFELINK</a> project website for <a href="http://www.uni-due.de/methods/bloom/index-en-bloom.html" rel="nofollow">programs</a> in Java and Python. Paper explaining method is <a href="http://dx.doi.org/10.1186/1472-6947-9-41" rel="nofollow">here</a>. </p>

<p>There is also an interesting approach to anaonymization of strings in the context of <a href="http://en.wikipedia.org/wiki/Record_linkage" rel="nofollow">record linkage</a> using n-grams developed by <a href="http://datamining.anu.edu.au/projects/linkage.html" rel="nofollow">ANU Data Mining Group</a>. The paper with description and sample Python code is available <a href="http://www.biomedcentral.com/1472-6947/4/9/" rel="nofollow">here</a>. </p>
",22,"2011-04-30 18:31:33",NULL,NULL,NULL,0,NULL,22,"2011-04-30 18:31:33",NULL,712,NULL,NULL,NULL
782,2,NULL,"2010-07-27 14:02:37",3,NULL,"<p>I think you missunderstood (half of) the use statistician make of the normal distribution but I really like your question. </p>

<p>I don't think it is a good idea to assume systematically normality and I  admit it is done sometime (maybe because the normal distribution is tractable, unimodal ...) without verification. Hence your remark about monotonic map is excellent ! </p>

<p>However the powerfull use of normality comes when you construct yourself new statistics such as the one that appears when you apply the empiriral counter part of expectation: <strong>the empirical mean</strong>. Hence empirical mean and more generally smoothing is what makes normality appear everywhere...  </p>
",223,"2010-07-27 15:44:37",NULL,NULL,NULL,0,NULL,223,"2010-07-27 15:44:37",NULL,779,NULL,NULL,NULL
783,2,NULL,"2010-07-27 14:03:52",35,NULL,"<blockquote>
  <p>"... surely, God loves the .06 nearly as much as the .05. Can there be any
  doubt that God views the strength of evidence for or against the null as a
  fairly continuous function of the magnitude of p?" (p.1277)</p>
</blockquote>

<p>Rosnow, R. L., &amp; Rosenthal, R. (1989). Statistical procedures and the justification of knowledge in psychological science. American Psychologist, 44(10), 1276-1284. <a href="http://socrates.berkeley.edu/~maccoun/PP279_Rosnow.pdf">pdf</a></p>
",442,"2012-11-03 05:47:42",NULL,NULL,NULL,6,NULL,9007,"2012-11-03 05:47:42","2010-07-27 14:03:52",726,NULL,NULL,NULL
784,2,NULL,"2010-07-27 14:04:13",3,NULL,"<p><a href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=12156" rel="nofollow">Street-Fighting Mathematics</a>. The Art of Educated Guessing and Opportunistic Problem Solving<br>
by Sanjoy Mahajan from MIT</p>
",22,"2010-07-27 14:04:13",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 14:04:13",614,NULL,NULL,NULL
785,2,NULL,"2010-07-27 14:10:31",15,NULL,"<blockquote>
  <p>The statistician cannot evade the responsibility for understanding the
  process he applies or recommends.</p>
</blockquote>

<p>-– Sir Ronald A. Fisher</p>
",447,"2010-12-03 04:02:46",NULL,NULL,NULL,0,NULL,795,"2010-12-03 04:02:46","2010-07-27 14:10:31",726,NULL,NULL,NULL
786,2,NULL,"2010-07-27 14:14:10",18,NULL,"<blockquote>
  <p>The death of one man is a tragedy. 
  The death of millions is a statistic.</p>
</blockquote>

<p>~Joseph Stalin, comment to Churchill at Potsdam, 1945</p>
",460,"2010-08-06 23:23:57",NULL,NULL,NULL,1,NULL,509,"2010-08-06 23:23:57","2010-07-27 14:14:10",726,NULL,NULL,NULL
787,2,NULL,"2010-07-27 14:20:42",3,NULL,"<blockquote>
  <p>Statistics are the triumph of the
  quantitative method, and the
  quantitative method is the victory of
  sterility and death.</p>
</blockquote>

<p>~ <strong>Hillaire Belloc</strong> <em>in</em> The Silence of the Sea</p>
",460,"2010-07-27 14:20:42",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-27 14:20:42",726,NULL,NULL,NULL
788,2,NULL,"2010-07-27 14:25:39",1,NULL,"<blockquote>
  <p>A witty statesman said, you might
  prove anything by figures.</p>
</blockquote>

<p>~ Thomas Carlyle, Chartism (1839) ch. 2</p>
",460,"2010-07-27 14:25:39",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-27 14:25:39",726,NULL,NULL,NULL
789,2,NULL,"2010-07-27 14:38:34",4,NULL,"<p><a href="http://psy.otago.ac.nz/miller/index.htm#GLMBook" rel="nofollow">Statistical Analysis with the General Linear Model</a></p>

<p>It covers basic linear models (ANOVA, ANCOVA, multiple regression). I can tell by personal experience that it is really really good book to get into the general framework of linear models, which are very useful in many advanced approaches (e.g., hierarchical modeling).</p>
",447,"2014-06-20 06:34:29",NULL,NULL,NULL,1,NULL,930,"2014-06-20 06:34:29","2010-07-27 14:38:34",614,NULL,NULL,NULL
790,1,NULL,"2010-07-27 14:46:49",3,1540,"<p>I am looking at a scientific paper in which a single measurement is calculated using a logarithmic mean</p>

<blockquote>
  <p>'triplicate spots were combined to
  produce one signal by taking the
  logarithmic mean of reliable spots'</p>
</blockquote>

<p><strong>Why choose the log-mean?</strong></p>

<p>Are the authors making an assumption about the underlying distribution? </p>

<p>That it is what...Log-Normal? </p>

<p>Have they just picked something they thought was reasonable... i.e. between a a mean and a geometric mean?</p>

<p>Any thoughts?</p>
",228,"2010-07-27 17:19:27","Why (or when) to use the log-mean?",<distributions><logarithm><mean>,2,3,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
791,2,NULL,"2010-07-27 14:52:19",3,NULL,"<p>I really dont think so, as there are fundamental aspects in statistics that are simply overlooked in machine learning. For instance, in statistics, when fitting a model to data, the discrpeancy function that is used (e.g., G^2, RMSEA) is essential because they have different statistical properties. In machine learning, it just doesnt matter, so it is not covered at all. </p>

<p>Of course one could argue that you could learn those things later, but IMHO, it is better to undestand and care about some issues, and possibly in the future not care about them, then the other way around. </p>

<p>cheers</p>
",447,"2010-07-27 14:52:19",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,770,NULL,NULL,NULL
792,2,NULL,"2010-07-27 15:03:28",4,NULL,"<p>Simply CLT (nor any other theorem) does not state that every quantity in the universe is normally distributed. Indeed, statisticians often use monotonic transformations to improve normality, so they could use their favorite tools.</p>
",88,"2010-07-27 15:03:28",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,779,NULL,NULL,NULL
793,2,NULL,"2010-07-27 15:04:58",0,NULL,"<blockquote>
  <p>Are the authors making an assumption
  about the underlying distribution?</p>
</blockquote>

<p>You are making an assumption whether you choose to use it or whether you choose against using it.
For Power Law distributions it usually makes sense to look at the logarithms.</p>
",3807,"2010-07-27 15:04:58",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,790,NULL,NULL,NULL
794,2,NULL,"2010-07-27 15:05:38",28,NULL,"<blockquote>
  <p>The subjectivist (i.e. Bayesian)
  states his judgements, whereas the
  objectivist sweeps them under the
  carpet by calling assumptions
  knowledge, and he basks in the
  glorious objectivity of science.</p>
</blockquote>

<p>I.J. Good</p>
",7620,"2010-07-27 15:05:38",NULL,NULL,NULL,2,NULL,7620,NULL,"2010-07-27 15:05:38",726,NULL,"John A. Ramey",NULL
795,1,NULL,"2010-07-27 15:08:11",-3,728,"<p>I have data compiled by someone else where score averages have been computed over time- averages range from 0-100. The original scores have negative values in many cases and the average would have been negative also, raw average ranges from -30 to 90.  How is this 'normalization' accomplished?<br>
Thanks</p>
",474,"2010-07-27 15:36:41","Normalization of series",<mean>,2,2,1,NULL,NULL,NULL,NULL,"2010-08-03 09:14:27",NULL,NULL
796,2,NULL,"2010-07-27 15:14:22",19,NULL,"<p>I would like to oppose the other two answers based on a paper (in German) by <a href="http://www.psycontent.com/content/t5726m72644gq457/">Kubinger, Rasch and Moder (2009)</a>.</p>

<p>They argue, based on "extensive" simulations from distributions either meeting or not meeting the assumptions imposed by a t-test, (normality and homogenity of variance) that the welch-tests performs equally well when the assumptions are met (i.e., basically same probability of committing alpha and beta errors) but outperforms the t-test if the assumptions are not met, especially in terms of power. Therefore, they recommend to always use the welch-test if the sample size exceeds 30.</p>

<p>As a meta-comment: For people interested in statistics (like me and probably most other here) an argument based on data (as mine) should at least count equally as arguments solely based on theoretical grounds (as the others here).</p>

<hr>

<p><strong>Update:</strong><br>
After thinking about this topic again, I found two further recommendations of which the newer one assists my point. Look at the original papers (which are both, at least for me, freely available) for the argumentations that lead to these recommendations.</p>

<p>The first recommendation comes from Graeme D. Ruxton in 2006: "<strong>If you want to compare the central tendency of 2 populations based on samples of unrelated data, then the unequal variance t-test should always be used in preference to the Student's t-test or Mann–Whitney U test.</strong>"<br>
In:<br>
Ruxton, G.D., 2006. <a href="http://beheco.oxfordjournals.org/content/17/4/688.full">The unequal variance t-test is an underused
alternative to Student’s t-test and the Mann–Whitney U test</a>.
<em>Behav. Ecol</em>. 17, 688–690.</p>

<p>The second (older) recommendation is from Coombs et al. (1996, p. 148): "<strong>In summary, the independent samples t test is generally acceptable in terms of controlling Type I error rates provided there are sufficiently large equal-sized samples, even when the equal population variance assumption is violated.</strong> For unequal-sized samples, however, an alternative that does not assume equal population variances is preferable. Use the James second-order test when distributions are either short-tailed symmetric or normal. Promising alternatives include the Wilcox H and Yuen trimmed means tests, which provide broader control of Type I error rates than either the Welch test or the James test and have greater power when data are long-tailed." (emphasis added)<br>
In:<br>
Coombs WT, Algina J, Oltman D. 1996. <a href="http://rer.sagepub.com/content/66/2/137.full.pdf">Univariate and multivariate omnibus hypothesis tests selected to control type I error rates when population variances are not necessarily equal</a>. <em>Rev Educ Res</em> 66:137–79.</p>
",442,"2010-10-19 15:28:42",NULL,NULL,NULL,1,NULL,442,"2010-10-19 15:28:42",NULL,305,NULL,NULL,NULL
797,2,NULL,"2010-07-27 15:19:43",4,NULL,"<p>Very good question. I feel that the answer depends on the whether you can identify the underlying process that gives rise to the measurement in question. If for example, you have evidence that height is a linear combination of several factors (e.g., height of parents, height of grandparents etc) then it would be natural to assume that height is normally distributed. On the other hand if you have evidence or perhaps even theory that the log of height is a linear combination of several variables (e.g., log parents heights, log of grandparents heights etc) then the log of height will be normally distributed.</p>

<p>In most situations, we do not know the underlying process that drives the measurement of interest. Thus, we can do one of several things:</p>

<p>(a) If the empirical distribution of heights looks normal then we use a the normal density for further analysis which implicitly assumes that height is a linear combination of several variables.</p>

<p>(b) If the empirical distribution does not look normal then we can try some transformation as suggested by <a href="http://stats.stackexchange.com/users/88/mbq" rel="nofollow">mbq</a> (e.g. log(height)). In this case we implicitly assume that the transformed variable (i.e., log(height)) is a linear combination of several variables.</p>

<p>(c) If (a) or (b) do not help then we have to abandon the advantages that CLT and an assumption of normality give us and model the variable using some other distribution.</p>
",NULL,"2010-07-27 15:25:00",NULL,NULL,NULL,0,NULL,NULL,"2010-07-27 15:25:00",NULL,779,NULL,user28,user28
798,1,862,"2010-07-27 15:21:48",21,28339,"<p>I'm interested in finding as optimal of a method as I can for determining how many bins I should use in a histogram. My data should range from 30 to 350 objects at most, and in particular I'm trying to apply thresholding (like Otsu's method) where "good" objects, which I should have fewer of and should be more spread out, are separated from "bad" objects, which should be more dense in value. A concrete value would have a score of 1-10 for each object. I'd had 5-10 objects with scores 6-10, and 20-25 objects with scores 1-4. I'd like to find a histogram binning pattern that generally allows something like Otsu's method to threshold off the low scoring objects. However, in the implementation of Otsu's I've seen, the bin size was 256, and often I have many fewer data points that 256, which to me suggests that 256 is not a good bin number. With so few data, what approaches should I take to calculating the number of bins to use?</p>
",476,"2014-05-29 15:58:50","Calculating optimal number of bins in a histogram for n, where n ranges from 30-350",<rule-of-thumb><histogram>,7,0,14,8,"2010-10-06 14:32:02",NULL,NULL,NULL,NULL,NULL
799,2,NULL,"2010-07-27 15:26:34",7,NULL,"<p>The main advantage of LME for analysing accuracy data is the ability to account for a series of random effects. In psychology experiments, researchers usually aggregate items and/or participants. Not only are people different from each other, but items also differ (some words might be more distinctive or memorable, for instance). Ignoring these sources of variability usually leads to underestimations of accuracy (for instance lower d' values). Although the participant aggregation issue can somehow be dealt with  individual estimation, the item effects are still there, and are commonly larger than participant effects. LME not only allows you to tackle both random effects simultaneously, but also to add specificy additional predictor variables (age, education level, word length, etc.) to them.</p>

<p>A really good reference for LMEs, especially focused in the fields of linguistics and experimental psychology, is 
<a href="http://rads.stackoverflow.com/amzn/click/0521709180" rel="nofollow">Analyzing Linguistic Data: A Practical Introduction to Statistics using R</a> </p>

<p>cheers</p>
",447,"2010-07-27 15:26:34",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,764,NULL,NULL,NULL
800,2,NULL,"2010-07-27 15:27:18",1,NULL,"<p>The question is a bit unclear. But, perhaps the normalization you are looking for is this:</p>

<p>Normalized Score = 100 * (Raw Score - min(Raw_score)) / (max(Raw Score) - min(Raw Score))</p>
",NULL,"2010-07-27 15:27:18",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,795,NULL,user28,NULL
801,2,NULL,"2010-07-27 15:30:31",2,NULL,"<p>I'm not sure this counts as strictly good practise, but I tend to produce more than one histogram with different bin widths and pick the histogram which histgram to use based on which histgram fits the interpretation I'm trying to communicate best. Whilst this introduces some objectivity into the choice of histogram I justify it on the basis I have had much more time to understnad the data than the person I'm giving the histogram to so I need to give them a very concise message. </p>

<p>I'm also a big fan of presenting histograms with the same number of points in each bin rather than the same bin width. I usually find these represent the data far better then the constant bin width although they are mopre difficult to produce. </p>
",210,"2010-07-27 15:30:31",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,798,NULL,NULL,NULL
802,2,NULL,"2010-07-27 15:35:14",4,NULL,"<p>The rescaling of a particular variable should, when possible, relate to some comprehensible scale for the reason that it helps make the resulting model interpretable.  However, the resulting transformation need not absolutely carry a physical significance.  Essentially you have to engage in a trade off between the violation of the normality assumption and the interpretability of your model.  What I like to do in these situations is have the original data, data transformed in a way that makes sense, and the data transformed in a way that is most normal.  If the data transformed in a way that makes sense is the same as the results when the data is transformed in a way that makes it most normal, I report it in a way that is interpretable with a side note that the results are the same in the case of the optimally transformed (and/or untransformed) data.  When the untransformed data is behaving particularly poorly, I conduct my analyses with the transformed data but do my best to report the results in untransformed units.</p>

<p>Also, I think you have a misconception in your statement that "quantities that occur in nature are normally distributed".  This only holds true in cases where the value is "determined by the additive effect of a large number" of independent factors.  That is, means and sums are normally distributed regardless of the underlying distribution from which they draw, where as individual values are not expected to be normally distributed.  As was of example, individual draws from a binomial distribution do not look at all normal, but a distribution of the sums of 30 draws from a binomial distribution does look rather normal.</p>
",196,"2010-07-27 15:35:14",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,779,NULL,NULL,NULL
803,2,NULL,"2010-07-27 15:36:41",2,NULL,"<p>Or more generically,</p>

<p><code>Index = ( ((RawValue - Min(Raw))/(Max(Raw)-Min(Raw)) * (Max(Out)-Min(Out) ) + Min(Out)</code></p>

<p>Where <code>Raw</code> is the input vector, <code>Out</code> is the output vector, and <code>RawValue</code> is the value in question.  Srikant's answer is the same for an output range of 0 to 100.</p>

<p>To convert back, rearrange to get:</p>

<p><code>RawValue = (((Index - Min(Out))*(Max(Raw)-Min(Raw)) / (Max(Out)-Min(Out)) + Min(Raw)</code></p>
",257,"2010-07-27 15:36:41",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,795,NULL,NULL,NULL
804,2,NULL,"2010-07-27 15:43:51",9,NULL,"<p>of course, one could ditch both tests, and start using a Bayesian t-test (Savage-Dickey ratio test), which can account for unequal and unequal variances, and best of all, it allows for a quantification of evidence in favor of the null hypothesis (which means, no more of old "failure to reject" talk)</p>

<p>This test is very simple (and fast) to implement, and there is a paper that clearly explains to readers unfamiliar with Bayesian statistics how to use it, along with an R script. you basically can just insert your data send the commands to the R console:</p>

<p><a href="http://www.ruudwetzels.com/articles/Wetzelsetal2009_SDtest.pdf">Wetzels, R., Raaijmakers, J. G. W., Jakab, E., &amp; Wagenmakers, E.-J. (2009). How to Quantify Support For and Against the Null Hypothesis: A Flexible WinBUGS Implementation of a Default Bayesian t-test.</a> </p>

<p>there is also a tutorial for all this, with example data:</p>

<p><a href="http://www.ruudwetzels.com/index.php?src=SDtest">http://www.ruudwetzels.com/index.php?src=SDtest</a></p>

<p>I know this is not a direct response to what was asked, but I thought readers might enjoy having this nice alternative</p>

<p>cheers</p>
",447,"2010-07-27 15:43:51",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,305,NULL,NULL,NULL
805,1,NULL,"2010-07-27 15:43:55",2,551,"<p>I am attempting to calculate the standard error (SE) for the positive predictive value (PPV), negative predictive value (NPV), and diagnostic odds ratio (DOR) that I have obtained using the rates of true positives, false positives, true negatives, and false negatives in a sample. I am able to get 95% CIs but not SE. </p>

<p>Thank you!</p>
",NULL,"2010-08-13 13:35:40","How do I calculate the SE for PPV, NPV, and DOR?",<error><standard-error>,2,0,2,8,"2010-08-13 13:35:40",NULL,NULL,NULL,Jay,NULL
806,1,829,"2010-07-27 15:56:38",4,1793,"<p>Lately, there have been numerous questions about <a href="http://en.wikipedia.org/wiki/Normalization_%28statistics%29" rel="nofollow">normalization</a></p>

<p>What are some of the situations where you never ever ever should normalize your data, and what are the alternatives?</p>
",59,"2010-09-27 23:40:16","When should normalization never be used?",<normality>,3,1,3,NULL,NULL,NULL,NULL,NULL,NULL,NULL
807,2,NULL,"2010-07-27 16:00:22",0,NULL,"<p>A statistic is any sample estimation of a population characteristic is it not?  Thus, I would suggest so long as inference from a characterized sample to a population is taking place what is occurring is, at least in part, statistics.  Under my definition machine learning would be a discipline that makes use of statistics.  For example, under many examples of machine learning it seems like part of what is happening is an attempt to take information provided to the system (a sample) in order to guess from which distribution (a population) it arose.</p>
",196,"2010-07-27 16:00:22",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,769,NULL,NULL,NULL
808,2,NULL,"2010-07-27 16:04:42",6,NULL,"<blockquote>
  <p>A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions.   </p>
</blockquote>

<p>-- M.J. Moroney</p>
",482,"2010-10-02 17:12:07",NULL,NULL,NULL,2,NULL,795,"2010-10-02 17:12:07","2010-07-27 16:04:42",726,NULL,NULL,NULL
809,2,NULL,"2010-07-27 16:14:26",2,NULL,"<blockquote>
  <p>An argument over the meaning of words
  is a matter of law, an argument
  grounded in empirical data and
  quantitative estimates is an argument
  about science.</p>
</blockquote>

<p>~ Razib Khan (though he is not a statistician or famous)</p>
",NULL,"2010-07-27 16:14:26",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 16:14:26",726,NULL,"Michael Bishop",NULL
810,2,NULL,"2010-07-27 16:17:43",7,NULL,"<p>If you use too few bins, the histogram doesn't really portray the data very well. If you have too many bins, you get a broken comb look, which also doesn't give a sense of the distribution.</p>

<p>One solution is to create a graph that shows every value. Either a dot plot, or a cumulative frequency distribution, which doesn't require any bins.  </p>

<p>If you want to create a frequency distribution with equally spaced bins, you need to decide how many bins (or the width of each). The decision clearly depends on the number of values. If you have lots of values, your graph will look better and be more informative if you have lots of bins. <a href="http://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width" rel="nofollow">This wikipedia page</a> lists several methods for deciding bin width from the number of observations. The simplest method is to set the number of bins equal to the square root of the number of values you are binning.</p>

<p><a href="http://toyoizumilab.brain.riken.jp/hideaki/res/histogram.html" rel="nofollow">This page from Hideaki Shimazaki</a> explains an alternative method. It is a bit more complicated to calculate, but seems to do a great job. The top part of the page is a Java app. Scroll past that to see the theory and explanation, then keep scrolling to find links to the papers that explain the method.</p>
",25,"2012-12-23 06:12:04",NULL,NULL,NULL,3,NULL,25,"2012-12-23 06:12:04",NULL,798,NULL,NULL,NULL
811,2,NULL,"2010-07-27 16:18:28",9,NULL,"<blockquote>
  <p>It is the mark of a truly intelligent
  person to be moved by statistics.</p>
</blockquote>

<p>George Bernard Shaw</p>
",NULL,"2010-07-27 16:18:28",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-27 16:18:28",726,NULL,"Michael Bishop",NULL
812,1,904,"2010-07-27 16:19:42",6,942,"<p>There are a lot of references in the statistic literature to "<strong>functional data</strong>" (i.e. data that are curves), and in parallel, to "<strong>high dimensional data</strong>" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. </p>

<p>When talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions, it can be polynomes, splines, wavelet, Fourier, .... and will translate the functional problem into a finite dimensional vectorial problem (since in applied mathematic everything comes to be finite at some point). </p>

<p><strong>My question is:</strong>  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? </p>

<p>If the answer is no, can you illustrate ?</p>

<p>EDIT/UPDATE with the help of Simon Byrne's answer:</p>

<ul>
<li>sparsity (S-sparse assumption, $l^p$ ball and weak $l^p$ ball for $p&lt;1$) is used as a structural assumption in high dimensional statistical analysis. </li>
<li>"smoothness" is used as a structural assumption in functional data analysis. </li>
</ul>

<p>On the other hand, inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity by wavelet and fourier transform. This make the critical difference mentionned by Simon not so critical?</p>
",223,"2011-02-10 08:14:49","What is the difference between functional data analysis and high dimensional data analysis",<data-mining><signal-processing><curve-fitting><wavelet>,2,1,6,223,"2011-02-10 08:14:49",NULL,NULL,NULL,NULL,NULL
813,2,NULL,"2010-07-27 16:23:12",57,NULL,"<blockquote>
  <p>Statistical thinking will one day be
  as necessary a qualification for
  efficient citizenship as the ability
  to read and write.</p>
</blockquote>

<p>--H.G. Wells</p>
",NULL,"2010-07-27 16:23:12",NULL,NULL,NULL,4,NULL,NULL,NULL,"2010-07-27 16:23:12",726,NULL,"Michael Bishop",NULL
814,2,NULL,"2010-07-27 16:26:53",4,NULL,"<blockquote>
  <p>These days the statistician is often
  asked such questions as "Are you a
  Bayesian?" "Are you a frequentist?"
  "Are you a data analyst?" "Are you a
  designer of experiments?". I will
  argue that the appropriate answer to
  ALL of these questions can be (and
  preferably should be) "yes", and that
  we can see why this is so if we
  consider the scientific context for
  what statisticians do.</p>
</blockquote>

<p>--G.E.P. Box</p>
",NULL,"2010-07-27 16:26:53",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-27 16:26:53",726,NULL,"Michael Bishop",NULL
815,2,NULL,"2010-07-27 16:34:22",4,NULL,"<p>I must admit that I do not really understand your question:</p>

<ul>
<li><p>your raindrops example is not very satisfying since this is not illustrating the fact that the Gaussian behaviour comes from the "average of a large number of iid random variables".</p></li>
<li><p>if the quantity $X$ that you are interested in is an average $\\frac{Y_1+\\ldots+Y_N}{N}$ that fluctuates around its mean in a Gaussian way, you can also expect that $\\frac{f(Y_1)+\\ldots+f(Y_N)}{N}$ has a Gaussian behaviour.</p></li>
<li><p>if the fluctuation of $X$ around its mean are approximately Gaussian and small, then so are the fluctuation of $f(X)$ around its mean (by Taylor expansion)</p></li>
<li><p>could you cite some true examples of (real life) Gaussian behaviour coming from averaging: this is not very common! Gaussian behaviour is often used in statistics as a first rough approximation because the computations are very tractable. As physicists uses the harmonic approximation, statisticians uses the Gaussian approximation.</p></li>
</ul>
",368,"2010-07-27 16:34:22",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,779,NULL,NULL,NULL
816,2,NULL,"2010-07-27 16:34:55",4,NULL,"<p>Of course one should never try to blindly normalize data if the data does not follow a (single) normal distribution.</p>

<p>For example one might want to rescale observables $X$ to all be normal with $(X-\\mu)/\\sigma$, but this can only work if the data is normal and if both $\\mu$ and $\\sigma$ are the same for all data points (e.g. $\\sigma$ doesn't depend on $\\mu$ in a particular $X$ range).</p>
",56,"2010-09-27 23:40:16",NULL,NULL,NULL,0,NULL,159,"2010-09-27 23:40:16",NULL,806,NULL,NULL,NULL
817,2,NULL,"2010-07-27 16:37:40",4,NULL,"<p>Vipul, you're not being totally precise in your question. </p>

<blockquote>
  <p>This is typically justified using the
  central limit theorem, which says that
  when you average a large number of iid
  random variables, you get a normal
  distribution.</p>
</blockquote>

<p>I'm not entirely sure this is what you're saying, but keep in mind that the raindrops in your example are not iid random variables. The mean calculated by sampling a certain number of those raindrops is a random variables, and as the means are calculated using a large enough sample size, the distribution of that sample mean is normal. </p>

<p>The law of large numbers says that the value of that sample mean converges to the average value of the population (strong or weak depending on type of convergence). </p>

<p>The CLT says that the sample mean, call it XM(n), which is a random variable, has a distribution, say G(n). As n approaches infintity, that distribution is the normal distribution. CLT is all about <em>convergence in distribution</em>, not a basic concept. </p>

<p>The observations you draw (diameter, area, volume) don't have to be normal at all. They probably won't be if you plot them. But, the sample mean from taking all three observations will have a normal distribution. And, the volume won't be the cube of the diameter, nor will the area be the square of the diameter. The square of the sums is not going to be the sum of the squares, unless you get oddly lucky. </p>
",62,"2010-07-27 16:37:40",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,779,NULL,NULL,NULL
818,2,NULL,"2010-07-27 16:37:42",1,NULL,"<p>We mostly use:</p>

<ul>
<li>ggplot - for charts</li>
<li>stats</li>
<li>e1071 - for <a href="http://en.wikipedia.org/wiki/Support_vector_machine" rel="nofollow">SVMs</a></li>
</ul>
",480,"2010-08-09 08:15:05",NULL,NULL,NULL,1,NULL,509,"2010-08-09 08:15:05","2010-07-27 16:37:42",73,NULL,NULL,NULL
819,2,NULL,"2010-07-27 16:40:37",3,NULL,"<p>The PPV and NPV are proportions. You know the numerator and denominator, which both are positive integers, so you can calculate the proportion. Your goal, I presume, is to quantify how well you have determined those values. If your sample size is huge, then those values are likely to be very close to their true population values. If the sample size is small, then there is more random variability and those values are more likely to be far from their true value. </p>

<p>The confidence intervals you calculated tell you what you want to know. Given a bunch of assumptions, you can be 95% confident that the confidence interval includes the population value. </p>

<p>What about the standard error? The problem is that the uncertainty of a proportion is not symmetrical. The asymmetry is very noticeable when the proportion is far from 0.50 and the sample size is small. Since the confidence interval is not symmetrical around the observed proportion, trying to express this uncertainty as a single plus/minus standard error won't be very informative. It makes more sense to report the confidence interval. </p>

<p>If you really want to calculate a standard error, even knowing it isn't very helpful, here is the formula to compute an approximate standard error from p (the proportion) and n (sample size; the denominator): </p>

<p><img src="http://upload.wikimedia.org/math/2/8/5/285265cbedc09f97b3c98762a3433393.png" alt=""></p>
",25,"2010-07-27 16:40:37",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,805,NULL,NULL,NULL
820,1,822,"2010-07-27 16:42:39",0,125,"<p>I am building a web application for used book trading and I am adding a feature to propose other book that would be interesting when they view an offer.</p>

<p>Currently the data that I store are the following (they are updated each time someone visit an offer) :</p>

<p>ISBN (book of the offer)<br>
SessId (a unique id that everyone has when visiting the website)<br>
NumberOfVisit (The number of time someone has view an offer of that book)</p>

<p>I also have access to some user update data which categorize the book by subject and course. It isn't necessarily up-to-date and precise, but it's nonetheless data.</p>

<p>What are the approach to list the most interesting books for a book ?</p>
",488,"2010-08-07 17:52:01","How can I link item by relevance?",<algorithms>,2,1,1,88,"2010-08-07 17:52:01",NULL,NULL,NULL,NULL,NULL
821,2,NULL,"2010-07-27 16:46:45",1,NULL,"<p>You probably want to look at <a href="http://en.wikipedia.org/wiki/Recommender_system" rel="nofollow">recommender systems</a>. </p>
",NULL,"2010-07-27 16:46:45",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,820,NULL,user28,NULL
822,2,NULL,"2010-07-27 16:50:45",2,NULL,"<p>There are many,many ways to do this....I'd suggest googling for some search terms to get started, such as "market basket analysis", or having  a look at Toby Segaran's "Programming Collective Intelligence" if you know python (even if you don't - it is pretty easy to understand).</p>
",247,"2010-07-27 16:50:45",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,820,NULL,NULL,NULL
823,2,NULL,"2010-07-27 16:50:51",-7,NULL,"<blockquote>
  <p>Let’s be clear: the work of science
  has nothing whatever to do with
  consensus. Consensus is the business
  of politics. Science, on the contrary,
  requires only one investigator who
  happens to be right, which means that
  he or she has results that are
  verifiable by reference to the real
  world. In science consensus is
  irrelevant. What is relevant is
  reproducible results. The greatest
  scientists in history are great
  precisely because they broke with the
  consensus.</p>
  
  <p>There is no such thing as consensus
  science. If it’s consensus, it isn’t
  science. If it’s science, it isn’t
  consensus. Period.</p>
</blockquote>

<p>Michael Crichton</p>
",25,"2010-07-27 16:50:51",NULL,NULL,NULL,8,NULL,NULL,NULL,"2010-07-27 16:50:51",726,NULL,NULL,NULL
824,2,NULL,"2010-07-27 16:56:35",15,NULL,"<p>Bayes' theorem is a relatively simple, but fundamental result of probability theory that allows for the calculation of certain conditional probabilities.  Conditional probabilities are just those probabilities that reflect the influence of one event on the probability of another. </p>

<p>Simply put, in its most famous form, it states that the probability of a hypothesis given new data (<strong>P(H|D)</strong>; called the posterior probability) is equal to the following equation: the probability of the observed data given the hypothesis (<strong>P(D|H)</strong>; called the conditional probability), times the probability of the theory being true prior to new evidence (<strong>P(H)</strong>; called the prior probability of H), divided by the probability of seeing that data, period (<strong>P(D</strong>); called the marginal probability of D).  </p>

<p>Formally, the equation looks like this:  </p>

<p><img src="http://upload.wikimedia.org/math/3/1/3/313f04bd948b0c467d53cfc822a0fc8a.png" alt="alt text"></p>

<p>The significance of Bayes theorem is largely due to its proper use being a point of contention between schools of thought on probability.  To a subjective Bayesian (that interprets probability as being subjective degrees of belief) Bayes' theorem provides the cornerstone for theory testing, theory selection and other practices, by plugging their subjective probability judgments into the equation, and running with it.  To a frequentist (that interprets probability as <a href="http://en.wikipedia.org/wiki/Frequency_%28statistics%29">limiting relative frequencies</a>), this use of Bayes' theorem is an abuse, and they strive to instead use meaningful (non-subjective) priors (as do objective Bayesians under yet another interpretation of probability). </p>
",39,"2010-07-27 17:15:57",NULL,NULL,NULL,2,NULL,39,"2010-07-27 17:15:57",NULL,672,NULL,NULL,NULL
825,1,827,"2010-07-27 17:04:37",22,5272,"<p>I have R-scripts for reading large amounts of csv data from different files and then perform machine learning tasks such as svm for classification.
<br>
Are there any libraries for making use of multiple cores on the server for R.
<br>
or 
<br>
What is most suitable way to achieve that?</p>
",480,"2010-10-14 18:01:00","Any suggestions for making R code use multiple processors?",<r><parallel-computing><multicore>,5,8,7,930,"2010-10-14 18:01:00",NULL,NULL,NULL,NULL,NULL
826,2,NULL,"2010-07-27 17:06:17",4,NULL,"<p>I like to demonstrate sampling variation and essentially the Central Limit Theorem through an "in-class" exercise. Everybody in the class of say 100 students writes their age on a piece of paper. All pieces of paper are the same size and folded in the same fashion after I've calculated the average. This is the population and I calculate the average age. Then each student randomly selects 10 pieces of paper, writes down the ages and returns them to the bag. (S)he calculates the mean and passes the bag along to the next student. Eventually we have 100 samples of 10 students each estimating the population mean which we can describe through a histogram and some descriptive statistics.</p>

<p>We then repeat the demonstration this time using a set of 100 "opinions" that replicate some Yes/No question from recent polls e.g. If the (British General) election were called tomorrow would you consider voting for the British National Party. Students them sample 10 of these opinions.</p>

<p>At the end we've demonstrated sampling variation, the Central Limit Theorem, etc with both continuous and binary data.</p>
",215,"2011-06-02 08:09:07",NULL,NULL,NULL,0,NULL,2116,"2011-06-02 08:09:07","2011-06-02 11:09:11",643,NULL,NULL,NULL
827,2,NULL,"2010-07-27 17:06:30",13,NULL,"<p>If it's on Linux, then the most straight-forward is <a href="http://cran.r-project.org/web/packages/multicore/index.html"><strong>multicore</strong></a>.  Beyond that, I suggest having a look at <a href="http://www.stats.uwo.ca/faculty/yu/Rmpi/">MPI</a> (especially with the <a href="http://cran.r-project.org/web/packages/snow/index.html"><strong>snow</strong></a> package).</p>

<p>More generally, have a look at:</p>

<ol>
<li>The <a href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">High-Performance Computing view</a> on CRAN.</li>
<li><a href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">"State of the Art in Parallel Computing with R"</a></li>
</ol>

<p>Lastly, I recommend using the <a href="http://cran.r-project.org/web/packages/foreach/index.html">foreach</a> package to abstract away the parallel backend in your code.  That will make it more useful in the long run.</p>
",5,"2010-07-27 17:06:30",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,825,NULL,NULL,NULL
828,2,NULL,"2010-07-27 17:12:15",4,NULL,"<p>Shane is correct. Both <a href="http://cran.r-project.org/package=multicore" rel="nofollow">multicore</a> and <a href="http://cran.r-project.org/package=Rmpi" rel="nofollow">Rmpi</a> are winners.</p>

<p>Slightly broader coverage of the topic is in the <a href="http://cran.r-project.org/web/views/HighPerformanceComputing.html" rel="nofollow">CRAN Task View on High-Performance Computing</a>.  This also links to a fairly recent survey article on <a href="http://www.jstatsoft.org/v31/i01/" rel="nofollow">Parallel Computing with R</a> from JSS.  </p>

<p>Lastly, a few hands-on examples and tips are in the <em>Intro to HPC with R</em> tutorial I give once in a while -- see my <a href="http://dirk.eddelbuettel.com" rel="nofollow">presentations page</a> for the most recent copy from last week at useR.</p>
",334,"2010-07-27 17:12:15",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,825,NULL,NULL,NULL
829,2,NULL,"2010-07-27 17:15:37",3,NULL,"<p>Whether one can normalize a non-normal data set depends on the application.  For example, data normalization is required for many statistical tests (i.e. calculating a z-score, t-score, etc.)  Some tests are more prone to failure when normalizing non-normal data, while some are more resistant ("robust" tests).  </p>

<p>One less-robust statistic is the mean, which is sensitive to outliers (i.e. non-normal data).  Alternatively, the median is less sensitive to outliers (and therefore more robust).</p>

<p>A great example of non-normal data when many statistics fail is bi-modally distributed data.  Because of this, it's always good practice to visualize your data as a frequency distribution (or even better, test for normality!)</p>
",491,"2010-07-27 17:15:37",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,806,NULL,NULL,NULL
830,2,NULL,"2010-07-27 17:18:36",3,NULL,"<p>Both Shane and Dirk's responses  are spot on. </p>

<p>Nevertheless, you might wanna take a look at a commercial version of R, called <a href="http://www.revolutionanalytics.com/" rel="nofollow">Revolution R</a> which is built to deal with big datasets and run on multiple cores. This software is free for academics (which might be your case, I dont know)</p>
",447,"2010-07-27 17:18:36",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,825,NULL,NULL,NULL
831,2,NULL,"2010-07-27 17:19:27",1,NULL,"<p>In the case of log-normally distributed data .... the geometric mean is a better measure of central tendancy than the arithmetic mean. I mean I would guess they look at the paper and have seen a log-normal distribution. </p>

<p>Spots ... makes me think its referring to probes from a microarray .. in which case they do tend to do log-normally distributed (can't find the reference though sorry).</p>
",494,"2010-07-27 17:19:27",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,790,NULL,NULL,NULL
832,2,NULL,"2010-07-27 17:37:35",2,NULL,"<p>If you use Stata, you can use the -clt- command that creates graphs of sampling distributions, see</p>

<p><a href="http://www.ats.ucla.edu/stat/stata/ado/teach/clt.htm" rel="nofollow">http://www.ats.ucla.edu/stat/stata/ado/teach/clt.htm</a></p>
",NULL,"2010-07-27 17:37:35",NULL,NULL,NULL,1,NULL,NULL,NULL,"2011-06-02 11:09:11",643,NULL,"Michael Mitchell",NULL
833,2,NULL,"2010-07-27 17:44:35",8,NULL,"<p>Yes and no. At the theoretical level, both cases can use similar techniques and frameworks (an excellent example being Gaussian process regression).</p>

<p>The critical difference is the assumptions used to prevent overfitting (regularization):</p>

<ul>
<li><p>In the functional case, there is usually some assumption of smoothness, in other words, values occurring close to each other should be similar in some systematic way. This leads to the use of techniques such as splines, loess, Gaussian processes, etc.</p></li>
<li><p>In the high-dimensional case, there is usually an assumption of sparsity: that is, only a subset of the dimensions will have any signal. This leads to techniques aiming at identifying those dimensions (Lasso, LARS, slab-and-spike priors, etc.)</p></li>
</ul>

<p>UPDATE:</p>

<p>I didn't really think about wavelet/Fourier methods, but yes, the thresholding techniques used for such methods are aiming for sparsity in the projected space. Conversely, some high-dimensional techniques assume a projection on to a lower-dimensional manifold (e.g. principal component analysis), which is a type of smoothness assumption.</p>
",495,"2010-07-28 11:37:58",NULL,NULL,NULL,1,NULL,495,"2010-07-28 11:37:58",NULL,812,NULL,NULL,NULL
834,1,NULL,"2010-07-27 17:46:25",3,3981,"<p>I've heard that AIC can be used to choose among several models (which regressor to use).</p>

<p>But i would like to understand formally what it is in a kind of "advanced undergraduated" level,which i think would be something formal but with intuition arising from the formula.</p>

<p>And is it possible to implement AIC in stata with complex survey data?</p>

<p>Thank you very much in advance!!</p>

<p>Regards!</p>
",498,"2011-04-29 00:34:45","What is AIC? Looking for a formal but intuitive answer",<stata><aic>,3,3,3,3911,"2011-04-29 00:34:45",NULL,NULL,NULL,NULL,NULL
835,2,NULL,"2010-07-27 17:57:48",9,NULL,"<p><a href="http://googleblog.blogspot.com/2009/02/from-height-of-this-place.html" rel="nofollow">Data is the sword of the 21st century, those who wield it well, the Samurai.</a></p>
",62,"2010-07-27 17:57:48",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-27 17:57:48",726,NULL,NULL,NULL
836,2,NULL,"2010-07-27 18:14:44",3,NULL,"<p>It is a heuristic, and as such, has been subjected to <em>extensive</em> testing.  So when to trust it or not is not simple clear-cut and always-true decision.</p>

<p>At a rough approximation, it trades off goodness of fit and number of variables ("degrees of freedom"). Much more, as usual, <a href="http://en.wikipedia.org/wiki/Akaike_information_criterion" rel="nofollow">at the Wikipedia article about AIC</a>.</p>
",334,"2010-07-27 18:14:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,834,NULL,NULL,NULL
837,1,858,"2010-07-27 18:25:40",0,1195,"<p>I am creating multiple logistic regression models using lrm from Harrell's Design package in R.  One model I would like to make is the model with no predictors.  For example, I want to predict a constant c such that: </p>

<pre><code>logit(Y) ~ c
</code></pre>

<p>I know I how to compute c (divide the number of "1"s by the total), what I would like is to use <code>lrm</code> so I can manipulate it as a model in a consistent way with the other models I am making.  Is this possible, and if so how?  </p>

<p>I have tried so far:</p>

<pre><code>library(Design)
data(mtcars)
lrm(am ~ 1, data=mtcars)
</code></pre>

<p>which gives the error:</p>

<pre><code>Error in dimnames(stats) &lt;- list(names(cof), c("Coef", "S.E.", "Wald Z",  :
    length of 'dimnames' [1] not equal to array extent
</code></pre>

<p>and I have tried:</p>

<pre><code>lrm(am ~ ., data=mtcars)
</code></pre>

<p>But this uses all the predictors, rather then none of the predictors.</p>
",501,"2013-08-09 00:48:47","R lrm model with no predictors",<r><logistic>,2,0,NULL,7290,"2013-08-09 00:47:53",NULL,NULL,NULL,NULL,NULL
838,2,NULL,"2010-07-27 18:28:36",2,NULL,"<p>Basically one needs a loss function in order to optimize anything. AIC provides the loss function which when minimized gives a "optimal"* model which fits the given data. The AIC loss function (2k-2*log(L)) tries to formulate the bias variance trade off that every statistical modeler faces when fitting a model to finite set of data.</p>

<p>In other words while fitting a model if you increase the number of parameters you will improve the log likelihood but will run into the danger of over fitting. The AIC penalizes for increasing the number of parameters thus minimizing the AIC selects the model where the improvement in log likelihood is not worth the penalty for increasing the number of parameters.</p>

<ul>
<li>Note that when I say optimal model it is optimal in the sense that the model minimizes the AIC. There are other criteria (e.g. BIC) which may give other "optimal" models.  </li>
</ul>

<p>I don't have any experience with stata so cannot help you with the other part of the question. </p>
",288,"2010-07-27 18:28:36",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,834,NULL,NULL,NULL
839,2,NULL,"2010-07-27 18:33:51",4,NULL,"<p>Though it is not generally labeled as Bayesian search theory, such methods are pretty widely used in oil exploration.  There are, however, important differences in the standard examples that drive different features of their respective modeling problems.  </p>

<p>In the case of lost vessel exploration (in Bayesian search theory), we are looking for a specific point on the sea floor (one elevation), with a  distributions modeling the likelihood of its resting location, and another distribution modeling the likelihood of finding the boat were it at that depth.  These distributions are then guide search, and are continuously updated through the results of the guided search.</p>

<p>Though similar, oil exploration is fraught with complicating features (multiple sampling depths, high sampling costs, variable yields, multiple geological indicators, drilling cost, etc.) that necessitate methods that go beyond what is considered in the prior example.  See <a href="http://www.nek.lu.se/ryde/NordicEcont09/Papers/levitt.pdf" rel="nofollow">Learning through Oil and Gas Exploration</a> for an overview of these complicating factors and a way to model them.</p>

<p>So, yes, it may be said that the oil exploration problem is different in magnitude, but not kind from lost vessel exploration, and thus similar methods may be fruitfully applied. Finally, a quick literature search reveals many different modeling approaches, which is not too surprising, given the complicated nature of the problem.</p>
",39,"2010-07-27 18:39:12",NULL,NULL,NULL,0,NULL,39,"2010-07-27 18:39:12",NULL,743,NULL,NULL,NULL
840,2,NULL,"2010-07-27 18:33:56",1,NULL,"<p>I work with both R and <a href="http://en.wikipedia.org/wiki/MATLAB" rel="nofollow">MATLAB</a> and I use <a href="http://cran.r-project.org/web/packages/R.matlab/index.html" rel="nofollow">R.matlab</a> a lot to transfer data between the two.</p>
",288,"2010-08-09 08:13:30",NULL,NULL,NULL,0,NULL,509,"2010-08-09 08:13:30","2010-07-27 18:33:56",73,NULL,NULL,NULL
841,1,844,"2010-07-27 18:38:02",14,1016,"<p>I have $N$ paired observations ($X_i$, $Y_i$) drawn from a common unknown distribution, which has finite first and second moments, and is symmetric around the mean.</p>

<p>Let $\\sigma_X$ the standard deviation of $X$ (unconditional on $Y$), and $\\sigma_Y$ the same for Y. I would like to test the hypothesis  </p>

<p>$H_0$: $\\sigma_X = \\sigma_Y$</p>

<p>$H_1$: $\\sigma_X \\neq \\sigma_Y$</p>

<p>Does anyone know of such a test? I can assume in first analysis that the distribution is normal, although the general case is more interesting. I am looking for a closed-form solution. Bootstrap is always a last resort.</p>
",30,"2011-03-13 20:31:13","Comparing the variance of paired observations",<distributions><hypothesis-testing><standard-deviation><normal-distribution>,5,2,2,223,"2011-02-10 07:50:44",NULL,NULL,NULL,NULL,NULL
842,2,NULL,"2010-07-27 18:39:32",5,NULL,"<p>I noticed that the previous answers lack some general HPC considerations.<br>
First of all, neither of those packages will enable you to run <strong>one</strong> SVM in parallel. So what you can speed up is parameter optimization or cross-validation, still you must write your own functions for that. Or of course you may run the job for different datasets in parallel, if it is a case.<br>
The second issue is memory; if you want to spread calculation over a few physical computers, there is no free lunch and you must copy the data -- here you must consider if it makes sense to predistribute a copy of data across computers to save some communication. On the other hand if you wish to use multiple cores on one computer, than the multicore is especially appropriate because it enables all child processes to access the memory of the parent process, so you can save some time and a lot of memory space.</p>
",88,"2010-07-27 18:39:32",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,825,NULL,NULL,NULL
843,2,NULL,"2010-07-27 18:47:25",0,NULL,"<p>If I need to determine the number of bins programmatically I usually start out with a histogram that has way more bins than needed. Once the histogram is filled I then combine bins until I have enough entries per bin for the method I am using, e.g. if I want to model Poisson-uncertainties in a counting experiment with uncertainties from a normal distribution until I have more than something like 10 entries.</p>
",56,"2010-07-27 18:47:25",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,798,NULL,NULL,NULL
844,2,NULL,"2010-07-27 18:48:54",3,NULL,"<p>You could use the fact that the <a href="http://en.wikipedia.org/wiki/Variance#Distribution_of_the_sample_variance" rel="nofollow">distribution of the sample variance</a> is a chi square distribution centered at the true variance. Under your null hypothesis, your test statistic would be the difference of two chi squared random variates centered at the same unknown true variance. I do not know whether the difference of two chi-squared random variates is an identifiable distribution but the above may help you to some extent.</p>
",NULL,"2010-07-27 18:48:54",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,841,NULL,user28,NULL
845,2,NULL,"2010-07-27 19:02:09",5,NULL,"<p>If you want to go down the non-parametric route you could always try the squared ranks test. </p>

<p>For the unpaired case, the assumptions for this test (taken from <a href="http://www.stat.wvu.edu/~xiawang/courses/stat551/hwk/hw14.pdf" rel="nofollow">here</a>) are:</p>

<ol>
<li>Both samples are random samples from their respective populations.</li>
<li>In addition to independence within each sample there is mutual independence between
the two samples.</li>
<li>The measurement scale is at least interval.</li>
</ol>

<p>These <a href="http://www.stat.wvu.edu/~xiawang/courses/stat551/hwk/hw14.pdf" rel="nofollow">lecture notes</a> describe the unpaired case in detail.</p>

<p>For the paired case you will have to change this procedure slightly. Midway down <a href="http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/signrank.htm" rel="nofollow">this page</a> should give you an idea of where to start.</p>
",8,"2010-07-28 00:08:20",NULL,NULL,NULL,4,NULL,8,"2010-07-28 00:08:20",NULL,841,NULL,NULL,NULL
846,1,851,"2010-07-27 19:10:53",1,437,"<p>I'm working on regression models in STATISTICA application and I need to know what is Fisher-Snedecor distribution for and how to analyze my regression model in this distribution. What the significance level means? What is v1 and v2? I need an explanation and little tutorial on real data.</p>

<p>Thanks in advance!</p>
",503,"2010-08-11 12:58:41","Regression output and Fisher-Snedecor distribution",<regression>,1,1,NULL,8,"2010-08-11 12:58:41",NULL,NULL,NULL,NULL,NULL
847,1,NULL,"2010-07-27 19:12:53",0,123,"<p>Google Website Optimizer (GWO) is a tool provided by Google to do <a href="http://en.wikipedia.org/wiki/A/B_testing" rel="nofollow">A/B</a> and <a href="http://en.wikipedia.org/wiki/Multivariate_testing" rel="nofollow">MVT</a> experiments on websites.</p>

<p>This has been an unanswered question for a long time so I thought I'd ask it here and see if I can get any help. Here is some documentation (clues) that Google has published about the statistics used:</p>

<ul>
<li><a href="http://www.google.com/support/websiteoptimizer/bin/answer.py?hl=en&amp;answer=61146#stats" rel="nofollow">All about statistics</a></li>
<li><a href="http://www.google.com/support/websiteoptimizer/bin/answer.py?hl=en&amp;answer=74818" rel="nofollow">Fractional versus Full Factorial analysis</a></li>
</ul>

<p>Example Data (<a href="http://dl.dropbox.com/u/2511052/gwoTestResults/WebOptimizer-summary.pdf" rel="nofollow">summary pdf</a>):</p>

<ul>
<li><a href="http://dl.dropbox.com/u/2511052/gwoTestResults/WebsiteOptimizer.xml" rel="nofollow">XML</a></li>
<li><a href="http://dl.dropbox.com/u/2511052/gwoTestResults/WebsiteOptimizer.csv" rel="nofollow">CSV</a></li>
</ul>

<p>I (as well as many others) are interested in discovering the math used by Google, because no one has been able to reproduce their calculations yet.</p>

<p>Thank you in advance for any help that you can provide.</p>
",500,"2013-09-16 22:37:44","What statistical model is used to calculate the test results for GWO?",<computational-statistics><model>,1,3,1,22468,"2013-09-16 22:37:44",NULL,NULL,NULL,NULL,NULL
848,2,NULL,"2010-07-27 19:19:46",0,NULL,"<p>As an addition to Harvey's response if you have a small sample of data then there are alternatives to the standard error approximation formula shown above. I would recommend using software, one example would be to use the <code>binom.test</code> function in R, to calculate these confidence intervals rather than doing it "by hand".</p>
",NULL,"2010-07-27 19:19:46",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,805,NULL,Ralph,NULL
849,2,NULL,"2010-07-27 19:22:33",21,NULL,"<p>Principal Components Analysis (PCA) and Common Factor Analysis (CFA) are distinct methods.  Often, they produce similar results and PCA is used as the default extraction method in the SPSS Factor Analysis routines.  This undoubtedly results in a lot of confusion about the distinction between the two.</p>

<p>The bottom line is, these are two different models, conceptually.  In PCA, the components are actual orthogonal linear combinations that maximize the total variance.  In FA, the factors are linear combinations that maximize the shared portion of the variance--underlying "latent constructs".  That's why FA is often called "common factor analysis".  FA uses a variety of optimization routines and the result, unlike PCA, depends on the optimization routine used and starting points for those routines.  Simply there is not a single unique solution.</p>

<p>In R, the factanal() function provides CFA with a maximum likelihood extraction.  So, you shouldn't expect it to reproduce an SPSS result which is based on a PCA extraction.  It's simply not the same model or logic.  I'm not sure if you would get the same result if you used SPSS's Maximum Likelihood extraction either as they may not use the same algorithm.  </p>

<p>For better or for worse in R, you can, however, reproduce the mixed up "factor analysis" that SPSS provides as its default.  Here's the process in R.  With this code, I'm able to reproduce the SPSS Principal Component "Factor Analysis" result using this dataset.  (With the exception of the sign, which is indeterminant).  That result could also then be rotated using any of Rs available rotation methods.</p>

<pre><code># Load the base dataset attitude to work with.
data(attitude)
# Compute eigenvalues and eigen vectors of the correlation matrix.
pfa.eigen&lt;-eigen(cor(attitude))
# Print and note that eigen values are those produced by SPSS.
# Also note that SPSS will extract 2 components as eigen values &gt; 1 = 2
pfa.eigen$values
# set a value for the number of factors (for clarity)
factors&lt;-2
# Extract and transform two components.
pfa.eigen$vectors [ , 1:factors ]  %*% 
+ diag ( sqrt (pfa.eigen$values [ 1:factors ] ),factors,factors )
</code></pre>
",485,"2010-07-27 19:27:45",NULL,NULL,NULL,4,NULL,485,"2010-07-27 19:27:45",NULL,612,NULL,NULL,NULL
850,2,NULL,"2010-07-27 19:28:38",0,NULL,"<p>Try creating a dummy variable with so many ones as the number of rows in mtcars and put it formula instead of one.</p>
",88,"2010-07-27 19:28:38",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,837,NULL,NULL,NULL
851,2,NULL,"2010-07-27 19:34:06",4,NULL,"<p>The Fisher Snedecor distribution is another name for the <a href="http://en.wikipedia.org/wiki/F-distribution" rel="nofollow">F-distribution</a>. The F-distribution comes from the ratio of two chi-squared variables.</p>

<p>In regression, you use the F-distribution in the <a href="http://en.wikipedia.org/wiki/Analysis_of_variance" rel="nofollow">ANOVA</a> table. This table gives you information about which variables/covariates should be in your model.</p>

<p>The v1 and v2 are called the degrees of freedom. In simple linear regression,o.e.</p>

<p>Y = a + bx + epsilon</p>

<p>we have a single extra variable. So v1 = p = 1 and v2 = n-p-1 = n-2.</p>

<p>I don't use STATISTICA, so I can't give you more detailed information.</p>
",8,"2010-07-27 19:34:06",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,846,NULL,NULL,NULL
852,1,NULL,"2010-07-27 19:52:26",2,361,"<p>The coefficient on a logged explanatory variable when the dependent variable also is in log form is an elasticity (or the percentage change in the dependent variable if the explanatory variable changes by one percent).  Suppose I estimate a regression without logging the dependent variable but I use a log link in a General Linear Model (and family gaussian) while the explanatory variable remains in log form.  Is the coefficient on that explanatory variable still an elasticity?</p>
",NULL,"2010-11-02 04:32:16","Elasticities Using GLM",<econometrics><generalized-linear-model>,1,0,NULL,159,"2010-11-02 04:32:16",NULL,NULL,NULL,"David Jacobs",NULL
853,2,NULL,"2010-07-27 19:55:30",1,NULL,"<p>It sounds like they simply use Choice Modeling. You would use conditional logistic regression to analyze it. </p>

<p>The people from JMP (SAS) have a nice (free) book about Design of Experiments and Choice Modeling.</p>
",74,"2010-07-27 19:55:30",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,847,NULL,NULL,NULL
854,2,NULL,"2010-07-27 19:58:44",2,NULL,"<p>I would use <strong>Fisher's Exact Test</strong>, even for large N. I wouldn't know why not. Any performance argument predates today's fast computers.</p>
",506,"2010-07-27 19:58:44",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,192,NULL,NULL,NULL
855,2,NULL,"2010-07-27 20:05:20",1,NULL,"<p>I thought this was too obvious, until I saw this <a href="http://stats.stackexchange.com/questions/795/normalization-of-series" rel="nofollow">question</a>!</p>

<p>When you normalise data, make sure you always have access to the raw data after normalisation. Of course, you could break this rule if you have a good reason, e.g. storage.</p>
",8,"2010-07-27 20:05:20",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,806,NULL,NULL,NULL
856,1,1306,"2010-07-27 20:24:45",8,1655,"<p>Does anyone know of a variation of Fisher's Exact Test which takes weights into account? For instance <a href="http://www.measuredhs.com/help/Datasets/sampling_weights.htm">sampling weights</a>.<br>
So instead of the usual 2x2 cross table, every data point has a "mass" or "size" value weighing the point.</p>

<p>Example data:</p>

<pre><code>A B weight
N N 1
N N 3
Y N 1
Y N 2
N Y 6
N Y 7
Y Y 1
Y Y 2
Y Y 3
Y Y 4
</code></pre>

<p>Fisher's Exact Test then uses this 2x2 cross table:</p>

<pre><code>A\\B  N  Y All
 N   2  2   4
 Y   2  4   6
All  4  6  10
</code></pre>

<p>If we would take the weight as an 'actual' number of data points, this would result in:</p>

<pre><code>A\\B  N  Y All
 N   4 13  17
 Y   3 10  13
All  7 23  30
</code></pre>

<p>But that would result in much too high a confidence. One data point changing from N/Y to N/N would make a very large difference in the statistic.<br>
Plus, it wouldn't work if any weight contained fractions.</p>
",506,"2011-04-24 08:04:05","Fisher's Exact Test with weights?",<hypothesis-testing>,3,0,1,506,"2010-07-29 15:21:16",NULL,NULL,NULL,NULL,NULL
857,2,NULL,"2010-07-27 20:25:16",6,NULL,"<p>Operations Research began during wartime in the 1940s with scientists and others addressing problems in Radar operations, Anti-Submarine Warfare (ASW), and air operations. It is really a methodology to help decision makers choose a course of action by using an analytic framework that includes statistics, linear and non-linear programming, game theory, decision theory, etc. Statistics is one of many tools it uses.</p>
",482,"2010-07-27 20:25:16",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,775,NULL,NULL,NULL
858,2,NULL,"2010-07-27 20:33:48",1,NULL,"<p>I'm not sure why this doesn't work with <code>lrm</code>.  However, R does Logistic Regression just fine with its own internal functions.  See GLM.  Here's your model, working...</p>

<pre><code>summary(glm(am~1, data = mtcars, family=binomial(link=logit)))
</code></pre>

<p>So, unless you need something that <code>lrm()</code> from design provides, then use GLM with the binomial logit link.</p>
",485,"2013-08-09 00:48:47",NULL,NULL,NULL,2,NULL,7290,"2013-08-09 00:48:47",NULL,837,NULL,NULL,NULL
859,1,867,"2010-07-27 21:07:20",11,1535,"<p>Sorry for the verbose background to this question:</p>

<p>Occasionally in investigations of animal behaviour, an experimenter is interested in the amount of time that a subject spends in different, pre-defined zones in a test apparatus. I've often seen this sort of data analyzed using ANOVA; however, I have never been entirely convinced of the validity of such analyses, given that ANOVA assumes the observations are independent, and they never actually are independent in these analyses (since more time spent in one zone means that less is spent in other zones!). </p>

<p>For example,</p>

<blockquote>
  <p>D. R. Smith, C. D. Striplin, A. M.
  Geller, R. B. Mailman, J. Drago, C. P.
  Lawler, M. Gallagher, <a href="http://www.sciencedirect.com/science/article/B6T0F-3THY168-F/2/007a74da6fbe693e297e50ec91dec141">Behavioural
  assessment of mice lacking D1A
  dopamine receptors</a>, Neuroscience,
  Volume 86, Issue 1, 21 May 1998, Pages
  135-146</p>
</blockquote>

<p>In the above article, they reduce the degrees of freedom by 1 in order to compensate for the non-independence. However, I am not sure how such a manipulation can actually ameliorate this violation of ANOVA assumptions. </p>

<p>Perhaps a chi-squared procedure might be more appropriate? What would you do to analyze data like this (preference for zones, based on time spent in zones)?</p>

<p>Thanks!</p>
",445,"2014-05-21 17:36:11","ANOVA with non-independent observations",<anova>,4,0,4,930,"2010-12-21 17:06:54",NULL,NULL,NULL,NULL,NULL
860,2,NULL,"2010-07-27 21:51:43",4,NULL,"<p>Let $f$ be your true distribution, and $g$ the family from which you are trying to fit your data. Then $\\theta$, the maximum likelihood estimator of parameters of $g$, is a random variable. You could formulate model selection as finding the distribution family $g$ that minimizes the expected KL divergence between $f$ and $g(\\theta)$, which can be written as</p>

<p>$$\\text{Entropy}(f)-E_x E_y[\\log(g(x|\\theta(y)))]$$</p>

<p>Since you are minimizing over $g$, the Entropy($f$) term doesn't matter and you look for $g$ that maximizes $E_x E_y[\\log(g(x|\\theta(y)))]$.</p>

<p>Let $L(\\theta(y)|y)$ be the likelihood of data $y$ according to $g(\\theta)$. You could estimate $E_x E_y[\\log(g(x|\\theta(y)))]$ as $\\log(L(\\theta(y)|y))$ but that estimator is biased.</p>

<p>Akaike's showed that when $f$ belongs to family $g$ with dimension $k$, the following estimator is asymptotically unbiased</p>

<p>$$\\log(L(\\theta(y)|y))-k$$</p>

<p>Burnham has more details in this <a href="http://faculty.washington.edu/skalski/classes/QERM597/papers_xtra/Burnham%20and%20Anderson.pdf" rel="nofollow">paper</a>, also blog <a href="http://www.emakalic.org/blog/?p=26" rel="nofollow">post</a> by Enes Makalic has further explanation and references</p>
",511,"2010-11-18 00:36:44",NULL,NULL,NULL,2,NULL,159,"2010-11-18 00:36:44",NULL,834,NULL,NULL,NULL
861,2,NULL,"2010-07-27 22:33:05",3,NULL,"<p>There is a free book on Geostatistical Mapping with R <a href="http://spatial-analyst.net/book/" rel="nofollow">here</a> it might help your problem.</p>
",481,"2010-07-27 22:33:05",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,743,NULL,NULL,NULL
862,2,NULL,"2010-07-28 00:23:22",16,NULL,"<p>The Freedman-Diaconis rule is very robust and works well in practice. The bin-width is set to $h=2*\\text{IQR}*n^{-1/3}$. So the number of bins is (max-min)/$h$.</p>

<p>In base R, you can use 
<code>
hist(x,breaks="FD")
</code></p>

<p>For other plotting libraries without this option (e.g. ggplot2), you can calculate binwidth as: </p>

<pre><code>bw &lt;- diff(range(x)) / (2 * IQR(x) * length(x)^(1/3)))
### for example
ggplot() + geom_histogram(aes(x), binwidth = bw)
</code></pre>
",159,"2014-05-29 15:58:50",NULL,NULL,NULL,4,NULL,1381,"2014-05-29 15:58:50",NULL,798,NULL,NULL,NULL
863,2,NULL,"2010-07-28 00:29:41",4,NULL,"<p>Mike,</p>

<p>I agree that an ANOVA based on total time probably isn't the correct approach here.  Further, I'm not convinced that Chi Sqaure solves your problem.  Chi square will respect the idea that you can't be in two locations at the same time, but it doesn't address the problem that there are likely dependencies between time N and time N+1.  In regards to this second issue, I see some analogies between your situation and what people run into with eye and mouse tracking data. A multinomial model of some sort may serve your purposes well.  Unfortunately, the details of that type of model are beyond my expertise.  I'm sure some statistics book somewhere has a nice little primer on that topic, but off the top of my head I'd point you towards:</p>

<ul>
<li>Barr D.J. (2008) Analyzing ‘visual world’ eyetracking data using multilevel logistic regression. Journal of Memory and Language, Special Issue: Emerging Data Analysis (59) pp 457-474</li>
<li><a href="https://r-forge.r-project.org/projects/gmpm/" rel="nofollow">https://r-forge.r-project.org/projects/gmpm/</a> is a non-parametric approach to the same issue being developed by Dr. Barr</li>
</ul>

<p>If anything, both of those sources should be more than complete because they get into how to analyze the time course of the position.</p>
",196,"2010-07-28 00:29:41",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,859,NULL,NULL,NULL
864,2,NULL,"2010-07-28 01:00:22",3,NULL,"<p>I am going to suggest an answer that is very different from that of a traditional ANOVA. Let T be the total time that is available for an animal to spend in all the zones. You could define T as the total amount of waking time or some such. Suppose that you have J zones. Then by definition you have:</p>

<p>Sum T_j = T</p>

<p>You could normalize the above by dividing the lhs and the rhs by T and you get</p>

<p>Sum P_j = 1</p>

<p>where P_j is the proportion of time that an animal spends in zone j.</p>

<p>Now the question you have is if P_j is significantly different from 1 / J for all j.</p>

<p>You could assume that P_j follows a <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution" rel="nofollow">dirichlet distribution</a> and estimate two models.</p>

<p><strong>Null Model</strong> </p>

<p>Set the parameters of the distribution such that P_j = 1 / J. (Setting the parameters of the distribution to 1 will do.)</p>

<p><strong>Alternative Model</strong></p>

<p>Set the parameters of the distribution to be a function of zone specific covariates. You could then estimate the model parameters.</p>

<p>You would choose the alternative model if it outperforms the null model on some critera (e.g., likelihood ratio).</p>
",NULL,"2010-07-28 01:00:22",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,859,NULL,user28,NULL
865,2,NULL,"2010-07-28 01:06:56",0,NULL,"<p>Just to expand on Rob's answer a bit, suppose that we want to know the cumulative distribution function (CDF) of the highest value of $N$ independent draws from a standard normal distribution, $X_1, ..., X_N$. Call this highest value $Y_1$, the first order statistic. Then the CDF is:</p>

<p>$$ \\begin{align*}P(Y_1 &lt; x) &amp;= P(\\max(X_1, ..., X_N) &lt; x) \\\\\\n&amp;= P(X_1 &lt; x, ..., X_N &lt; x)\\n\\\\\\n&amp;= P(X_1 &lt; x) \\cdot \\cdot \\cdot  P(X_N &lt; x) \\\\\\n&amp;= P(X &lt; x)^{100},\\n\\end{align*} $$
where the second line follows by independence of the draws. We can also write this as
$$F_{Y_1}(x) = F_X(x)^{100},$$
where $F$ represents the CDF and $f$ represents the PDF of the random variable given as a subscript to this function.</p>

<p>Rob uses the standard notation that $\\Phi(x)$ is defined as $P(X &lt; x)$ for a standard normal---<i>i.e.,</i> $\\Phi(x)$ is the standard normal CDF.</p>

<p>The probability density function (PDF) of the first order statistic is just the derivative of the CDF with respect to $X$:
$$f_{Y_1}(x) = 100 \\cdot F_X(x)^{99} f_X(x)$$
the CDF at $x$ raised to 99 (that is, $N-1$) times the PDF at $x$ times 100 (that is, $N$).</p>
",401,"2012-04-01 04:18:55",NULL,NULL,NULL,2,NULL,401,"2012-04-01 04:18:55",NULL,225,NULL,NULL,NULL
866,1,876,"2010-07-28 01:10:18",55,11646,"<p>Say I want to estimate a large number of parameters, and I want to penalize some of them because I believe they should have little effect compared to the others. How do I decide what penalization scheme to use? When is ridge regression more appropriate? When should I use lasso?</p>
",455,"2013-07-26 13:52:42","When should I use lasso vs ridge?",<regression><lasso><ridge-regression>,3,3,39,159,"2010-08-09 12:25:13",NULL,NULL,NULL,NULL,NULL
867,2,NULL,"2010-07-28 01:17:57",3,NULL,"<p>(Caveat Emptor: I'm not an expert in this area)</p>

<p>If you just want to talk about differences in time spent per location, then submitting the "time-per-location" data as counts in a multinomial mixed model (see the MCMCglmm package for R), using subject as a random effect, should do the trick. </p>

<p>If you want to talk about differences in location preference <em>through</em> time, then maybe bin time to reasonable intervals (maybe to the resolution of your timing device?), classify each interval according to the mouse's location at that time (eg. if 3 locations, each interval gets labelled either 1, 2, or 3), and again use a multinomial mixed effects model with subject as a random effect but this time add interval as a fixed effect (though possibly only after factorizing interval, which drops power but should help capture non-linearities through time).</p>
",364,"2010-07-28 01:17:57",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,859,NULL,NULL,NULL
868,1,917,"2010-07-28 03:15:34",3,792,"<p>I created a quick fun Excel Spreadsheet tonight to try and predict which video games I'll enjoy if I buy them. I'm wondering if this quick example makes sense from a Logistic Regression perspective and if I am computing all of the values correctly.</p>

<p>Unfortunately, if I did everything correctly I doubt I have much to look forward to on my XBOX or PS3 ;)</p>

<p>I laid out a few categories and weighted them like so (Real spreadsheet lists twice as many or so):</p>

<p><code>
4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</p>

<p>Visually Stunning &nbsp; Exhilirating&nbsp; Artistic&nbsp; Sporty
</code></p>

<p>Then I went through some games I have and rated them in each category (ratings of 0-4). I then set a separate cell to be the value of Beta_0 and tuned that until the resulting percentages all looked about right.</p>

<p>Next I entered in my expected ratings for the new games I <strong>was</strong> looking forward to and got percentages for those.</p>

<p>Example:
Beta_0 := -35</p>

<p><code>
4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</p>

<p>Visually Stunning &nbsp; Exhilirating&nbsp; Artistic&nbsp; Sporty</p>

<p>4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1
</code></p>

<p>Would be calculated as</p>

<p>P = 1 / [1 + e^(-35 + (4*4 + 4*4 + 3*0 + 1*1)]
P = 88.1%</p>

<p>If I were to automate the regression am I correct in thinking I'd be tuning Beta_0 to make it so the positive training examples come out high and the negative training examples come out low?</p>

<p>I'm completely new to this (just started today thanks to this site actually!) so please have no concern about bruising my ego, I'm eager to learn more.</p>

<p>Thanks!</p>
",9426,"2011-01-07 01:55:24","Training a Logistic Regression Model",<logit><logistic>,4,1,2,NULL,NULL,NULL,NULL,NULL,NULL,NULL
869,2,NULL,"2010-07-28 03:49:27",2,NULL,"<p>Usually in logistic regression you'd want "successes" to be 1 and "failures" to be 0, but so long as you are consistent in how you enter your data and interpret it, the coefficients don't really care.</p>
",196,"2010-07-28 03:49:27",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,868,NULL,NULL,NULL
870,1,956,"2010-07-28 03:54:56",13,14181,"<p>Given a list of p-values generated from independent tests, sorted in ascending order, one can use the <a href="http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf">Benjamini-Hochberg procedure</a> for <a href="http://en.wikipedia.org/wiki/False_discovery_rate#Independent_tests">multiple testing correction</a>. For each p-value, the Benjamini-Hochberg procedure allows you to calculate the False Discovery Rate (FDR) for each of the p-values. That is, at each "position" in the sorted list of p-values, it will tell you what proportion of those are likely to be false rejections of the null hypothesis.</p>

<p>My question is, are these FDR values to be referred to as "<strong><a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1074290335">q-values</a></strong>", or as "<strong>corrected p-values</strong>", or as something else entirely?</p>

<p><strong>EDIT 2010-07-12:</strong> I would like to more fully describe the correction procedure we are using. First, we sort the test results in increasing order by their un-corrected original p-value. Then, we iterate over the list, calculating what I have been interpreting as "the FDR expected if we were to reject the null hypothesis for this and all tests prior in the list," using the B-H correction, with an alpha equal to the observed, un-corrected p-value for the respective iteration. We then take, as what we've been calling our "q-value", the maximum of the previously corrected value (FDR at iteration i - 1) or the current value (at i), to preserve monotonicity.</p>

<p>Below is some Python code which represents this procedure:</p>

<pre><code>def calc_benjamini_hochberg_corrections(p_values, num_total_tests):
    """
    Calculates the Benjamini-Hochberg correction for multiple hypothesis
    testing from a list of p-values *sorted in ascending order*.

    See
    http://en.wikipedia.org/wiki/False_discovery_rate#Independent_tests
    for more detail on the theory behind the correction.

    **NOTE:** This is a generator, not a function. It will yield values
    until all calculations have completed.

    :Parameters:
    - `p_values`: a list or iterable of p-values sorted in ascending
      order
    - `num_total_tests`: the total number of tests (p-values)

    """
    prev_bh_value = 0
    for i, p_value in enumerate(p_values):
        bh_value = p_value * num_total_tests / (i + 1)
        # Sometimes this correction can give values greater than 1,
        # so we set those values at 1
        bh_value = min(bh_value, 1)

        # To preserve monotonicity in the values, we take the
        # maximum of the previous value or this one, so that we
        # don't yield a value less than the previous.
        bh_value = max(bh_value, prev_bh_value)
        prev_bh_value = bh_value
        yield bh_value
</code></pre>
",520,"2013-03-07 01:26:38","Multiple hypothesis testing correction with Benjamini-Hochberg, p-values or q-values?",<hypothesis-testing>,1,4,7,520,"2010-08-12 16:07:55",NULL,NULL,NULL,NULL,NULL
871,1,875,"2010-07-28 04:08:23",19,9102,"<p>I realize this is pedantic and trite, but as a researcher in a field outside of statistics, with limited formal education in statistics, I always wonder if I'm writing "p-value" correctly. Specifically:</p>

<ol>
<li>Is the "p" supposed to be capitalized?</li>
<li>Is the "p" supposed to be italicized? (Or in mathematical font, in TeX?)</li>
<li>Is there supposed to be a hyphen between "p" and "value"?</li>
<li>Alternatively, is there no "proper" way of writing "p-value" at all, and any dolt will understand what I mean if I just place "p" next to "value" in some permutation of these options?</li>
</ol>
",520,"2010-07-29 19:39:02","Correct formatting (capitalization, italicization, hyphenation) of "p-value"?",<hypothesis-testing>,4,1,5,NULL,NULL,NULL,NULL,NULL,NULL,NULL
872,2,NULL,"2010-07-28 04:21:06",6,NULL,"<p>This seems to be a style issue with different journals and publishers adopting different conventions (or allowing a mixed muddle of styles depending on authors' preferences). My own preference, for what it's worth, is p-value, hyphenated with no italics and no capitalization.</p>
",159,"2010-07-28 04:21:06",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,871,NULL,NULL,NULL
873,2,NULL,"2010-07-28 04:24:05",1,NULL,"<p>The <a href="http://www.amstat.org/publications/chance/assets/style.pdf" rel="nofollow">ASA House Style</a> seems to recommend italicizing the p with hyphen: <em>p</em>-value.  A google scholar search shows <a href="http://scholar.google.com/scholar?q=p+value&amp;hl=en&amp;btnG=Search&amp;as_sdt=80001&amp;as_sdtp=on" rel="nofollow">varied spellings</a>.</p>
",251,"2010-07-28 04:24:05",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,871,NULL,NULL,NULL
874,2,NULL,"2010-07-28 04:26:17",9,NULL,"<p>Ridge or lasso are forms of regularized linear regressions.  The regularization can also be interpreted as prior in a maximum a posteriori estimation method.  Under this interpretation, the ridge and the lasso make different assumptions on the class of linear transformation they infer to relate input and output data.   In the ridge, the coefficients of the linear transformation are normal distributed and in the lasso they are  Laplace distributed.  In the lasso, this makes it easier for the coefficients to be zero and therefore easier to eliminate some of your input variable as not contributing to the output.</p>

<p>There are also some practical considerations.  The ridge is a bit easier to implement and faster to compute, which may matter depending on the type of data you have. </p>

<p>If you have both implemented, use subsets of your data to find the ridge and the lasso and compare how well they work on the left out data.  The errors should give you an idea of which to use.   </p>
",260,"2010-07-28 04:26:17",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,866,NULL,NULL,NULL
875,2,NULL,"2010-07-28 04:29:37",16,NULL,"<p>There do not appear to be "standards".  For example:</p>

<ul>
<li>The <a href="http://www.nature.com/nature/authors/gta/#a5.6">Nature style guide</a> refers to "P value"</li>
<li>This <a href="http://my.ilstu.edu/~jhkahn/apastats.html">APA style guide</a> refers to "<em>p</em> value"</li>
<li>The <a href="http://bloodjournal.hematologylibrary.org/authors/stylecheckforfigs.dtl">Blood style guide</a> says:
<ul>
<li>Capitalize and italicize the <em>P</em> that introduces a <em>P</em> value</li>
<li>Italicize the <em>p</em> that represents the Spearman rank correlation test</li>
</ul></li>
<li><a href="http://en.wikipedia.org/wiki/P-value">Wikipedia</a> uses "p-value", but italicizes the <em>p</em> if used by itself</li>
</ul>

<p>My brief, unscientific survey suggests that the most common combination is lower-case, italicized <em>p</em> without a hyphen.</p>
",163,"2010-07-28 04:29:37",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,871,NULL,NULL,NULL
876,2,NULL,"2010-07-28 05:55:31",31,NULL,"<p>Keep in mind that ridge regression can't zero out coefficients; thus, you either end up including all the coefficients in the model, or none of them. In contrast, the LASSO does both parameter shrinkage and variable selection automatically. If some of your covariates are highly correlated, you may want to look at the Elastic Net [3] instead of the LASSO.</p>

<p>I'd personally recommend using the Non-Negative Garotte (NNG) [1] as it's consistent in terms of estimation and variable selection [2]. Unlike LASSO and ridge regression, NNG requires an initial estimate that is then shrunk towards the origin. In the original paper, Breiman recommends the least-squares solution for the initial estimate (you may however want to start the search from a ridge regression solution and use something like GCV to select the penalty parameter).</p>

<p>In terms of available software, I've implemented the original NNG in MATLAB (based on Breiman's original FORTRAN code). You can download it from: </p>

<p><a href="http://www.emakalic.org/blog/wp-content/uploads/2010/04/nngarotte.zip">http://www.emakalic.org/blog/wp-content/uploads/2010/04/nngarotte.zip</a></p>

<p>BTW, if you prefer a Bayesian solution, check out [4,5].</p>

<p>References:</p>

<p>[1] Breiman, L. Better Subset Regression Using the Nonnegative Garrote Technometrics, 1995, 37, 373-384</p>

<p>[2] Yuan, M. &amp; Lin, Y. On the non-negative garrotte estimator Journal of the Royal Statistical Society (Series B), 2007, 69, 143-161</p>

<p>[3] Zou, H. &amp; Hastie, T. Regularization and variable selection via the elastic net Journal of the Royal Statistical Society (Series B), 2005, 67, 301-320</p>

<p>[4] Park, T. &amp; Casella, G. The Bayesian Lasso Journal of the American Statistical Association, 2008, 103, 681-686</p>

<p>[5] Kyung, M.; Gill, J.; Ghosh, M. &amp; Casella, G. Penalized Regression, Standard Errors, and Bayesian Lassos Bayesian Analysis, 2010, 5, 369-412</p>
",530,"2013-07-26 13:52:42",NULL,NULL,NULL,0,NULL,17230,"2013-07-26 13:52:42",NULL,866,NULL,NULL,NULL
877,1,NULL,"2010-07-28 06:31:36",3,716,"<p>Has anyone gone through some papers using Vector Error Correction Models in causality applications with more than one cointegration vectors, say two. I guess there will be more than one ECM terms. How to assess the endogeneity of the  left hand variables if t-stats on different ECM coefficients yield different (conflicting) results. For left hand side variables to be endogenous should we have both ECM coefficients negative and significant?</p>

<p>Javed Iqbal</p>
",531,"2010-07-28 06:31:36","Time Series Econometrics: VECM with multiple cointegration vectors",<econometrics>,0,1,2,NULL,NULL,NULL,NULL,"2013-02-15 16:21:19",NULL,NULL
878,2,NULL,"2010-07-28 06:49:12",38,NULL,"<blockquote>
  <p>Absence of evidence is not evidence of
  absence.</p>
</blockquote>

<p>Carl Sagan</p>
",NULL,"2010-08-07 10:06:32",NULL,NULL,NULL,6,NULL,380,"2010-08-07 10:06:32","2010-07-28 06:49:12",726,NULL,Tzippy,NULL
879,2,NULL,"2010-07-28 07:23:22",6,NULL,"<p>Maybe the paper "<a href="http://pubs.research.avayalabs.com/pdfs/ALR-2007-003-paper.pdf">Variations on the histogram</a>" by Denby and Mallows will be of interest:</p>

<blockquote>
  <p>This new display which we term "dhist" (for diagonally-cut histogram) preserves the desirable features of both the equal-width hist and the equal-area hist. It will show tall narrow bins like the e-a hist when there are spikes in the data and will show isolated outliers just like the usual histogram.</p>
</blockquote>

<p>They also mention that code in R is available on request.</p>
",251,"2010-07-28 07:23:22",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,798,NULL,NULL,NULL
880,1,NULL,"2010-07-28 08:15:40",6,392,"<p>My question is about cross validation when there are many more variables than observations. To fix ideas, I propose to restrict to the classification framework in very high dimension (more features than observation).</p>

<p><strong>Problem:</strong> Assume that for each variable $i=1,\\dots,p$ you have a measure of importance $T[i]$ than exactly measure the interest of feature $i$ for the classification problem. The problem of selecting a subset of feature to reduce optimally the classification error is then reduced to that of finding the number of features. </p>

<p><strong>Question:</strong> What is the most efficient way to run cross validation in this case (cross validation scheme)? My question is not about how to write the code but on the version of cross validation to use when trying to find the number of selected feature (to minimize the classification error) but how to deal with the high dimension when doing cross validation (hence the problem above may be a bit like a 'toy problem' to discuss CV in high dimension). </p>

<p><strong>Notations:</strong>  $n$ is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example $p=10000$ and $n=100$). </p>
",223,"2010-09-02 07:13:57","Cross validation in very high dimension (to select the number of used variables in very high dimensional classification)",<machine-learning><classification><cross-validation>,1,2,4,223,"2010-09-02 07:13:57",NULL,NULL,NULL,NULL,NULL
881,1,1189,"2010-07-28 08:15:51",5,264,"<p>Here's something I've wondered about for a while, but haven't been able to discover the correct terminology. Say you have a relatively complicated density function that you suspect might have a close approximation as a sum of (properly weighted) simpler density functions. Have such things been studied? I'm particularly interested in reading about any applications.</p>

<p>Here's one example I've found:</p>

<p><a href="http://www.soa.org/library/research/transactions-of-society-of-actuaries/1966/january/tsa66v18pt1n5211.pdf" rel="nofollow">Expansion of probability density functions as a sum of gamma densities with applications in risk theory</a></p>
",34,"2011-03-28 08:58:47","Series expansion of a density function",<probability><mixture><pdf>,3,0,1,88,"2011-03-28 08:58:47",NULL,NULL,NULL,NULL,NULL
882,2,NULL,"2010-07-28 08:21:47",4,NULL,"<p><strong>Histogram density estimator</strong> is estimating the density with a sum of <strong>piecewise functions</strong> (density of a uniform).</p>

<p><strong>KDE</strong> is using a sum of <strong>smooth function</strong> (gaussian  is an example) (as long as they are positive they can be transformed into a density by normalization) </p>

<p>The use of "<strong>mixture</strong>" in statistic is about convex combination of densities.</p>
",223,"2010-08-03 19:20:24",NULL,NULL,NULL,0,NULL,223,"2010-08-03 19:20:24",NULL,881,NULL,NULL,NULL
883,2,NULL,"2010-07-28 09:09:51",3,NULL,"<p>You can do this with <strong>mixture modeling</strong>. There are a number of R packages on CRAN for doing this. Search for "mixture" at <a href="http://cran.r-project.org/web/packages/" rel="nofollow">http://cran.r-project.org/web/packages/</a></p>
",159,"2010-07-28 09:09:51",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,881,NULL,NULL,NULL
884,1,NULL,"2010-07-28 09:54:14",28,1102,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href="http://stats.stackexchange.com/questions/16921/how-to-understand-degrees-of-freedom">How to understand degrees of freedom?</a>  </p>
</blockquote>



<p>I was at a talk a few months back where the speaker used the term 'degrees of freedom'. She briefly said something along the lines of it meaning the number of values used to form a statistic that are free to vary.</p>

<p>What does this mean? I'm specifically looking for an intuitive explanation. </p>
",541,"2012-08-07 09:37:08","What are "degrees of freedom"?",<degrees-of-freedom>,0,0,0,88,"2010-08-07 17:51:33",NULL,NULL,"2012-08-07 14:36:18",NULL,NULL
886,1,934,"2010-07-28 11:31:59",9,1538,"<p>The 'fundamental' idea of statistics for estimating parameters is <a href="http://en.wikipedia.org/wiki/Maximum_likelihood">maximum likelihood</a>. I am wondering what is the corresponding idea in machine learning.</p>

<p>Qn 1. Would it be fair to say that the 'fundamental' idea in machine learning for estimating parameters is: 'Loss Functions'</p>

<p>[Note: It is my impression that machine learning algorithms often optimize a loss function and hence the above question.]</p>

<p>Qn 2: Is there any literature that attempts to bridge the gap between statistics and machine learning?</p>

<p>[Note: Perhaps, by way of relating loss functions to maximum likelihood. (e.g., OLS is equivalent to maximum likelihood for normally distributed errors etc)]</p>
",NULL,"2010-08-04 09:07:44","What is the 'fundamental' idea of machine learning for estimating parameters?",<machine-learning><maximum-likelihood><loss-functions>,5,4,3,NULL,NULL,NULL,NULL,NULL,user28,NULL
887,1,NULL,"2010-07-28 12:02:21",3,723,"<p>Suppose there is a very big (infinite?) population of normally distributed values with unknown mean and variance.</p>

<p>Suppose also that we have a sample, <em>S</em>, of <em>n</em> values from the entire population. We can calculate mean and standard deviation for this sample (we use <em>n-1</em> for stdev calculation).</p>

<p>The first and most important question is how is stdev(S) related to the standard deviation of the entire population?</p>

<p>An illustration for this issue is the second question:</p>

<p>Suppose we have an additional number, <em>x</em>, and we would like to test whether it is an vis-a-vis the general population. My intuitive approach is to calculate Z as follows:</p>

<p>Z = (x - mean(S))/stdev(S)</p>

<p>and then test it against standard distribution if <em>n>30</em> or against t-distribution if <em>n&lt;30</em>.</p>

<p>However, this approach doesn't account for <em>n</em>, the size of the sample. What is the right way to solve this question provided there is only single sample <em>S</em>?</p>
",213,"2012-11-02 17:12:58","Basic question regarding variance and stdev of a sample",<standard-deviation><variance><normality><sample><unbiased-estimator>,6,1,2,449,"2010-10-19 07:21:44",NULL,NULL,NULL,NULL,NULL
889,2,NULL,"2010-07-28 12:12:22",1,NULL,"<p>My first answer was full of errors. Here is a corrected version:</p>

<p>The correct way to test is as follows:</p>

<p>z = (mean(S) - mu) / (stdev(S) / sqrt(n) )</p>

<p>See: <a href="http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_one-sample_t-test" rel="nofollow">Student's t-test</a></p>

<p>Note the following:</p>

<ol>
<li><p>The sample size is accounted for when you divide the standard deviation by the square root of the sample size.</p></li>
<li><p>You should also note that the z-test is for testing whether the true mean of the population is some particular value. It does not make sense to substitute x instead of mu in the above statistic.</p></li>
</ol>
",NULL,"2010-07-28 12:20:09",NULL,NULL,NULL,0,NULL,NULL,"2010-07-28 12:20:09",NULL,887,NULL,user28,user28
890,1,NULL,"2010-07-28 12:13:03",2,913,"<p>I am struggling a little bit at the moment with a question related to logistic regression. I have a model that predicts the occurrence of animal based on land cover with reference to forest. I am not grasping the concept of a reference class and struggle to extrapolate the model onto a new area. Any explanations or guidance towards papers, lecture notes etc would be highly appreciated. </p>
",NULL,"2012-10-27 22:09:44","Reference category and prediction",<logistic>,2,1,NULL,88,"2012-10-27 22:09:44",NULL,NULL,NULL,Mike,NULL
893,2,NULL,"2010-07-28 12:48:14",12,NULL,"<p>I really like first sentence from 
<a href="http://www.jerrydallal.com/LHSP/dof.htm">The Little Handbook of Statistical Practice. Degrees of Freedom Chapter</a></p>

<blockquote>
  <p>One of the questions an instrutor
  dreads most from a mathematically
  unsophisticated audience is, "What
  exactly is degrees of freedom?"</p>
</blockquote>

<p>I think you can get really good understanding about degrees of freedom from reading this chapter.</p>
",236,"2010-07-28 12:48:14",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,16921,NULL,NULL,NULL
894,2,NULL,"2010-07-28 12:49:31",22,NULL,"<p>Or simply: the number of elements in a numerical array that you're allowed to change so that the value of the statistic remains unchanged.</p>

<pre><code># for instance if:
x + y + z = 10
</code></pre>

<p>you can change, for instance, <em>x</em> and <em>y</em> at random, but you cannot change <em>z</em> (you can, but not at random, therefore you're not <em>free</em> to change it - see Harvey's comment), 'cause you'll change the value of the statistic (&Sigma; = 10). So, in this case df = 2.</p>
",1356,"2010-07-28 17:34:31",NULL,NULL,NULL,3,NULL,1356,"2010-07-28 17:34:31",NULL,16921,NULL,NULL,NULL
895,2,NULL,"2010-07-28 12:50:26",2,NULL,"<p>I'm finding it rather tricky to see what you are asking:</p>

<ol>
<li>If you want to know whether the Var(S) is different from the population variance, then see this previous <a href="http://stats.stackexchange.com/questions/841/comparing-the-variance-of-paired-observations/845#845">answer</a>.</li>
<li>If you want to determine whether the mean(S) and the mean(X) are the same, then look at <a href="http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test" rel="nofollow">Independent two-sample t-tests</a>.</li>
<li>If you want to test whether mean(S) is equal to the population mean, then see @Srikant answer above, i.e. a <a href="http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_one-sample_t-test" rel="nofollow">one-sample t-test</a>.</li>
</ol>
",8,"2010-07-28 12:50:26",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,887,NULL,NULL,NULL
897,1,905,"2010-07-28 13:32:32",8,2528,"<p>What is the difference between offline and <a href="http://en.wikipedia.org/wiki/Online_machine_learning">online learning</a>?  Is it just a matter of learning over the entire dataset (offline) vs. learning incrementally (one instance at a time)?  What are examples of algorithms used in both?</p>
",284,"2014-08-21 13:48:17","Online vs offline learning?",<machine-learning>,1,0,4,88,"2010-08-07 17:50:51",NULL,NULL,NULL,NULL,NULL
898,1,918,"2010-07-28 13:49:07",5,725,"<p>I am interested in tools/techniques that can be used for analysis of <a href="http://en.wikipedia.org/wiki/Real-time_data">streaming data in "real-time"</a>*, where latency is an issue.  The most common example of this is probably price data from a financial market, although it also occurs in other fields (e.g. finding trends on Twitter or in Google searches).</p>

<p>In my experience, the most common software category for this is <a href="http://en.wikipedia.org/wiki/Complex_event_processing">"<strong>complex event processing</strong>"</a>.  This includes commercial software such as <a href="http://www.streambase.com/index.htm">Streambase</a> and <a href="http://www.sybase.com/products/financialservicessolutions/aleristreamingplatform">Aleri</a> or open-source ones such as <a href="http://esper.codehaus.org/">Esper</a> or <a href="http://telegraph.cs.berkeley.edu/">Telegraph</a> (which was the basis for <a href="http://www.truviso.com/">Truviso</a>).  </p>

<p>Many existing models are not suited to this kind of analysis because they're too computationally expensive.  Are any models** specifically designed to deal with real-time data?  What tools can be used for this?</p>

<p><i>
* By "real-time", I mean "analysis on data <em>as it is created</em>".  So I do not mean "data that has a time-based relevance" (as in <a href="http://www.hilarymason.com/blog/conference-web2-expo-sf/">this talk by Hilary Mason</a>).</p>

<p>** By "model", I mean a mathematical abstraction that describe the behavior of an object of study (e.g. in terms of random variables and their associated probability distributions), either for description or forecasting.  This could be a machine learning or statistical model.
</i></p>
",5,"2010-11-11 05:30:32","Modeling of real-time streaming data?",<modeling><software><real-time>,6,1,6,NULL,NULL,NULL,NULL,NULL,NULL,NULL
899,1,NULL,"2010-07-28 13:53:18",11,860,"<p>I'm trying to separate two groups of values from a single data set. I can assume that one of the populations is normally distributed and is at least half the size of the sample. The values of the second one are both lower or higher than the values from the first one (distribution is unknown). What I'm trying to do is to find the upper and lower limits that would enclose the normally-distributed population from the other.</p>

<p>My assumption provide me with starting point:</p>

<ul>
<li>all points within the interquartile range of the sample are from the normally-distributed population.</li>
</ul>

<p>I'm trying to test for outliers taking them from the rest of the sample until they don't fit into the 3 st.dev of the normally-distributed population. Which is not ideal, but seem to produce reasonable enough result.</p>

<p>Is my assumption statistically sound? What would be a better way to go about this?</p>

<p>p.s. please fix the tags someone.</p>
",219,"2013-03-17 16:07:09","Separating two populations from the sample",<dataset><outliers><em-algorithm>,4,3,1,686,"2012-10-27 20:15:59",NULL,NULL,NULL,NULL,NULL
900,2,NULL,"2010-07-28 13:57:09",1,NULL,"<p>I am not sure how far this would be relevant to what you want to do but see the paper on adaptive question design called <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.8664&amp;rep=rep1&amp;type=pdf" rel="nofollow">FASTPACE</a>. The goal of the algorithm is to ask the next question from a survey respondent based on his/her previous questions and answers.</p>

<p>The data does not arrive as fast as stock prices but nevertheless latency is an issue as most survey respondents expect the next question to appear within a few seconds.</p>
",NULL,"2010-07-28 13:57:09",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,898,NULL,user28,NULL
901,2,NULL,"2010-07-28 13:59:49",1,NULL,"<p><strong>RODBC</strong> for accessing data from databases, <strong>sqldf</strong> for performing simple SQL queries on dataframes (although I am forcing myself to use native R commands), and <strong>ggplot2</strong> and <strong>plyr</strong></p>
",11,"2010-07-28 13:59:49",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-28 13:59:49",73,NULL,NULL,NULL
902,2,NULL,"2010-07-28 14:01:46",2,NULL,"<p>It is going to depend a lot on what exactly you are looking for, but start at <a href="http://www.cs.rutgers.edu/~muthu/stream-1-1.ps" rel="nofollow">Data Streams: Algorithms and Application by Muthukrishnan </a>.</p>

<p>There are many others that can be found by googling "data stream algorithms", or following the references in the paper.</p>
",247,"2010-07-28 16:49:31",NULL,NULL,NULL,1,NULL,247,"2010-07-28 16:49:31",NULL,898,NULL,NULL,NULL
903,2,NULL,"2010-07-28 14:06:35",3,NULL,"<p>There is a series of Google Tech Talk videos called <a href="http://video.google.com/videosearch?q=mease+stats+202&amp;sitesearch=#" rel="nofollow">Stats 202 - Statistical Aspects of Data Mining</a></p>
",11,"2010-07-28 14:06:35",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-28 14:06:35",485,NULL,NULL,NULL
904,2,NULL,"2010-07-28 14:16:38",7,NULL,"<p>Functional Data often involves different question.  I've been reading Functional Data Analysis, Ramsey and Silverman, and they spend a lot of times discussing curve registration, warping functions, and estimating derivatives of curves.  These tend to be very different questions than those asked by people interested in studying high-dimensional data.</p>
",549,"2010-07-28 14:16:38",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,812,NULL,NULL,NULL
905,2,NULL,"2010-07-28 14:37:17",9,NULL,"<p>Online learning means that you are doing it as the data comes in...offline means that you have a static dataset.</p>

<p>So for online learning, you (typically) have more data, but you have time constraints.  Another wrinkle that can affect online learning is that your concepts might change through time.</p>

<p>Let's say you want to build a classifier to recognize spam.  You can acquire a large corpus of e-mail, label it, and train a classifier on it.  This would be offline learning.  Or, you can take all the e-mail coming into your system, and continuously update your classifier (labels may be a bit tricky...).  This would be online learning.</p>
",549,"2010-07-28 14:37:17",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,897,NULL,NULL,NULL
908,2,NULL,"2010-07-28 14:58:10",6,NULL,"<p>You miss one important issue -- there is almost never such thing as T[i]. Think of a simple problem in which the sum of two attributes (of a similar amplitude) is important; if you'd remove one of them the importance of the other will suddenly drop. Also, big amount of irrelevant attributes is the accuracy of most classifiers, so along their ability to assess importance. Last but not least, stochastic algorithms will return stochastic results, and so even the T[i] ranking can be unstable. So in principle you should at least recalculate T[i] after each (or at least after each non trivially redundant) attribute is removed.</p>

<p>Going back to the topic, the question which CV to choose is mostly problem dependent; with very small number of cases LOO may be the best choice because all other start to reduce to it; still small is rather n=10 not n=100. So I would just recommend random subsampling (which I use most) or K-fold (then with recreating splits on each step). Still, you should also collect not only mean but also the standard deviation of error estimates; this can be used to (approximately) judge which changes of mean are significant ans so help you decide when to cease the process.</p>
",88,"2010-07-28 14:58:10",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,880,NULL,NULL,NULL
909,2,NULL,"2010-07-28 15:24:38",2,NULL,"<p>This assumes that you don't even know if the second distribution is normal or not; I basically handle this uncertainty by focusing only on the normal distribution. This may or may not be the best approach.</p>

<p>If you can assume that the two populations are completely separated (i.e., all values from distribution A are less than all values from distribution B), then one approach is to use the optimize() function in R to search for the break-point that yields estimates of the mean and sd of the normal distribution that make the data most likely:</p>

<pre><code>#generate completely separated data
a = rnorm(100)
b = rnorm(100,10)
while(!all(a&lt;b)){
    a = rnorm(100)
    b = rnorm(100,10)
}

#create a mix
mix = c(a,b)

#"forget" the original distributions
rm(a)
rm(b)

#try to find the break point between the distributions
break_point = optimize(
    f = function(x){
        data_from_a = mix[mix&lt;x]
        likelihood = dnorm(data_from_a,mean(data_from_a),sd(data_from_a))
        SLL = sum(log(likelihood))
        return(SLL)
    }
    , interval = c(sort(mix)[2],max(mix))
    , maximum = TRUE
)$maximum

#label the data
labelled_mix = data.frame(
    x = mix
    , source = ifelse(mix&lt;break_point,'A','B')
)
print(labelled_mix)
</code></pre>

<p>If you can't assume complete separation, then I think you'll have to assume some distribution for the second distribution and then use mixture modelling. Note that mixture modelling won't actually label the individual data points, but will give you the mixture proportion and estimates of the parameters of each distribution (eg. mean, sd, etc.).</p>
",364,"2010-07-28 15:24:38",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,899,NULL,NULL,NULL
910,2,NULL,"2010-07-28 15:29:33",3,NULL,"<p>There is a trivial answer -- there is no parameter estimation in machine learning! We don't assume that our models are equivalent to some hidden background models; we treat both reality and the model as black boxes and we try to shake the model box (train in official terminology) so that its output will be similar to that of the reality box.</p>

<p>The concept of not only likelihood but the whole model selection based on the training data is replaced by optimizing the accuracy (whatever defined; in principle the goodness in desired use) on the unseen data; this allows to optimize both precision and recall in a coupled manner. This leads to the concept of an ability to generalize, which is achieved in different ways depending on the learner type.</p>

<p>The answer to the question two depends highly on definitions; still I think that the nonparametric statistics is something that connects the two. </p>
",88,"2010-07-28 15:29:33",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,886,NULL,NULL,NULL
911,2,NULL,"2010-07-28 15:32:38",3,NULL,"<p>Interesting question.  What do you mean by weight?  </p>

<p>I would be inclined to do a bootstrap...pick your favorite statistic (i.e. Fisher's Exact), and compute it on your data.  Then assign new cells to each instance according to your null hypothesis, and repeat the process 999 times.  This should give a pretty good empirical distribution for your test statistic under the null hypothesis, and allow easy computation of your p-value!</p>
",549,"2010-07-28 15:32:38",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,856,NULL,NULL,NULL
912,2,NULL,"2010-07-28 15:37:01",1,NULL,"<p>The other issue is that you put in your data, and the algorithm learns the weights and the Beta_0 for you...I don't know if Excel can do logistic regression...if it doesn't, I'd be inclined to use R to learn your model (and predict future cases for you!).</p>
",549,"2010-07-28 15:37:01",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,868,NULL,NULL,NULL
913,1,916,"2010-07-28 16:15:07",3,600,"<p>Comparing two variables, I came up with the following chart. the x, y pairs represent independent observations of data on the field.  I've doen <a href="http://en.wikipedia.org/wiki/Correlation_and_dependence" rel="nofollow">Pearson correlation</a> on it and have found one of 0.6. </p>

<p>My end goal is to establish a relationship between y and x such that y = f(x).</p>

<p>What analaysis would you recommend to obtain some form ofa relationship between the two variables?</p>

<p><img src="http://koopics.com/ask_math_chart.jpg" alt="Graph"></p>
",59,"2010-09-19 01:47:02","Relationships between two variables",<regression>,8,2,1,88,"2010-09-16 07:04:38",NULL,NULL,NULL,NULL,NULL
914,2,NULL,"2010-07-28 16:21:41",3,NULL,"<p>What you are looking for is called regression; there are a lot of methods you can do it, both statistical and machine learning ones. If you want to find f, you must use statistics; in that case you must first assume that f is of some form, like f:y=a*x+b and then use some regression method to fit the parameters.<br>
The plot suggests there are a lot of outliers (elements that does not follow f(x)); you may need robust regression to get rid of them.</p>
",88,"2010-07-28 16:31:04",NULL,NULL,NULL,0,NULL,88,"2010-07-28 16:31:04",NULL,913,NULL,NULL,NULL
915,2,NULL,"2010-07-28 16:24:38",3,NULL,"<p>And just eyeballing the data, you are probably going to want to transform the data, as (at least to me) it looks skewed. Looking at the histograms of the two variables should suggest which transforms may be beneficial.</p>

<p>As suggested by mbq, more text <a href="http://en.wikipedia.org/wiki/Data_transformation_%28statistics%29" rel="nofollow">here</a>.</p>
",247,"2010-07-28 16:48:05",NULL,NULL,NULL,0,NULL,247,"2010-07-28 16:48:05",NULL,913,NULL,NULL,NULL
916,2,NULL,"2010-07-28 16:31:53",4,NULL,"<p>Normality seems to be strongly violated at least by your y variable. I would log transform y to see if that cleans things up a bit. Then, fit a regression to log(y) ~ x.  The formula the regression will return will be of the form log(y) = \\alpha +  \\beta*x which you can transform back to the original scale by y = exp(\\alpha +  \\beta*x)</p>
",287,"2010-07-28 16:31:53",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,913,NULL,NULL,NULL
917,2,NULL,"2010-07-28 17:01:12",5,NULL,"<p>Like drknexus said, for a logistic regression, your outcome measure needs to be 0 and 1. I'd go back and recode your outcome as 0 (didn't like it), or 1 (did like it). Then, abandon excel and load the data into R (it's really not as intimidating as it looks).  Your regression will look something like this:</p>

<pre><code>glm(Liked ~ Visually.Stunning + Exhilarating + Artistic + Sporty, family = binomial, data = data)
</code></pre>

<p>The regression will return betas for each feature in terms of log-odds. So, for every 1 point increase in <code>Artistic</code>, for instance, you'll have a value for how much that increases or decreases the log-odds of your enjoyment. Most of the betas will be positive, unless you <em>dislike</em> sporty games or something.</p>

<p>Now, you'll have to ask yourself some interesting questions. The assumption of the model is that the values on each of these scores affect your enjoyment <em>independently</em>, which probably isn't true! A game that is very Visually.Stunning and Exhilarating is probably way better than you would expect given those component parts. And it's probably the case that if a game gets scores of 1 on all features except Sporty, which gets a 4, that high Sporty score is worth less than if the other scores were higher.</p>

<p>That is, many or all of your features probably <em>interact</em>. To fit an accurate model, then, you'll want to add in these interactions. That formula would look like this:</p>

<pre><code>glm(Liked ~ Visually.Stunning * Exhilarating * Artistic * Sporty, family = binomial, data = data)
</code></pre>

<p>Now, there are two points of difficulty here. First, you need to have more data to fit a good model with this many interactions than the pure independence model. Second, you risk overfitting, which means that the model will very accurately describe the original data, but will be less good at making accurate predictions for future data.</p>

<p>Needless to say, some people spend all day fitting and refitting models like this one. </p>
",287,"2010-07-28 17:01:12",NULL,NULL,NULL,5,NULL,NULL,NULL,NULL,868,NULL,NULL,NULL
918,2,NULL,"2010-07-28 17:04:38",3,NULL,"<p>This area roughly falls into two categories.  The first concerns stream processing and querying issues and associated models and algorithms.  The second is efficient algorithms and models for learning from data streams (or data stream mining).</p>

<p>It's my impression that the CEP industry is connected to the first area.  For example, StreamBase originated from the <a href="http://www.cs.brown.edu/research/aurora/" rel="nofollow">Aurora</a> project at Brown/Brandeis/MIT.  A similar project was Widom's <a href="http://infolab.stanford.edu/stream/" rel="nofollow">STREAM</a> at Stanford.  Reviewing the publications at either of those projects' sites should help exploring the area.  </p>

<p>A nice paper summarizing the research issues (in 2002) from the first area is <em><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.9846" rel="nofollow">Models and issues in data stream systems</a></em> by Babcock et al.  In stream mining, I'd recommend starting with <em><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.80.798" rel="nofollow">Mining Data Streams: A Review</a></em> by Gaber et al.</p>

<p>BTW, I'm not sure exactly what you're interested in as far as specific models.  If it's stream mining and classification in particular, the <a href="http://en.wikipedia.org/wiki/Incremental_decision_tree#VFDT" rel="nofollow">VFDT</a> is a popular choice.  The two review papers (linked above) point to many other models and it's very contextual.</p>
",251,"2010-07-28 20:07:08",NULL,NULL,NULL,2,NULL,251,"2010-07-28 20:07:08",NULL,898,NULL,NULL,NULL
919,2,NULL,"2010-07-28 17:14:08",1,NULL,"<p>I'm not sure I exactly understand your question, but I'm assuming your confusion involves a categorical predictor in your model. When it comes to continuous variables in a regression, the coefficients for each predictor are weights for the value of the predictor to produce the predicted y value:e.g.  y = 2*x   </p>

<p>However, with a categorical variable, weights are meaningless. What does 2*Male mean, or in your case, 2*forest.</p>

<p>So, the coefficients returned for levels of categorical variables represent how different they are from some reference level. In experimental settings, your reference level would be the control group, and then you would get a coefficient for every treatment group, indicating what the size of the effect of each treatment was.</p>

<p>In my own research, and I'm guessing in yours too, there isn't always a meaningful control category for the reference level. So, what I'd do is set the reference level to whatever category would make exposition of the comparisons easiest. Or, you could use different contrasts, like sum contrasts, but those have their own difficulties especially if you have sparse data for one or more categories.</p>
",287,"2010-07-28 17:14:08",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,890,NULL,NULL,NULL
920,1,940,"2010-07-28 17:15:09",4,653,"<p>I completed a Monte Carlo simulation that consisted of one million ($10^6$) individual simulations. The simulation returns a variable, $p$, that can be either 1 or 0. I then weight the simulations based on predefined criteria and calculate the probability of $p$. I also calculate a risk ratio using $p$:</p>

<p>$$\\text{Risk ratio} = P(p|\\text{test case}) / P(p|\\text{control case})$$</p>

<p>I had eight Monte Carlo runs, which consist of one control case and seven test cases. </p>

<p>I need to know if the probabilities of $p$ are statistically different compared to the other cases. I know I can use a multiple comparison test or nonparametric ANOVA to test individual variables, but how do I do this for probabilities? </p>

<hr>

<p>For example are these two probabilities statistically different?:</p>

<p><em>Probabilities</em>:</p>

<p>$P(p|\\text{test #3}) = 4.08 \\times 10^{-5}$</p>

<p>$P(p|\\text{test #4}) = 6.10 \\times 10^{-5}$</p>

<p><em>Risk Ratios</em>:</p>

<p>$\\text{Risk Ratio}(\\text{test #3}) = 0.089$</p>

<p>$\\text{Risk Ratio}(\\text{test #4}) = 0.119$ </p>
",559,"2013-06-05 04:11:12","Test if probabilities are statistically different?",<hypothesis-testing>,1,0,2,805,"2013-06-05 04:11:12",NULL,NULL,NULL,NULL,NULL
922,2,NULL,"2010-07-28 17:28:47",2,NULL,"<p>I don't think there is a fundamental idea around parameter estimation in Machine Learning.  The ML crowd will happily maximize the likelihood or the posterior, as long as the algorithms are efficient and predict "accurately".  The focus is on computation, and results from statistics are widely used.</p>

<p>If you're looking for fundamental ideas in general, then in computational learning theory, <a href="http://en.wikipedia.org/wiki/Probably_approximately_correct_learning" rel="nofollow">PAC</a> is central; in statistical learning theory, <a href="http://en.wikipedia.org/wiki/Structural_risk_minimization" rel="nofollow">structural risk miniminization</a>; and there are other areas (for example, see the <em><a href="http://hunch.net/?p=612" rel="nofollow">Prediction Science</a></em> post by John Langford).</p>

<p>On bridging statistics/ML, the divide seems exagerrated.  I liked gappy's <a href="http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning/607#607">answer</a> to the "Two Cultures" question.</p>
",251,"2010-07-28 17:28:47",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,886,NULL,NULL,NULL
924,1,926,"2010-07-28 17:36:24",6,719,"<p>For 1,000,000 observations, I observed a discrete event, X, 3 times for the control group and 10 times for the test group. How do I determine for a large number of observations (1,000,000), if three is statistically different than ten?</p>
",559,"2010-10-08 16:05:48","Determine if three is statistically different than ten for a very large number of observations (1,000,000)",<hypothesis-testing><large-data>,7,2,2,8,"2010-10-08 16:05:48",NULL,NULL,NULL,NULL,NULL
925,2,NULL,"2010-07-28 17:40:56",0,NULL,"<p>I would be really surprised if you find the difference statistically significant. Having said that you may want to use a test for a difference of proportions (3 out of 1M vs 10 out of 1M).</p>
",NULL,"2010-07-28 17:40:56",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,924,NULL,user28,NULL
926,2,NULL,"2010-07-28 17:43:48",5,NULL,"<p>I think a simple chi-squared test will do the trick. Do you have 1,000,000 observations for both control and test? If so, your table of observations will be (in R code)</p>

<p>Edit: Woops! Left off a zero!</p>

<pre><code>m &lt;- rbind(c(3, 1000000-3), c(10, 1000000-10))
#      [,1]   [,2] 
# [1,]    3 999997
# [2,]   10 999990
</code></pre>

<p>And chi-squared test will be</p>

<pre><code>chisq.test(m)
</code></pre>

<p>Which returns chi-squared = 2.7692, df = 1, p-value = 0.0961, which is not statistically significant at the p &lt; 0.05 level. I'd be surprised if these could be clinically significant anyway.</p>
",287,"2010-07-28 17:51:05",NULL,NULL,NULL,6,NULL,287,"2010-07-28 17:51:05",NULL,924,NULL,NULL,NULL
927,1,937,"2010-07-28 17:43:49",17,778,"<p>What are some podcasts related to statistical analysis?  I've found some audio recordings of college lectures on ITunes U, but I'm not aware of any statistical podcasts.  The closest thing I'm aware of is an operations research podcast <a href="http://www.scienceofbetter.org/podcast/">The Science of Better</a>.  It touches on statistical issues, but it's not specifically a statistical show.</p>
",319,"2014-04-19 11:54:36","Statistical podcasts",<books>,6,0,13,88,"2011-04-13 08:20:38","2010-07-28 17:43:49",NULL,NULL,NULL,NULL
928,1,932,"2010-07-28 17:57:51",5,2162,"<p>This one is bothering me for a while, and a great dispute was held around it. In psychology (as well as in other social sciences), we deal with different ways of dealing with numbers :-) i.e. <strong>the levels of measurement</strong>. It's also common practice in psychology to standardize some questionnaire, hence transform the data into percentile scores (in order to assess a respondent's position within the representative sample).</p>

<p>Long story short, if you have a variable that holds the data expressed in percentile scores, how should you treat it? As an ordinal, interval, or even ratio variable?!</p>

<p>It's not ratio, cause there no real 0 (0<sup>th</sup> percentile doesn't imply absence of measured property, but the variable's smallest value). I advocate the view that percentile scores are ordinal, since P<sub>70</sub> - P<sub>50</sub> is not equal to P<sub>50</sub> - P<sub>30</sub>, while the other side says it's interval. </p>

<p>Please gentlemen, cut the cord. Ordinal or interval?</p>
",1356,"2014-08-21 19:29:36","Measurement level of percentile scores",<measurement>,3,3,1,88,"2010-08-07 17:48:50",NULL,NULL,NULL,NULL,NULL
929,1,NULL,"2010-07-28 17:59:42",0,191,"<p>How comprehensive is the following book - What interpretations are missing?</p>

<p>Interpretations of Probability, Andrei Khrennikov, 2009, de Gruyter, ISBN 978-3-11-020748-4 </p>

<p><a href="http://www.degruyter.com/cont/fb/ma/detailEn.cfm?isbn=9783110207484&amp;sel=pi" rel="nofollow">http://www.degruyter.com/cont/fb/ma/detailEn.cfm?isbn=9783110207484&amp;sel=pi</a></p>

<p>Contents:http://www.degruyter.com/files/pdf/9783110207484Contents.pdf</p>
",560,"2010-07-28 19:14:49","Probability Interpretations",<probability>,1,4,1,NULL,NULL,NULL,NULL,"2010-07-29 23:43:25",NULL,NULL
930,2,NULL,"2010-07-28 18:05:11",1,NULL,"<p>Continuous (interval); this is a method how to convert ordinal data to something that may have some distribution that makes sense.</p>
",88,"2010-07-28 18:05:11",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,928,NULL,NULL,NULL
931,2,NULL,"2010-07-28 18:17:46",4,NULL,"<p>You may be interested in the following link: <a href="http://www.ats.ucla.edu/stat/seminars/" rel="nofollow">http://www.ats.ucla.edu/stat/seminars/</a> where the UCLA Statistical Computing unit of the UCLA has <em>very nice</em> screen-casts available. I have found them very useful in the past. They function essentially as lectures. Top-quality teaching.</p>
",561,"2010-07-28 18:17:46",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-28 18:17:46",927,NULL,NULL,NULL
932,2,NULL,"2010-07-28 18:18:26",3,NULL,"<p><strong>Background to understand my answer</strong></p>

<p>The critical property that distinguishes between ordinal and interval scale is whether we can take <em>ratio of differences</em>. While you cannot take ratio of direct measures for either scale the ratio of differences is meaningful for interval but not ordinal (See: <a href="http://en.wikipedia.org/wiki/Level_of_measurement#Interval_scale" rel="nofollow">http://en.wikipedia.org/wiki/Level_of_measurement#Interval_scale</a>).</p>

<p>Temperature is the classic example for an interval scale. Consider the following:</p>

<p>80 f = 26.67 c</p>

<p>40 f = 4.44 c and</p>

<p>20 f = -6.67 c</p>

<p>Differences between the first and the second is:</p>

<p>40 f and 22.23 c</p>

<p>Difference between the second and the third is:</p>

<p>20 f and 11.11 c</p>

<p>Notice that the ratio is the same irrespective of the scale on which we measure temperature. </p>

<p>A classic example of ordinal data is ranks. If three teams are ranked 1, 2 and 4 then a statement like so does not make sense: "Team 1's difference in strength vis-a-vis team 2 is half of team 2's difference in strength relative to team 4." (Abuse of notation: I am using the same number as rank and team index).</p>

<p><strong>Answer to your question</strong></p>

<p>Is ratio of differences in percentiles meaningful? In other words, is the ratio of difference in percentiles invariant to the underlying scale? Consider, for example: (P<sub>70</sub>-P<sub>50</sub>) / (P<sub>50</sub>-P<sub>30</sub>)?</p>

<p>Suppose that these percentiles are based on an underlying score between 0-100 and we compute the above ratio. Clearly, we would obtain the same ratio of percentile differences under arbitrary linear transformation of the score (e.g., multiply all scores by 10 so that the range is between 0-1000 and compute the percentiles).</p>

<p>Thus, my answer: <strong>Interval</strong></p>
",NULL,"2010-07-28 18:18:26",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,928,NULL,user28,NULL
933,2,NULL,"2010-07-28 18:21:10",2,NULL,"<p>I have done very well with reading the official documentation. It is well-written, sometimes injected with humour (!) and, if you're willing to spend the time to learn Stata properly, is an absolute goldmine.</p>
",561,"2010-07-28 18:21:10",NULL,NULL,NULL,0,NULL,NULL,NULL,"2011-11-16 20:17:12",290,NULL,NULL,NULL
934,2,NULL,"2010-07-28 18:25:00",6,NULL,"<p>If statistics is all about maximizing likelihood, then machine learning is all about minimizing loss. Since you don't know the loss you will incur on future data, you minimize an approximation, ie empirical loss.</p>

<p>For instance, if you have a prediction task and are evaluated by the number of misclassifications, you could train parameters so that resulting model produces the smallest number of misclassifications on the training data. "Number of misclassifications" (ie, 0-1 loss) is a hard loss function to work with because it's not differentiable, so you approximate it with a smooth "surrogate". For instance, log loss is an upper bound on 0-1 loss, so you could minimize that instead, and this will turn out to be the same as maximizing conditional likelihood of the data. With parametric model this approach becomes equivalent to logistic regression.</p>

<p>In a structured modeling task, and log-loss approximation of 0-1 loss, you get something different from maximum conditional likelihood, you will instead maximize <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.8122">product</a> of (conditional) marginal likelihoods.</p>

<p>To get better approximation of loss, people noticed that training model to minimize loss and using that loss as an estimate of future loss is an overly optimistic estimate. So for more accurate (true future loss) minimization they add a bias correction term to empirical loss and minimize that, this is known as structured risk minimization.</p>

<p>In practice, figuring out the right bias correction term may be too hard, so you add an expression "in the spirit" of the bias correction term, for instance, sum of squares of parameters. In the end, almost all parametric machine learning supervised classification approaches end up training the model to minimize the following</p>

<p>Sum_i L(m(x_i,w),y_i) + P(w)</p>

<p>where m is your model parametrized by vector w, i is taken over all datapoints {x_i,y_i}, L is some computationally nice approximation of your true loss and P(w) is some bias-correction/regularization term</p>

<p>For instance if you x \\in {-1,1}^d, y \\in {-1,1}, a typical approach would be to let m(x)=sign(w.x), L(m(x),y)=-log(y x.w), P(w)=q w.w, and choose q by cross validation</p>
",511,"2010-07-28 18:35:23",NULL,NULL,NULL,4,NULL,511,"2010-07-28 18:35:23",NULL,886,NULL,NULL,NULL
935,2,NULL,"2010-07-28 18:36:09",1,NULL,"<p>Though quantum probability and negative probability models are quite interesting, this is hardly exhaustive of nonstandard models of probability.  There are for instance, <a href="http://en.wikipedia.org/wiki/Imprecise_probability" rel="nofollow">imprecise probability models</a>, and models that violate Kolmogorov's countable additivity axiom, and more.</p>

<p>As an aside, the book may be more properly called 'Models of Probability'.  <a href="http://plato.stanford.edu/entries/probability-interpret/" rel="nofollow">Interpretations of probability</a>, generally involve characterizing the competing understandings of probability as logically prescribed values, limiting frequencies, propensities, subjective beliefs, etc.   Models, or axiomatizations can certainly be motivated by these understandings, but the problem of creating a variant system is different than arguing for a particular interpretation.</p>
",39,"2010-07-28 19:14:49",NULL,NULL,NULL,1,NULL,39,"2010-07-28 19:14:49",NULL,929,NULL,NULL,NULL
936,2,NULL,"2010-07-28 19:22:44",2,NULL,"<p>There is <a href="http://www.econtalk.org/" rel="nofollow">econtalk</a>, it is mostly about economics, but delves very often to issues of research, science, and statistics.</p>
",253,"2010-07-28 19:22:44",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-28 19:22:44",927,NULL,NULL,NULL
937,2,NULL,"2010-07-28 19:34:00",7,NULL,"<p>BBC's <a href="http://news.bbc.co.uk/2/hi/programmes/more_or_less/default.stm">More or Less</a> is often concerned with numeracy and statistical literacy issues.  But it's not specifically about statistics.  Their <a href="http://news.bbc.co.uk/2/hi/programmes/more_or_less/1628489.stm">About</a> page has some background.</p>

<blockquote>
  <p>More or Less is devoted to the powerful, sometimes beautiful, often abused but ever ubiquitous world of numbers.
    The programme was an idea born of the sense that numbers were the principal language of public argument.
    [...]</p>
</blockquote>
",251,"2010-07-28 19:34:00",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-28 19:34:00",927,NULL,NULL,NULL
938,2,NULL,"2010-07-28 20:43:32",1,NULL,"<p>Gelman et al. is well-regarded but explictly intended for a graduate course. If you don't have substantial prior coursework in statistics, it is largely a waste.</p>
",NULL,"2010-07-28 20:43:32",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-28 20:43:32",125,NULL,"Paul Riedesel",NULL
939,1,941,"2010-07-28 20:43:33",3,556,"<p>Is the Yates' correction for continuity used only for 2X2 matrices? </p>
",559,"2010-10-20 21:10:16","Yates' correction for continuity only for 2X2?",<contingency-tables><yates-correction>,1,2,1,8,"2010-10-20 21:10:16",NULL,NULL,NULL,NULL,NULL
940,2,NULL,"2010-07-28 20:50:25",7,NULL,"<p>If you have 1,000,000 independent "coin flips" that can produce 1 with probabilty (prob) and 0 with probability (1-prob), then the number of 1's observed will follow a <a href="http://en.wikipedia.org/wiki/Binomial_distribution" rel="nofollow">Binomial distribution</a>.  </p>

<p>Tests of statistical significance are rejection tests, i.e. reject the hypothesis that the two parameters are equal if the probability that param2 is observed in test2 when the true value is param1 is less than a certain number, like 5%, 1%, or 0.1%.  These tests are typically constructed from the cumulative distribution function.  </p>

<p>The cumulative distribution function for a binomial is ugly, but can be found in R and probably some other statistics packages as well.  </p>

<p>But the good news is that with 1,000,000 cases you don't need to do that.... you would if you had a relatively small number of cases.  </p>

<p>Because you have 1,000,000 independent flips, the CDF of a normal distribution is a good approximation (the Law of Large Numbers plays a role here).  The mean and variance you need to use are the obvious ones, and are in the <a href="http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation" rel="nofollow">Binomial Wikipedia</a> article... You are then comparing two normally distributed variables and can use all the standard tests you would use with normally distributed variables. </p>

<p>For instance, if the true probability were 40*10^-6 then in 1,000,000 tests you would expect to see 40 +/- 6  positive cases. If the acceptance interval for a test is, for instance, 5 standard deviations wide on each side, then this would be compatible with both observations.  If it were just 3 std dev wide on each side, one case would fit and the other would be statistically different.  </p>
",87,"2010-07-28 21:19:40",NULL,NULL,NULL,0,NULL,87,"2010-07-28 21:19:40",NULL,920,NULL,NULL,NULL
941,2,NULL,"2010-07-28 21:12:03",6,NULL,"<p>It's derived for binomial/hypergeometric distributions, so it's applicable to 2x2 or 2x1 cases.</p>
",251,"2010-07-28 21:12:03",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,939,NULL,NULL,NULL
942,1,962,"2010-07-28 21:13:23",15,1955,"<p>I've been beginning to work my way through <a href="http://www.autonlab.org/tutorials/">Statistical Data Mining Tutorials by Andrew Moore</a> (highly recommended for anyone else first venturing into this field).  I started by reading this <a href="http://www.autonlab.org/tutorials/biosurv01.pdf">extremely interesting PDF entitled "Introductory overview of time-series-based anomaly detection algorithms"</a> in which Moore traces through many of the techniques used in the creation of an algorithm to detect disease outbreaks.  Halfway through the slides, on page 27, he lists a number of other "state of the art methods" used to detect outbreaks.  The first one listed is <strong>wavelets</strong>.  Wikipeida describes a wavelet as</p>

<blockquote>
  <p>a wave-like oscillation with an
  amplitude that starts out at zero,
  increases, and then decreases back to
  zero. It can typically be visualized
  as a "brief oscillation"</p>
</blockquote>

<p>but does not describe their application to statistics and my Google searches yield highly academic papers that assume a knowledge of how wavelets relate to statistics or full books on the subject.</p>

<p>I would like a basic understanding of how wavelets are applied to time-series anomaly detection, much in the way Moore illustrates the other techniques in his tutorial.  Can someone provide an explanation of how detection methods using wavelets work or a link to an understandable article on the matter?</p>
",75,"2014-06-06 08:57:07","Application of wavelets to time-series-based anomaly detection algorithms",<time-series><outliers><signal-processing><wavelet>,3,0,11,223,"2011-02-08 16:48:14",NULL,NULL,NULL,NULL,NULL
943,2,NULL,"2010-07-28 21:31:14",2,NULL,"<p>If you can assume bivariate normality, then you can develop a likelihood-ratio test comparing the two possible covariance matrix structures. The unconstrained (H_a) maximum likelihood estimates are well known - just the sample covariance matrix, the constrained ones (H_0) can be derived by writing out the likelihood (and will probably be some sort of "pooled" estimate). </p>

<p>If you don't want to derive the formulas, you can use SAS or R to fit a repeated measures model with unstructured and compound symmetry covariance structures and compare the likelihoods.</p>
",279,"2010-07-28 21:31:14",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,841,NULL,NULL,NULL
944,1,947,"2010-07-28 21:45:46",1,168,"<p>When I type a left paren or any quote in the R console, it automatically creates a matching one to the right of my cursor. I guess the idea is that I can just type the expression I want inside without having to worry about matching, but I find it annoying, and would rather just type it myself. How can I disable this feature?</p>

<p>I am using R 2.8.0 on OSX 10.5.8.</p>
",NULL,"2010-07-29 02:11:34","How can I get R to stop autocompleting my quotes/parens?",<r>,3,5,NULL,NULL,NULL,NULL,NULL,NULL,anonymous,NULL
945,2,NULL,"2010-07-28 21:49:13",2,NULL,"<p>Well either use a different IDE -- this is entirely a feature of the OS X app -- or try to configure the feature in question. </p>

<p>As for IDEs / R environments, I'm rather happy with <a href="http://ess.r-project.org" rel="nofollow">ESS</a>  which works on every platform R works on. </p>
",334,"2010-07-28 21:49:13",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,944,NULL,NULL,NULL
946,1,963,"2010-07-28 21:50:29",2,633,"<p>New to the site.  I am just getting started with R, and want to replicate a feature that is available in SPSS.  </p>

<p>Simply, I build a "Custom Table" in SPSS with a single categorical variable in the column and many continous/scale variables in the rows (no interactions, just stacked on top of each other).  </p>

<p>The table reports the means and valid N's for each column (summary statistics are in the rows), and select the option to generate significance tests for column means (each column against the others) using alpha .05 and adjust for unequal variances.</p>

<p>Here is my question.</p>

<p>How can I replicate this in R?  What is my best option to build this table  and what tests are available that will get me to the same spot?  Since I am getting used to R, I am still trying to navigate around what is available.</p>

<p>Many thanks in advance!</p>
",569,"2010-07-29 08:22:43","Column Means Significance Tests in R",<r><mean><statistical-significance><spss>,2,0,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
947,2,NULL,"2010-07-28 21:51:15",5,NULL,"<p>On OSX, go to <code>R &gt; Preferences &gt; Editor</code> and deselect <code>Match braces/quotes</code></p>
",287,"2010-07-28 21:51:15",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,944,NULL,NULL,NULL
948,2,NULL,"2010-07-28 21:54:16",1,NULL,"<pre><code>summary(df)
</code></pre>

<p>Will give you 5 number summaries and counts of <code>NA</code> for continuous variables, and counts for categorical variables.</p>

<p>As for the significance tests, you'll have to do that by hand with <code>t.test()</code> or <code>wilcox.test()</code>.</p>
",287,"2010-07-28 21:54:16",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,946,NULL,NULL,NULL
949,1,NULL,"2010-07-28 21:59:02",15,1880,"<p>Take $x \\in \\{0,1\\}^d$ and $y \\in \\{0,1\\}$ and suppose we model the task of predicting y given x using logistic regression. When can logistic regression coefficients be written in closed form?</p>

<p>One example is when we use a saturated model.</p>

<p>That is, define $P(y|x) \\propto \\exp(\\sum_i w_i f_i(x_i))$, where $i$ indexes sets in the power-set of $\\{x_1,\\ldots,x_d\\}$, and $f_i$ returns 1 if all variables in the $i$'th set are 1, and 0 otherwise. Then you can express each $w_i$ in this logistic regression model as a logarithm of a rational function of statistics of the data. </p>

<p>Are there other interesting examples when closed form exists?</p>
",511,"2013-07-04 18:23:03","When is logistic regression solved in closed form?",<logistic><generalized-linear-model>,2,9,4,2970,"2012-09-20 12:49:01",NULL,NULL,NULL,NULL,NULL
950,2,NULL,"2010-07-28 23:28:44",1,NULL,"<p>One does not need to know about conditional probability or Bayes Theorem to figure out that it is best to switch your answer.</p>

<p>Suppose you initially pick Door 1.   Then the probability of Door 1 being a winner is 1/3 and the probability of Doors 2 or 3 being a winner is 2/3.   If Door 2 is shown to be a loser by the host's choice then the probabilty that 2 or 3 is a winner is still 2/3.   But since Door 2 is a loser, Door 3 must have a 2/3 probability of being a winner.     </p>
",99,"2010-07-28 23:28:44",NULL,NULL,NULL,12,NULL,NULL,NULL,NULL,373,NULL,NULL,NULL
951,1,1000,"2010-07-29 00:48:11",6,574,"<p>What is the relationship between a Nonhomogeneous Poisson process and a process that has heavy tail distribution for its inter arrival times?</p>

<p>Any pointer to a resource that can shed some light on this question would be hugely appreciated</p>
",172,"2012-02-06 22:48:39","Nonhomogeneous Poisson and Heavy tail inter arrival time distribution",<distributions><poisson>,3,0,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL
952,1,NULL,"2010-07-29 01:45:55",3,322,"<h3>Context</h3>

<p>I have a survey of 16 questions, each with four possible responses. The purpose of the survey is to measure the respondent's propensity towards four categories (which we will denote A, B, C, D). Each of the four responses per question are representative of an aspect of the four categories, A, B, C, D.</p>

<p>The respondent rank orders each of the four responses (we will denote the first response by "4", the second by "3", etc).</p>

<p>To score the categories, we add the responses up based on the coding above. There are 16 x (4 + 3 + 2 + 1) = 160 total points. The sums for each category are computed, and the maximum score is deemed the respondent's dominant category.</p>

<p>Therefore each survey looks like the following (in CSV format)</p>

<pre><code>question_num, A, B, C, D
1, 4, 3, 1, 2
2, 3, 4, 1, 2
3, 3, 4, 2, 1
4, 4, 3, 1, 2
5, 4, 3, 1, 2
6, 4, 3, 2, 1
7, 4, 3, 1, 2
.
.
.
16, 3, 4, 1, 2
sums, 64, 48, 24, 24
</code></pre>

<p>I have about 325 surveys completed.</p>

<h3>Aim</h3>

<p>I want to remove possible redundant items in the survey so I can reduce the burden on future respondents.</p>

<h3>Questions</h3>

<ul>
<li>My first strategy was to do a multi-logistic regression with the response as the dominant category (described above). Is this a good idea?</li>
<li>Would PCA be helpful?</li>
<li>Are there any other strategies for identifying redundant items?</li>
</ul>
",513,"2011-06-08 15:35:27","How to reduce number of items on a multi-item scale where each item requires ranking four response options",<logistic><scales><survey><ranking>,2,2,NULL,183,"2011-06-08 15:35:27",NULL,NULL,NULL,NULL,NULL
953,2,NULL,"2010-07-29 02:00:32",1,NULL,"<p>I agree with the suggestions about running a regression possibly with log(y) as the outcome variable or some other suitable transformation. I just wanted to add one comment, if you are reporting the bivariate association, you might prefer:
(a) to correlate log(x) and log(y),
(b) Spearman's rho, which correlates the ranks of the two variables.</p>
",183,"2010-07-29 02:00:32",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,913,NULL,NULL,NULL
954,2,NULL,"2010-07-29 02:11:34",4,NULL,"<p>To follow on from Dirk's comment, if you don't like your current IDE, check out some of the existing discussion on R IDEs:
<a href="http://stackoverflow.com/questions/1439059/best-ide-texteditor-for-r">http://stackoverflow.com/questions/1439059/best-ide-texteditor-for-r</a></p>
",183,"2010-07-29 02:11:34",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,944,NULL,NULL,NULL
955,1,NULL,"2010-07-29 03:25:03",0,1267,"<p>Just wonder, is there any data analysis/ statistic/ data mining work that are available on freelance basis?</p>

<p>This could be subjective and argumentative, which is why I put it as CW. </p>
",175,"2010-09-16 07:08:24","Data Analysis Work-- Is there Any Freelance Opportunity?",<job>,1,6,NULL,88,"2010-09-16 07:08:24",NULL,NULL,"2010-07-29 12:38:50",NULL,NULL
956,2,NULL,"2010-07-29 04:06:29",10,NULL,"<p>As Robin said, you've got the Benjamini-Hochberg method backwards. With that method, you set a value for Q (upper case Q; the maximum desired FDR) and it then sorts your comparisons into two piles. The goal is that no more than Q% of the comparisons in the "discovery" pile are false, and thus at least 100%-Q% are true. </p>

<p>If you computed a new value for each comparison, which is the value of Q at which that comparisons would just barely be considered a discovery, then those new values are q-values (lower case q; see the link to a paper by John Storey in the original question).</p>
",25,"2013-03-07 01:26:38",NULL,NULL,NULL,1,NULL,25,"2013-03-07 01:26:38",NULL,870,NULL,NULL,NULL
957,2,NULL,"2010-07-29 04:15:44",0,NULL,"<p>I think you need to nail down the question you are asking, before you can compute an answer. I think this question is way too vague to answer: "test whether it is an vis-a-vis the general population". </p>

<p>The only question I think you can answer is this one: If the new value came from the same population as the others, what is the chance that it will be so far (or further) from the sample mean? That is the question that your equation will begin to answer, although it is not quite right. Here is a corrected equation that includes n.</p>

<p>t = (x - mean(S))/(stdev(S)/sqrt(n))</p>

<p>Compute the corresponding P value (with n-1 degrees of freedom) and you've answered the question.</p>
",25,"2010-07-29 04:15:44",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,887,NULL,NULL,NULL
958,2,NULL,"2010-07-29 05:55:55",1,NULL,"<p>The short answer is "yes". Such work does exist.
For example, there is sometimes consulting work in and around universities.
Also, some companies wish to outsource data analysis and statistical activities.</p>

<p>In general, I found that word of mouth was a powerful tool. Once you build up a good reputation in a given community, additional requests for work will follow.</p>

<p>see: <a href="http://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=statistics+jobs" rel="nofollow">http://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=statistics+jobs</a></p>
",183,"2010-07-29 05:55:55",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,955,NULL,NULL,NULL
959,2,NULL,"2010-07-29 06:01:01",2,NULL,"<p>I've never had to perform such analyses, but there is an academic literature on the factor analysis of ipsative tests that would be relevant:
e.g.,</p>

<ul>
<li><a href="http://deepblue.lib.umich.edu/bitstream/2027.42/68736/2/10.1177_004912418000900206.pdf" rel="nofollow">http://deepblue.lib.umich.edu/bitstream/2027.42/68736/2/10.1177_004912418000900206.pdf</a></li>
<li><a href="http://www.informaworld.com/smpp/content~db=all~content=a785042624" rel="nofollow">http://www.informaworld.com/smpp/content~db=all~content=a785042624</a></li>
</ul>
",183,"2010-07-29 06:01:01",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,952,NULL,NULL,NULL
960,2,NULL,"2010-07-29 07:05:38",2,NULL,"<p>So restructure your data merging all user responses, so in such form:</p>

<pre><code>Q1 Q2 Q3 ...
user1 rank for option1 for Q1, user1 rank for option1 for Q2, ...
user1 rank for option2, ...
...
user2 rank for option1, ...
...
user325 rank for option4, ...
</code></pre>

<p>And then cluster the questions. I recommend agglomerative clustering, there it is easy to see what questions can be removed.</p>
",88,"2010-07-29 07:05:38",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,952,NULL,NULL,NULL
961,1,971,"2010-07-29 07:07:36",10,1991,"<p>I have a dataset made up of elements from three groups, let's call them G1, G2, and G3.
I analysed certain characteristics of these elements and divided them into 3 types of "behaviour" T1, T2, and T3 (I used cluster analysis to do that).</p>

<p>So, now I have a 3 x 3 contingency table like this with the counts of elements in the three groups divided by type:</p>

<pre><code>      |    T1   |    T2   |    T3   |
------+---------+---------+---------+---
  G1  |   18    |   15    |   65    | 
------+---------+---------+---------+---
  G2  |   20    |   10    |   70    |
------+---------+---------+---------+---
  G3  |   15    |   55    |   30    |
</code></pre>

<p>Now, I can run a Fisher test on these data in R</p>

<pre><code>data &lt;- matrix(c(18, 20, 15, 15, 10, 55, 65, 70, 30), nrow=3)
fisher.test(data)
</code></pre>

<p>and I get</p>

<pre><code>   Fisher's Exact Test for Count Data

data:  data 
p-value = 9.028e-13
alternative hypothesis: two.sided     
</code></pre>

<p>So my questions are:</p>

<ul>
<li><p>is it correct to use Fisher test this way?</p></li>
<li><p>how do I know who is different from who? Is there a post-hoc test I can use? Looking at the data I would say the 3<sup>rd</sup> group has a different behaviour from the first two, how do I show that statistically? </p></li>
<li><p>someone pointed me to logit models: are they a viable option for this type of analysis?</p></li>
<li><p>any other option to analyse this type of data?</p></li>
</ul>

<p>Thank you a lot</p>

<p>nico</p>
",582,"2010-09-16 06:54:07","Statistical test for n x m contingency tables",<contingency-tables>,2,0,7,88,"2010-09-16 06:54:07",NULL,NULL,NULL,NULL,NULL
962,2,NULL,"2010-07-29 07:10:39",10,NULL,"<p>Wavelets are useful to detect singularities in a signal (see for example the paper <a href="http://www.math.univ-toulouse.fr/~bigot/Site/Publications_files/Spectrometry.pdf" rel="nofollow">here</a> (see figure 3 for an illustration) and the references mentioned in this paper.  I guess singularities can sometimes be an anomaly? </p>

<p>The idea here is that the Continuous wavelet transform <strong>(CWT)</strong> has maxima lines that propagates along frequencies, i.e. the longer the line is, the higher is the singularity. See Figure 3 in the paper to see what I mean! note that there is free Matlab code related to that paper, it should be <a href="http://www.math.univ-toulouse.fr/~bigot/Site/Software.html" rel="nofollow">here</a>.</p>

<hr>

<p>Additionally, I can give you some heuristics detailing <strong>why</strong> the DISCRETE (preceding example is about the continuous one) wavelet transform (<strong>DWT</strong>) <strong>is interesting for a statistician</strong> (excuse non-exhaustivity) :   </p>

<ul>
<li>There is a wide class of (realistic (Besov space)) signals that are transformed into a <strong>sparse</strong> sequence by the wavelet transform. (<strong>compression property</strong>)</li>
<li>A wide class of (quasi-stationary) processes that are transformed into a sequence with almost uncorrelated features (<strong>decorrelation property</strong>)</li>
<li>Wavelet coefficients contain information that is localized in <strong>time and in frequency</strong> (at different scales). (multi-scale property)</li>
<li>Wavelet coefficients of a signal <strong>concentrate on its singularities</strong>. </li>
</ul>
",223,"2014-06-06 08:57:07",NULL,NULL,NULL,1,NULL,13465,"2014-06-06 08:57:07",NULL,942,NULL,NULL,NULL
963,2,NULL,"2010-07-29 08:15:59",3,NULL,"<p>As I read in help for the <code>t.test</code>, it is only applicable for the 2-sample tests. If you want to perform it to every combination of the columns of a matrix A, taken 2 at a time, you could do something like this (for the moment, I can't recall a better way)</p>

<pre><code>apply(combn(1:dim(A)[2],2),2,function(x) t.test(A[,x[1]],A[,x[2]])) 
</code></pre>

<p>or, if you just want the p.values</p>

<pre><code>t(apply(combn(1:dim(A)[2],2),2,function(x) c(x[1],x[2],(t.test(A[,x[1]],A[,x[2]]))$p.value)))
</code></pre>
",339,"2010-07-29 08:22:43",NULL,NULL,NULL,2,NULL,339,"2010-07-29 08:22:43",NULL,946,NULL,NULL,NULL
964,1,1011,"2010-07-29 08:28:36",3,393,"<p>So in R, for instance, this would be:</p>

<pre><code>my_ts_logged_diffed = diff(log(some_ts_object))
plot(my_ts_logged_diffed)
</code></pre>

<p>This seems to be part of every experienced analyst/forecaster analytical workflow--in particular, a <em>visual examination of the plotted data</em>. What are they looking for--i.e., what useful information does this transformation help reveal?</p>

<p>Similarly, I have a pretty good selection of time series textbooks, tutorials, and the like; nearly all of them mention this analytical step, but none of them say why it's done (i am sure there's a good reason, and one that's apparently too obvious to even mention).</p>

<p>(i do indeed routinely rely on this transformation but only for the limited purpose of testing for a normal distribution (i think the test is called <em>Shapiro-Wilk</em>). The application of the test just involves (assuming i am applying it correctly) comparing a couple of parameters (a 'W' parameter and the p-value) against a baseline--the Test doesn't appear to require plotting the data).</p>
",438,"2010-11-02 01:47:17","What are analysts looking for when they plot a differenced, logged time series?",<time-series><data-transformation>,6,0,1,159,"2010-07-31 00:48:08",NULL,NULL,NULL,NULL,NULL
965,2,NULL,"2010-07-29 08:46:34",4,NULL,"<p>Most growth/decay processes will at most change the moving quantity at an exponential rate. The differences of the logs of the quantity relate to the local slope &lambda;, so for a underlying exponential growth or decay process it would be flat in t. Any deviation from flat gives you hints if and where there are switchovers between different purly exponential pieces; also for chaotic behavior you would expect more than linear growth or decay, i.e. a "local &lambda;" curve that is not flat for small t-ranges.</p>
",56,"2010-07-29 08:46:34",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,964,NULL,NULL,NULL
966,2,NULL,"2010-07-29 10:22:37",2,NULL,"<p>This is often used for a price to return transformation based on assuming continuously compounded returns. The Campbell, Lo, and MacKinlay book (Econometrics of Financial Markets, 1997) lays it out quite nicely:</p>

<p>Define r_t as the log of gross returns 1 + R_t:</p>

<pre><code>r_t := log(1 + R_t) 
</code></pre>

<p>which is the same as the log of the previous prices</p>

<pre><code>log(P_t / P_{t-1}) 
</code></pre>

<p>which is the same as </p>

<pre><code>p_t - p_{t-1}
</code></pre>

<p>which is what you asked about. </p>
",334,"2010-07-29 10:22:37",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,964,NULL,NULL,NULL
967,2,NULL,"2010-07-29 10:50:53",23,NULL,"<blockquote>
  <p>I keep saying that the sexy job in the next 10 years will be statisticians. And I'm not kidding.</p>
</blockquote>

<p><a href="http://en.wikipedia.org/wiki/Hal_Varian">Hal Varian</a></p>
",215,"2011-08-15 05:59:45",NULL,NULL,NULL,4,NULL,4479,"2011-08-15 05:59:45","2010-07-29 10:50:53",726,NULL,NULL,NULL
968,2,NULL,"2010-07-29 10:57:18",58,NULL,"<blockquote>
  <p>All generalizations are false,
  including this one.</p>
</blockquote>

<p><em>Mark Twain</em></p>
",1356,"2010-07-29 10:57:18",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-07-29 10:57:18",726,NULL,NULL,NULL
969,2,NULL,"2010-07-29 11:25:13",7,NULL,"<blockquote>
  <p>An ecologist is a statistician who likes to be outside. </p>
</blockquote>

<p>-- apparently a good friend of <a href="http://r.789695.n4.nabble.com/Why-software-fails-in-scientific-research-td1573062.html#a2275423">Murray Cooper</a>.</p>
",144,"2010-12-03 04:04:09",NULL,NULL,NULL,0,NULL,795,"2010-12-03 04:04:09","2010-07-29 11:25:13",726,NULL,NULL,NULL
970,2,NULL,"2010-07-29 11:41:41",1,NULL,"<p>In general there is none. A Poisson process has inter-arrival times that are exponentially distributed, which does not have heavy tails. </p>
",247,"2010-07-29 11:41:41",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,951,NULL,NULL,NULL
971,2,NULL,"2010-07-29 11:41:49",10,NULL,"<p>At first I think that the Fisher test is used correctly.</p>

<p>Count data are better handled using log-linear models (not logit, to ensure that the fitted values are bounded below). In R you can specify <code>family=poisson</code> (which sets errors = Poisson and link = log). The log link ensures that all the fitted values are positive, while the Poisson errors take account of the fact that the data are integer and have variances that are equal to their means.
e.g. <code>glm(y~x,poisson)</code> and the model is fitted with a log link and Poisson errors (to account for the non-normality). </p>

<p>In cases where there is overdispersion (the residual deviance should be equal to the residual degrees of freedom, if the Poisson errors assumption is appropriate), instead of using <code>quasipoisson</code> as the error family, you could fit a negative binomial model.
(This involves the function <code>glm.nb</code> from package <code>MASS</code>)</p>

<p>In your case you could fit and compare models using using commands like the following:</p>

<pre><code>observed &lt;- as.vector(data)
Ts&lt;-factor(rep(c("T1","T2","T3"),each=3))
Gs&lt;-factor(rep(c("G1","G2","G3"),3))

model1&lt;-glm(observed~Ts*Gs,poisson)

#or and a model without the interaction terms
model2&lt;-glm(observed~Ts+Gs,poisson)


#you can compare the two models using anova with a chi-squared test
anova(model1,model2,test="Chi")
summary(model1)
</code></pre>

<p>Always make sure that your minimal model contains all the nuisance variables.</p>

<p>As for how do we know who is different from who, there are some plots that may help you. R function <code>assocplot</code> produces an association plot indicating deviations from independence of rows and columns in a two dimensional contingency table.</p>

<p>Here are the same data plotted as a mosaic plot</p>

<pre><code>mosaicplot(data, shade = TRUE)
</code></pre>
",339,"2010-07-29 11:41:49",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,961,NULL,NULL,NULL
972,2,NULL,"2010-07-29 11:43:54",1,NULL,"<p>There are no general heuristics, you should make a grid search, especially since the value of nu must be between 0-1.</p>
",566,"2010-07-29 11:43:54",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,602,NULL,NULL,NULL
973,1,1020,"2010-07-29 12:02:28",16,4214,"<p>What are the <strong>freely available data set for classification with more than 1000 features</strong> (or sample points if it contains curves)? </p>

<p>There is already a community wiki about free data sets:
<a href="http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/">http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/</a></p>

<p>But here, it would be nice to have a <strong>more focused list that can be used more conveniently</strong>, also I propose the following rules:</p>

<ol>
<li>One post <strong>per dataset</strong> </li>
<li><strong>No</strong> link to set of dataset </li>
<li><p>each data set <strong>must</strong> be associated with</p>

<ul>
<li><p>a <strong>name</strong> (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)</p></li>
<li><p>the number of features (let say it is <strong>p</strong>) the size of the dataset (let say it is <strong>n</strong>) and the number of labels/class (let say it is <strong>k</strong>)    </p></li>
<li><p>a typical <strong>error rate</strong> from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) </p></li>
</ul></li>
</ol>
",223,"2013-07-03 01:34:41","Free data set for very high dimensional classification",<machine-learning><classification><dataset><large-data>,5,2,12,223,"2010-12-21 13:04:43","2010-07-29 12:02:28",NULL,NULL,NULL,NULL
975,2,NULL,"2010-07-29 12:44:50",2,NULL,"<p>Another good podcast is <a href="http://www.bbc.co.uk/radio4/features/in-our-time/" rel="nofollow">In our time</a> by the BBC. It's a weekly podcast (off air for the summer) that deals with topics in History, Religion and Science. I would say that about 1 in 12 podcasts deal with Mathematics and Statistics. Take a look at the podcast archive for <a href="http://www.bbc.co.uk/radio4/features/in-our-time/archive/science" rel="nofollow">Science subjects</a>.</p>
",8,"2010-07-29 12:44:50",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-29 12:44:50",927,NULL,NULL,NULL
977,1,978,"2010-07-29 13:43:53",2,373,"<p>When we are monitoring movements of structures we normally install monitoring points onto the structure before we do any work which might cause movement. This gives us chance to take a few readings before we start doing the work to 'baseline' the readings. </p>

<p>Quite often the data is quite variable (the variations in the reading can easily be between 10 and 20% of the fianl movement). The measurements are also often affected by the environment in which they are taken so one set of measurements taken on one project may not have the same accuracy as measurements on another project. </p>

<p>Is there any statisitcal method, or rule of thumb that can be applied to say how many baseline readings need to be taken to give a certain accuracy before the first reading is taken? Are there any rules of humb that can be applied to this situation? </p>
",210,"2010-07-30 15:37:06","How many measurements are needed to 'baseline' a measurement?",<variance><measurement>,3,0,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
978,2,NULL,"2010-07-29 13:51:07",3,NULL,"<p>I think you should look at <a href="http://en.wikipedia.org/wiki/Statistical_power" rel="nofollow">power calculations</a>. These are often used to decide the sample size of survey or clinical trial. Taken from wikipedia:</p>

<blockquote>
  <p>A priori power analysis is conducted
  prior to the research study, and is
  typically used to determine an
  appropriate sample size to achieve
  adequate power.</p>
</blockquote>
",8,"2010-07-29 13:51:07",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,977,NULL,NULL,NULL
980,1,NULL,"2010-07-29 14:04:20",4,3839,"<p>I haven't studied statistics for over 10 years (and then just a basic course), so maybe my question is a bit hard to understand.</p>

<p>Anyway, what I want to do is reduce the number of data points in a series. The x-axis is number of milliseconds since start of measurement and the y-axis is the reading for that point. </p>

<p>Often there is thousands of data points, but I might only need a few hundreds. So my question is: How do I accurately reduce the number of data points?</p>

<p>What is the process called? (So I can google it)
Are there any prefered algorithms (I will implement it in C#)</p>

<p>Hope you got some clues. Sorry for my lack of proper terminology.</p>

<hr>

<p>Edit: More details comes here:</p>

<p>The raw data I got is heart rate data, and in the form of number of milliseconds since last beat. Before plotting the data I calculate number of milliseconds from first sample, and the bpm (beats per minute) at each data point (60000/timesincelastbeat). </p>

<p>I want to visualize the data, i.e. plot it in a line graph. I want to reduce the number of points in the graph from thousands to some hundreds. </p>

<p>One option would be to calculate the average bpm for every second in the series, or maybe every 5 seconds or so. That would have been quite easy if I knew I would have at least one sample for each of those periods (seconds of 5-seconds-intervals).</p>
",NULL,"2013-03-14 20:28:45","How do I reduce the number of data points in a series?",<data-visualization>,6,7,4,159,"2010-07-31 00:49:57",NULL,NULL,NULL,Pete,NULL
981,2,NULL,"2010-07-29 14:15:09",6,NULL,"<p>You have two problems: too many points and how to smooth over the remaining points.</p>

<p><strong>Thinning your sample</strong></p>

<p>If you have too many observations arriving in real time, you could always use <a href="http://en.wikipedia.org/wiki/Simple_random_sample" rel="nofollow">simple random sampling</a> to thin your sample. Note, for this too be true, the number of points would have to be very large.</p>

<p>Suppose you have <em>N</em> points and you only want <em>n</em> of them. Then generate <em>n</em> random numbers from a discrete uniform <em>U(0, N-1)</em> distribution. These would be the points you use.</p>

<p>If you want to do this sequentially, i.e. at each point you decide to use it or not, then just accept a point with probability <em>p</em>. So if you set <em>p=0.01</em> you would accept (on average) 1 point in a hundred. </p>

<p>If your data is unevenly spread and you only want to thin dense regions of points, then just make your thinning function a bit more sophisticated. For example, instead of <em>p</em>, what about:</p>

<p><img src="http://mathurl.com/2vlkhjs.png" alt="alt text"></p>

<p>where <img src="http://mathurl.com/343luwd.png" alt="alt text"> is a positive number and <em>t</em> is the time since the last observation. If the time between two points is large, i.e. large <em>t</em>, the probability of accepting a point will be one. Conversely, if two points are close together, the probability of accepting a point will be <em>1-p</em>.</p>

<p>You will need to experiment with values of <img src="http://mathurl.com/343luwd.png" alt="alt text"> and <em>p</em>.</p>

<p><strong>Smoothing</strong></p>

<p>Possibly something like a simple moving average type scheme. Or you could go for something more advanced like a <a href="http://en.wikipedia.org/wiki/Kernel_smoother" rel="nofollow">kernel smoother</a> (as others suggested). You will need to be careful that you don't smooth too much, since I assume that a sudden drop should be picked up very quickly in your scenario. </p>

<p>There should be <em>C#</em> libraries available for this sort of stuff.</p>

<p><strong>Conclusion</strong></p>

<p>Thin if necessary, then smooth.</p>
",8,"2010-07-30 11:00:12",NULL,NULL,NULL,11,NULL,8,"2010-07-30 11:00:12",NULL,980,NULL,NULL,NULL
982,2,NULL,"2010-07-29 14:18:02",6,NULL,"<p>Well, I think the word you're looking for is "sampling," but I'm not sure why you want to do it. Thousands of data points isn't very many. Or are you looking just to plot a smaller number of equally-spaced points? That's usually called "binning." </p>

<p>Is your goal to generate a visualization? In that case, you might want to keep the raw data, plot it as a scattergraph, then overlay some sort of central tendency (regression line, spline, whatever) to communicate whatever the takehome message ought to be.</p>

<p>Or is your goal to numerically summarize the results in some way? In that case, you might want to explain your problem in more detail!</p>
",6,"2010-07-29 14:18:02",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,980,NULL,NULL,NULL
983,2,NULL,"2010-07-29 14:26:53",2,NULL,"<p>It really depends on the amount of variance relative to the size of a measurement that you care about. If you need to be able to tell the difference between a mean of 2 and a mean of 0, and your data look like this:</p>

<pre><code>-4.4 3.8 -2.0 -5.1 0.2 7.1 0.9 -5.4 2.8 0.5
</code></pre>

<p>Then you're going to need a lot more data! But if you only care if the average is less than or more than 10, then that much data is adequate. </p>

<p>@cgillespie gives the technically correct response. You need to have some idea what size of effect you care about, as well as some idea how much variance your measurements have. If the equations of power analysis are more than you can deal with, you can always use random numbers in an Excel spreadsheet! Generate columns of random numbers with a normal distribution and various means and variances, then figure out whether the confidence intervals (2 * standard deviation / sqrt(N)) around the mean of each set of numbers include differences that you might are about. Do that a bunch of times. That'll give you a good idea of how many measurements you need, and it's not too hard to explain to others.</p>
",6,"2010-07-29 14:26:53",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,977,NULL,NULL,NULL
984,2,NULL,"2010-07-29 14:29:16",1,NULL,"<p>You're not providing enough information.  Why do you want to reduce the data points.  A few thousand is nothing these days.</p>

<p>Given that you want the same result each time you view the same data perhaps you want to simply bin averages.  You have variable spacing on your x-axis.  Maybe you're trying to make that consistent?  In that case you would set a bin width of perhaps 50 msec, or 100, and then average all the points in there.  Make the bin width as large as you need to reduce the data points to the size of the set you want.</p>

<p>It's really a hard question to answer without a reason for why you're getting rid of data.</p>
",601,"2010-07-29 14:29:16",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,980,NULL,NULL,NULL
986,2,NULL,"2010-07-29 15:20:56",3,NULL,"<p>You can use <em>multinom</em> from nnet package for multinomial regression. 
Post-hoc tests you can use <em>linearHypothesis</em> from car package.
You can conduct test of independence using <em>linearHypothesis</em> (Wald test) or <em>anova</em> (LR test).</p>
",419,"2010-07-29 15:20:56",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,961,NULL,NULL,NULL
990,2,NULL,"2010-07-29 15:43:35",4,NULL,"<blockquote>
  <p>You may be too vague to be wrong and
  that's really bad cause that's just
  obscuring the issue.</p>
</blockquote>

<p>Bruce Sterling </p>
",3807,"2010-07-29 15:43:35",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-29 15:43:35",726,NULL,NULL,NULL
991,2,NULL,"2010-07-29 15:45:39",1,NULL,"<p>usually, you plot such a series to check the extend to which it exhibits heteroskedasticity.
Depending on the answer, you may have to model the residuals, even if you are only interested in the mean. </p>

<p>On the top of my head, this article is a practical example:</p>

<p><a href="http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBIQFjAA&amp;url=http%3A%2F%2Fdss.ucsd.edu%2F~jhamilto%2FJHamilton_Engle.pdf&amp;ei=CKJRTN-uO4XeOO_LhL4E&amp;usg=AFQjCNEP4dL3_uRf28371SnBS4lhhnYsSw&amp;sig2=tJRjwUetH8XTHuijcXYmbg" rel="nofollow">http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBIQFjAA&amp;url=http%3A%2F%2Fdss.ucsd.edu%2F~jhamilto%2FJHamilton_Engle.pdf&amp;ei=CKJRTN-uO4XeOO_LhL4E&amp;usg=AFQjCNEP4dL3_uRf28371SnBS4lhhnYsSw&amp;sig2=tJRjwUetH8XTHuijcXYmbg</a> </p>
",603,"2010-07-29 15:45:39",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,964,NULL,NULL,NULL
992,2,NULL,"2010-07-29 15:50:24",1,NULL,"<p>Try a bivariate robust regression 
(see <a href="http://cran.r-project.org/web/packages/rrcov/vignettes/rrcov.pdf" rel="nofollow">http://cran.r-project.org/web/packages/rrcov/vignettes/rrcov.pdf</a> for an intro).</p>

<p>If your data points are all positive, you might want to try to regress log(y) on log(x).
Note that log() is <em>not</em> a substitute for a robust regression, but it sometimes makes the results more interpretable.</p>
",603,"2010-07-29 15:50:24",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,913,NULL,NULL,NULL
993,2,NULL,"2010-07-29 15:57:21",6,NULL,"<p>A (two-sided) Fisher's Exact test gives p-value = <strong>0.092284</strong>.</p>

<pre><code>function p = fexact(k, x, m, n)
%FEXACT Fisher's Exact test.
%   Y = FEXACT(K, X, M, N) calculates the P-value for Fisher's
%   Exact Test.
%   K, X, M and N must be nonnegative integer vectors of the same
%   length.  The following must also hold:
%   X &lt;= N &lt;= M, X &lt;= K &lt;= M and K + N - M &lt;= X.  Here:
%   K is the number of items in the group,
%   X is the number of items in the group with the feature,
%   M is the total number of items,
%   N is the total number of items with the feature,

if nargin &lt; 4
   help(mfilename);
   return;
end
nr = length(k);
if nr ~= length(x) | nr ~= length(m) | nr ~= length(n)
   help(mfilename);
   return;
end

na = nan;
v = na(ones(nr, 1));
mi = max(0, k + n - m);
ma = min(k, n);

d = hygepdf(x, m, k, n) * (1 + 5.8e-11);
for i = 1:nr
  y = hygepdf(mi(i):ma(i), m(i), k(i), n(i));
  v(i) = sum(y(y &lt;= d(i)));
end
p = max(min(v, 1), 0);
p(isnan(v)) = nan;
</code></pre>

<p>For your example, try <code>fexact(1e6, 3, 2e6, 13)</code>.</p>
",506,"2010-07-29 16:28:09",NULL,NULL,NULL,2,NULL,506,"2010-07-29 16:28:09",NULL,924,NULL,NULL,NULL
994,2,NULL,"2010-07-29 16:00:17",6,NULL,"<p><a href="http://rads.stackoverflow.com/amzn/click/0713999225">The Drunkard's Walk: How Randomness Rules Our Lives</a> by Leonard Mlodinow is an excellent book for laypeople. Enjoyable and educational.</p>

<p>It might not be a textbook, but it makes you think about the world in the right way.</p>
",NULL,"2010-07-29 16:00:17",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-29 16:00:17",421,NULL,sjcockell,NULL
995,2,NULL,"2010-07-29 16:02:18",3,NULL,"<p><em>Collaborative Statistics</em> is CC BY: <a href="http://cnx.org/content/col10522/latest/" rel="nofollow">http://cnx.org/content/col10522/latest/</a></p>
",NULL,"2010-07-29 16:02:18",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-29 16:02:18",614,NULL,Nicole,NULL
996,2,NULL,"2010-07-29 16:13:41",13,NULL,"<p>You can find a pedagogical summary of the various methods available in <a href="http://download.springer.com/static/pdf/216/chp%253A10.1007%252F978-3-642-35494-6_4.pdf?auth66=1386938034_5f6480148bf543fd3993a8b2ff1ea06a&amp;ext=.pdf" rel="nofollow">(1)</a></p>

<p>For some --recent-- numerical comparisons of the various methods listed there, you can check 
<a href="http://link.springer.com/article/10.1007%2Fs00362-013-0544-8#page-1" rel="nofollow">(2)</a> and <a href="http://www.sciencedirect.com/science/article/pii/S0167947313002661" rel="nofollow">(3)</a>.</p>

<p>there are many older (and less exhaustive) numerical comparisons, typically found in books. You will find one on pages 142-143 of (4), for example.</p>

<p>Note that all the methods discussed here have an open source R implementation, mainly through the <a href="http://cran.r-project.org/web/packages/rrcov/vignettes/rrcov.pdf" rel="nofollow">rrcov</a>
 package.</p>

<ul>
<li>(1) P. Rousseeuw and M. Hubert (2013)  High-Breakdown Estimators of
Multivariate Location and Scatter.</li>
<li>(2) M. Hubert, P. Rousseeuw, K. Vakili (2013).
Shape bias of robust covariance estimators: an empirical study.
Statistical Papers.</li>
<li>(3) K. Vakili and E. Schmitt (2014). Finding multivariate outliers with FastPCS. Computational Statistics &amp; Data Analysis.</li>
<li>(4) Maronna R. A., Martin R. D. and Yohai V. J. (2006).
Robust Statistics: Theory and Methods. Wiley, New York.</li>
</ul>
",603,"2013-12-11 18:58:18",NULL,NULL,NULL,0,NULL,603,"2013-12-11 18:58:18",NULL,213,NULL,NULL,NULL
997,2,NULL,"2010-07-29 16:50:11",9,NULL,"<p>The huge denominators throw off one's intuition. Since the sample sizes are identical, and the proportions low, the problem can be recast: 13 events occurred, and were expected (by null hypothesis) to occur equally in both groups. In fact the split was 3 in one group and 10 in the other. How rare is that? The binomial test answers.</p>

<p>Enter this line into R:
binom.test(3,13,0.5,alternative="two.sided")</p>

<p>The two-tail P value is 0.09229, identical to four digits to the results of Fisher's test. </p>

<p>Looked at that way, the results are not surprising. The problem is equivalent to this one: If you flipped a coin 13 times, how surprising would it be to see three or fewer, or ten or more, heads. One of those outcomes would occur 9.23% of the time. </p>
",25,"2010-08-22 01:43:00",NULL,NULL,NULL,2,NULL,25,"2010-08-22 01:43:00",NULL,924,NULL,NULL,NULL
998,2,NULL,"2010-07-29 18:06:31",0,NULL,"<p>In addition to the other answers:
If you have 1,000,000 observations and when your event comes up only a few times, you are likely to want to look at a lot of different events.
If you look at 100 different events you will run into problems if you work with p&lt;0.05 as criteria for significance. </p>
",3807,"2010-07-29 18:06:31",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,924,NULL,NULL,NULL
999,2,NULL,"2010-07-29 18:33:43",3,NULL,"<p>If you are coming from a SAS or SPSS background, check out:</p>

<p><a href="http://sites.google.com/site/r4statistics/" rel="nofollow">http://sites.google.com/site/r4statistics/</a></p>

<p>This is the companion site to the book, R for SAS and SPSS Users by Robert Muenchen and a free version of the book can be found here.</p>
",NULL,"2010-07-29 18:33:43",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,alex,NULL
1000,2,NULL,"2010-07-29 18:39:45",4,NULL,"<p>Well, if you have a point process that you try modeling as a Poisson process, and find it has heavy tails, there are several possibilities.  What are the key assumptions for a Poisson Process:</p>

<p>-There is a constant rate function
-Events are memoryless, that is P(E in (t,t+d)) is independent of t and when other events are.
-The waiting time until the next event is exponentially distributed (kinda what the previous two are saying)</p>

<p>So, how can you violate these assumptions to get heavy tails?</p>

<p>-Non-constant rate function.  If the rate function switches between, say, two values, you'll have too many short wait-times, and too many long wait-times, given the overall rate function.  This can show itself as having heavy tails.
-The waiting time is not exponentially distributed.  In which case, you don't have a Poisson process.  You have some other sort of point process.</p>

<p>Note that in the extreme case, any point process can be modeled by a NHPP - put a delta function at each event, and set the rate to 0 elsewhere.  I think we can all agree that this is a poor model, having little predictive power.   So if you are interested in a NHPP, you'll want to think a bit about whether that is the right model, or whether you are overly-adjusting a model to fit your data.</p>
",549,"2010-07-29 18:39:45",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,951,NULL,NULL,NULL
1001,1,3078,"2010-07-29 18:46:47",3,1036,"<p>I have distributions from two different data sets and I would like to
measure how similar their distributions (in terms of their bin
frequencies) are. In other words, I am not interested in the correlation of 
data point sequences but rather in the their distributional properties with respect to similarity. Currently I can only observe a similarity in eye-balling which is not enough. I don't want to assume causality and I don't want to predict at this point. So, I assume that correlation is the way to go. </p>

<p>Spearman's Correlation Coefficient is used to compare non-normal data and since I don't know anything about the real underlying distribution in my data, I think it would be a save bet. I wonder if this measure can also be used to
compare distributional data rather than the data poitns that are
summarized in a distribution. Here the example code in R that exemplifies
what I would like to check:</p>

<pre><code>aNorm &lt;- rnorm(1000000)
bNorm &lt;- rnorm(1000000)
cUni &lt;- runif(1000000)
ha &lt;- hist(aNorm)
hb &lt;- hist(bNorm)
hc &lt;- hist(cUni)
print(ha$counts)
print(hb$counts)
print(hc$counts)
# relatively similar
n &lt;- min(c(NROW(ha$counts),NROW(hb$counts)))
cor.test(ha$counts[1:n], hb$counts[1:n], method="spearman")
# quite different
n &lt;- min(c(NROW(ha$counts),NROW(hc$counts)))
cor.test(ha$counts[1:n], hc$counts[1:n], method="spearman")
</code></pre>

<p>Does this make sense or am I violating some assumptions of the coefficient?</p>

<p>Thanks,
R.</p>
",608,"2012-01-18 13:23:58","Is Spearman's correlation coefficient usable to compare distributions?",<distributions><spearman><spearman-rho><paired-data>,3,6,2,88,"2012-01-18 13:23:58",NULL,NULL,NULL,NULL,NULL
1003,2,NULL,"2010-07-29 18:59:01",8,NULL,"<p>Try IPSUR, Introduction to Probability and Statistics Using R. It's free in the GNU sense of the word. </p>

<p><a href="http://ipsur.r-forge.r-project.org/book/" rel="nofollow">http://ipsur.r-forge.r-project.org/book/</a></p>

<p>It's definitely open source - on the download page you can download the LaTeX source or the lyx source used to generate this. </p>
",36,"2014-06-20 07:54:48",NULL,NULL,NULL,0,NULL,28791,"2014-06-20 07:54:48","2010-07-29 18:59:01",614,NULL,NULL,NULL
1004,2,NULL,"2010-07-29 18:59:32",10,NULL,"<p>Rather use <a href="http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test" rel="nofollow">Kolmogorov–Smirnov test</a>, which is exactly what you need. R function <code>ks.test</code> implements it.</p>

<p>Also check <a href="http://stats.stackexchange.com/questions/411/motivation-for-kolmogorov-distance-between-distributions">this question</a>.</p>
",88,"2010-07-29 19:14:52",NULL,NULL,NULL,6,NULL,88,"2010-07-29 19:14:52",NULL,1001,NULL,NULL,NULL
1005,2,NULL,"2010-07-29 19:01:51",10,NULL,"<p>Try IPSUR, Introduction to Probability and Statistics Using R. It's a free book, free in the GNU sense of the word. </p>

<p><a href="http://ipsur.r-forge.r-project.org/book/index.php">http://ipsur.r-forge.r-project.org/book/index.php</a></p>

<p>It's definitely open source - on the download page you can download the LaTeX source or the lyx source used to generate this. </p>
",36,"2010-07-29 19:01:51",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,NULL,NULL
1006,2,NULL,"2010-07-29 19:22:17",4,NULL,"<p>In this case Poisson is good approximation for distribution for number of cases.
There is simple formula to approximate variance of log RR (delta method) .</p>

<p>log RR = 10/3 = 1.2, 
se log RR = sqrt(1/3+1/10) = 0.66, so 95%CI = (-0.09; 2.5)</p>

<p>It is not significant difference at 0.05 level using two-sided test.
LR based Chi-square test for Poisson model gives p=0.046 and Wald test p=0.067.
This results are similar to Pearson Chi-square test without continuity correction (Chi2 with correction p=0.096).
Another possibility is chisq.test with option simulate.p.value=T, in this case p=0.092 (for 100 000 simulations).</p>

<p>In this case test statistics is rather discrete, so Fisher test can be conservative.
There is some evidence that difference can be significant. Before final conclusion data collecting process should be taken into account.</p>
",419,"2010-07-29 19:22:17",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,924,NULL,NULL,NULL
1008,2,NULL,"2010-07-29 19:39:02",1,NULL,"<p>P value from theoretical point of view is some realization of random variable.
There is some standard (in probability) to use upper case letters for random variables and lower case for realizations.
In table headers we should use P (maybe <em>italicize</em>), in text together with its value p=0.0012 and in text describing for example methodology p-value .</p>
",419,"2010-07-29 19:39:02",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,871,NULL,NULL,NULL
1009,2,NULL,"2010-07-29 20:09:37",3,NULL,"<ol>
<li>For data in IQR range you should use
truncated normal distribution (for
example R package gamlss.tr) to
estimate parameters of this
distribution.  </li>
<li>Another approach is using mixture models with 2 or 3 components (distributions). You can fit   such models using gamlss.mx package (distributions from package gamlss.dist can be specified for
each component of mixture).</li>
</ol>
",419,"2010-07-29 20:09:37",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,899,NULL,NULL,NULL
1010,2,NULL,"2010-07-29 21:20:25",8,NULL,"<p>If I understand correctly, then you can just fit a mixture of two Normals to the data. There are lots of R packages that are available to do this. This example uses the <a href="http://cran.r-project.org/web/packages/mixtools/index.html" rel="nofollow">mixtools</a> package:</p>

<pre><code>#Taken from the documentation
library(mixtools)
data(faithful)
attach(faithful)

#Fit two Normals
wait1 = normalmixEM(waiting, lambda = 0.5)
plot(wait1, density=TRUE, loglik=FALSE)
</code></pre>

<p>This gives:</p>

<p><img src="http://img294.imageshack.us/img294/4213/kernal.jpg" alt="Mixture of two Normals"></p>

<p>The package also contains more sophisticated methods - check the documentation.</p>
",8,"2010-07-29 21:20:25",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,899,NULL,NULL,NULL
1011,2,NULL,"2010-07-29 21:21:02",4,NULL,"<p>Essentially they're looking for the log of the fold-change from one time point to the next because intuitively, it's easier to think about log-fold changes visually than actual fold-changes.</p>

<p>Log-fold changes make decreases and increases simply a difference in sign, so that a log-2-fold increase is the same distance as a log-2-fold decrease (i.e. |log 2| = |log 0.5|). In addition, many systems exhibit multiplicative effects for independent events, e.g. biological systems and economic systems, where two independent x-fold increases results in an x^2 fold increase, whereas on a log scale, this becomes an additive (and thus easier to see) 2*log(x) increase.</p>

<p>Also, <code>diff(log(x))</code> is prettier than <code>x[-1]/x[-length(x)]</code>.</p>
",378,"2010-07-29 21:21:02",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,964,NULL,NULL,NULL
1012,1,1014,"2010-07-29 21:29:57",0,538,"<p>A colleague wants to compare models that use either a Gaussian distribution or a uniform distribution and for other reasons needs the standard devation of these two distributions to be equal.  In R I can do a simulation...</p>

<pre><code>sd(runif(100000000))
sd(runif(100000000,min=0,max=2))
</code></pre>

<p>and see that the calculated standard deviation is likely to be ~.2887 * the range of the uniform distribution.  However, I was wondering if there was an equation that could yield the exact value, and if so, what that formula was.</p>
",196,"2010-09-29 14:34:45","What would the calculated value of the standard deviation of a uniform distribution be?",<distributions><uniform><normal-distribution>,2,3,1,196,"2010-08-12 07:19:30",NULL,NULL,NULL,NULL,NULL
1013,2,NULL,"2010-07-29 21:47:00",1,NULL,"<p>The standard deviation of the continous uniform distribution on the interval [0,1] is 12<sup>-1/2</sup>&asymp;0.288675. The <a href="http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29" rel="nofollow">Wikipedia article</a> lists of it's more properties.</p>
",56,"2010-09-29 14:34:45",NULL,NULL,NULL,2,NULL,56,"2010-09-29 14:34:45",NULL,1012,NULL,NULL,NULL
1014,2,NULL,"2010-07-29 21:54:24",8,NULL,"<p>In general, the standard deviation of a continous uniform distribution is (max - min) / sqrt(12).</p>
",614,"2010-07-29 21:54:24",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,1012,NULL,NULL,NULL
1015,1,3667,"2010-07-29 22:03:55",7,699,"<p>I am trying to calculate the reliability in an elicitation exercise by analysing some test-retest questions given to the experts. The experts elicited a series of probability distributions which were then compared with the true value (found at a later date) by computing the standardized quadratic scores. These scores are the values that I am using to calculate the reliability between the test-retest results. </p>

<p>Which reliability method would be appropriate here?  I was looking mostly at Pearson's correlation and Chronbach's alpha (and got some negative values using both methods) but I am not sure this is the right approach.</p>

<hr>

<p><strong>UPDATE:</strong>
Background information</p>

<p>The data were collected from a number of students who were asked to predict their own actual exam mark in four chosen modules by giving a probability distribution of the marks. One module was then repeated at a later date (hence the test-retest exercise).</p>

<p>Once the exam was taken, and the real results were available, the standardized quadratic scores were computed. These scores are proper scoring rules used to compare assessed probability distributions with the observed data which might be known at a later stage.</p>

<p>The probability score <em>Q</em> is defined as:</p>

<p><img src="http://img717.imageshack.us/img717/9424/chart2j.png" alt="Quadratic score"></p>

<p>where <em>k</em> is the total number of elicited probabilities and <em>j</em> is the true outcome.</p>

<p>My question is which reliability method would be more appropriate when it comes to assessing the reliability between the scores of the repeated modules? I calculated Pearson's correlation and Chronbach's alpha (and got some negative values using both methods) but there might be a better approach.</p>
",108,"2010-11-01 16:32:27","Reliability in Elicitation Exercise",<psychometrics><reliability><elicitation>,2,2,2,930,"2010-10-18 15:21:53",NULL,NULL,NULL,NULL,NULL
1016,1,1026,"2010-07-29 22:04:32",8,4921,"<p>I've got a linear regression model with the sample and variable observations and I want to know:</p>

<ol>
<li>Whether a specific variable is significant enough to remain included in the model.</li>
<li>Whether another variable (with observations) ought to be included in the model.</li>
</ol>

<p>Which statistics can help me out? How can get them most efficiently?</p>
",614,"2010-09-16 23:06:05","Is a variable significant in a linear regression model?",<regression>,5,0,5,8,"2010-08-09 10:57:45",NULL,NULL,NULL,NULL,NULL
1018,2,NULL,"2010-07-29 22:30:13",2,NULL,"<p><a href="http://archive.ics.uci.edu/ml/datasets/Arcene" rel="nofollow"><strong>Arcene</strong></a><br>
n=900<br>
p=10000 (3k is artificially added noise)<br>
k=2 (~balanced)<br>
From <a href="http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf" rel="nofollow">NIPS2003</a>.</p>
",88,"2010-07-30 18:00:06",NULL,NULL,NULL,0,NULL,190,"2010-07-30 18:00:06","2010-07-29 22:30:13",973,NULL,NULL,NULL
1019,2,NULL,"2010-07-29 22:32:44",2,NULL,"<p><a href="http://archive.ics.uci.edu/ml/datasets/Dexter" rel="nofollow"><strong>Dexter</strong></a><br>
n=2600<br>
p=20000 (10k+53 is artificial noise)<br>
k=2 (balanced)<br>
From <a href="http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf" rel="nofollow">NIPS2003</a>.</p>
",88,"2010-07-29 22:41:53",NULL,NULL,NULL,3,NULL,88,"2010-07-29 22:41:53","2010-07-29 22:32:44",973,NULL,NULL,NULL
1020,2,NULL,"2010-07-29 22:35:28",2,NULL,"<p><a href="http://archive.ics.uci.edu/ml/datasets/Dorothea" rel="nofollow"><strong>Dorothea</strong></a><br>
n=1950<br>
p=100000 (0.1M, half is artificially added noise)<br>
k=2 (~10x unbalanced)<br>
From <a href="http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf" rel="nofollow">NIPS2003</a>.</p>
",88,"2010-07-29 22:41:33",NULL,NULL,NULL,0,NULL,88,"2010-07-29 22:41:33","2010-07-29 22:35:28",973,NULL,NULL,NULL
1021,2,NULL,"2010-07-29 22:38:21",2,NULL,"<p><a href="http://archive.ics.uci.edu/ml/datasets/Gisette" rel="nofollow"><strong>Gisette</strong></a><br>
n=13500<br>
p=5000 (half is artificially added noise)<br>
k=2 (balanced)<br>
From <a href="http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf" rel="nofollow">NIPS2003</a>.</p>
",88,"2010-07-29 22:38:21",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-29 22:38:21",973,NULL,NULL,NULL
1023,1,1029,"2010-07-29 23:04:08",7,692,"<p>Introductory, advanced, and even obscure, please.</p>

<p>Mostly to test myself. I like to make sure I know what the heck I'm talking about :)</p>

<p>Thanks</p>
",74,"2010-08-17 20:09:56","Where can I find good statistics quizzes?",<teaching>,2,1,4,NULL,NULL,"2010-07-30 01:01:24",NULL,NULL,NULL,NULL
1024,2,NULL,"2010-07-29 23:25:12",3,NULL,"<p>For part 1, you're looking for the <a href="http://en.wikipedia.org/wiki/F-test#Regression_problems" rel="nofollow">F-test</a>. Calculate your residual sum of squares from each model fit and calculate an F-statistic, which you can use to find p-values from either an F-distribution or some other null distribution that you generate yourself.</p>
",378,"2010-07-29 23:25:12",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,1016,NULL,NULL,NULL
1025,2,NULL,"2010-07-29 23:45:46",4,NULL,"<p>Calculating averages leads to a different dataset than simply reducing the number of data points.
If one heartbeat per minute is much faster than the other heart beats you will lose the signal through your smoothing process.</p>

<p>If you summary 125-125-0-125-125 as 100 than the story that the data tells is different through your smoothing.</p>

<p>Sometimes the heart even skips beats and I believe that's an event that interesting for however wants to look at plotted heart rate data.</p>

<p>I would therefore propose that you calculate the distance between two points with a formula like <code>d=sqrt((time1-time2)^2 + (bpm1-bpm2))</code>.</p>

<p>You set a minimum distance in your program.
Then you iterate through your data and after every point you delete all following points for which d is smaller than your minimum distance.</p>

<p>As the unit of time and bpm isn't the same you might want to think about how you can find a way to scale the units meaningfully. To do this task right you should speak to the doctors who in the end have to interpret your graphs and ask them what information they consider to be essential.</p>
",3807,"2010-07-31 02:09:30",NULL,NULL,NULL,1,NULL,3807,"2010-07-31 02:09:30",NULL,980,NULL,NULL,NULL
1026,2,NULL,"2010-07-30 00:00:15",24,NULL,"<p>Statistical significance is not usually a good basis for determining whether a variable should be included in a model. Statistical tests were designed to test hypotheses, not select variables. I know a lot of textbooks discuss variable selection using statistical tests, but this is generally a bad approach. See Harrell's book <em><a href="http://rads.stackoverflow.com/amzn/click/0387952322">Regression Modelling Strategies</a></em> for some of the reasons why. These days, variable selection based on the AIC (or something similar) is usually preferred.</p>
",159,"2010-07-30 00:00:15",NULL,NULL,NULL,6,NULL,NULL,NULL,NULL,1016,NULL,NULL,NULL
1027,2,NULL,"2010-07-30 00:44:09",0,NULL,"<p>You would use Cronbach alpha if you do not know the true value but if you do know the true value then it seems a bit pointless to use Cronbach alpha. The use of Pearson correlation also seems a bit odd as you do not actually have a paired set of values. I would suggest using something like the <a href="http://en.wikipedia.org/wiki/Mean_squared_error" rel="nofollow">Mean Squared Error (MSE)</a>. Suppose that you have N experts and that the  expected estimate for the expert i is given by $\\hat{\\theta_i}$ and your true value is $\\theta$. Then,</p>

<p>$MSE = \\frac{\\sum_i (\\hat{\\theta_i} - \\theta)^2}{N}$</p>
",NULL,"2010-07-30 00:44:09",NULL,NULL,NULL,3,NULL,NULL,NULL,NULL,1015,NULL,user28,NULL
1028,1,NULL,"2010-07-30 00:55:20",10,3899,"<p>I am comparing two distributions with KL divergence which returns me a non-standardized number that, according to what I read about this measure, is the amount of information that is required to transform one hypothesis into the other. I have two questions:</p>

<p>a) Is there a way to quantify a KL divergence so that it has a more meaningful interpretation, e.g. like an effect size or a R^2? Any form of standardization?</p>

<p>b) In R, when using KLdiv (flexmix package) one can set the 'esp' value (standard esp=1e-4) that sets all points smaller than esp to some standard in order to provide numerical stability. I have been playing with different esp values and, for my data set, I am getting an increasingly larger KL divergence the smaller a number I pick. What is going on? I would expect that the smaller the esp, the more reliable the results should be since they let more 'real values' become part of the statistic. No? I have to change the esp since it otherwise does not calculate the statistic but simply shows up as NA in the result table...</p>

<p>Thanks in advance,
Ampleforth </p>
",608,"2014-01-03 16:55:30","Questions about KL divergence?",<distributions><kullback-leibler>,4,0,8,3911,"2011-04-29 00:29:23",NULL,NULL,NULL,NULL,NULL
1029,2,NULL,"2010-07-30 02:42:38",3,NULL,"<p>I wrote a post compiling links of  Practice Questions for Statistics in Psychology (Undergraduate Level).
<a href="http://jeromyanglim.blogspot.com/2009/12/practice-questions-for-statistics-in.html" rel="nofollow">http://jeromyanglim.blogspot.com/2009/12/practice-questions-for-statistics-in.html</a></p>

<p>The questions would fall into the introductory category.</p>
",183,"2010-07-30 02:42:38",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-30 02:42:38",1023,NULL,NULL,NULL
1030,2,NULL,"2010-07-30 03:53:08",6,NULL,"<p>The KL(p,q) divergence between distributions p(.) and q(.) has an intuitive information theoretic interpretation which you may find useful. </p>

<p>Suppose we observe data x generated by some probability distribution p(.). A lower bound on the average codelength in bits required to state the data generated by p(.) is given by the entropy of p(.). </p>

<p>Now, since we don't know p(.) we choose another distribution, say, q(.) to encode (or describe, state) the data. The average codelength of data generated by p(.) and encoded using q(.) will necessarily be longer than if the true distribution p(.) was used for the coding. The KL divergence tells us about the inefficiencies of this alternative code. In other words, the KL divergence between p(.) and q(.) is the average number of <strong>extra</strong> bits required to encode data generated by p(.) using coding distribution q(.). The KL divergence is non-negative and equal to zero iff the actual data generating distribution is used to encode the data.</p>
",530,"2010-07-30 03:53:08",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,1028,NULL,NULL,NULL
1031,2,NULL,"2010-07-30 05:29:11",6,NULL,"<p>KL has a deep meaning when you visualize a set of <strong>dentities as a manifold</strong> within the fisher metric tensor, it gives the geodesic distance between two "close" distributions. Formally: </p>

<p>$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$</p>

<p>The following lines are here to explain with details what is meant by this las mathematical formulae. </p>

<p><strong>Definition of the Fisher metric.</strong> </p>

<p>Consider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_{ij})$ is </p>

<p>$F_{ij}=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$</p>

<p>With this notation $D$ is a riemannian manifold and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao lower bound theorem)</p>

<p>You may say ... OK mathematical abstraction but where is KL ? </p>

<p>It is not mathematical abstraction, if $p=1$ you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and $F_{11}$ is connected to the curvature of that curve...
(see the seminal paper of Bradley Efron <a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1176343282" rel="nofollow">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1176343282</a>) </p>

<p><strong>The geometric answer to part of point a/ in your question :</strong> the squared distance $ds^2$ between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:</p>

<p>$ds^2= \\sum F_{ij} d \\theta^i d \\theta^j$</p>

<p>and it is known to be twice the Kullback Leibler Divergence:</p>

<p>$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$</p>

<p>If you want to learn more about that I suggest reading the paper from Amari
<a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1176345779" rel="nofollow">http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1176345779</a>
(I think there is also a book from Amari about riemannian geometry in statistic but I don't remember the name) </p>
",223,"2010-08-04 17:27:55",NULL,NULL,NULL,4,NULL,223,"2010-08-04 17:27:55",NULL,1028,NULL,NULL,NULL
1032,2,NULL,"2010-07-30 08:04:50",2,NULL,"<p>You will find many applications of Mathematical Statistics in 'Mathematical Statistics and Data Analysis' by John A. Rice. The 'Application Index' lists all applications discussed in the text. </p>

<p>Javed</p>
",531,"2010-07-30 08:04:50",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-30 08:04:50",414,NULL,NULL,NULL
1033,2,NULL,"2010-07-30 09:49:02",0,NULL,"<p>"how is stdev(S) related to the standard deviation of the entire population?"</p>

<p>I don't know if the "Confidence Interval" concept might be what you are looking for? </p>

<p>Stdev(S) is an Estimate of the standard deviation of the entire population. To see how good an estimate, confidence intervals could be computed, and these would be dependent on the sample size.</p>

<p>See for e.g., Simulation and the Monte Carlo Method, Rubinstein &amp; Kroese.</p>
",NULL,"2010-07-30 09:49:02",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,887,NULL,"Anubala Varikat",NULL
1035,2,NULL,"2010-07-30 09:54:10",4,NULL,"<p>Another solution to your problem (without transforming variables) is regression with error distribution other then <strong>Gaussian</strong> for example <strong>Gamma</strong> or <strong>skewed t-Student</strong>.
Gamma is in GLM family, so there is a lot of software to fit model with this error distribution.</p>
",419,"2010-07-30 09:54:10",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,913,NULL,NULL,NULL
1036,2,NULL,"2010-07-30 10:03:37",14,NULL,"<p>Sivia and Skilling, Data analysis: a Bayesian tutorial (2ed) 2006 246p 0198568320
<a href="http://books.google.com/books?id=zN-yliq6eZ4C&amp;dq=isbn:0198568320&amp;source=gbs_navlinks_s">books.goo</a>:</p>

<blockquote>
  <p>Statistics lectures have been a source
  of much bewilderment and frustration
  for generations of students. This book
  attempts to remedy the situation by
  expounding a logical and unified
  approach to the whole subject of data
  analysis. This text is intended as a
  tutorial guide for senior
  undergraduates and research students
  in science and engineering ...</p>
</blockquote>

<p>I don't know the other recommendations though.</p>
",557,"2010-07-30 10:03:37",NULL,NULL,NULL,2,NULL,NULL,NULL,"2010-07-30 10:03:37",125,NULL,NULL,NULL
1038,2,NULL,"2010-07-30 12:11:18",1,NULL,"<ol>
<li><em><strong>Wilcox, Rand R.</em></strong> - <em>BASIC STATISTICS - Understanding
Conventional Methods and Modern
Insights</em>, Oxford University Press,
2009</li>
<li><p><em><strong>Hoff, Peter D.</em></strong> - <em>A First Course in
Bayesian Statistical Methods</em>,
Springer, 2009</p></li>
<li><p><em><strong>Dalgaard, Peter</em></strong> - <em>Introductory
Statistics with R, Second Edition</em>, Springer, 2008</p></li>
</ol>

<p>also take a glance at <a href="http://stackoverflow.com/questions/192369/books-for-learning-the-r-language/2270793#2270793">this link</a>, though it's R-specific, there are plenty of books that can guide you through basic statistical techniques.</p>
",1356,"2010-07-30 12:11:18",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-30 12:11:18",652,NULL,NULL,NULL
1039,2,NULL,"2010-07-30 12:15:45",4,NULL,"<p>If you're looking for an elementary text, i.e. one that doesn't have a calculus prerequisite, there's Don Berry's <a href="http://rads.stackoverflow.com/amzn/click/0534234720" rel="nofollow">Statistics: A Bayesian Perspective</a>.</p>
",319,"2010-07-30 12:15:45",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-07-30 12:15:45",125,NULL,NULL,NULL
1040,1,1041,"2010-07-30 14:24:17",1,759,"<p>Consider the following model </p>

<p>$Y_i = f(X_i) + e_i$</p>

<p>from which we observe n iid data points $\\left( X_i, Y_i \\right)_{i=1}^n$. Suppose that $X_i \\in \\mathbb{R}^d$ is a $d$ dimensional feature vector. And suppose that a ordinary least squares estimate is fit to data, that is,</p>

<p>$\\hat \\beta = {\\rm arg} \\min_{\\beta \\in \\mathbb{R}^d} \\sum_i (Y_i - \\sum_j X_{ij} \\beta_j)^2$</p>

<p>Since a wrong model is estimated, what is the interpretation for the confidence interval around estimated coefficients? </p>

<p>More generally, does it make sense to estimate confidence intervals around parameters in a misspecified model? And what does the confidence interval tell us in such a case?</p>
",168,"2011-04-16 10:55:20","What is the interpretation/meaning of confidence intervals in misspecified models?",<modeling><estimation><model-selection><confidence-interval>,3,3,1,168,"2010-07-30 15:15:21",NULL,NULL,NULL,NULL,NULL
1041,2,NULL,"2010-07-30 14:38:39",2,NULL,"<p>The confidence interval that you obtain is conditional on the model being correct and the interpretation is also conditional on the model being the correct one. If you <strong>know</strong> that the model is incorrect then obviously you would not use it to compute the confidence interval. </p>

<p>In reality, you do not know the <strong>true</strong> model and so you have no way to tell if you have a misspecified model (although you do have ways to assess misspecification, e.g., examine if residuals are normally distributed, diagnostic plots of fitted vs observed values etc). So, to my mind, the real question is if the model is misspecified, to what extent can you rely on confidence intervals as a way to assess where the true parameter is. I suspect that the answer is specific to the degree of misspecification that is coming from f(x) i.e., the degree to which f(x) departs from the assumptions of OLS.</p>
",NULL,"2010-07-30 14:38:39",NULL,NULL,NULL,1,NULL,NULL,NULL,NULL,1040,NULL,user28,NULL
1043,2,NULL,"2010-07-30 15:37:06",1,NULL,"<p>OK, so your data is very expensive to get.  If you have some indication of the shape of the data then perhaps a bootstrap / bayesian / optimization (sticking in keywords :)) approach would work best.  See the optim command in R as an example.  You would need to know some things though.  For example, could we assume something like normality?  If so then fitting a small number of data points to the normal distribution will likely give you a much better estimate of your parameters than simple mean and sdev values.</p>
",601,"2010-07-30 15:37:06",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,977,NULL,NULL,NULL
1044,2,NULL,"2010-07-30 15:43:28",5,NULL,"<p><strong>Sweave</strong> lets you embed R code in a LaTeX document.  The results of executing the code, and optionally the source code, become part of the final document.</p>

<p>So instead of, for example, pasting an image produced by R into a LaTeX file, you can paste the R <em>code</em> into the file and keep everything in one place.</p>
",319,"2010-07-30 15:43:28",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-07-30 15:43:28",73,NULL,NULL,NULL
1045,1,1051,"2010-07-30 16:06:54",3,427,"<p>The wiki article on <a href="http://en.wikipedia.org/wiki/Credible_interval" rel="nofollow">credible intervals</a> has the following statement:</p>

<blockquote>
  <p>credible intervals and confidence intervals treat <a href="http://en.wikipedia.org/wiki/Nuisance_parameter" rel="nofollow">nuisance parameters</a> in radically different ways.</p>
</blockquote>

<p>What is the <strong>radical</strong> difference that the wiki talks about?</p>

<p>Credible intervals are based on the posterior distribution of the parameter and confidence interval is based on the maximum likelihood associated with the data generating process. It seems to me that how credible and confidence intervals are computed is <strong>not dependent</strong> on whether the parameters are nuisance or not. So, I am a bit puzzled by this statement.</p>

<p>PS: I am aware of alternative approaches to dealing with nuisance parameters under frequentist inference but I think they are less common than standard maximum likelihood. (See this question on the difference between <a href="http://stats.stackexchange.com/questions/622/what-is-the-difference-between-a-partial-likelihood-profile-likelihood-and-margi">partial, profile and marginal likelihoods</a>.)</p>
",NULL,"2010-07-30 23:53:35","Is there a radical difference in how bayesian and frequentist approaches treat nuisance parameters?",<confidence-interval><credible-interval>,1,1,5,NULL,NULL,NULL,NULL,NULL,user28,NULL
1046,2,NULL,"2010-07-30 16:58:42",5,NULL,"<p>If you are using GNU/Linux previous answers by Shane and Dirk are great.</p>

<p>If you need a solution for windows, there is one in this post:</p>

<p><a href="http://www.r-statistics.com/2010/04/parallel-multicore-processing-with-r-on-windows/" rel="nofollow">Parallel Multicore Processing with R (on Windows)</a></p>

<p>Although the package is not yet on CRAN. it can be downloaded from that link.</p>
",253,"2010-07-30 16:58:42",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,825,NULL,NULL,NULL
1047,1,1048,"2010-07-30 17:00:57",13,4458,"<p>I'm comparing a sample and checking whether it distributes as some, discrete, distribution.  However, I'm not enterily sure that Kolmogorov-Smirnov applies. <a href="http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Wikipedia</a> seems to imply it does not. If it does not, how can I test the sample's distribution?</p>
",614,"2010-07-30 17:10:09","Is Kolmogorov-Smirnov test valid with discrete distributions?",<hypothesis-testing>,1,0,1,NULL,NULL,NULL,NULL,NULL,NULL,NULL
1048,2,NULL,"2010-07-30 17:10:09",8,NULL,"<p>It does not apply to discrete distributions. See <a href="http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm">http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm</a> for example.</p>

<p>Is there any reason you can't use a chi-square goodness of fit test?
see <a href="http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm">http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm</a> for more info.</p>
",247,"2010-07-30 17:10:09",NULL,NULL,NULL,2,NULL,NULL,NULL,NULL,1047,NULL,NULL,NULL
1049,2,NULL,"2010-07-30 17:10:24",7,NULL,"<p>One resource is 'Some hints for the R beginner' at
<a href="http://www.burns-stat.com/pages/Tutor/hints_R_begin.html">http://www.burns-stat.com/pages/Tutor/hints_R_begin.html</a></p>
",NULL,"2010-07-30 17:10:24",NULL,NULL,NULL,1,NULL,NULL,NULL,"2010-08-01 18:56:25",138,NULL,"Patrick Burns",NULL
1050,2,NULL,"2010-07-30 20:00:45",9,NULL,"<p>Here's a fresh one: <a href="http://www.lulu.com/product/file-download/introduction-to-probability-and-statistics-using-r/12037733">Introduction to Probability and Statistics Using R </a>. It's R-specific, though, but it's a great one. I haven't read it yet, but it seems fine so far...</p>
",1356,"2010-07-30 20:00:45",NULL,NULL,NULL,3,NULL,NULL,NULL,"2010-07-30 20:00:45",170,NULL,NULL,NULL
1051,2,NULL,"2010-07-30 21:15:43",5,NULL,"<p>The fundamental difference is that in maximum likelihood based methods we can't integrate the nuisance parameters out (because the likelihood function is not a PDF and doesn't obey probability laws).</p>

<p>In maximum likelihood methods, the ideal way to deal with nuisance parameters is through marginal/conditional likelihoods, but these are defined differently from the question you linked.  (There is a notion of an <em>integrated</em> (marginal/conditional) likelihood function as in the linked question, but this is not strictly the marginal likelihood function.)  </p>

<p>Say you have a parameter of interest, $\\theta$, a nuisance parameter, $\\lambda$.  Suppose a transformation of your data $X$ to $(Y, Z)$ exists such that either $Y$ or $Y|Z$ depends only on $\\theta$.  If $Y$ depends on $\\theta$, then the joint density can be written </p>

<p>$f(Y, Z; \\theta, \\lambda) = f_{Y}(Y; \\theta) f_{Z|Y}(Z|Y; \\theta, \\lambda)$.  </p>

<p>In the latter case, we have </p>

<p>$f(Y, Z; \\theta, \\lambda) = f_{Y|Z}(Y|Z; \\theta) f_{Z}(Z; \\theta, \\lambda)$.  </p>

<p>In either case, the factor depending on $\\theta$ alone is of interest.  In the former, it's the basis for the definition of the marginal likelihood and in the latter, the conditional likelihood.  The important point here is to isolate a component that depends on $\\theta$ alone.</p>

<p>If we can't find such a transformation, we look at other likelihood functions to eliminate the nuisance.  We usually start with a profile likelihood.  To eliminate bias in the MLE, we try to obtain approximations for marginal or conditional likelihoods, usually through a "modified profile likelihood" function (yet another likelihood function!).  </p>

<p>There are many details, but the short story is that the likelihood methods treat nuisance parameters quite differently than Bayesian methods.  In particular, the estimated likelihoods don't account for uncertainty in the nuisance.  Bayesian methods do account for it through the specification of a prior.</p>

<p>There are arguments in favor of an integrated likelihood function and lead to something resembling the Bayesian framework.  If you're interested, I can dig up some references.</p>
",251,"2010-07-30 23:53:35",NULL,NULL,NULL,6,NULL,251,"2010-07-30 23:53:35",NULL,1045,NULL,NULL,NULL
1052,1,NULL,"2010-07-30 22:38:06",10,3519,"<p>my question particularly applies to network reconstruction</p>
",NULL,"2010-07-31 03:04:03","What is the major difference between correlation and mutual information?",<correlation><mutual-information>,2,0,6,NULL,"2010-07-31 02:05:21",NULL,NULL,NULL,puzzled,user28
1053,1,1056,"2010-07-31 00:51:52",17,1552,"<p>I am looking for a good book/tutorial to learn the survival analysis. I am also interested in references on doing survival analysis in R</p>
",172,"2014-02-15 08:47:36","References for Survival Analysis",<books><survival><big-list>,11,1,11,8,"2010-10-21 13:39:03","2010-09-16 12:35:43",NULL,NULL,NULL,NULL
1054,1,1058,"2010-07-31 00:59:11",4,508,"<p>What is the equivalent command in R for the <code>stcox</code> command in Stata? </p>
",172,"2013-07-20 22:57:03","R command for stcox in Stata",<r><survival><stata>,2,0,1,22047,"2013-07-20 22:57:03",NULL,NULL,NULL,NULL,NULL
1055,2,NULL,"2010-07-31 01:08:08",20,NULL,"<p>Correlation measures the <em>linear</em> relationship (Pearson's correlation) or <em>monotonic</em> relationship (Spearman's correlation) between two variables, X and Y. </p>

<p>Mutual information is more general and measures the reduction of uncertainty in Y after observing X. It is the KL distance between the joint density and the product of the individual densities. So MI can measure non-monotonic relationships and other more complicated relationships.</p>
",159,"2010-07-31 03:04:03",NULL,NULL,NULL,2,NULL,159,"2010-07-31 03:04:03",NULL,1052,NULL,NULL,NULL
1056,2,NULL,"2010-07-31 01:41:53",10,NULL,"<p>I like:</p>

<ul>
<li><a href="http://www.powells.com/biblio/72-9780387239187-0">Survival Analysis: Techniques for Censored and Truncated Data</a> (Klein &amp; Moeschberger)</li>
<li><a href="http://www.powells.com/biblio/72-9780387239187-0">Modeling Survival Data: Extending the Cox Model</a> (Therneau)</li>
</ul>

<p>The first does a good job of straddling theory and model building issues.  It's mostly focused on semi-parametric techniques, but there is reasonable coverage of parametric methods.  It doesn't really provide any R or other code examples, if that's what you're after.</p>

<p>The second is heavy with modeling on the Cox PH side (as the title might indicate).  It's by the author of the <a href="http://cran.r-project.org/web/views/Survival.html">survival</a> package in R and there are plenty of R examples and mini-case studies.  I think both books complement each other, but I'd recommend the first for getting started.  </p>

<p>A quick way to get started in R is David Diez's <a href="http://www.statgrad.com/teac/surv/R_survival.pdf">guide</a>.</p>
",251,"2010-07-31 01:41:53",NULL,NULL,NULL,0,NULL,NULL,NULL,"2010-09-16 12:35:43",1053,NULL,NULL,NULL
1057,2,NULL,"2010-07-31 02:05:44",2,NULL,"<p>To add to Rob's answer ... with respect to reverse engineering a network, MI may be preferred over correlation when you want to extract causal rather than associative links in your network.  Correlation networks are purely associative.  But for MI, you need more data and computing power.</p>
",251,"2010-07-31 02:05:44",NULL,NULL,NULL,0,NULL,NULL,NULL,NULL,1052,NULL,NULL,NULL
1058,2,NULL,"2010-07-31 02:08:06",9,NULL,"<p>In package <a href="http://cran.r-project.org/web/packages/survival/index.html" rel="nofollow">survival</a>, it's <code>coxph</code>.  John Fox has a nice introduction to using coxph in R:</p>

<ul>
<li><a href="http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf" rel="nofollow">Cox Proportional-Hazards Regression for Survival Data</a></li>
</ul>
",251,"2010-09-19 07:23:51",NULL,NULL,NULL,0,NULL,930,"2010-09-19 07:23:51",NULL,1054,NULL,NULL,NULL
1059,2,NULL,"2010-07-31 05:49:03",25,NULL,"<blockquote>
  <p>"Million to one chances crop up nine times out of ten."</p>
</blockquote>

<p>-<a href="http://www.goodreads.com/quotes/95458-scientists-have-calculated-that-the-chances-of-something-so-patently">Terry Pratchett</a></p>
",183,"2012-11-03 06:15:39",NULL,NULL,NULL,0,NULL,9007,"2012-11-03 06:15:39","2010-07-31 05:49:03",726,NULL,NULL,NULL
1060,1,1061,"2010-07-31 09:39:47",6,958,"<p>I'll use an example so that you can reproduce the results </p>

<pre><code># mortality 
mort = ts(scan("http://www.stat.pitt.edu/stoffer/tsa2/data/cmort.dat"),start=1970, frequency=52)

# temperature
temp = ts(scan("http://www.stat.pitt.edu/stoffer/tsa2/data/temp.dat"), start=1970, frequency=52)

#pollutant particulates
part = ts(scan("http://www.stat.pitt.edu/stoffer/tsa2/data/part.dat"), start=1970, frequency=52)

temp = temp-mean(temp)
temp2 = temp^2
trend = time(mort)
</code></pre>

<p>Now, fit a model for mortality data</p>

<pre><code>fit = lm(mort ~ trend + temp + temp2 + part, na.action=NULL)
</code></pre>

<p>What I want now is to reproduce the result of the AIC command </p>

<pre><code>AIC(fit)
[1] 3332.282
</code></pre>

<p>According to R's help file for AIC, AIC = -2 * log.likelihood + 2 * npar.
If I'm correct I think that log.likelihood is given using the following formula:</p>

<pre><code>n = length(mort)
RSS = anova(fit)[length(anova(fit)[,2]),2] # there must be better ways to get this, anyway
(log.likelihood &lt;- -n/2*(log(2*pi)+log(RSS/n)+1))

 [1] -1660.135
</code></pre>

<p>This is approximately equal to</p>

<pre><code>logLik(fit)
'log Lik.' -1660.141 (df=6)
</code></pre>

<p>As far as I can tell, the number of parameters in the model are 5 (how can I get this number programmatically ??). So AIC should be given by:</p>

<pre><code>-2 * log.likelihood + 2 * 5
[1] 3330.271
</code></pre>

<p>Ooops, it seems like I should have used 6 instead of 5 as the number of parameters. What is wrong with those calculations? </p>
",339,"2011-04-29 05:03:59","Why does AIC formula in R appear to use one extra parameter than expected?",<r><time-series><modeling><aic>,1,0,2,183,"2011-04-29 05:03:59",NULL,NULL,NULL,NULL,NULL
1061,2,NULL,"2010-07-31 10:04:04",11,NULL,"<pre><code>&gt; -2*logLik(fit)+2*(length(fit$coef)+1)
[1] 3332.282
</code></pre>

<p>(you forgot; you have 6 parameter because $\\sigma_{\\epsilon}$ also has to be estimated!&nbsp;</p>
",603,"2011-04-29 01:10:38",NULL,NULL,NULL,4,NULL,3911,"2011-04-29 01:10:38",NULL,1060,NULL,NULL,NULL
1062,1,NULL,"2010-07-31 14:19:13",4,713,"<p>In general inference, why orthogonal parameters are useful, and why is it worth trying to find a new parametrization that makes the parameters orthogonal ? </p>

<p>I have seen some textbook examples, not so many, and would be interested in more concrete examples and/or motivation.</p>
",368,"2010-10-12 16:05:37","Orthogonal parametrization",<multivariable>,2,1,2,88,"2010-08-07 17:48:25",NULL,NULL,NULL,NULL,NULL
1063,1,1065,"2010-07-31 14:29:15",5,11865,"<p>My stats has been self taught, but a lot of material I read point to a dataset having mean 0 and standard deviation of 1.</p>

<p>If that is the case then:</p>

<ol>
<li><p>Why is mean 0 and SD 1 a nice property to have?</p></li>
<li><p>Why does a random variable drawn from this sample equal 0.5?  The chance of drawing 0.001 is the same as 0.5 so this should be flat distribution...</p></li>
<li><p>When people talk about Z Scores what do they actually mean here?   </p></li>
</ol>
",353,"2012-08-20 17:02:53","Why are mean 0 and standard deviation 1 distributions always used?",<probability>,2,0,2,2116,"2012-08-20 04:54:05","2010-07-31 14:29:30",NULL,NULL,NULL,NULL
1064,2,NULL,"2010-07-31 15:36:12",51,NULL,"<p>A nice one I came about:</p>

<blockquote>
  <p>I think it is much more interesting to
  live with uncertainty than to live with
  answers that might be wrong.</p>
</blockquote>

<p>By Richard Feynman (<a href="http://www.youtube.com/watch?v=zeCHiUe1et0&amp;feature=player_embedded#!">link</a>)</p>
",253,"2010-08-17 17:33:06",NULL,NULL,NULL,2,NULL,74,"2010-08-17 17:33:06","2010-07-31 15:36:12",726,NULL,NULL,NULL
1065,2,NULL,"2010-07-31 15:46:05",6,NULL,"<ol>
<li><p>At the beginning the most useful answer is probably that mean of 0 and sd of 1 are mathematically convenient.  If you can work out the probabilities for a distribution with a mean of 0 and standard deviation of 1 you can work them out for any similar distribution of scores with a very simple equation.</p></li>
<li><p>I'm not following this question.  The mean of 0 and standard deviation of 1 usually applies to the standard normal distribution, often called the bell curve.  The most likely value is the mean and it falls off as you get farther away.  If you have a truly flat distribution then there is no value more likely than another.  Your question here is poorly formed.  Were you looking at questions about coin flips perhaps?  Look up binomial distribution and central limit theorem.</p></li>
<li><p>"mean here"?  Where?  The simple answer for z-scores is that they are your scores scaled as if your mean were 0 and standard deviation were 1.  Another way of thinking about it is that it takes an individual score as the number of standard deviations that score is from the mean.  The equation is calculating the (score - mean) / standard deviation.  The reasons you'd do that are quite varied but one is that in intro statistics courses you have tables of probabilities for different z-scores (see answer 1).</p></li>
</ol>

<p>If you looked up z-score first, even in wikipedia, you would have gotten pretty good answers.</p>
",601,"2012-08-20 04:56:16",NULL,NULL,NULL,1,NULL,2116,"2012-08-20 04:56:16","2010-07-31 15:46:05",1063,NULL,NULL,NULL
1066,1,1068,"2010-07-31 18:20:26",1,268,"<p>My question is actually quite short, but I'll have to start by describing the context since I am not sure how to directly ask it.</p>

<p>Consider the following "game":</p>

<p>We have a segment of length n ("large segment") and m integers ("lengths"), all considerably smaller than n. For each of the m lengths we draw a random  sub-segment of its length on the large segment. For example, if the large segment is of size 1000 (i.e. 1..1000) and we are given lengths 20, 10, 50, than a possible solution would be: 31..50, 35..44, 921..970 (sub-segments of lengths 20, 10 and 50 respectively).</p>

<p>Notes:
1. This is just a toy example. We usually have many more lengths so there are many overlaps and each position in the large segment is covered by multiple sub-segments. 
2. Remember that the lengths are given; only their mapping to the large segment is random.
3. Drawing a sub-segment of length k is done bu simply drawing a number from a uniform distribution over 1..n-k (a sub-segment of size k can start at position 1, 2, ... n-k).</p>

<p>Now, we conduct many simulations of the process an d record the data. We finally examine for each position the distribution of number of sub-segments covering this position. If we look at positions that are relatively far from the edges of the large segment, the distribution in each such position is normal, and all the distributions look the same.
The "problem" is that the positions at the ends do not look normal at all. This is not surprising, since, for example, if we are now drawing a sub-segment of length 10, the only way the very first position in the large segment will be covered is if we draw 1, whereas, for example, the 10th position will be covered if we draw 1,2,3,..10.</p>

<p>What I am trying to figure out is what is the kind of distribution we see in the "edge" positions (it's not normal, but I think it usually looks like a normal distribution with its tail cut in one direction), and also how can I approximate this distribution density function from my simulations. For the "center" positions, I just estimate the mean and standard deviation and since I beleive the distributions there are normal - I can use the normal density function. This alos makes me think if I really need to treat the positions in a categorical way - "near the edges" and "not near the edges", or whether there are actually the same in some sort (some generalization of the normal distribution?).</p>

<p>Thank you, and sorry again for the length of the post.</p>
",634,"2012-05-18 12:35:06","Approximating density function for a non-normal distribution",<distributions><normality>,1,5,NULL,930,"2010-09-30 21:24:14",NULL,NULL,NULL,NULL,NULL
